[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "nameThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameThread()\n{\r\n    Thread.currentThread().setName(\"JUnit-\" + methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    nameThread();\r\n    testAccount = AzureBlobStorageTestAccount.create();\r\n    if (testAccount != null) {\r\n        fs = testAccount.getFileSystem();\r\n    }\r\n    assumeNotNull(fs);\r\n    basePath = fs.makeQualified(AzureTestUtils.createTestPath(new Path(\"NativeAzureFileSystemContractLive\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    super.tearDown();\r\n    testAccount = AzureTestUtils.cleanup(testAccount);\r\n    fs = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestBaseDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTestBaseDir()\n{\r\n    return basePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getGlobalTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getGlobalTimeout()\n{\r\n    return AzureTestConstants.AZURE_TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMoveFileUnderParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMoveFileUnderParent() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameFileToSelf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameFileToSelf() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameChildDirForbidden",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameChildDirForbidden() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMoveDirUnderParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMoveDirUnderParent() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameDirToSelf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameDirToSelf() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.set(KEY_AUTH_SERVICE_CACHING_ENABLE, \"true\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCachePut",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCachePut() throws Throwable\n{\r\n    CachingAuthorizer<String, Integer> cache = new CachingAuthorizer<>(DUMMY_TTL_VALUE, \"TEST\");\r\n    cache.init(createConfiguration());\r\n    cache.put(\"TEST\", 1);\r\n    cache.put(\"TEST\", 3);\r\n    int result = cache.get(\"TEST\");\r\n    assertEquals(\"Cache returned unexpected result\", 3, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteOneByteToFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testWriteOneByteToFile() throws Exception\n{\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    testWriteOneByteToFile(testFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWriteBytesToFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReadWriteBytesToFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    testWriteOneByteToFile(testFilePath);\r\n    try (FSDataInputStream inputStream = fs.open(testFilePath, TEST_DEFAULT_BUFFER_SIZE)) {\r\n        assertEquals(TEST_BYTE, inputStream.read());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOOBWritesAndReadFail",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOOBWritesAndReadFail() throws Exception\n{\r\n    Configuration conf = this.getRawConfiguration();\r\n    conf.setBoolean(AZURE_TOLERATE_CONCURRENT_APPEND, false);\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    int readBufferSize = fs.getAbfsStore().getAbfsConfiguration().getReadBufferSize();\r\n    byte[] bytesToRead = new byte[readBufferSize];\r\n    final byte[] b = new byte[2 * readBufferSize];\r\n    new Random().nextBytes(b);\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    try (FSDataOutputStream writeStream = fs.create(testFilePath)) {\r\n        writeStream.write(b);\r\n        writeStream.flush();\r\n    }\r\n    try (FSDataInputStream readStream = fs.open(testFilePath)) {\r\n        assertEquals(readBufferSize, readStream.read(bytesToRead, 0, readBufferSize));\r\n        try (FSDataOutputStream writeStream = fs.create(testFilePath)) {\r\n            writeStream.write(b);\r\n            writeStream.flush();\r\n        }\r\n        assertEquals(readBufferSize, readStream.read(bytesToRead, 0, readBufferSize));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOOBWritesAndReadSucceed",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOOBWritesAndReadSucceed() throws Exception\n{\r\n    Configuration conf = this.getRawConfiguration();\r\n    conf.setBoolean(AZURE_TOLERATE_CONCURRENT_APPEND, true);\r\n    final AzureBlobFileSystem fs = getFileSystem(conf);\r\n    int readBufferSize = fs.getAbfsStore().getAbfsConfiguration().getReadBufferSize();\r\n    byte[] bytesToRead = new byte[readBufferSize];\r\n    final byte[] b = new byte[2 * readBufferSize];\r\n    new Random().nextBytes(b);\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    try (FSDataOutputStream writeStream = fs.create(testFilePath)) {\r\n        writeStream.write(b);\r\n        writeStream.flush();\r\n    }\r\n    try (FSDataInputStream readStream = fs.open(testFilePath)) {\r\n        assertEquals(readBufferSize, readStream.read(bytesToRead, 0, readBufferSize));\r\n        try (FSDataOutputStream writeStream = fs.create(testFilePath)) {\r\n            writeStream.write(b);\r\n            writeStream.flush();\r\n        }\r\n        assertEquals(readBufferSize, readStream.read(bytesToRead, 0, readBufferSize));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteWithBufferOffset",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testWriteWithBufferOffset() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    final byte[] b = new byte[1024 * 1000];\r\n    new Random().nextBytes(b);\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        stream.write(b, TEST_OFFSET, b.length - TEST_OFFSET);\r\n    }\r\n    final byte[] r = new byte[TEST_DEFAULT_READ_BUFFER_SIZE];\r\n    FSDataInputStream inputStream = fs.open(testFilePath, TEST_DEFAULT_BUFFER_SIZE);\r\n    int result = inputStream.read(r);\r\n    assertNotEquals(-1, result);\r\n    assertArrayEquals(r, Arrays.copyOfRange(b, TEST_OFFSET, b.length));\r\n    inputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWriteHeavyBytesToFileWithSmallerChunks",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadWriteHeavyBytesToFileWithSmallerChunks() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    final byte[] writeBuffer = new byte[5 * 1000 * 1024];\r\n    new Random().nextBytes(writeBuffer);\r\n    write(testFilePath, writeBuffer);\r\n    final byte[] readBuffer = new byte[5 * 1000 * 1024];\r\n    FSDataInputStream inputStream = fs.open(testFilePath, TEST_DEFAULT_BUFFER_SIZE);\r\n    int offset = 0;\r\n    while (inputStream.read(readBuffer, offset, TEST_OFFSET) > 0) {\r\n        offset += TEST_OFFSET;\r\n    }\r\n    assertArrayEquals(readBuffer, writeBuffer);\r\n    inputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWithFileNotFoundException",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testReadWithFileNotFoundException() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    testWriteOneByteToFile(testFilePath);\r\n    try (FSDataInputStream inputStream = fs.open(testFilePath, TEST_DEFAULT_BUFFER_SIZE)) {\r\n        fs.delete(testFilePath, true);\r\n        assertPathDoesNotExist(fs, \"This path should not exist\", testFilePath);\r\n        intercept(FileNotFoundException.class, () -> inputStream.read(new byte[1]));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteWithFileNotFoundException",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testWriteWithFileNotFoundException() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        assertPathExists(fs, \"Path should exist\", testFilePath);\r\n        stream.write(TEST_BYTE);\r\n        fs.delete(testFilePath, true);\r\n        assertPathDoesNotExist(fs, \"This path should not exist\", testFilePath);\r\n        intercept(FileNotFoundException.class, () -> stream.close());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFlushWithFileNotFoundException",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFlushWithFileNotFoundException() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    if (fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(testFilePath).toString())) {\r\n        return;\r\n    }\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        assertPathExists(fs, \"This path should exist\", testFilePath);\r\n        fs.delete(testFilePath, true);\r\n        assertPathDoesNotExist(fs, \"This path should not exist\", testFilePath);\r\n        intercept(FileNotFoundException.class, () -> stream.close());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteOneByteToFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testWriteOneByteToFile(Path testFilePath) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        stream.write(TEST_BYTE);\r\n    }\r\n    FileStatus fileStatus = fs.getFileStatus(testFilePath);\r\n    assertEquals(1, fileStatus.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testConvert403ToAccessDenied",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConvert403ToAccessDenied() throws Throwable\n{\r\n    assertTranslated(HttpURLConnection.HTTP_FORBIDDEN, AUTHORIZATION_PERMISSION_MISS_MATCH, AccessDeniedException.class, AUTHORIZATION_PERMISSION_MISS_MATCH.getErrorCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testConvert404ToFNFE",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConvert404ToFNFE() throws Throwable\n{\r\n    assertTranslated(HttpURLConnection.HTTP_NOT_FOUND, PATH_NOT_FOUND, FileNotFoundException.class, PATH_NOT_FOUND.getErrorCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testConvert409ToFileAlreadyExistsException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConvert409ToFileAlreadyExistsException() throws Throwable\n{\r\n    assertTranslated(HttpURLConnection.HTTP_CONFLICT, PATH_ALREADY_EXISTS, FileAlreadyExistsException.class, PATH_ALREADY_EXISTS.getErrorCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertTranslated",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertTranslated(int httpStatus, AzureServiceErrorCode exitCode, Class<E> clazz, String expectedText) throws Exception\n{\r\n    AbfsRestOperationException ex = new AbfsRestOperationException(httpStatus, exitCode.getErrorCode(), \"\", null);\r\n    intercept(clazz, expectedText, () -> {\r\n        checkException(PATH, ex);\r\n        return \"expected exception translation from \" + ex;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testDifferentMaxIORetryCount",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testDifferentMaxIORetryCount() throws Exception\n{\r\n    AbfsConfiguration abfsConfig = getAbfsConfig();\r\n    abfsConfig.setMaxIoRetries(noRetryCount);\r\n    testMaxIOConfig(abfsConfig);\r\n    abfsConfig.setMaxIoRetries(retryCount);\r\n    testMaxIOConfig(abfsConfig);\r\n    abfsConfig.setMaxIoRetries(retryCountBeyondMax);\r\n    testMaxIOConfig(abfsConfig);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testDefaultMaxIORetryCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultMaxIORetryCount() throws Exception\n{\r\n    AbfsConfiguration abfsConfig = getAbfsConfig();\r\n    Assert.assertEquals(String.format(\"default maxIORetry count is %s.\", maxRetryCount), maxRetryCount, abfsConfig.getMaxIoRetries());\r\n    testMaxIOConfig(abfsConfig);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testAbfsConfigConstructor",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testAbfsConfigConstructor() throws Exception\n{\r\n    ExponentialRetryPolicy template = new ExponentialRetryPolicy(getAbfsConfig().getMaxIoRetries());\r\n    int testModifier = 1;\r\n    int expectedMaxRetries = template.getRetryCount() + testModifier;\r\n    int expectedMinBackoff = template.getMinBackoff() + testModifier;\r\n    int expectedMaxBackoff = template.getMaxBackoff() + testModifier;\r\n    int expectedDeltaBackoff = template.getDeltaBackoff() + testModifier;\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.setInt(AZURE_MAX_IO_RETRIES, expectedMaxRetries);\r\n    config.setInt(AZURE_MIN_BACKOFF_INTERVAL, expectedMinBackoff);\r\n    config.setInt(AZURE_MAX_BACKOFF_INTERVAL, expectedMaxBackoff);\r\n    config.setInt(AZURE_BACKOFF_INTERVAL, expectedDeltaBackoff);\r\n    ExponentialRetryPolicy policy = new ExponentialRetryPolicy(new AbfsConfiguration(config, \"dummyAccountName\"));\r\n    Assert.assertEquals(\"Max retry count was not set as expected.\", expectedMaxRetries, policy.getRetryCount());\r\n    Assert.assertEquals(\"Min backoff interval was not set as expected.\", expectedMinBackoff, policy.getMinBackoff());\r\n    Assert.assertEquals(\"Max backoff interval was not set as expected.\", expectedMaxBackoff, policy.getMaxBackoff());\r\n    Assert.assertEquals(\"Delta backoff interval was not set as expected.\", expectedDeltaBackoff, policy.getDeltaBackoff());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsConfiguration getAbfsConfig() throws Exception\n{\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    return new AbfsConfiguration(config, \"dummyAccountName\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMaxIOConfig",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMaxIOConfig(AbfsConfiguration abfsConfig)\n{\r\n    ExponentialRetryPolicy retryPolicy = new ExponentialRetryPolicy(abfsConfig.getMaxIoRetries());\r\n    int localRetryCount = 0;\r\n    while (localRetryCount < abfsConfig.getMaxIoRetries()) {\r\n        Assert.assertTrue(\"Retry should be allowed when retryCount less than max count configured.\", retryPolicy.shouldRetry(localRetryCount, -1));\r\n        localRetryCount++;\r\n    }\r\n    Assert.assertEquals(\"When all retries are exhausted, the retryCount will be same as max configured\", abfsConfig.getMaxIoRetries(), localRetryCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testScriptPathNotSpecified",
  "errType" : [ "KeyProviderException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testScriptPathNotSpecified() throws Exception\n{\r\n    assumeWindows();\r\n    ShellDecryptionKeyProvider provider = new ShellDecryptionKeyProvider();\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    String key = \"key\";\r\n    conf.set(SimpleKeyProvider.KEY_ACCOUNT_KEY_PREFIX + account, key);\r\n    try {\r\n        provider.getStorageAccountKey(account, conf);\r\n        Assert.fail(\"fs.azure.shellkeyprovider.script is not specified, we should throw\");\r\n    } catch (KeyProviderException e) {\r\n        LOG.info(\"Received an expected exception: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testValidScript",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testValidScript() throws Exception\n{\r\n    assumeWindows();\r\n    String expectedResult = \"decretedKey\";\r\n    File scriptFile = new File(TEST_ROOT_DIR, \"testScript.cmd\");\r\n    FileUtils.writeStringToFile(scriptFile, \"@echo %1 \" + expectedResult, StandardCharsets.UTF_8);\r\n    ShellDecryptionKeyProvider provider = new ShellDecryptionKeyProvider();\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    String key = \"key1\";\r\n    conf.set(SimpleKeyProvider.KEY_ACCOUNT_KEY_PREFIX + account, key);\r\n    conf.set(ShellDecryptionKeyProvider.KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT, \"cmd /c \" + scriptFile.getAbsolutePath());\r\n    String result = provider.getStorageAccountKey(account, conf);\r\n    assertEquals(key + \" \" + expectedResult, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    fSys = AzureBlobStorageTestAccount.createMock().getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem createFileSystem() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.createMock().getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testListStatusThrowsExceptionForUnreadableDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListStatusThrowsExceptionForUnreadableDir() throws Exception\n{\r\n    System.out.println(\"Skipping testListStatusThrowsExceptionForUnreadableDir since WASB\" + \" doesn't honor directory permissions.\");\r\n    assumeNotWindows();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testGlobStatusThrowsExceptionForUnreadableDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGlobStatusThrowsExceptionForUnreadableDir() throws Exception\n{\r\n    System.out.println(\"Skipping testGlobStatusThrowsExceptionForUnreadableDir since WASB\" + \" doesn't honor directory permissions.\");\r\n    assumeNotWindows();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestRootDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTestRootDir()\n{\r\n    return TEST_ROOT_DIR;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestRootPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestRootPath(FileSystem fSys)\n{\r\n    return fSys.makeQualified(new Path(TEST_ROOT_DIR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestRootPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestRootPath(FileSystem fSys, String pathString)\n{\r\n    return fSys.makeQualified(new Path(TEST_ROOT_DIR, pathString));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getAbsoluteTestRootPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getAbsoluteTestRootPath(FileSystem fSys)\n{\r\n    Path testRootPath = new Path(TEST_ROOT_DIR);\r\n    if (testRootPath.isAbsolute()) {\r\n        return testRootPath;\r\n    } else {\r\n        return new Path(fSys.getWorkingDirectory(), TEST_ROOT_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AbfsCommitTestHelper.prepareTestConfiguration(binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create(\"CleanupTestContainers\", EnumSet.noneOf(AzureBlobStorageTestAccount.CreateOptions.class), createConfiguration(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "testEnumContainers",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testEnumContainers() throws Throwable\n{\r\n    describe(\"Enumerating all the WASB test containers\");\r\n    int count = 0;\r\n    CloudStorageAccount storageAccount = getTestAccount().getRealAccount();\r\n    CloudBlobClient blobClient = storageAccount.createCloudBlobClient();\r\n    Iterable<CloudBlobContainer> containers = blobClient.listContainers(CONTAINER_PREFIX);\r\n    for (CloudBlobContainer container : containers) {\r\n        count++;\r\n        LOG.info(\"Container {} URI {}\", container.getName(), container.getUri());\r\n    }\r\n    LOG.info(\"Found {} test containers\", count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "testDeleteContainers",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testDeleteContainers() throws Throwable\n{\r\n    describe(\"Delete all the WASB test containers\");\r\n    int count = 0;\r\n    CloudStorageAccount storageAccount = getTestAccount().getRealAccount();\r\n    CloudBlobClient blobClient = storageAccount.createCloudBlobClient();\r\n    Iterable<CloudBlobContainer> containers = blobClient.listContainers(CONTAINER_PREFIX);\r\n    for (CloudBlobContainer container : containers) {\r\n        LOG.info(\"Container {} URI {}\", container.getName(), container.getUri());\r\n        if (container.deleteIfExists()) {\r\n            count++;\r\n        }\r\n    }\r\n    LOG.info(\"Deleted {} test containers\", count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"/src\", \"/dest\", \"filename\" }, { \"/%2c%26\", \"/abcÖ⇒123\", \"%2c%27\" }, { \"/ÖáΠ⇒\", \"/abcÖáΠ⇒123\", \"中文\" }, { \"/A +B\", \"/B+ C\", \"C +D\" }, { \"/A~`!@#$%^&*()-_+={};:'>,<B\", \"/B~`!@#$%^&*()-_+={};:'>,<C\", \"C~`!@#$%^&*()-_+={};:'>,<D\" } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameFileUsingUnicode",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRenameFileUsingUnicode() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path folderPath1 = path(srcDir);\r\n    assertMkdirs(fs, folderPath1);\r\n    assertIsDirectory(fs, folderPath1);\r\n    Path filePath = new Path(folderPath1 + \"/\" + filename);\r\n    touch(filePath);\r\n    assertIsFile(fs, filePath);\r\n    Path folderPath2 = new Path(destDir);\r\n    assertRenameOutcome(fs, folderPath1, folderPath2, true);\r\n    assertPathDoesNotExist(fs, \"renamed\", folderPath1);\r\n    assertIsDirectory(fs, folderPath2);\r\n    assertPathExists(fs, \"renamed file\", new Path(folderPath2 + \"/\" + filename));\r\n    FileStatus[] fileStatus = fs.listStatus(folderPath2);\r\n    assertNotNull(fileStatus);\r\n    assertTrue(\"Empty listing returned from listStatus(\\\"\" + folderPath2 + \"\\\")\", fileStatus.length > 0);\r\n    assertEquals(fileStatus[0].getPath().getName(), filename);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.createMock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testLeaseAsDistributedLock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testLeaseAsDistributedLock()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSelfRenewingLease",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testSelfRenewingLease()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRedoFolderRenameAll",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRedoFolderRenameAll()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreateNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testCreateNonRecursive()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSelfRenewingLeaseFileDelete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testSelfRenewingLeaseFileDelete()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameRedoFolderAlreadyDone",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameRedoFolderAlreadyDone() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamAsyncFlushWithRetainUncommittedData",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAbfsOutputStreamAsyncFlushWithRetainUncommittedData() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    final byte[] b;\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        b = new byte[TEST_BUFFER_SIZE];\r\n        new Random().nextBytes(b);\r\n        for (int i = 0; i < 2; i++) {\r\n            stream.write(b);\r\n            for (int j = 0; j < FLUSH_TIMES; j++) {\r\n                stream.flush();\r\n                Thread.sleep(10);\r\n            }\r\n        }\r\n    }\r\n    final byte[] r = new byte[TEST_BUFFER_SIZE];\r\n    try (FSDataInputStream inputStream = fs.open(testFilePath, 4 * ONE_MB)) {\r\n        while (inputStream.available() != 0) {\r\n            int result = inputStream.read(r);\r\n            assertNotEquals(\"read returned -1\", -1, result);\r\n            assertArrayEquals(\"buffer read from stream\", r, b);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamSyncFlush",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAbfsOutputStreamSyncFlush() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    final byte[] b;\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        b = new byte[TEST_BUFFER_SIZE];\r\n        new Random().nextBytes(b);\r\n        stream.write(b);\r\n        for (int i = 0; i < FLUSH_TIMES; i++) {\r\n            stream.hsync();\r\n            stream.hflush();\r\n            Thread.sleep(10);\r\n        }\r\n    }\r\n    final byte[] r = new byte[TEST_BUFFER_SIZE];\r\n    try (FSDataInputStream inputStream = fs.open(testFilePath, 4 * ONE_MB)) {\r\n        int result = inputStream.read(r);\r\n        assertNotEquals(-1, result);\r\n        assertArrayEquals(r, b);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteHeavyBytesToFileSyncFlush",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testWriteHeavyBytesToFileSyncFlush() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    ExecutorService es;\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        es = Executors.newFixedThreadPool(10);\r\n        final byte[] b = new byte[TEST_BUFFER_SIZE];\r\n        new Random().nextBytes(b);\r\n        List<Future<Void>> tasks = new ArrayList<>();\r\n        for (int i = 0; i < FLUSH_TIMES; i++) {\r\n            Callable<Void> callable = new Callable<Void>() {\r\n\r\n                @Override\r\n                public Void call() throws Exception {\r\n                    stream.write(b);\r\n                    return null;\r\n                }\r\n            };\r\n            tasks.add(es.submit(callable));\r\n        }\r\n        boolean shouldStop = false;\r\n        while (!shouldStop) {\r\n            shouldStop = true;\r\n            for (Future<Void> task : tasks) {\r\n                if (!task.isDone()) {\r\n                    stream.hsync();\r\n                    shouldStop = false;\r\n                    Thread.sleep(THREAD_SLEEP_TIME);\r\n                }\r\n            }\r\n        }\r\n        tasks.clear();\r\n    }\r\n    es.shutdownNow();\r\n    FileStatus fileStatus = fs.getFileStatus(testFilePath);\r\n    long expectedWrites = (long) TEST_BUFFER_SIZE * FLUSH_TIMES;\r\n    assertEquals(\"Wrong file length in \" + testFilePath, expectedWrites, fileStatus.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteHeavyBytesToFileAsyncFlush",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testWriteHeavyBytesToFileAsyncFlush() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    ExecutorService es = Executors.newFixedThreadPool(10);\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        final byte[] b = new byte[TEST_BUFFER_SIZE];\r\n        new Random().nextBytes(b);\r\n        List<Future<Void>> tasks = new ArrayList<>();\r\n        for (int i = 0; i < FLUSH_TIMES; i++) {\r\n            Callable<Void> callable = new Callable<Void>() {\r\n\r\n                @Override\r\n                public Void call() throws Exception {\r\n                    stream.write(b);\r\n                    return null;\r\n                }\r\n            };\r\n            tasks.add(es.submit(callable));\r\n        }\r\n        boolean shouldStop = false;\r\n        while (!shouldStop) {\r\n            shouldStop = true;\r\n            for (Future<Void> task : tasks) {\r\n                if (!task.isDone()) {\r\n                    stream.flush();\r\n                    shouldStop = false;\r\n                }\r\n            }\r\n        }\r\n        Thread.sleep(THREAD_SLEEP_TIME);\r\n        tasks.clear();\r\n    }\r\n    es.shutdownNow();\r\n    FileStatus fileStatus = fs.getFileStatus(testFilePath);\r\n    assertEquals((long) TEST_BUFFER_SIZE * FLUSH_TIMES, fileStatus.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFlushWithOutputStreamFlushEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFlushWithOutputStreamFlushEnabled() throws Exception\n{\r\n    testFlush(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFlushWithOutputStreamFlushDisabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFlushWithOutputStreamFlushDisabled() throws Exception\n{\r\n    testFlush(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFlush",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFlush(boolean disableOutputStreamFlush) throws Exception\n{\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) getFileSystem();\r\n    fs.getAbfsStore().getAbfsConfiguration().setDisableOutputStreamFlush(disableOutputStreamFlush);\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    byte[] buffer = getRandomBytesArray();\r\n    assertTrue(fs.getAbfsStore().getAbfsConfiguration().getWriteBufferSize() <= buffer.length);\r\n    boolean isAppendBlob = true;\r\n    if (!fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(testFilePath).toString())) {\r\n        isAppendBlob = false;\r\n    }\r\n    try (FSDataOutputStream stream = fs.create(testFilePath)) {\r\n        stream.write(buffer);\r\n        AbfsOutputStream abfsStream = (AbfsOutputStream) stream.getWrappedStream();\r\n        abfsStream.waitForPendingUploads();\r\n        stream.flush();\r\n        validate(fs.open(testFilePath), buffer, !disableOutputStreamFlush || isAppendBlob);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testHflushWithFlushEnabled",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testHflushWithFlushEnabled() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    byte[] buffer = getRandomBytesArray();\r\n    String fileName = UUID.randomUUID().toString();\r\n    final Path testFilePath = path(fileName);\r\n    try (FSDataOutputStream stream = getStreamAfterWrite(fs, testFilePath, buffer, true)) {\r\n        stream.hflush();\r\n        validate(fs, testFilePath, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testHflushWithFlushDisabled",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testHflushWithFlushDisabled() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    byte[] buffer = getRandomBytesArray();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    boolean isAppendBlob = false;\r\n    if (fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(testFilePath).toString())) {\r\n        isAppendBlob = true;\r\n    }\r\n    try (FSDataOutputStream stream = getStreamAfterWrite(fs, testFilePath, buffer, false)) {\r\n        stream.hflush();\r\n        validate(fs, testFilePath, buffer, isAppendBlob);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testHsyncWithFlushEnabled",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testHsyncWithFlushEnabled() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    byte[] buffer = getRandomBytesArray();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    try (FSDataOutputStream stream = getStreamAfterWrite(fs, testFilePath, buffer, true)) {\r\n        stream.hsync();\r\n        validate(fs, testFilePath, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testTracingHeaderForAppendBlob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testTracingHeaderForAppendBlob() throws Exception\n{\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(FS_AZURE_APPEND_BLOB_KEY, \"abfss:/\");\r\n    config.set(TestConfigurationKeys.FS_AZURE_TEST_APPENDBLOB_ENABLED, \"true\");\r\n    AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(config);\r\n    byte[] buf = new byte[10];\r\n    new Random().nextBytes(buf);\r\n    try (FSDataOutputStream out = fs.create(new Path(\"/testFile\"))) {\r\n        ((AbfsOutputStream) out.getWrappedStream()).registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.WRITE, false, 0, ((AbfsOutputStream) out.getWrappedStream()).getStreamID()));\r\n        out.write(buf);\r\n        out.hsync();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testStreamCapabilitiesWithFlushDisabled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testStreamCapabilitiesWithFlushDisabled() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    byte[] buffer = getRandomBytesArray();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    try (FSDataOutputStream stream = getStreamAfterWrite(fs, testFilePath, buffer, false)) {\r\n        assertLacksStreamCapabilities(stream, StreamCapabilities.HFLUSH, StreamCapabilities.HSYNC, StreamCapabilities.DROPBEHIND, StreamCapabilities.READAHEAD, StreamCapabilities.UNBUFFER);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testStreamCapabilitiesWithFlushEnabled",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testStreamCapabilitiesWithFlushEnabled() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    byte[] buffer = getRandomBytesArray();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    try (FSDataOutputStream stream = getStreamAfterWrite(fs, testFilePath, buffer, true)) {\r\n        assertHasStreamCapabilities(stream, StreamCapabilities.HFLUSH, StreamCapabilities.HSYNC);\r\n        assertLacksStreamCapabilities(stream, StreamCapabilities.DROPBEHIND, StreamCapabilities.READAHEAD, StreamCapabilities.UNBUFFER);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testHsyncWithFlushDisabled",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testHsyncWithFlushDisabled() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    byte[] buffer = getRandomBytesArray();\r\n    final Path testFilePath = path(methodName.getMethodName());\r\n    boolean isAppendBlob = false;\r\n    if (fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(testFilePath).toString())) {\r\n        isAppendBlob = true;\r\n    }\r\n    try (FSDataOutputStream stream = getStreamAfterWrite(fs, testFilePath, buffer, false)) {\r\n        stream.hsync();\r\n        validate(fs, testFilePath, buffer, isAppendBlob);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRandomBytesArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getRandomBytesArray()\n{\r\n    final byte[] b = new byte[TEST_FILE_LENGTH];\r\n    new Random().nextBytes(b);\r\n    return b;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getStreamAfterWrite",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FSDataOutputStream getStreamAfterWrite(AzureBlobFileSystem fs, Path path, byte[] buffer, boolean enableFlush) throws IOException\n{\r\n    fs.getAbfsStore().getAbfsConfiguration().setEnableFlush(enableFlush);\r\n    FSDataOutputStream stream = fs.create(path);\r\n    stream.write(buffer);\r\n    return stream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validate(InputStream stream, byte[] writeBuffer, boolean isEqual) throws IOException\n{\r\n    try {\r\n        byte[] readBuffer = new byte[writeBuffer.length];\r\n        int numBytesRead = stream.read(readBuffer, 0, readBuffer.length);\r\n        if (isEqual) {\r\n            assertArrayEquals(\"Bytes read do not match bytes written.\", writeBuffer, readBuffer);\r\n        } else {\r\n            assertThat(\"Bytes read unexpectedly match bytes written.\", readBuffer, IsNot.not(IsEqual.equalTo(writeBuffer)));\r\n        }\r\n    } finally {\r\n        stream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void validate(FileSystem fs, Path path, byte[] writeBuffer, boolean isEqual) throws IOException\n{\r\n    String filePath = path.toUri().toString();\r\n    try (FSDataInputStream inputStream = fs.open(path)) {\r\n        byte[] readBuffer = new byte[TEST_FILE_LENGTH];\r\n        int numBytesRead = inputStream.read(readBuffer, 0, readBuffer.length);\r\n        if (isEqual) {\r\n            assertArrayEquals(String.format(\"Bytes read do not match bytes written to %1$s\", filePath), writeBuffer, readBuffer);\r\n        } else {\r\n            assertThat(String.format(\"Bytes read unexpectedly match bytes written to %1$s\", filePath), readBuffer, IsNot.not(IsEqual.equalTo(writeBuffer)));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.createOutOfBandStore(UPLOAD_BLOCK_SIZE, DOWNLOAD_BLOCK_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testReadOOBWrites",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testReadOOBWrites() throws Exception\n{\r\n    byte[] dataBlockWrite = new byte[UPLOAD_BLOCK_SIZE];\r\n    byte[] dataBlockRead = new byte[UPLOAD_BLOCK_SIZE];\r\n    String key = \"WASB_String\" + AzureTestUtils.getForkID() + \".txt\";\r\n    OutputStream outputStream = testAccount.getStore().storefile(key, new PermissionStatus(\"\", \"\", FsPermission.getDefault()), key);\r\n    Arrays.fill(dataBlockWrite, (byte) 255);\r\n    for (int i = 0; i < NUMBER_OF_BLOCKS; i++) {\r\n        outputStream.write(dataBlockWrite);\r\n    }\r\n    outputStream.flush();\r\n    outputStream.close();\r\n    DataBlockWriter writeBlockTask = new DataBlockWriter(testAccount, key);\r\n    writeBlockTask.startWriting();\r\n    int count = 0;\r\n    for (int i = 0; i < 5; i++) {\r\n        try (InputStream inputStream = testAccount.getStore().retrieve(key)) {\r\n            count = 0;\r\n            int c = 0;\r\n            while (c >= 0) {\r\n                c = inputStream.read(dataBlockRead, 0, UPLOAD_BLOCK_SIZE);\r\n                if (c < 0) {\r\n                    break;\r\n                }\r\n                count += c;\r\n            }\r\n        } catch (IOException e) {\r\n            System.out.println(e.getCause().toString());\r\n            e.printStackTrace();\r\n            fail();\r\n        }\r\n    }\r\n    writeBlockTask.stopWriting();\r\n    assertEquals(NUMBER_OF_BLOCKS * UPLOAD_BLOCK_SIZE, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAppendBlockOperations",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAppendBlockOperations() throws Exception\n{\r\n    CloudBlobContainer container = getTestAccount().getRealContainer();\r\n    OperationContext context = new OperationContext();\r\n    context.getResponseReceivedEventHandler().addListener(new ResponseReceivedEventHandler());\r\n    context.getSendingRequestEventHandler().addListener(new SendingRequestEventHandler());\r\n    CloudAppendBlob appendBlob = container.getAppendBlobReference(\"testAppendBlockOperations\");\r\n    assertNull(lastOperationTypeSent);\r\n    assertNull(lastOperationTypeReceived);\r\n    assertEquals(0, lastContentLengthReceived);\r\n    try (BlobOutputStream output = appendBlob.openWriteNew(null, null, context)) {\r\n        assertEquals(BlobOperationDescriptor.OperationType.CreateBlob, lastOperationTypeReceived);\r\n        assertEquals(0, lastContentLengthReceived);\r\n        String message = \"this is a test\";\r\n        output.write(message.getBytes(\"UTF-8\"));\r\n        output.flush();\r\n        assertEquals(BlobOperationDescriptor.OperationType.AppendBlock, lastOperationTypeSent);\r\n        assertEquals(BlobOperationDescriptor.OperationType.AppendBlock, lastOperationTypeReceived);\r\n        assertEquals(message.length(), lastContentLengthReceived);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPutBlockOperations",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testPutBlockOperations() throws Exception\n{\r\n    CloudBlobContainer container = getTestAccount().getRealContainer();\r\n    OperationContext context = new OperationContext();\r\n    context.getResponseReceivedEventHandler().addListener(new ResponseReceivedEventHandler());\r\n    context.getSendingRequestEventHandler().addListener(new SendingRequestEventHandler());\r\n    CloudBlockBlob blockBlob = container.getBlockBlobReference(\"testPutBlockOperations\");\r\n    assertNull(lastOperationTypeSent);\r\n    assertNull(lastOperationTypeReceived);\r\n    assertEquals(0, lastContentLengthReceived);\r\n    try (BlobOutputStream output = blockBlob.openOutputStream(null, null, context)) {\r\n        assertNull(lastOperationTypeReceived);\r\n        assertEquals(0, lastContentLengthReceived);\r\n        String message = \"this is a test\";\r\n        output.write(message.getBytes(\"UTF-8\"));\r\n        output.flush();\r\n        assertEquals(BlobOperationDescriptor.OperationType.PutBlock, lastOperationTypeSent);\r\n        assertEquals(BlobOperationDescriptor.OperationType.PutBlock, lastOperationTypeReceived);\r\n        assertEquals(message.length(), lastContentLengthReceived);\r\n    }\r\n    assertEquals(BlobOperationDescriptor.OperationType.PutBlockList, lastOperationTypeSent);\r\n    assertEquals(BlobOperationDescriptor.OperationType.PutBlockList, lastOperationTypeReceived);\r\n    assertEquals(0, lastContentLengthReceived);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPutPageOperations",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPutPageOperations() throws Exception\n{\r\n    CloudBlobContainer container = getTestAccount().getRealContainer();\r\n    OperationContext context = new OperationContext();\r\n    context.getResponseReceivedEventHandler().addListener(new ResponseReceivedEventHandler());\r\n    context.getSendingRequestEventHandler().addListener(new SendingRequestEventHandler());\r\n    CloudPageBlob pageBlob = container.getPageBlobReference(\"testPutPageOperations\");\r\n    assertNull(lastOperationTypeSent);\r\n    assertNull(lastOperationTypeReceived);\r\n    assertEquals(0, lastContentLengthReceived);\r\n    try (BlobOutputStream output = pageBlob.openWriteNew(1024, null, null, context)) {\r\n        assertEquals(BlobOperationDescriptor.OperationType.CreateBlob, lastOperationTypeReceived);\r\n        assertEquals(0, lastContentLengthReceived);\r\n        final int pageSize = 512;\r\n        byte[] buffer = new byte[pageSize];\r\n        output.write(buffer);\r\n        output.flush();\r\n        assertEquals(BlobOperationDescriptor.OperationType.PutPage, lastOperationTypeSent);\r\n        assertEquals(BlobOperationDescriptor.OperationType.PutPage, lastOperationTypeReceived);\r\n        assertEquals(buffer.length, lastContentLengthReceived);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testGetBlobOperations",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testGetBlobOperations() throws Exception\n{\r\n    CloudBlobContainer container = getTestAccount().getRealContainer();\r\n    OperationContext context = new OperationContext();\r\n    context.getResponseReceivedEventHandler().addListener(new ResponseReceivedEventHandler());\r\n    context.getSendingRequestEventHandler().addListener(new SendingRequestEventHandler());\r\n    CloudBlockBlob blockBlob = container.getBlockBlobReference(\"testGetBlobOperations\");\r\n    assertNull(lastOperationTypeSent);\r\n    assertNull(lastOperationTypeReceived);\r\n    assertEquals(0, lastContentLengthReceived);\r\n    String message = \"this is a test\";\r\n    try (BlobOutputStream output = blockBlob.openOutputStream(null, null, context)) {\r\n        assertNull(lastOperationTypeReceived);\r\n        assertEquals(0, lastContentLengthReceived);\r\n        output.write(message.getBytes(\"UTF-8\"));\r\n        output.flush();\r\n        assertEquals(BlobOperationDescriptor.OperationType.PutBlock, lastOperationTypeSent);\r\n        assertEquals(BlobOperationDescriptor.OperationType.PutBlock, lastOperationTypeReceived);\r\n        assertEquals(message.length(), lastContentLengthReceived);\r\n    }\r\n    assertEquals(BlobOperationDescriptor.OperationType.PutBlockList, lastOperationTypeSent);\r\n    assertEquals(BlobOperationDescriptor.OperationType.PutBlockList, lastOperationTypeReceived);\r\n    assertEquals(0, lastContentLengthReceived);\r\n    try (BlobInputStream input = blockBlob.openInputStream(null, null, context)) {\r\n        assertEquals(BlobOperationDescriptor.OperationType.GetProperties, lastOperationTypeSent);\r\n        assertEquals(BlobOperationDescriptor.OperationType.GetProperties, lastOperationTypeReceived);\r\n        assertEquals(0, lastContentLengthReceived);\r\n        byte[] buffer = new byte[1024];\r\n        int numBytesRead = input.read(buffer);\r\n        assertEquals(BlobOperationDescriptor.OperationType.GetBlob, lastOperationTypeSent);\r\n        assertEquals(BlobOperationDescriptor.OperationType.GetBlob, lastOperationTypeReceived);\r\n        assertEquals(message.length(), lastContentLengthReceived);\r\n        assertEquals(numBytesRead, lastContentLengthReceived);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "responseReceived",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void responseReceived(ResponseReceivedEvent event)\n{\r\n    HttpURLConnection conn = (HttpURLConnection) event.getConnectionObject();\r\n    BlobOperationDescriptor.OperationType operationType = BlobOperationDescriptor.getOperationType(conn);\r\n    lastOperationTypeReceived = operationType;\r\n    switch(operationType) {\r\n        case AppendBlock:\r\n        case PutBlock:\r\n        case PutPage:\r\n            lastContentLengthReceived = BlobOperationDescriptor.getContentLengthIfKnown(conn, operationType);\r\n            break;\r\n        case GetBlob:\r\n            lastContentLengthReceived = BlobOperationDescriptor.getContentLengthIfKnown(conn, operationType);\r\n            break;\r\n        default:\r\n            lastContentLengthReceived = 0;\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "sendingRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendingRequest(SendingRequestEvent event)\n{\r\n    this.lastOperationTypeSent = BlobOperationDescriptor.getOperationType((HttpURLConnection) event.getConnectionObject());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteRoot",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testDeleteRoot() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testPath = path(\"/testFolder\");\r\n    fs.mkdirs(new Path(testPath + \"_0\"));\r\n    fs.mkdirs(new Path(testPath + \"_1\"));\r\n    fs.mkdirs(new Path(testPath + \"_2\"));\r\n    touch(new Path(testPath + \"_1/testfile\"));\r\n    touch(new Path(testPath + \"_1/testfile2\"));\r\n    touch(new Path(testPath + \"_1/testfile3\"));\r\n    Path root = new Path(\"/\");\r\n    FileStatus[] ls = fs.listStatus(root);\r\n    assertEquals(3, ls.length);\r\n    fs.delete(root, true);\r\n    ls = fs.listStatus(root);\r\n    assertEquals(\"listing size\", 0, ls.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOpenFileAfterDelete",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testOpenFileAfterDelete() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testfile = path(\"/testFile\");\r\n    touch(testfile);\r\n    assertDeleted(fs, testfile, false);\r\n    intercept(FileNotFoundException.class, () -> fs.open(testfile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureFileIsDeleted",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEnsureFileIsDeleted() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testfile = path(\"testfile\");\r\n    touch(testfile);\r\n    assertDeleted(fs, testfile, false);\r\n    assertPathDoesNotExist(fs, \"deleted\", testfile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteDirectory",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testDeleteDirectory() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path dir = path(\"testfile\");\r\n    fs.mkdirs(dir);\r\n    fs.mkdirs(new Path(dir + \"/test1\"));\r\n    fs.mkdirs(new Path(dir + \"/test1/test2\"));\r\n    assertDeleted(fs, dir, true);\r\n    assertPathDoesNotExist(fs, \"deleted\", dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteFirstLevelDirectory",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDeleteFirstLevelDirectory() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final List<Future<Void>> tasks = new ArrayList<>();\r\n    ExecutorService es = Executors.newFixedThreadPool(10);\r\n    Path dir = path(\"/test\");\r\n    for (int i = 0; i < 1000; i++) {\r\n        final Path fileName = new Path(dir + \"/\" + i);\r\n        Callable<Void> callable = new Callable<Void>() {\r\n\r\n            @Override\r\n            public Void call() throws Exception {\r\n                touch(fileName);\r\n                return null;\r\n            }\r\n        };\r\n        tasks.add(es.submit(callable));\r\n    }\r\n    for (Future<Void> task : tasks) {\r\n        task.get();\r\n    }\r\n    es.shutdownNow();\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.DELETE, false, 0));\r\n    intercept(FileAlreadyExistsException.class, () -> fs.delete(dir, false));\r\n    fs.registerListener(null);\r\n    assertDeleted(fs, dir, true);\r\n    assertPathDoesNotExist(fs, \"deleted\", dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteIdempotency",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testDeleteIdempotency() throws Exception\n{\r\n    Assume.assumeTrue(DEFAULT_DELETE_CONSIDERED_IDEMPOTENT);\r\n    AbfsConfiguration abfsConfig = TestAbfsConfigurationFieldsValidation.updateRetryConfigs(getConfiguration(), REDUCED_RETRY_COUNT, REDUCED_MAX_BACKOFF_INTERVALS_MS);\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    AbfsClient abfsClient = fs.getAbfsStore().getClient();\r\n    AbfsClient testClient = TestAbfsClient.createTestClientFromCurrentContext(abfsClient, abfsConfig);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    when(op.isARetriedRequest()).thenReturn(true);\r\n    AbfsHttpOperation http404Op = mock(AbfsHttpOperation.class);\r\n    when(http404Op.getStatusCode()).thenReturn(HTTP_NOT_FOUND);\r\n    when(op.getResult()).thenReturn(http404Op);\r\n    when(op.hasResult()).thenReturn(true);\r\n    Assertions.assertThat(testClient.deleteIdempotencyCheckOp(op).getResult().getStatusCode()).describedAs(\"Delete is considered idempotent by default and should return success.\").isEqualTo(HTTP_OK);\r\n    AbfsHttpOperation http400Op = mock(AbfsHttpOperation.class);\r\n    when(http400Op.getStatusCode()).thenReturn(HTTP_BAD_REQUEST);\r\n    when(op.getResult()).thenReturn(http400Op);\r\n    when(op.hasResult()).thenReturn(true);\r\n    Assertions.assertThat(testClient.deleteIdempotencyCheckOp(op).getResult().getStatusCode()).describedAs(\"Idempotency check to happen only for HTTP 404 response.\").isEqualTo(HTTP_BAD_REQUEST);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteIdempotencyTriggerHttp404",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testDeleteIdempotencyTriggerHttp404() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    AbfsClient client = TestAbfsClient.createTestClientFromCurrentContext(fs.getAbfsStore().getClient(), this.getConfiguration());\r\n    intercept(AbfsRestOperationException.class, () -> fs.getAbfsStore().delete(new Path(\"/NonExistingPath\"), false, getTestTracingContext(fs, false)));\r\n    intercept(AbfsRestOperationException.class, () -> client.deletePath(\"/NonExistingPath\", false, null, getTestTracingContext(fs, true)));\r\n    AbfsClient mockClient = TestAbfsClient.getMockAbfsClient(fs.getAbfsStore().getClient(), this.getConfiguration());\r\n    AzureBlobFileSystemStore mockStore = mock(AzureBlobFileSystemStore.class);\r\n    mockStore = TestMockHelpers.setClassField(AzureBlobFileSystemStore.class, mockStore, \"client\", mockClient);\r\n    mockStore = TestMockHelpers.setClassField(AzureBlobFileSystemStore.class, mockStore, \"abfsPerfTracker\", TestAbfsPerfTracker.getAPerfTrackerInstance(this.getConfiguration()));\r\n    doCallRealMethod().when(mockStore).delete(new Path(\"/NonExistingPath\"), false, getTestTracingContext(fs, false));\r\n    AbfsRestOperation idempotencyRetOp = TestAbfsClient.getRestOp(DeletePath, mockClient, HTTP_METHOD_DELETE, TestAbfsClient.getTestUrl(mockClient, \"/NonExistingPath\"), TestAbfsClient.getTestRequestHeaders(mockClient));\r\n    idempotencyRetOp.hardSetResult(HTTP_OK);\r\n    doReturn(idempotencyRetOp).when(mockClient).deleteIdempotencyCheckOp(any());\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    when(mockClient.deletePath(\"/NonExistingPath\", false, null, tracingContext)).thenCallRealMethod();\r\n    Assertions.assertThat(mockClient.deletePath(\"/NonExistingPath\", false, null, tracingContext).getResult().getStatusCode()).describedAs(\"Idempotency check reports successful \" + \"delete. 200OK should be returned\").isEqualTo(idempotencyRetOp.getResult().getStatusCode());\r\n    mockStore.delete(new Path(\"/NonExistingPath\"), false, getTestTracingContext(fs, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    testPath = path(\"testfile.dat\");\r\n    testFolderPath = path(\"testfolder\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setupInputStreamToTest",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setupInputStreamToTest(AzureBlobStorageTestAccount testAccount) throws Exception\n{\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    Path base = methodPath();\r\n    Path testFilePath1 = new Path(base, \"test1.dat\");\r\n    Path testFilePath2 = new Path(base, \"test2.dat\");\r\n    FSDataOutputStream outputStream = fs.create(testFilePath1);\r\n    String testString = \"This is a test string\";\r\n    outputStream.write(testString.getBytes());\r\n    outputStream.close();\r\n    inputStream = fs.open(testFilePath1);\r\n    fs.rename(testFilePath1, testFilePath2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobReadScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedPageBlobReadScenario() throws Throwable\n{\r\n    AzureBlobStorageTestAccount testAccount = getPageBlobTestStorageAccount();\r\n    setupInputStreamToTest(testAccount);\r\n    byte[] readBuffer = new byte[512];\r\n    inputStream.read(readBuffer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobSeekScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedPageBlobSeekScenario() throws Throwable\n{\r\n    AzureBlobStorageTestAccount testAccount = getPageBlobTestStorageAccount();\r\n    setupInputStreamToTest(testAccount);\r\n    inputStream.seek(5);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadBlockBlobSeekScenario",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSingleThreadBlockBlobSeekScenario() throws Throwable\n{\r\n    AzureBlobStorageTestAccount testAccount = createTestAccount();\r\n    setupInputStreamToTest(testAccount);\r\n    inputStream.seek(5);\r\n    inputStream.read();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingledThreadBlockBlobReadScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingledThreadBlockBlobReadScenario() throws Throwable\n{\r\n    AzureBlobStorageTestAccount testAccount = createTestAccount();\r\n    setupInputStreamToTest(testAccount);\r\n    byte[] readBuffer = new byte[512];\r\n    inputStream.read(readBuffer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedBlockBlobSetPermissionScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedBlockBlobSetPermissionScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    fs.setPermission(testPath, new FsPermission(FsAction.EXECUTE, FsAction.READ, FsAction.READ));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobSetPermissionScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedPageBlobSetPermissionScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    fs.setOwner(testPath, \"testowner\", \"testgroup\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedBlockBlobSetOwnerScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedBlockBlobSetOwnerScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    fs.setOwner(testPath, \"testowner\", \"testgroup\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobSetOwnerScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedPageBlobSetOwnerScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    fs.setPermission(testPath, new FsPermission(FsAction.EXECUTE, FsAction.READ, FsAction.READ));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedBlockBlobListStatusScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedBlockBlobListStatusScenario() throws Throwable\n{\r\n    createTestFolder(createTestAccount(), testFolderPath);\r\n    fs.delete(testFolderPath, true);\r\n    fs.listStatus(testFolderPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobListStatusScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedPageBlobListStatusScenario() throws Throwable\n{\r\n    createTestFolder(getPageBlobTestStorageAccount(), testFolderPath);\r\n    fs.delete(testFolderPath, true);\r\n    fs.listStatus(testFolderPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedBlockBlobRenameScenario",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSingleThreadedBlockBlobRenameScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    Path dstPath = new Path(\"dstFile.dat\");\r\n    fs.delete(testPath, true);\r\n    boolean renameResult = fs.rename(testPath, dstPath);\r\n    assertFalse(renameResult);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobRenameScenario",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSingleThreadedPageBlobRenameScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    Path dstPath = new Path(\"dstFile.dat\");\r\n    fs.delete(testPath, true);\r\n    boolean renameResult = fs.rename(testPath, dstPath);\r\n    assertFalse(renameResult);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedBlockBlobDeleteScenario",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSingleThreadedBlockBlobDeleteScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    boolean deleteResult = fs.delete(testPath, true);\r\n    assertFalse(deleteResult);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobDeleteScenario",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSingleThreadedPageBlobDeleteScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    boolean deleteResult = fs.delete(testPath, true);\r\n    assertFalse(deleteResult);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedBlockBlobOpenScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedBlockBlobOpenScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    inputStream = fs.open(testPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleThreadedPageBlobOpenScenario",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSingleThreadedPageBlobOpenScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    fs.delete(testPath, true);\r\n    inputStream = fs.open(testPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testWriteAfterClose",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWriteAfterClose() throws Throwable\n{\r\n    FSDataOutputStream out = fs.create(testPath);\r\n    out.close();\r\n    intercept(IOException.class, STREAM_IS_CLOSED, () -> out.write('a'));\r\n    intercept(IOException.class, STREAM_IS_CLOSED, () -> out.write(new byte[] { 'a' }));\r\n    out.hsync();\r\n    out.flush();\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (inputStream != null) {\r\n        inputStream.close();\r\n    }\r\n    ContractTestUtils.rm(fs, testPath, true, true);\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.createOutOfBandStore(UPLOAD_BLOCK_SIZE, DOWNLOAD_BLOCK_SIZE, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return isSecure ? FileSystemUriSchemes.ABFS_SECURE_SCHEME : FileSystemUriSchemes.ABFS_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "getTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestPath()\n{\r\n    return new Path(UriUtils.generateUniqueTestPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AbfsFileSystemContract{\");\r\n    sb.append(\"isSecureScheme=\").append(isSecure);\r\n    sb.append(super.toString());\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    fs = getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertPathDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPathDoesNotExist(String message, Path path) throws IOException\n{\r\n    ContractTestUtils.assertPathDoesNotExist(fs, message, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertPathExists",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPathExists(String message, Path path) throws IOException\n{\r\n    ContractTestUtils.assertPathExists(fs, message, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCheckingNonExistentOneLetterFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCheckingNonExistentOneLetterFile() throws Exception\n{\r\n    assertPathDoesNotExist(\"one letter file\", new Path(\"/a\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testStoreRetrieveFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testStoreRetrieveFile() throws Exception\n{\r\n    Path testFile = methodPath();\r\n    writeString(testFile, \"Testing\");\r\n    assertTrue(fs.exists(testFile));\r\n    FileStatus status = fs.getFileStatus(testFile);\r\n    assertNotNull(status);\r\n    assertEquals(new FsPermission((short) 0644), status.getPermission());\r\n    assertEquals(\"Testing\", readString(testFile));\r\n    fs.delete(testFile, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetGetXAttr",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetGetXAttr() throws Exception\n{\r\n    byte[] attributeValue1 = \"hi\".getBytes(StandardCharsets.UTF_8);\r\n    byte[] attributeValue2 = \"你好\".getBytes(StandardCharsets.UTF_8);\r\n    String attributeName1 = \"user.asciiAttribute\";\r\n    String attributeName2 = \"user.unicodeAttribute\";\r\n    Path testFile = methodPath();\r\n    createEmptyFile(testFile, FsPermission.createImmutable(READ_WRITE_PERMISSIONS));\r\n    assertNull(fs.getXAttr(testFile, attributeName1));\r\n    fs.setXAttr(testFile, attributeName1, attributeValue1);\r\n    assertArrayEquals(attributeValue1, fs.getXAttr(testFile, attributeName1));\r\n    fs.setXAttr(testFile, attributeName2, attributeValue2);\r\n    assertArrayEquals(attributeValue1, fs.getXAttr(testFile, attributeName1));\r\n    assertArrayEquals(attributeValue2, fs.getXAttr(testFile, attributeName2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetGetXAttrCreateReplace",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetGetXAttrCreateReplace() throws Exception\n{\r\n    byte[] attributeValue = \"one\".getBytes(StandardCharsets.UTF_8);\r\n    String attributeName = \"user.someAttribute\";\r\n    Path testFile = methodPath();\r\n    createEmptyFile(testFile, FsPermission.createImmutable(READ_WRITE_PERMISSIONS));\r\n    fs.setXAttr(testFile, attributeName, attributeValue, CREATE_FLAG);\r\n    assertArrayEquals(attributeValue, fs.getXAttr(testFile, attributeName));\r\n    intercept(IOException.class, () -> fs.setXAttr(testFile, attributeName, attributeValue, CREATE_FLAG));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetGetXAttrReplace",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetGetXAttrReplace() throws Exception\n{\r\n    byte[] attributeValue1 = \"one\".getBytes(StandardCharsets.UTF_8);\r\n    byte[] attributeValue2 = \"two\".getBytes(StandardCharsets.UTF_8);\r\n    String attributeName = \"user.someAttribute\";\r\n    Path testFile = methodPath();\r\n    createEmptyFile(testFile, FsPermission.createImmutable(READ_WRITE_PERMISSIONS));\r\n    intercept(IOException.class, () -> fs.setXAttr(testFile, attributeName, attributeValue1, REPLACE_FLAG));\r\n    fs.setXAttr(testFile, attributeName, attributeValue1, CREATE_FLAG);\r\n    fs.setXAttr(testFile, attributeName, attributeValue2, REPLACE_FLAG);\r\n    assertArrayEquals(attributeValue2, fs.getXAttr(testFile, attributeName));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testStoreDeleteFolder",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testStoreDeleteFolder() throws Exception\n{\r\n    Path testFolder = methodPath();\r\n    assertFalse(fs.exists(testFolder));\r\n    assertTrue(fs.mkdirs(testFolder));\r\n    assertTrue(fs.exists(testFolder));\r\n    FileStatus status = fs.getFileStatus(testFolder);\r\n    assertNotNull(status);\r\n    assertTrue(status.isDirectory());\r\n    assertEquals(new FsPermission((short) 0755), status.getPermission());\r\n    Path innerFile = new Path(testFolder, \"innerFile\");\r\n    assertTrue(fs.createNewFile(innerFile));\r\n    assertPathExists(\"inner file\", innerFile);\r\n    assertTrue(fs.delete(testFolder, true));\r\n    assertPathDoesNotExist(\"inner file\", innerFile);\r\n    assertPathDoesNotExist(\"testFolder\", testFolder);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileOwnership",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFileOwnership() throws Exception\n{\r\n    Path testFile = methodPath();\r\n    writeString(testFile, \"Testing\");\r\n    testOwnership(testFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFolderOwnership",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFolderOwnership() throws Exception\n{\r\n    Path testFolder = methodPath();\r\n    fs.mkdirs(testFolder);\r\n    testOwnership(testFolder);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testOwnership",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testOwnership(Path pathUnderTest) throws IOException\n{\r\n    FileStatus ret = fs.getFileStatus(pathUnderTest);\r\n    UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\r\n    assertTrue(ret.getOwner().equals(currentUser.getShortUserName()));\r\n    fs.delete(pathUnderTest, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "ignoreStickyBit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FsPermission ignoreStickyBit(FsPermission original)\n{\r\n    return new FsPermission(original.getUserAction(), original.getGroupAction(), original.getOtherAction());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertEqualsIgnoreStickyBit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertEqualsIgnoreStickyBit(FsPermission expected, FsPermission actual)\n{\r\n    assertEquals(ignoreStickyBit(expected), ignoreStickyBit(actual));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFilePermissions",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFilePermissions() throws Exception\n{\r\n    Path testFile = methodPath();\r\n    FsPermission permission = FsPermission.createImmutable((short) 644);\r\n    createEmptyFile(testFile, permission);\r\n    FileStatus ret = fs.getFileStatus(testFile);\r\n    assertEqualsIgnoreStickyBit(permission, ret.getPermission());\r\n    fs.delete(testFile, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFolderPermissions",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFolderPermissions() throws Exception\n{\r\n    Path testFolder = methodPath();\r\n    FsPermission permission = FsPermission.createImmutable((short) 644);\r\n    fs.mkdirs(testFolder, permission);\r\n    FileStatus ret = fs.getFileStatus(testFolder);\r\n    assertEqualsIgnoreStickyBit(permission, ret.getPermission());\r\n    fs.delete(testFolder, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeepFileCreationBase",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testDeepFileCreationBase(String testFilePath, String firstDirPath, String middleDirPath, short permissionShort, short umaskedPermissionShort) throws Exception\n{\r\n    Path testFile = new Path(testFilePath);\r\n    Path firstDir = new Path(firstDirPath);\r\n    Path middleDir = new Path(middleDirPath);\r\n    FsPermission permission = FsPermission.createImmutable(permissionShort);\r\n    FsPermission umaskedPermission = FsPermission.createImmutable(umaskedPermissionShort);\r\n    createEmptyFile(testFile, permission);\r\n    FsPermission rootPerm = fs.getFileStatus(firstDir.getParent()).getPermission();\r\n    FsPermission inheritPerm = FsPermission.createImmutable((short) (rootPerm.toShort() | 0300));\r\n    assertPathExists(\"test file\", testFile);\r\n    assertPathExists(\"firstDir\", firstDir);\r\n    assertPathExists(\"middleDir\", middleDir);\r\n    FileStatus directoryStatus = fs.getFileStatus(middleDir);\r\n    assertTrue(directoryStatus.isDirectory());\r\n    assertEqualsIgnoreStickyBit(inheritPerm, directoryStatus.getPermission());\r\n    FileStatus fileStatus = fs.getFileStatus(testFile);\r\n    assertFalse(fileStatus.isDirectory());\r\n    assertEqualsIgnoreStickyBit(umaskedPermission, fileStatus.getPermission());\r\n    assertTrue(fs.delete(firstDir, true));\r\n    assertPathDoesNotExist(\"deleted file\", testFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeepFileCreation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDeepFileCreation() throws Exception\n{\r\n    testDeepFileCreationBase(\"deep/file/creation/test\", \"deep\", \"deep/file/creation\", (short) 0644, (short) 0644);\r\n    testDeepFileCreationBase(\"deep/file/creation/test\", \"deep\", \"deep/file/creation\", (short) 0777, (short) 0755);\r\n    testDeepFileCreationBase(\"/deep/file/creation/test\", \"/deep\", \"/deep/file/creation\", (short) 0644, (short) 0644);\r\n    testDeepFileCreationBase(\"/deep/file/creation/test\", \"/deep\", \"/deep/file/creation\", (short) 0700, (short) 0700);\r\n    testDeepFileCreationBase(\"/deep/file\", \"/deep\", \"/deep\", (short) 0644, (short) 0644);\r\n    testDeepFileCreationBase(\"deep/file\", \"deep\", \"deep\", (short) 0644, (short) 0644);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRename",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRename() throws Exception\n{\r\n    for (RenameVariation variation : RenameVariation.values()) {\r\n        System.out.printf(\"Rename variation: %s\\n\", variation);\r\n        Path originalFile;\r\n        switch(variation) {\r\n            case NormalFileName:\r\n                originalFile = new Path(\"fileToRename\");\r\n                break;\r\n            case SourceInAFolder:\r\n                originalFile = new Path(\"file/to/rename\");\r\n                break;\r\n            case SourceWithSpace:\r\n                originalFile = new Path(\"file to rename\");\r\n                break;\r\n            case SourceWithPlusAndPercent:\r\n                originalFile = new Path(\"file+to%rename\");\r\n                break;\r\n            default:\r\n                throw new Exception(\"Unknown variation\");\r\n        }\r\n        Path destinationFile = new Path(\"file/resting/destination\");\r\n        assertTrue(fs.createNewFile(originalFile));\r\n        assertTrue(fs.exists(originalFile));\r\n        assertFalse(fs.rename(originalFile, destinationFile));\r\n        assertTrue(fs.mkdirs(destinationFile.getParent()));\r\n        boolean result = fs.rename(originalFile, destinationFile);\r\n        assertTrue(result);\r\n        assertTrue(fs.exists(destinationFile));\r\n        assertFalse(fs.exists(originalFile));\r\n        fs.delete(destinationFile.getParent(), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameImplicitFolder",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRenameImplicitFolder() throws Exception\n{\r\n    Path testFile = new Path(\"deep/file/rename/test\");\r\n    FsPermission permission = FsPermission.createImmutable((short) 644);\r\n    createEmptyFile(testFile, permission);\r\n    boolean renameResult = fs.rename(new Path(\"deep/file\"), new Path(\"deep/renamed\"));\r\n    assertTrue(renameResult);\r\n    assertFalse(fs.exists(testFile));\r\n    FileStatus newStatus = fs.getFileStatus(new Path(\"deep/renamed/rename/test\"));\r\n    assertNotNull(newStatus);\r\n    assertEqualsIgnoreStickyBit(permission, newStatus.getPermission());\r\n    assertTrue(fs.delete(new Path(\"deep\"), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameFolder",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameFolder() throws Exception\n{\r\n    for (RenameFolderVariation variation : RenameFolderVariation.values()) {\r\n        Path originalFolder = new Path(\"folderToRename\");\r\n        if (variation != RenameFolderVariation.CreateJustInnerFile) {\r\n            assertTrue(fs.mkdirs(originalFolder));\r\n        }\r\n        Path innerFile = new Path(originalFolder, \"innerFile\");\r\n        Path innerFile2 = new Path(originalFolder, \"innerFile2\");\r\n        if (variation != RenameFolderVariation.CreateJustFolder) {\r\n            assertTrue(fs.createNewFile(innerFile));\r\n            assertTrue(fs.createNewFile(innerFile2));\r\n        }\r\n        Path destination = new Path(\"renamedFolder\");\r\n        assertTrue(fs.rename(originalFolder, destination));\r\n        assertTrue(fs.exists(destination));\r\n        if (variation != RenameFolderVariation.CreateJustFolder) {\r\n            assertTrue(fs.exists(new Path(destination, innerFile.getName())));\r\n            assertTrue(fs.exists(new Path(destination, innerFile2.getName())));\r\n        }\r\n        assertFalse(fs.exists(originalFolder));\r\n        assertFalse(fs.exists(innerFile));\r\n        assertFalse(fs.exists(innerFile2));\r\n        fs.delete(destination, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCopyFromLocalFileSystem",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCopyFromLocalFileSystem() throws Exception\n{\r\n    Path localFilePath = new Path(System.getProperty(\"test.build.data\", \"azure_test\"));\r\n    FileSystem localFs = FileSystem.get(new Configuration());\r\n    localFs.delete(localFilePath, true);\r\n    try {\r\n        writeStringToFile(localFs, localFilePath, \"Testing\");\r\n        Path dstPath = methodPath();\r\n        assertTrue(FileUtil.copy(localFs, localFilePath, fs, dstPath, false, fs.getConf()));\r\n        assertPathExists(\"coied from local\", dstPath);\r\n        assertEquals(\"Testing\", readStringFromFile(fs, dstPath));\r\n        fs.delete(dstPath, true);\r\n    } finally {\r\n        localFs.delete(localFilePath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testListDirectory",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testListDirectory() throws Exception\n{\r\n    Path rootFolder = new Path(\"testingList\");\r\n    assertTrue(fs.mkdirs(rootFolder));\r\n    FileStatus[] listed = fs.listStatus(rootFolder);\r\n    assertEquals(0, listed.length);\r\n    Path innerFolder = new Path(rootFolder, \"inner\");\r\n    assertTrue(fs.mkdirs(innerFolder));\r\n    listed = fs.listStatus(rootFolder);\r\n    assertEquals(1, listed.length);\r\n    assertTrue(listed[0].isDirectory());\r\n    Path innerFile = new Path(innerFolder, \"innerFile\");\r\n    writeString(innerFile, \"testing\");\r\n    listed = fs.listStatus(rootFolder);\r\n    assertEquals(1, listed.length);\r\n    assertTrue(listed[0].isDirectory());\r\n    listed = fs.listStatus(innerFolder);\r\n    assertEquals(1, listed.length);\r\n    assertFalse(listed[0].isDirectory());\r\n    assertTrue(fs.delete(rootFolder, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testUriEncoding",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUriEncoding() throws Exception\n{\r\n    fs.create(new Path(\"p/t%5Fe\")).close();\r\n    FileStatus[] listing = fs.listStatus(new Path(\"p\"));\r\n    assertEquals(1, listing.length);\r\n    assertEquals(\"t%5Fe\", listing[0].getPath().getName());\r\n    assertTrue(fs.rename(new Path(\"p\"), new Path(\"q\")));\r\n    assertTrue(fs.delete(new Path(\"q\"), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testUriEncodingMoreComplexCharacters",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testUriEncodingMoreComplexCharacters() throws Exception\n{\r\n    String fileName = \"!#$'()*;=[]%\";\r\n    String directoryName = \"*;=[]%!#$'()\";\r\n    fs.create(new Path(directoryName, fileName)).close();\r\n    FileStatus[] listing = fs.listStatus(new Path(directoryName));\r\n    assertEquals(1, listing.length);\r\n    assertEquals(fileName, listing[0].getPath().getName());\r\n    FileStatus status = fs.getFileStatus(new Path(directoryName, fileName));\r\n    assertEquals(fileName, status.getPath().getName());\r\n    InputStream stream = fs.open(new Path(directoryName, fileName));\r\n    assertNotNull(stream);\r\n    stream.close();\r\n    assertTrue(fs.delete(new Path(directoryName, fileName), true));\r\n    assertTrue(fs.delete(new Path(directoryName), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testChineseCharacters",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testChineseCharacters() throws Exception\n{\r\n    String chinese = \"\" + '\\u963f' + '\\u4db5';\r\n    String fileName = \"filename\" + chinese;\r\n    String directoryName = chinese;\r\n    fs.create(new Path(directoryName, fileName)).close();\r\n    FileStatus[] listing = fs.listStatus(new Path(directoryName));\r\n    assertEquals(1, listing.length);\r\n    assertEquals(fileName, listing[0].getPath().getName());\r\n    FileStatus status = fs.getFileStatus(new Path(directoryName, fileName));\r\n    assertEquals(fileName, status.getPath().getName());\r\n    InputStream stream = fs.open(new Path(directoryName, fileName));\r\n    assertNotNull(stream);\r\n    stream.close();\r\n    assertTrue(fs.delete(new Path(directoryName, fileName), true));\r\n    assertTrue(fs.delete(new Path(directoryName), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testChineseCharactersFolderRename",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testChineseCharactersFolderRename() throws Exception\n{\r\n    String chinese = \"\" + '\\u963f' + '\\u4db5';\r\n    String fileName = \"filename\" + chinese;\r\n    String srcDirectoryName = chinese;\r\n    String targetDirectoryName = \"target\" + chinese;\r\n    fs.create(new Path(srcDirectoryName, fileName)).close();\r\n    fs.rename(new Path(srcDirectoryName), new Path(targetDirectoryName));\r\n    FileStatus[] listing = fs.listStatus(new Path(targetDirectoryName));\r\n    assertEquals(1, listing.length);\r\n    assertEquals(fileName, listing[0].getPath().getName());\r\n    FileStatus status = fs.getFileStatus(new Path(targetDirectoryName, fileName));\r\n    assertEquals(fileName, status.getPath().getName());\r\n    assertTrue(fs.delete(new Path(targetDirectoryName, fileName), true));\r\n    assertTrue(fs.delete(new Path(targetDirectoryName), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testReadingDirectoryAsFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReadingDirectoryAsFile() throws Exception\n{\r\n    Path dir = methodPath();\r\n    assertTrue(fs.mkdirs(dir));\r\n    try {\r\n        fs.open(dir).close();\r\n        assertTrue(\"Should've thrown\", false);\r\n    } catch (FileNotFoundException ex) {\r\n        assertExceptionContains(\"a directory not a file.\", ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreatingFileOverDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCreatingFileOverDirectory() throws Exception\n{\r\n    Path dir = methodPath();\r\n    assertTrue(fs.mkdirs(dir));\r\n    try {\r\n        fs.create(dir).close();\r\n        assertTrue(\"Should've thrown\", false);\r\n    } catch (IOException ex) {\r\n        assertExceptionContains(\"Cannot create file\", ex);\r\n        assertExceptionContains(\"already exists as a directory\", ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInputStreamReadWithZeroSizeBuffer",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testInputStreamReadWithZeroSizeBuffer() throws Exception\n{\r\n    Path newFile = methodPath();\r\n    OutputStream output = fs.create(newFile);\r\n    output.write(10);\r\n    output.close();\r\n    InputStream input = fs.open(newFile);\r\n    int result = input.read(new byte[2], 0, 0);\r\n    assertEquals(0, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInputStreamReadWithBufferReturnsMinusOneOnEof",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testInputStreamReadWithBufferReturnsMinusOneOnEof() throws Exception\n{\r\n    Path newFile = methodPath();\r\n    OutputStream output = fs.create(newFile);\r\n    output.write(10);\r\n    output.close();\r\n    InputStream input = fs.open(newFile);\r\n    byte[] buff = new byte[1];\r\n    int result = input.read(buff, 0, 1);\r\n    assertEquals(1, result);\r\n    assertEquals(10, buff[0]);\r\n    buff[0] = 2;\r\n    result = input.read(buff, 0, 1);\r\n    assertEquals(-1, result);\r\n    assertEquals(2, buff[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInputStreamReadWithBufferReturnsMinusOneOnEofForLargeBuffer",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testInputStreamReadWithBufferReturnsMinusOneOnEofForLargeBuffer() throws Exception\n{\r\n    Path newFile = methodPath();\r\n    OutputStream output = fs.create(newFile);\r\n    byte[] outputBuff = new byte[97331];\r\n    for (int i = 0; i < outputBuff.length; ++i) {\r\n        outputBuff[i] = (byte) (Math.random() * 255);\r\n    }\r\n    output.write(outputBuff);\r\n    output.close();\r\n    InputStream input = fs.open(newFile);\r\n    byte[] buff = new byte[131072];\r\n    int result = input.read(buff, 0, buff.length);\r\n    assertEquals(outputBuff.length, result);\r\n    for (int i = 0; i < outputBuff.length; ++i) {\r\n        assertEquals(outputBuff[i], buff[i]);\r\n    }\r\n    buff = new byte[131072];\r\n    result = input.read(buff, 0, buff.length);\r\n    assertEquals(-1, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInputStreamReadIntReturnsMinusOneOnEof",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testInputStreamReadIntReturnsMinusOneOnEof() throws Exception\n{\r\n    Path newFile = methodPath();\r\n    OutputStream output = fs.create(newFile);\r\n    output.write(10);\r\n    output.close();\r\n    InputStream input = fs.open(newFile);\r\n    int value = input.read();\r\n    assertEquals(10, value);\r\n    value = input.read();\r\n    assertEquals(-1, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionOnFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetPermissionOnFile() throws Exception\n{\r\n    Path newFile = methodPath();\r\n    OutputStream output = fs.create(newFile);\r\n    output.write(13);\r\n    output.close();\r\n    FsPermission newPermission = new FsPermission((short) 0700);\r\n    fs.setPermission(newFile, newPermission);\r\n    FileStatus newStatus = fs.getFileStatus(newFile);\r\n    assertNotNull(newStatus);\r\n    assertEquals(newPermission, newStatus.getPermission());\r\n    assertEquals(\"supergroup\", newStatus.getGroup());\r\n    assertEquals(UserGroupInformation.getCurrentUser().getShortUserName(), newStatus.getOwner());\r\n    if (!(this instanceof ITestNativeAzureFSPageBlobLive)) {\r\n        assertEquals(1, newStatus.getLen());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionOnFolder",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSetPermissionOnFolder() throws Exception\n{\r\n    Path newFolder = methodPath();\r\n    assertTrue(fs.mkdirs(newFolder));\r\n    FsPermission newPermission = new FsPermission((short) 0600);\r\n    fs.setPermission(newFolder, newPermission);\r\n    FileStatus newStatus = fs.getFileStatus(newFolder);\r\n    assertNotNull(newStatus);\r\n    assertEquals(newPermission, newStatus.getPermission());\r\n    assertTrue(newStatus.isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetOwnerOnFile",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testSetOwnerOnFile() throws Exception\n{\r\n    Path newFile = methodPath();\r\n    OutputStream output = fs.create(newFile);\r\n    output.write(13);\r\n    output.close();\r\n    fs.setOwner(newFile, \"newUser\", null);\r\n    FileStatus newStatus = fs.getFileStatus(newFile);\r\n    assertNotNull(newStatus);\r\n    assertEquals(\"newUser\", newStatus.getOwner());\r\n    assertEquals(\"supergroup\", newStatus.getGroup());\r\n    if (!(this instanceof ITestNativeAzureFSPageBlobLive)) {\r\n        assertEquals(1, newStatus.getLen());\r\n    }\r\n    fs.setOwner(newFile, null, \"newGroup\");\r\n    newStatus = fs.getFileStatus(newFile);\r\n    assertNotNull(newStatus);\r\n    assertEquals(\"newUser\", newStatus.getOwner());\r\n    assertEquals(\"newGroup\", newStatus.getGroup());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetOwnerOnFolder",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSetOwnerOnFolder() throws Exception\n{\r\n    Path newFolder = methodPath();\r\n    assertTrue(fs.mkdirs(newFolder));\r\n    fs.setOwner(newFolder, \"newUser\", null);\r\n    FileStatus newStatus = fs.getFileStatus(newFolder);\r\n    assertNotNull(newStatus);\r\n    assertEquals(\"newUser\", newStatus.getOwner());\r\n    assertTrue(newStatus.isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testModifiedTimeForFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testModifiedTimeForFile() throws Exception\n{\r\n    Path testFile = methodPath();\r\n    fs.create(testFile).close();\r\n    testModifiedTime(testFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testModifiedTimeForFolder",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testModifiedTimeForFolder() throws Exception\n{\r\n    Path testFolder = methodPath();\r\n    assertTrue(fs.mkdirs(testFolder));\r\n    testModifiedTime(testFolder);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFolderLastModifiedTime",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testFolderLastModifiedTime() throws Exception\n{\r\n    Path parentFolder = methodPath();\r\n    Path innerFile = new Path(parentFolder, \"innerfile\");\r\n    assertTrue(fs.mkdirs(parentFolder));\r\n    long lastModifiedTime = fs.getFileStatus(parentFolder).getModificationTime();\r\n    Thread.sleep(modifiedTimeErrorMargin + 1);\r\n    assertTrue(fs.createNewFile(innerFile));\r\n    assertFalse(testModifiedTime(parentFolder, lastModifiedTime));\r\n    testModifiedTime(parentFolder);\r\n    lastModifiedTime = fs.getFileStatus(parentFolder).getModificationTime();\r\n    Path destFolder = new Path(\"testDestFolder\");\r\n    assertTrue(fs.mkdirs(destFolder));\r\n    long destLastModifiedTime = fs.getFileStatus(destFolder).getModificationTime();\r\n    Thread.sleep(modifiedTimeErrorMargin + 1);\r\n    Path destFile = new Path(destFolder, \"innerfile\");\r\n    assertTrue(fs.rename(innerFile, destFile));\r\n    assertFalse(testModifiedTime(parentFolder, lastModifiedTime));\r\n    assertFalse(testModifiedTime(destFolder, destLastModifiedTime));\r\n    testModifiedTime(parentFolder);\r\n    testModifiedTime(destFolder);\r\n    destLastModifiedTime = fs.getFileStatus(destFolder).getModificationTime();\r\n    Thread.sleep(modifiedTimeErrorMargin + 1);\r\n    fs.delete(destFile, false);\r\n    assertFalse(testModifiedTime(destFolder, destLastModifiedTime));\r\n    testModifiedTime(destFolder);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testListSlash",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testListSlash() throws Exception\n{\r\n    Path testFolder = new Path(\"/testFolder\");\r\n    Path testFile = new Path(testFolder, \"testFile\");\r\n    assertTrue(fs.mkdirs(testFolder));\r\n    assertTrue(fs.createNewFile(testFile));\r\n    FileStatus status;\r\n    status = fs.getFileStatus(new Path(\"/testFolder\"));\r\n    assertTrue(status.isDirectory());\r\n    status = fs.getFileStatus(new Path(\"/testFolder/\"));\r\n    assertTrue(status.isDirectory());\r\n    status = fs.getFileStatus(new Path(\"/testFolder/.\"));\r\n    assertTrue(status.isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCannotCreatePageBlobByDefault",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCannotCreatePageBlobByDefault() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String[] rawPageBlobDirs = conf.getStrings(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES);\r\n    assertTrue(rawPageBlobDirs == null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRedoRenameFolder",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testRedoRenameFolder() throws IOException\n{\r\n    String srcKey = \"folderToRename\";\r\n    Path originalFolder = new Path(srcKey);\r\n    assertTrue(fs.mkdirs(originalFolder));\r\n    Path innerFile = new Path(originalFolder, \"innerFile\");\r\n    assertTrue(fs.createNewFile(innerFile));\r\n    Path innerFile2 = new Path(originalFolder, \"innerFile2\");\r\n    assertTrue(fs.createNewFile(innerFile2));\r\n    String dstKey = \"renamedFolder\";\r\n    Path home = fs.getHomeDirectory();\r\n    String relativeHomeDir = getRelativePath(home.toString());\r\n    NativeAzureFileSystem.FolderRenamePending pending = new NativeAzureFileSystem.FolderRenamePending(relativeHomeDir + \"/\" + srcKey, relativeHomeDir + \"/\" + dstKey, null, (NativeAzureFileSystem) fs);\r\n    String renameDescription = pending.makeRenamePendingFileContents();\r\n    assertTrue(fs.delete(innerFile, false));\r\n    Path destination = new Path(dstKey);\r\n    Path innerDest = new Path(destination, \"innerFile\");\r\n    assertTrue(fs.createNewFile(innerDest));\r\n    final String renamePendingStr = \"folderToRename-RenamePending.json\";\r\n    Path renamePendingFile = new Path(renamePendingStr);\r\n    FSDataOutputStream out = fs.create(renamePendingFile, true);\r\n    assertTrue(out != null);\r\n    writeStringToStream(out, renameDescription);\r\n    assertFalse(fs.exists(originalFolder));\r\n    assertTrue(fs.exists(destination));\r\n    assertTrue(fs.exists(new Path(destination, innerFile.getName())));\r\n    assertTrue(fs.exists(new Path(destination, innerFile2.getName())));\r\n    assertFalse(fs.exists(originalFolder));\r\n    assertFalse(fs.exists(innerFile));\r\n    assertFalse(fs.exists(innerFile2));\r\n    assertFalse(fs.exists(renamePendingFile));\r\n    FileStatus[] listed = fs.listStatus(destination);\r\n    assertEquals(2, listed.length);\r\n    Path root = fs.getHomeDirectory();\r\n    listed = fs.listStatus(root);\r\n    assertEquals(1, listed.length);\r\n    assertTrue(listed[0].isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRedoRenameFolderInFolderListing",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testRedoRenameFolderInFolderListing() throws IOException\n{\r\n    String parent = \"parent\";\r\n    Path parentFolder = new Path(parent);\r\n    assertTrue(fs.mkdirs(parentFolder));\r\n    Path inner = new Path(parentFolder, \"innerFolder\");\r\n    assertTrue(fs.mkdirs(inner));\r\n    Path inner2 = new Path(parentFolder, \"innerFolder2\");\r\n    assertTrue(fs.mkdirs(inner2));\r\n    Path innerFile = new Path(inner2, \"file\");\r\n    assertTrue(fs.createNewFile(innerFile));\r\n    Path inner2renamed = new Path(parentFolder, \"innerFolder2Renamed\");\r\n    Path home = fs.getHomeDirectory();\r\n    String relativeHomeDir = getRelativePath(home.toString());\r\n    NativeAzureFileSystem.FolderRenamePending pending = new NativeAzureFileSystem.FolderRenamePending(relativeHomeDir + \"/\" + inner2, relativeHomeDir + \"/\" + inner2renamed, null, (NativeAzureFileSystem) fs);\r\n    final String renamePendingStr = inner2 + FolderRenamePending.SUFFIX;\r\n    Path renamePendingFile = new Path(renamePendingStr);\r\n    FSDataOutputStream out = fs.create(renamePendingFile, true);\r\n    assertTrue(out != null);\r\n    writeStringToStream(out, pending.makeRenamePendingFileContents());\r\n    FileStatus[] listed = fs.listStatus(parentFolder);\r\n    assertEquals(2, listed.length);\r\n    assertTrue(listed[0].isDirectory());\r\n    assertTrue(listed[1].isDirectory());\r\n    assertFalse(fs.exists(inner2));\r\n    assertTrue(fs.exists(inner2renamed));\r\n    assertTrue(fs.exists(new Path(inner2renamed, \"file\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRedoRenameFolderRenameInProgress",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testRedoRenameFolderRenameInProgress() throws IOException\n{\r\n    String parent = \"parent\";\r\n    Path parentFolder = new Path(parent);\r\n    assertTrue(fs.mkdirs(parentFolder));\r\n    Path folderToBeRenamed = new Path(parentFolder, \"folderToBeRenamed\");\r\n    assertTrue(fs.mkdirs(folderToBeRenamed));\r\n    String innerFolderName = \"innerFolder\";\r\n    Path inner = new Path(folderToBeRenamed, innerFolderName);\r\n    assertTrue(fs.mkdirs(inner));\r\n    String innerFileName = \"file\";\r\n    Path innerFile = new Path(inner, innerFileName);\r\n    assertTrue(fs.createNewFile(innerFile));\r\n    Path renamedFolder = new Path(parentFolder, \"renamedFolder\");\r\n    Path home = fs.getHomeDirectory();\r\n    String relativeHomeDir = getRelativePath(home.toString());\r\n    NativeAzureFileSystem.FolderRenamePending pending = new NativeAzureFileSystem.FolderRenamePending(relativeHomeDir + \"/\" + folderToBeRenamed, relativeHomeDir + \"/\" + renamedFolder, null, (NativeAzureFileSystem) fs);\r\n    final String renamePendingStr = folderToBeRenamed + FolderRenamePending.SUFFIX;\r\n    Path renamePendingFile = new Path(renamePendingStr);\r\n    FSDataOutputStream out = fs.create(renamePendingFile, true);\r\n    assertTrue(out != null);\r\n    writeStringToStream(out, pending.makeRenamePendingFileContents());\r\n    ((NativeAzureFileSystem) fs).getStoreInterface().rename(relativeHomeDir + \"/\" + inner, relativeHomeDir + \"/\" + renamedFolder + \"/\" + innerFolderName, true, null);\r\n    assertFalse(((NativeAzureFileSystem) fs).getStoreInterface().explicitFileExists(relativeHomeDir + \"/\" + inner));\r\n    assertTrue(((NativeAzureFileSystem) fs).getStoreInterface().explicitFileExists(relativeHomeDir + \"/\" + innerFile));\r\n    FileStatus[] listed = fs.listStatus(parentFolder);\r\n    assertEquals(1, listed.length);\r\n    assertTrue(listed[0].isDirectory());\r\n    assertFalse(fs.exists(inner));\r\n    assertFalse(fs.exists(innerFile));\r\n    assertTrue(fs.exists(renamedFolder));\r\n    assertTrue(fs.exists(new Path(renamedFolder, innerFolderName + \"/\" + innerFileName)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRedoRenameFolderInFolderListingWithZeroByteRenameMetadata",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testRedoRenameFolderInFolderListingWithZeroByteRenameMetadata() throws IOException\n{\r\n    String parent = \"parent\";\r\n    Path parentFolder = new Path(parent);\r\n    assertTrue(fs.mkdirs(parentFolder));\r\n    Path inner = new Path(parentFolder, \"innerFolder\");\r\n    assertTrue(fs.mkdirs(inner));\r\n    Path inner2 = new Path(parentFolder, \"innerFolder2\");\r\n    assertTrue(fs.mkdirs(inner2));\r\n    Path innerFile = new Path(inner2, \"file\");\r\n    assertTrue(fs.createNewFile(innerFile));\r\n    Path inner2renamed = new Path(parentFolder, \"innerFolder2Renamed\");\r\n    final String renamePendingStr = inner2 + FolderRenamePending.SUFFIX;\r\n    Path renamePendingFile = new Path(renamePendingStr);\r\n    FSDataOutputStream out = fs.create(renamePendingFile, true);\r\n    assertTrue(out != null);\r\n    out.close();\r\n    FileStatus[] listed = fs.listStatus(parentFolder);\r\n    assertEquals(2, listed.length);\r\n    assertTrue(listed[0].isDirectory());\r\n    assertTrue(listed[1].isDirectory());\r\n    assertFalse(fs.exists(renamePendingFile));\r\n    Path home = fs.getHomeDirectory();\r\n    String relativeHomeDir = getRelativePath(home.toString());\r\n    NativeAzureFileSystem.FolderRenamePending pending = new NativeAzureFileSystem.FolderRenamePending(relativeHomeDir + \"/\" + inner2, relativeHomeDir + \"/\" + inner2renamed, null, (NativeAzureFileSystem) fs);\r\n    pending.deleteRenamePendingFile(fs, renamePendingFile);\r\n    assertTrue(fs.exists(inner2));\r\n    assertFalse(fs.exists(inner2renamed));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameRedoFolderAlreadyDone",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameRedoFolderAlreadyDone() throws IOException\n{\r\n    String orig = \"originalFolder\";\r\n    String dest = \"renamedFolder\";\r\n    Path destPath = new Path(dest);\r\n    assertTrue(fs.mkdirs(destPath));\r\n    Path home = fs.getHomeDirectory();\r\n    String relativeHomeDir = getRelativePath(home.toString());\r\n    NativeAzureFileSystem.FolderRenamePending pending = new NativeAzureFileSystem.FolderRenamePending(relativeHomeDir + \"/\" + orig, relativeHomeDir + \"/\" + dest, null, (NativeAzureFileSystem) fs);\r\n    final String renamePendingStr = orig + FolderRenamePending.SUFFIX;\r\n    Path renamePendingFile = new Path(renamePendingStr);\r\n    FSDataOutputStream out = fs.create(renamePendingFile, true);\r\n    assertTrue(out != null);\r\n    writeStringToStream(out, pending.makeRenamePendingFileContents());\r\n    try {\r\n        pending.redo();\r\n    } catch (Exception e) {\r\n        fail();\r\n    }\r\n    FileStatus[] listed = fs.listStatus(new Path(\"/\"));\r\n    assertEquals(\"Pending directory still found\", 1, listed.length);\r\n    assertTrue(listed[0].isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRedoFolderRenameAll",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testRedoFolderRenameAll() throws IllegalArgumentException, IOException\n{\r\n    {\r\n        FileFolder original = new FileFolder(\"folderToRename\");\r\n        original.add(\"innerFile\").add(\"innerFile2\");\r\n        FileFolder partialSrc = original.copy();\r\n        FileFolder partialDst = original.copy();\r\n        partialDst.setName(\"renamedFolder\");\r\n        partialSrc.setPresent(0, false);\r\n        partialDst.setPresent(1, false);\r\n        testRenameRedoFolderSituation(original, partialSrc, partialDst);\r\n    }\r\n    {\r\n        FileFolder original = new FileFolder(\"folderToRename\");\r\n        original.add(\"file1\").add(\"file2\").add(\"file3\");\r\n        FileFolder partialSrc = original.copy();\r\n        FileFolder partialDst = original.copy();\r\n        partialDst.setName(\"renamedFolder\");\r\n        partialSrc.setPresent(1, false);\r\n        partialDst.setPresent(2, false);\r\n        testRenameRedoFolderSituation(original, partialSrc, partialDst);\r\n    }\r\n    {\r\n        final int SIZE = 5;\r\n        assertTrue(SIZE >= 3);\r\n        FileFolder original = new FileFolder(\"folderToRename\");\r\n        for (int i = 0; i < SIZE; i++) {\r\n            original.add(\"file\" + Integer.toString(i));\r\n        }\r\n        FileFolder partialSrc = original.copy();\r\n        FileFolder partialDst = original.copy();\r\n        partialDst.setName(\"renamedFolder\");\r\n        for (int i = 0; i < SIZE; i++) {\r\n            partialSrc.setPresent(i, i >= SIZE / 2);\r\n            partialDst.setPresent(i, i <= SIZE / 2);\r\n        }\r\n        testRenameRedoFolderSituation(original, partialSrc, partialDst);\r\n    }\r\n    {\r\n        FileFolder original = new FileFolder(\"folderToRename\");\r\n        FileFolder nested = new FileFolder(\"nestedFolder\");\r\n        nested.add(\"a\").add(\"b\").add(\"c\");\r\n        original.add(nested).add(\"p\").add(\"q\");\r\n        FileFolder partialSrc = original.copy();\r\n        FileFolder partialDst = original.copy();\r\n        partialDst.setName(\"renamedFolder\");\r\n        partialSrc.getMember(0).setPresent(0, false);\r\n        partialDst.getMember(0).setPresent(1, false);\r\n        partialDst.getMember(0).setPresent(2, false);\r\n        testRenameRedoFolderSituation(original, partialSrc, partialDst);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameRedoFolderSituation",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRenameRedoFolderSituation(FileFolder fullSrc, FileFolder partialSrc, FileFolder partialDst) throws IllegalArgumentException, IOException\n{\r\n    fullSrc.create();\r\n    fullSrc.makeRenamePending(partialDst);\r\n    partialSrc.prune();\r\n    partialDst.create();\r\n    assertFalse(fullSrc.exists());\r\n    partialDst.verifyExists();\r\n    fullSrc.verifyGone();\r\n    fs.delete(new Path(partialDst.getName()), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getRelativePath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getRelativePath(String path)\n{\r\n    int slashCount = 0;\r\n    int i;\r\n    for (i = 0; i < path.length(); i++) {\r\n        if (path.charAt(i) == '/') {\r\n            slashCount++;\r\n            if (slashCount == 3) {\r\n                return path.substring(i + 1, path.length());\r\n            }\r\n        }\r\n    }\r\n    throw new RuntimeException(\"Incorrect path prefix -- expected wasb://.../...\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCloseFileSystemTwice",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCloseFileSystemTwice() throws Exception\n{\r\n    fs.close();\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAvailable",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAvailable() throws IOException\n{\r\n    FSDataOutputStream out = fs.create(PATH);\r\n    byte[] data = new byte[FILE_SIZE];\r\n    Arrays.fill(data, (byte) 5);\r\n    out.write(data, 0, FILE_SIZE);\r\n    out.close();\r\n    verifyAvailable(1);\r\n    verifyAvailable(100);\r\n    verifyAvailable(5000);\r\n    verifyAvailable(FILE_SIZE);\r\n    verifyAvailable(MAX_STRIDE);\r\n    fs.delete(PATH, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyAvailable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyAvailable(int readStride) throws IOException\n{\r\n    FSDataInputStream in = fs.open(PATH);\r\n    try {\r\n        byte[] inputBuffer = new byte[MAX_STRIDE];\r\n        int position = 0;\r\n        int bytesRead = 0;\r\n        while (bytesRead != FILE_SIZE) {\r\n            bytesRead += in.read(inputBuffer, position, readStride);\r\n            int available = in.available();\r\n            if (bytesRead < FILE_SIZE) {\r\n                if (available < 1) {\r\n                    fail(String.format(\"expected available > 0 but got: \" + \"position = %d, bytesRead = %d, in.available() = %d\", position, bytesRead, available));\r\n                }\r\n            }\r\n        }\r\n        int available = in.available();\r\n        assertTrue(available == 0);\r\n    } finally {\r\n        in.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testGetFileSizeFromListing",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetFileSizeFromListing() throws IOException\n{\r\n    Path path = new Path(\"file.dat\");\r\n    final int PAGE_SIZE = 512;\r\n    final int FILE_SIZE = PAGE_SIZE + 1;\r\n    FSDataOutputStream out = fs.create(path);\r\n    byte[] data = new byte[FILE_SIZE];\r\n    Arrays.fill(data, (byte) 5);\r\n    out.write(data, 0, FILE_SIZE);\r\n    out.close();\r\n    FileStatus[] status = fs.listStatus(path);\r\n    assertEquals(1, status.length);\r\n    assertEquals(FILE_SIZE, status[0].getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testModifiedTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean testModifiedTime(Path testPath, long time) throws Exception\n{\r\n    FileStatus fileStatus = fs.getFileStatus(testPath);\r\n    final long errorMargin = modifiedTimeErrorMargin;\r\n    long lastModified = fileStatus.getModificationTime();\r\n    return (lastModified > (time - errorMargin) && lastModified < (time + errorMargin));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreateNonRecursive",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateNonRecursive() throws Exception\n{\r\n    Path testFolder = new Path(\"/testFolder\");\r\n    Path testFile = new Path(testFolder, \"testFile\");\r\n    try {\r\n        fs.createNonRecursive(testFile, true, 1024, (short) 1, 1024, null);\r\n        assertTrue(\"Should've thrown\", false);\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n    fs.mkdirs(testFolder);\r\n    fs.createNonRecursive(testFile, true, 1024, (short) 1, 1024, null).close();\r\n    assertTrue(fs.exists(testFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileEndingInDot",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFileEndingInDot() throws Exception\n{\r\n    Path testFolder = new Path(\"/testFolder.\");\r\n    Path testFile = new Path(testFolder, \"testFile.\");\r\n    assertTrue(fs.mkdirs(testFolder));\r\n    assertTrue(fs.createNewFile(testFile));\r\n    assertTrue(fs.exists(testFile));\r\n    FileStatus[] listed = fs.listStatus(testFolder);\r\n    assertEquals(1, listed.length);\r\n    assertEquals(\"testFile.\", listed[0].getPath().getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testModifiedTime",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testModifiedTime(Path testPath) throws Exception\n{\r\n    Calendar utc = Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"));\r\n    long currentUtcTime = utc.getTime().getTime();\r\n    FileStatus fileStatus = fs.getFileStatus(testPath);\r\n    final long errorMargin = 60 * 1000;\r\n    assertTrue(\"Modification time \" + new Date(fileStatus.getModificationTime()) + \" is not close to now: \" + utc.getTime(), fileStatus.getModificationTime() > (currentUtcTime - errorMargin) && fileStatus.getModificationTime() < (currentUtcTime + errorMargin));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createEmptyFile(Path testFile, FsPermission permission) throws IOException\n{\r\n    FSDataOutputStream outputStream = fs.create(testFile, permission, true, 4096, (short) 1, 1024, null);\r\n    outputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "readString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String readString(Path testFile) throws IOException\n{\r\n    return readStringFromFile(fs, testFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeString(Path path, String value) throws IOException\n{\r\n    writeStringToFile(fs, path, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSelfRenewingLease",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testSelfRenewingLease() throws IllegalArgumentException, IOException, InterruptedException, StorageException\n{\r\n    SelfRenewingLease lease;\r\n    final String FILE_KEY = \"file\";\r\n    fs.create(new Path(FILE_KEY));\r\n    NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;\r\n    String fullKey = nfs.pathToKey(nfs.makeAbsolute(new Path(FILE_KEY)));\r\n    AzureNativeFileSystemStore store = nfs.getStore();\r\n    lease = store.acquireLease(fullKey);\r\n    assertTrue(lease.getLeaseID() != null);\r\n    Thread.sleep(42000);\r\n    lease.free();\r\n    CloudBlob blob = lease.getCloudBlob();\r\n    String differentLeaseID = null;\r\n    try {\r\n        differentLeaseID = blob.acquireLease(15, null);\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n        fail(\"Caught exception trying to directly re-acquire lease from Azure\");\r\n    } finally {\r\n        assertTrue(differentLeaseID != null);\r\n        AccessCondition accessCondition = AccessCondition.generateEmptyCondition();\r\n        accessCondition.setLeaseID(differentLeaseID);\r\n        blob.releaseLease(accessCondition);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSelfRenewingLeaseFileDelete",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSelfRenewingLeaseFileDelete() throws IllegalArgumentException, IOException, InterruptedException, StorageException\n{\r\n    SelfRenewingLease lease;\r\n    final String FILE_KEY = \"file\";\r\n    final Path path = new Path(FILE_KEY);\r\n    fs.create(path);\r\n    NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;\r\n    String fullKey = nfs.pathToKey(nfs.makeAbsolute(path));\r\n    lease = nfs.getStore().acquireLease(fullKey);\r\n    assertTrue(lease.getLeaseID() != null);\r\n    Thread.sleep(42000);\r\n    nfs.getStore().delete(fullKey, lease);\r\n    assertTrue(!fs.exists(path));\r\n    assertTrue(lease.isFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testLeaseAsDistributedLock",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLeaseAsDistributedLock() throws IllegalArgumentException, IOException\n{\r\n    final String LEASE_LOCK_FILE_KEY = \"file\";\r\n    fs.create(new Path(LEASE_LOCK_FILE_KEY));\r\n    NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;\r\n    String fullKey = nfs.pathToKey(nfs.makeAbsolute(new Path(LEASE_LOCK_FILE_KEY)));\r\n    Thread first = new Thread(new LeaseLockAction(\"first-thread\", fullKey));\r\n    first.start();\r\n    Thread second = new Thread(new LeaseLockAction(\"second-thread\", fullKey));\r\n    second.start();\r\n    try {\r\n        first.join();\r\n        second.join();\r\n        assertTrue(firstEndTime < secondStartTime);\r\n    } catch (InterruptedException e) {\r\n        fail(\"Unable to wait for threads to finish\");\r\n        Thread.currentThread().interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.createMock();\r\n    fs = testAccount.getFileSystem();\r\n    backingStore = testAccount.getMockStorage().getBackingStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount.cleanup();\r\n    fs = null;\r\n    backingStore = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getNumTempBlobs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumTempBlobs()\n{\r\n    int count = 0;\r\n    for (String key : backingStore.getKeys()) {\r\n        if (key.contains(NativeAzureFileSystem.AZURE_TEMP_FOLDER)) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRecover",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRecover() throws Exception\n{\r\n    Path danglingFile = new Path(\"/crashedInTheMiddle\");\r\n    FSDataOutputStream stream = fs.create(danglingFile);\r\n    stream.write(new byte[] { 1, 2, 3 });\r\n    stream.flush();\r\n    FileStatus fileStatus = fs.getFileStatus(danglingFile);\r\n    assertNotNull(fileStatus);\r\n    assertEquals(0, fileStatus.getLen());\r\n    assertEquals(1, getNumTempBlobs());\r\n    runFsck(\"-move\");\r\n    fileStatus = fs.getFileStatus(new Path(\"/lost+found\", danglingFile.getName()));\r\n    assertNotNull(fileStatus);\r\n    assertEquals(3, fileStatus.getLen());\r\n    assertEquals(0, getNumTempBlobs());\r\n    assertFalse(fs.exists(danglingFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "runFsck",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void runFsck(String command) throws Exception\n{\r\n    Configuration conf = fs.getConf();\r\n    conf.setInt(NativeAzureFileSystem.AZURE_TEMP_EXPIRY_PROPERTY_NAME, 0);\r\n    WasbFsck fsck = new WasbFsck(conf);\r\n    fsck.setMockFileSystemForTesting(fs);\r\n    fsck.run(new String[] { AzureBlobStorageTestAccount.MOCK_WASB_URI, command });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDelete() throws Exception\n{\r\n    Path danglingFile = new Path(\"/crashedInTheMiddle\");\r\n    FSDataOutputStream stream = fs.create(danglingFile);\r\n    stream.write(new byte[] { 1, 2, 3 });\r\n    stream.flush();\r\n    FileStatus fileStatus = fs.getFileStatus(danglingFile);\r\n    assertNotNull(fileStatus);\r\n    assertEquals(0, fileStatus.getLen());\r\n    assertEquals(1, getNumTempBlobs());\r\n    runFsck(\"-delete\");\r\n    assertEquals(0, getNumTempBlobs());\r\n    assertFalse(fs.exists(danglingFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "sizes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> sizes()\n{\r\n    return Arrays.asList(new Object[][] { { DEFAULT_WRITE_BUFFER_SIZE, DataBlocks.DATA_BLOCKS_BUFFER_DISK }, { HUGE_FILE, DataBlocks.DATA_BLOCKS_BUFFER_DISK }, { DEFAULT_WRITE_BUFFER_SIZE, DataBlocks.DATA_BLOCKS_BUFFER_ARRAY }, { HUGE_FILE, DataBlocks.DATA_BLOCKS_BUFFER_ARRAY }, { DEFAULT_WRITE_BUFFER_SIZE, DataBlocks.DATA_BLOCKS_BYTEBUFFER }, { HUGE_FILE, DataBlocks.DATA_BLOCKS_BYTEBUFFER } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration configuration = getRawConfiguration();\r\n    configuration.unset(DATA_BLOCKS_BUFFER);\r\n    configuration.set(DATA_BLOCKS_BUFFER, blockFactoryName);\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testHugeFileWrite",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testHugeFileWrite() throws IOException\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path filePath = path(getMethodName());\r\n    final byte[] b = new byte[size];\r\n    new Random().nextBytes(b);\r\n    try (FSDataOutputStream out = fs.create(filePath)) {\r\n        out.write(b);\r\n    }\r\n    assertEquals(\"Mismatch in content length of file uploaded\", size, fs.getFileStatus(filePath).getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testLotsOfWrites",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testLotsOfWrites() throws IOException\n{\r\n    assume(\"If the size isn't a multiple of 8M this test would not pass, so \" + \"skip\", size % EIGHT_MB == 0);\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path filePath = path(getMethodName());\r\n    final byte[] b = new byte[size];\r\n    new Random().nextBytes(b);\r\n    try (FSDataOutputStream out = fs.create(filePath)) {\r\n        int offset = 0;\r\n        for (int i = 0; i < size / EIGHT_MB; i++) {\r\n            out.write(b, offset, EIGHT_MB);\r\n            offset += EIGHT_MB;\r\n        }\r\n    }\r\n    assertEquals(\"Mismatch in content length of file uploaded\", size, fs.getFileStatus(filePath).getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "test",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void test() throws IOException\n{\r\n    AbfsConfiguration conf = getConfiguration();\r\n    assumeThat(conf.get(FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT), not(isEmptyOrNullString()));\r\n    assumeThat(conf.get(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT), not(isEmptyOrNullString()));\r\n    assumeThat(conf.get(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID), not(isEmptyOrNullString()));\r\n    assumeThat(conf.get(FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY), not(isEmptyOrNullString()));\r\n    String tenantGuid = conf.getPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT);\r\n    String clientId = conf.getPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID);\r\n    String authEndpoint = getTrimmedPasswordString(conf, FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\r\n    String authority = getTrimmedPasswordString(conf, FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);\r\n    AccessTokenProvider tokenProvider = new MsiTokenProvider(authEndpoint, tenantGuid, clientId, authority);\r\n    AzureADToken token = null;\r\n    token = tokenProvider.getToken();\r\n    assertThat(token.getAccessToken(), not(isEmptyString()));\r\n    assertThat(token.getExpiry().after(new Date()), is(true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTrimmedPasswordString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getTrimmedPasswordString(AbfsConfiguration conf, String key, String defaultValue) throws IOException\n{\r\n    String value = conf.getPasswordString(key);\r\n    if (StringUtils.isBlank(value)) {\r\n        value = defaultValue;\r\n    }\r\n    return value.trim();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AbfsCommitTestHelper.prepareTestConfiguration(binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOnlyOneServerCallIsMadeWhenTheConfIsTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception\n{\r\n    testNumBackendCalls(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMultipleServerCallsAreMadeWhenTheConfIsFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMultipleServerCallsAreMadeWhenTheConfIsFalse() throws Exception\n{\r\n    testNumBackendCalls(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testNumBackendCalls",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testNumBackendCalls(boolean readSmallFilesCompletely) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem(readSmallFilesCompletely);\r\n    for (int i = 1; i <= 4; i++) {\r\n        String fileName = methodName.getMethodName() + i;\r\n        int fileSize = i * ONE_MB;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        int length = ONE_KB;\r\n        try (FSDataInputStream iStream = fs.open(testFilePath)) {\r\n            byte[] buffer = new byte[length];\r\n            Map<String, Long> metricMap = getInstrumentationMap(fs);\r\n            long requestsMadeBeforeTest = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n            iStream.seek(seekPos(SeekTo.END, fileSize, length));\r\n            iStream.read(buffer, 0, length);\r\n            iStream.seek(seekPos(SeekTo.MIDDLE, fileSize, length));\r\n            iStream.read(buffer, 0, length);\r\n            iStream.seek(seekPos(SeekTo.BEGIN, fileSize, length));\r\n            iStream.read(buffer, 0, length);\r\n            metricMap = getInstrumentationMap(fs);\r\n            long requestsMadeAfterTest = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n            if (readSmallFilesCompletely) {\r\n                assertEquals(1, requestsMadeAfterTest - requestsMadeBeforeTest);\r\n            } else {\r\n                assertEquals(3, requestsMadeAfterTest - requestsMadeBeforeTest);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeginingAndReadSmallFileWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeginingAndReadSmallFileWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.BEGIN, 2, 4, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeginingAndReadSmallFileWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeginingAndReadSmallFileWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.BEGIN, 2, 4, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeginingAndReadBigFileWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeginingAndReadBigFileWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.BEGIN, 5, 6, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeginingAndReadBigFileWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeginingAndReadBigFileWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.BEGIN, 5, 6, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToEndAndReadSmallFileWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToEndAndReadSmallFileWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.END, 2, 4, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToEndAndReadSmallFileWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToEndAndReadSmallFileWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.END, 2, 4, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToEndAndReadBigFileWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToEndAndReadBigFileWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.END, 5, 6, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToEndAndReaBigFiledWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToEndAndReaBigFiledWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.END, 5, 6, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToMiddleAndReadSmallFileWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToMiddleAndReadSmallFileWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.MIDDLE, 2, 4, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToMiddleAndReadSmallFileWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToMiddleAndReadSmallFileWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.MIDDLE, 2, 4, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToMiddleAndReaBigFileWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToMiddleAndReaBigFileWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.MIDDLE, 5, 6, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToMiddleAndReadBigFileWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToMiddleAndReadBigFileWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(SeekTo.MIDDLE, 5, 6, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekAndReadWithConf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSeekAndReadWithConf(SeekTo seekTo, int startFileSizeInMB, int endFileSizeInMB, boolean readSmallFilesCompletely) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem(readSmallFilesCompletely);\r\n    for (int i = startFileSizeInMB; i <= endFileSizeInMB; i++) {\r\n        String fileName = methodName.getMethodName() + i;\r\n        int fileSize = i * ONE_MB;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        int length = ONE_KB;\r\n        int seekPos = seekPos(seekTo, fileSize, length);\r\n        seekReadAndTest(fs, testFilePath, seekPos, length, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int seekPos(SeekTo seekTo, int fileSize, int length)\n{\r\n    if (seekTo == SeekTo.BEGIN) {\r\n        return 0;\r\n    }\r\n    if (seekTo == SeekTo.END) {\r\n        return fileSize - length;\r\n    }\r\n    return fileSize / 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekReadAndTest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void seekReadAndTest(FileSystem fs, Path testFilePath, int seekPos, int length, byte[] fileContent) throws IOException, NoSuchFieldException, IllegalAccessException\n{\r\n    AbfsConfiguration conf = getAbfsStore(fs).getAbfsConfiguration();\r\n    try (FSDataInputStream iStream = fs.open(testFilePath)) {\r\n        seek(iStream, seekPos);\r\n        byte[] buffer = new byte[length];\r\n        int bytesRead = iStream.read(buffer, 0, length);\r\n        assertEquals(bytesRead, length);\r\n        assertContentReadCorrectly(fileContent, seekPos, length, buffer, testFilePath);\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        final int readBufferSize = conf.getReadBufferSize();\r\n        final int fileContentLength = fileContent.length;\r\n        final boolean smallFile = fileContentLength <= readBufferSize;\r\n        int expectedLimit, expectedFCursor;\r\n        int expectedBCursor;\r\n        if (conf.readSmallFilesCompletely() && smallFile) {\r\n            assertBuffersAreEqual(fileContent, abfsInputStream.getBuffer(), conf, testFilePath);\r\n            expectedFCursor = fileContentLength;\r\n            expectedLimit = fileContentLength;\r\n            expectedBCursor = seekPos + length;\r\n        } else {\r\n            if ((seekPos == 0)) {\r\n                assertBuffersAreEqual(fileContent, abfsInputStream.getBuffer(), conf, testFilePath);\r\n            } else {\r\n                assertBuffersAreNotEqual(fileContent, abfsInputStream.getBuffer(), conf, testFilePath);\r\n            }\r\n            expectedBCursor = length;\r\n            expectedFCursor = (fileContentLength < (seekPos + readBufferSize)) ? fileContentLength : (seekPos + readBufferSize);\r\n            expectedLimit = (fileContentLength < (seekPos + readBufferSize)) ? (fileContentLength - seekPos) : readBufferSize;\r\n        }\r\n        assertEquals(expectedFCursor, abfsInputStream.getFCursor());\r\n        assertEquals(expectedFCursor, abfsInputStream.getFCursorAfterLastRead());\r\n        assertEquals(expectedBCursor, abfsInputStream.getBCursor());\r\n        assertEquals(expectedLimit, abfsInputStream.getLimit());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPartialReadWithNoData",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPartialReadWithNoData() throws Exception\n{\r\n    for (int i = 2; i <= 4; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(true);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        partialReadWithNoData(fs, testFilePath, fileSize / 2, fileSize / 4, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "partialReadWithNoData",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void partialReadWithNoData(final FileSystem fs, final Path testFilePath, final int seekPos, final int length, final byte[] fileContent) throws IOException\n{\r\n    FSDataInputStream iStream = fs.open(testFilePath);\r\n    try {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        abfsInputStream = spy(abfsInputStream);\r\n        doReturn(10).doReturn(10).doCallRealMethod().when(abfsInputStream).readRemote(anyLong(), any(), anyInt(), anyInt(), any(TracingContext.class));\r\n        iStream = new FSDataInputStream(abfsInputStream);\r\n        seek(iStream, seekPos);\r\n        byte[] buffer = new byte[length];\r\n        int bytesRead = iStream.read(buffer, 0, length);\r\n        assertEquals(bytesRead, length);\r\n        assertContentReadCorrectly(fileContent, seekPos, length, buffer, testFilePath);\r\n        assertEquals(fileContent.length, abfsInputStream.getFCursor());\r\n        assertEquals(fileContent.length, abfsInputStream.getFCursorAfterLastRead());\r\n        assertEquals(length, abfsInputStream.getBCursor());\r\n        assertTrue(abfsInputStream.getLimit() >= length);\r\n    } finally {\r\n        iStream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPartialReadWithSomeData",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPartialReadWithSomeData() throws Exception\n{\r\n    for (int i = 2; i <= 4; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(true);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        partialReadWithSomeData(fs, testFilePath, fileSize / 2, fileSize / 4, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "partialReadWithSomeData",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void partialReadWithSomeData(final FileSystem fs, final Path testFilePath, final int seekPos, final int length, final byte[] fileContent) throws IOException, NoSuchFieldException, IllegalAccessException\n{\r\n    FSDataInputStream iStream = fs.open(testFilePath);\r\n    try {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        abfsInputStream = spy(abfsInputStream);\r\n        int someDataLength = 10;\r\n        int secondReturnSize = seekPos - 10 + someDataLength;\r\n        doReturn(10).doReturn(secondReturnSize).doCallRealMethod().when(abfsInputStream).readRemote(anyLong(), any(), anyInt(), anyInt(), any(TracingContext.class));\r\n        iStream = new FSDataInputStream(abfsInputStream);\r\n        seek(iStream, seekPos);\r\n        byte[] buffer = new byte[length];\r\n        int bytesRead = iStream.read(buffer, 0, length);\r\n        assertEquals(length, bytesRead);\r\n        assertTrue(abfsInputStream.getFCursor() > seekPos + length);\r\n        assertTrue(abfsInputStream.getFCursorAfterLastRead() > seekPos + length);\r\n        assertEquals(length - someDataLength, abfsInputStream.getBCursor());\r\n        assertTrue(abfsInputStream.getLimit() > length - someDataLength);\r\n    } finally {\r\n        iStream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount = AzureTestUtils.cleanupTestAccount(testAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlobMd5StoreOffByDefault",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBlobMd5StoreOffByDefault() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.create();\r\n    testStoreBlobMd5(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testStoreBlobMd5",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testStoreBlobMd5() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(KEY_STORE_BLOB_MD5, true);\r\n    testAccount = AzureBlobStorageTestAccount.create(conf);\r\n    testStoreBlobMd5(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "trim",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String trim(String s, String toTrim)\n{\r\n    return StringUtils.removeEnd(StringUtils.removeStart(s, toTrim), toTrim);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testStoreBlobMd5",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testStoreBlobMd5(boolean expectMd5Stored) throws Exception\n{\r\n    assumeNotNull(testAccount);\r\n    NativeAzureFileSystem fs = testAccount.getFileSystem();\r\n    Path testFilePath = AzureTestUtils.pathForTests(fs, methodName.getMethodName());\r\n    String testFileKey = trim(testFilePath.toUri().getPath(), \"/\");\r\n    OutputStream outStream = fs.create(testFilePath);\r\n    outStream.write(new byte[] { 5, 15 });\r\n    outStream.close();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(testFileKey);\r\n    blob.downloadAttributes();\r\n    String obtainedMd5 = blob.getProperties().getContentMD5();\r\n    if (expectMd5Stored) {\r\n        assertNotNull(obtainedMd5);\r\n    } else {\r\n        assertNull(\"Expected no MD5, found: \" + obtainedMd5, obtainedMd5);\r\n    }\r\n    String newBlockId = Base64.encode(new byte[] { 55, 44, 33, 22 });\r\n    blob.uploadBlock(newBlockId, new ByteArrayInputStream(new byte[] { 6, 45 }), 2);\r\n    blob.commitBlockList(Arrays.asList(new BlockEntry[] { new BlockEntry(newBlockId, BlockSearchMode.UNCOMMITTED) }));\r\n    InputStream inStream = fs.open(testFilePath);\r\n    try {\r\n        byte[] inBuf = new byte[100];\r\n        while (inStream.read(inBuf) > 0) {\r\n        }\r\n        inStream.close();\r\n        if (expectMd5Stored) {\r\n            fail(\"Should've thrown because of data corruption.\");\r\n        }\r\n    } catch (IOException ex) {\r\n        if (!expectMd5Stored) {\r\n            throw ex;\r\n        }\r\n        StorageException cause = (StorageException) ex.getCause();\r\n        assertNotNull(cause);\r\n        assertEquals(\"Unexpected cause: \" + cause, StorageErrorCodeStrings.INVALID_MD5, cause.getErrorCode());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCheckBlockMd5",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCheckBlockMd5() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.create();\r\n    testCheckBlockMd5(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDontCheckBlockMd5",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDontCheckBlockMd5() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(KEY_CHECK_BLOCK_MD5, false);\r\n    testAccount = AzureBlobStorageTestAccount.create(conf);\r\n    testCheckBlockMd5(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCheckBlockMd5",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCheckBlockMd5(final boolean expectMd5Checked) throws Exception\n{\r\n    assumeNotNull(testAccount);\r\n    Path testFilePath = new Path(\"/testFile\");\r\n    testAccount.getFileSystem().getStore().addTestHookToOperationContext(new TestHookOperationContext() {\r\n\r\n        @Override\r\n        public OperationContext modifyOperationContext(OperationContext original) {\r\n            original.getResponseReceivedEventHandler().addListener(new ContentMD5Checker(expectMd5Checked));\r\n            return original;\r\n        }\r\n    });\r\n    OutputStream outStream = testAccount.getFileSystem().create(testFilePath);\r\n    outStream.write(new byte[] { 5, 15 });\r\n    outStream.close();\r\n    InputStream inStream = testAccount.getFileSystem().open(testFilePath);\r\n    byte[] inBuf = new byte[100];\r\n    while (inStream.read(inBuf) > 0) {\r\n    }\r\n    inStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testIntegerConfigValidator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testIntegerConfigValidator() throws Exception\n{\r\n    IntegerConfigurationBasicValidator integerConfigurationValidator = new IntegerConfigurationBasicValidator(MIN_BUFFER_SIZE, MAX_BUFFER_SIZE, DEFAULT_READ_BUFFER_SIZE, FAKE_KEY, false);\r\n    assertEquals(MIN_BUFFER_SIZE, (int) integerConfigurationValidator.validate(\"3072\"));\r\n    assertEquals(DEFAULT_READ_BUFFER_SIZE, (int) integerConfigurationValidator.validate(null));\r\n    assertEquals(MAX_BUFFER_SIZE, (int) integerConfigurationValidator.validate(\"104857600\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testIntegerConfigValidatorThrowsIfMissingValidValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testIntegerConfigValidatorThrowsIfMissingValidValue() throws Exception\n{\r\n    IntegerConfigurationBasicValidator integerConfigurationValidator = new IntegerConfigurationBasicValidator(MIN_BUFFER_SIZE, MAX_BUFFER_SIZE, DEFAULT_READ_BUFFER_SIZE, FAKE_KEY, true);\r\n    integerConfigurationValidator.validate(\"3072\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testIntegerWithOutlierConfigValidator",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testIntegerWithOutlierConfigValidator() throws Exception\n{\r\n    IntegerConfigurationBasicValidator integerConfigurationValidator = new IntegerConfigurationBasicValidator(INFINITE_LEASE_DURATION, MIN_LEASE_DURATION, MAX_LEASE_DURATION, DEFAULT_LEASE_DURATION, FAKE_KEY, false);\r\n    assertEquals(INFINITE_LEASE_DURATION, (int) integerConfigurationValidator.validate(\"-1\"));\r\n    assertEquals(DEFAULT_LEASE_DURATION, (int) integerConfigurationValidator.validate(null));\r\n    assertEquals(MIN_LEASE_DURATION, (int) integerConfigurationValidator.validate(\"15\"));\r\n    assertEquals(MAX_LEASE_DURATION, (int) integerConfigurationValidator.validate(\"60\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testIntegerWithOutlierConfigValidatorThrowsIfMissingValidValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testIntegerWithOutlierConfigValidatorThrowsIfMissingValidValue() throws Exception\n{\r\n    IntegerConfigurationBasicValidator integerConfigurationValidator = new IntegerConfigurationBasicValidator(INFINITE_LEASE_DURATION, MIN_LEASE_DURATION, MAX_LEASE_DURATION, DEFAULT_LEASE_DURATION, FAKE_KEY, true);\r\n    integerConfigurationValidator.validate(\"14\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testLongConfigValidator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLongConfigValidator() throws Exception\n{\r\n    LongConfigurationBasicValidator longConfigurationValidator = new LongConfigurationBasicValidator(MIN_BUFFER_SIZE, MAX_BUFFER_SIZE, DEFAULT_WRITE_BUFFER_SIZE, FAKE_KEY, false);\r\n    assertEquals(DEFAULT_WRITE_BUFFER_SIZE, (long) longConfigurationValidator.validate(null));\r\n    assertEquals(MIN_BUFFER_SIZE, (long) longConfigurationValidator.validate(\"3072\"));\r\n    assertEquals(MAX_BUFFER_SIZE, (long) longConfigurationValidator.validate(\"104857600\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testLongConfigValidatorThrowsIfMissingValidValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLongConfigValidatorThrowsIfMissingValidValue() throws Exception\n{\r\n    LongConfigurationBasicValidator longConfigurationValidator = new LongConfigurationBasicValidator(MIN_BUFFER_SIZE, MAX_BUFFER_SIZE, DEFAULT_READ_BUFFER_SIZE, FAKE_KEY, true);\r\n    longConfigurationValidator.validate(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testBooleanConfigValidator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testBooleanConfigValidator() throws Exception\n{\r\n    BooleanConfigurationBasicValidator booleanConfigurationValidator = new BooleanConfigurationBasicValidator(FAKE_KEY, false, false);\r\n    assertEquals(true, booleanConfigurationValidator.validate(\"true\"));\r\n    assertEquals(false, booleanConfigurationValidator.validate(\"False\"));\r\n    assertEquals(false, booleanConfigurationValidator.validate(null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testBooleanConfigValidatorThrowsIfMissingValidValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBooleanConfigValidatorThrowsIfMissingValidValue() throws Exception\n{\r\n    BooleanConfigurationBasicValidator booleanConfigurationValidator = new BooleanConfigurationBasicValidator(FAKE_KEY, false, true);\r\n    booleanConfigurationValidator.validate(\"almostTrue\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testStringConfigValidator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStringConfigValidator() throws Exception\n{\r\n    StringConfigurationBasicValidator stringConfigurationValidator = new StringConfigurationBasicValidator(FAKE_KEY, \"value\", false);\r\n    assertEquals(\"value\", stringConfigurationValidator.validate(null));\r\n    assertEquals(\"someValue\", stringConfigurationValidator.validate(\"someValue\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testStringConfigValidatorThrowsIfMissingValidValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testStringConfigValidatorThrowsIfMissingValidValue() throws Exception\n{\r\n    StringConfigurationBasicValidator stringConfigurationValidator = new StringConfigurationBasicValidator(FAKE_KEY, \"value\", true);\r\n    stringConfigurationValidator.validate(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testBase64StringConfigValidator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testBase64StringConfigValidator() throws Exception\n{\r\n    String encodedVal = Base64.encode(\"someValue\".getBytes());\r\n    Base64StringConfigurationBasicValidator base64StringConfigurationValidator = new Base64StringConfigurationBasicValidator(FAKE_KEY, \"\", false);\r\n    assertEquals(\"\", base64StringConfigurationValidator.validate(null));\r\n    assertEquals(encodedVal, base64StringConfigurationValidator.validate(encodedVal));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "testBase64StringConfigValidatorThrowsIfMissingValidValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBase64StringConfigValidatorThrowsIfMissingValidValue() throws Exception\n{\r\n    Base64StringConfigurationBasicValidator base64StringConfigurationValidator = new Base64StringConfigurationBasicValidator(FAKE_KEY, \"value\", true);\r\n    base64StringConfigurationValidator.validate(\"some&%Value\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamUploadingBytes",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testAbfsOutputStreamUploadingBytes() throws IOException\n{\r\n    describe(\"Testing bytes uploaded successfully by AbfsOutputSteam\");\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path uploadBytesFilePath = path(getMethodName());\r\n    String testBytesToUpload = \"bytes\";\r\n    try (AbfsOutputStream outForSomeBytes = createAbfsOutputStreamWithFlushEnabled(fs, uploadBytesFilePath)) {\r\n        AbfsOutputStreamStatisticsImpl abfsOutputStreamStatisticsForUploadBytes = getAbfsOutputStreamStatistics(outForSomeBytes);\r\n        assertEquals(\"Mismatch in bytes to upload\", 0, abfsOutputStreamStatisticsForUploadBytes.getBytesToUpload());\r\n        outForSomeBytes.write(testBytesToUpload.getBytes());\r\n        outForSomeBytes.flush();\r\n        abfsOutputStreamStatisticsForUploadBytes = getAbfsOutputStreamStatistics(outForSomeBytes);\r\n        assertEquals(\"Mismatch in bytes to upload\", testBytesToUpload.getBytes().length, abfsOutputStreamStatisticsForUploadBytes.getBytesToUpload());\r\n        assertEquals(\"Mismatch in successful bytes uploaded\", testBytesToUpload.getBytes().length, abfsOutputStreamStatisticsForUploadBytes.getBytesUploadSuccessful());\r\n    }\r\n    try (AbfsOutputStream outForLargeBytes = createAbfsOutputStreamWithFlushEnabled(fs, uploadBytesFilePath)) {\r\n        for (int i = 0; i < OPERATIONS; i++) {\r\n            outForLargeBytes.write(testBytesToUpload.getBytes());\r\n        }\r\n        outForLargeBytes.flush();\r\n        AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = getAbfsOutputStreamStatistics(outForLargeBytes);\r\n        assertEquals(\"Mismatch in bytes to upload\", OPERATIONS * (testBytesToUpload.getBytes().length), abfsOutputStreamStatistics.getBytesToUpload());\r\n        assertEquals(\"Mismatch in successful bytes uploaded\", OPERATIONS * (testBytesToUpload.getBytes().length), abfsOutputStreamStatistics.getBytesUploadSuccessful());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamQueueShrink",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAbfsOutputStreamQueueShrink() throws IOException\n{\r\n    describe(\"Testing queue shrink operations by AbfsOutputStream\");\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path queueShrinkFilePath = path(getMethodName());\r\n    String testQueueShrink = \"testQueue\";\r\n    if (fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(queueShrinkFilePath).toString())) {\r\n        return;\r\n    }\r\n    try (AbfsOutputStream outForOneOp = createAbfsOutputStreamWithFlushEnabled(fs, queueShrinkFilePath)) {\r\n        AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = getAbfsOutputStreamStatistics(outForOneOp);\r\n        assertEquals(\"Mismatch in queue shrunk operations\", 0, abfsOutputStreamStatistics.getQueueShrunkOps());\r\n    }\r\n    try (AbfsOutputStream outForLargeOps = createAbfsOutputStreamWithFlushEnabled(fs, queueShrinkFilePath)) {\r\n        for (int i = 0; i < OPERATIONS; i++) {\r\n            outForLargeOps.write(testQueueShrink.getBytes());\r\n            outForLargeOps.flush();\r\n        }\r\n        AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = getAbfsOutputStreamStatistics(outForLargeOps);\r\n        assertEquals(\"Mismatch in queue shrunk operations\", OPERATIONS - outForLargeOps.getWriteOperationsSize(), abfsOutputStreamStatistics.getQueueShrunkOps());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamWriteBuffer",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAbfsOutputStreamWriteBuffer() throws IOException\n{\r\n    describe(\"Testing write current buffer operations by AbfsOutputStream\");\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path writeBufferFilePath = path(getMethodName());\r\n    String testWriteBuffer = \"Buffer\";\r\n    try (AbfsOutputStream outForOneOp = createAbfsOutputStreamWithFlushEnabled(fs, writeBufferFilePath)) {\r\n        AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = getAbfsOutputStreamStatistics(outForOneOp);\r\n        assertEquals(\"Mismatch in write current buffer operations\", 0, abfsOutputStreamStatistics.getWriteCurrentBufferOperations());\r\n        outForOneOp.write(testWriteBuffer.getBytes());\r\n        outForOneOp.flush();\r\n        abfsOutputStreamStatistics = getAbfsOutputStreamStatistics(outForOneOp);\r\n        assertEquals(\"Mismatch in write current buffer operations\", 1, abfsOutputStreamStatistics.getWriteCurrentBufferOperations());\r\n    }\r\n    try (AbfsOutputStream outForLargeOps = createAbfsOutputStreamWithFlushEnabled(fs, writeBufferFilePath)) {\r\n        for (int i = 0; i < OPERATIONS; i++) {\r\n            outForLargeOps.write(testWriteBuffer.getBytes());\r\n            outForLargeOps.flush();\r\n        }\r\n        AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = getAbfsOutputStreamStatistics(outForLargeOps);\r\n        assertEquals(\"Mismatch in write current buffer operations\", OPERATIONS, abfsOutputStreamStatistics.getWriteCurrentBufferOperations());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamDurationTrackerPutRequest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAbfsOutputStreamDurationTrackerPutRequest() throws IOException\n{\r\n    describe(\"Testing to check if DurationTracker for PUT request is working \" + \"correctly.\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path pathForPutRequest = path(getMethodName());\r\n    try (AbfsOutputStream outputStream = createAbfsOutputStreamWithFlushEnabled(fs, pathForPutRequest)) {\r\n        outputStream.write('a');\r\n        outputStream.hflush();\r\n        IOStatistics ioStatistics = extractStatistics(fs);\r\n        LOG.info(\"AbfsOutputStreamStats info: {}\", ioStatisticsToPrettyString(ioStatistics));\r\n        Assertions.assertThat(lookupMeanStatistic(ioStatistics, AbfsStatistic.HTTP_PUT_REQUEST.getStatName() + StoreStatisticNames.SUFFIX_MEAN).mean()).describedAs(\"Mismatch in timeSpentOnPutRequest DurationTracker\").isGreaterThan(0.0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfsOutputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsOutputStreamStatisticsImpl getAbfsOutputStreamStatistics(AbfsOutputStream out)\n{\r\n    return (AbfsOutputStreamStatisticsImpl) out.getOutputStreamStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testClientCorrelationId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testClientCorrelationId() throws Exception\n{\r\n    checkCorrelationConfigValidation(CLIENT_CORRELATIONID_LIST[0], true);\r\n    checkCorrelationConfigValidation(CLIENT_CORRELATIONID_LIST[1], false);\r\n    checkCorrelationConfigValidation(CLIENT_CORRELATIONID_LIST[2], false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getOctalNotation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getOctalNotation(FsPermission fsPermission)\n{\r\n    Preconditions.checkNotNull(fsPermission, \"fsPermission\");\r\n    return String.format(AbfsHttpConstants.PERMISSION_FORMAT, fsPermission.toOctal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRelativePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRelativePath(final Path path)\n{\r\n    Preconditions.checkNotNull(path, \"path\");\r\n    return path.toUri().getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkCorrelationConfigValidation",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void checkCorrelationConfigValidation(String clientCorrelationId, boolean includeInHeader) throws Exception\n{\r\n    Configuration conf = getRawConfiguration();\r\n    conf.set(FS_AZURE_CLIENT_CORRELATIONID, clientCorrelationId);\r\n    AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n    String correlationID = fs.getClientCorrelationId();\r\n    if (includeInHeader) {\r\n        Assertions.assertThat(correlationID).describedAs(\"Correlation ID should match config when valid\").isEqualTo(clientCorrelationId);\r\n    } else {\r\n        Assertions.assertThat(correlationID).describedAs(\"Invalid ID should be replaced with empty string\").isEqualTo(EMPTY_STRING);\r\n    }\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fs.getFileSystemId(), FSOperationType.TEST_OP, TracingHeaderFormat.ALL_ID_FORMAT, null);\r\n    boolean isNamespaceEnabled = fs.getIsNamespaceEnabled(tracingContext);\r\n    String path = getRelativePath(new Path(\"/testDir\"));\r\n    String permission = isNamespaceEnabled ? getOctalNotation(FsPermission.getDirDefault()) : null;\r\n    String umask = isNamespaceEnabled ? getOctalNotation(FsPermission.getUMask(fs.getConf())) : null;\r\n    AbfsRestOperation op = fs.getAbfsClient().createPath(path, false, true, permission, umask, false, null, tracingContext);\r\n    int statusCode = op.getResult().getStatusCode();\r\n    Assertions.assertThat(statusCode).describedAs(\"Request should not fail\").isEqualTo(HTTP_CREATED);\r\n    String requestHeader = op.getResult().getClientRequestId().replace(\"[\", \"\").replace(\"]\", \"\");\r\n    Assertions.assertThat(requestHeader).describedAs(\"Client Request Header should match TracingContext\").isEqualTo(tracingContext.getHeader());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "runCorrelationTestForAllMethods",
  "errType" : [ "InvocationTargetException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void runCorrelationTestForAllMethods() throws Exception\n{\r\n    Map<AbstractAbfsIntegrationTest, Method> testClasses = new HashMap<>();\r\n    testClasses.put(new ITestAzureBlobFileSystemListStatus(), ITestAzureBlobFileSystemListStatus.class.getMethod(\"testListPath\"));\r\n    testClasses.put(new ITestAbfsReadWriteAndSeek(MIN_BUFFER_SIZE), ITestAbfsReadWriteAndSeek.class.getMethod(\"testReadAheadRequestID\"));\r\n    testClasses.put(new ITestAbfsReadWriteAndSeek(MIN_BUFFER_SIZE), ITestAbfsReadWriteAndSeek.class.getMethod(\"testReadAndWriteWithDifferentBufferSizesAndSeek\"));\r\n    testClasses.put(new ITestAzureBlobFileSystemAppend(), ITestAzureBlobFileSystemAppend.class.getMethod(\"testTracingForAppend\"));\r\n    testClasses.put(new ITestAzureBlobFileSystemFlush(), ITestAzureBlobFileSystemFlush.class.getMethod(\"testTracingHeaderForAppendBlob\"));\r\n    testClasses.put(new ITestAzureBlobFileSystemCreate(), ITestAzureBlobFileSystemCreate.class.getMethod(\"testDefaultCreateOverwriteFileTest\"));\r\n    testClasses.put(new ITestAzureBlobFilesystemAcl(), ITestAzureBlobFilesystemAcl.class.getMethod(\"testDefaultAclRenamedFile\"));\r\n    testClasses.put(new ITestAzureBlobFileSystemDelete(), ITestAzureBlobFileSystemDelete.class.getMethod(\"testDeleteFirstLevelDirectory\"));\r\n    testClasses.put(new ITestAzureBlobFileSystemCreate(), ITestAzureBlobFileSystemCreate.class.getMethod(\"testCreateNonRecursive\"));\r\n    testClasses.put(new ITestAzureBlobFileSystemAttributes(), ITestAzureBlobFileSystemAttributes.class.getMethod(\"testSetGetXAttr\"));\r\n    testClasses.put(new ITestAzureBlobFilesystemAcl(), ITestAzureBlobFilesystemAcl.class.getMethod(\"testEnsureAclOperationWorksForRoot\"));\r\n    for (AbstractAbfsIntegrationTest testClass : testClasses.keySet()) {\r\n        try {\r\n            testClass.setup();\r\n            testClasses.get(testClass).invoke(testClass);\r\n            testClass.teardown();\r\n        } catch (InvocationTargetException e) {\r\n            if (!(e.getCause() instanceof AssumptionViolatedException)) {\r\n                throw new IOException(testClasses.get(testClass).getName() + \" failed tracing context validation test\");\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testExternalOps",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testExternalOps() throws Exception\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.HAS_PATH_CAPABILITY, false, 0));\r\n    fs.getAbfsStore().setNamespaceEnabled(Trilean.UNKNOWN);\r\n    fs.hasPathCapability(new Path(\"/\"), CommonPathCapabilities.FS_ACLS);\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    Assume.assumeTrue(getConfiguration().isCheckAccessEnabled());\r\n    Assume.assumeTrue(getAuthType() == AuthType.OAuth);\r\n    fs.setListenerOperation(FSOperationType.ACCESS);\r\n    fs.getAbfsStore().setNamespaceEnabled(Trilean.TRUE);\r\n    fs.access(new Path(\"/\"), FsAction.READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initialize(final Configuration configuration, final String account) throws IOException\n{\r\n    state = INITED;\r\n    accountName = account;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getAccessToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAccessToken() throws IOException\n{\r\n    return ACCESS_TOKEN;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getExpiryTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Date getExpiryTime()\n{\r\n    return new Date(System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void bind(final URI fsURI, final Configuration conf) throws IOException\n{\r\n    state = BOUND;\r\n    uri = fsURI;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    state = CLOSED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getUserAgentSuffix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUserAgentSuffix()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "enable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void enable(Configuration conf)\n{\r\n    conf.setEnum(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.Custom);\r\n    conf.set(FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME, NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(NativeAzureFileSystem.KEY_AZURE_AUTHORIZATION, \"true\");\r\n    conf.set(RemoteWasbAuthorizerImpl.KEY_REMOTE_AUTH_SERVICE_URLS, \"http://localhost1/,http://localhost2/,http://localhost:8080\");\r\n    return AzureBlobStorageTestAccount.create(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    boolean useSecureMode = fs.getConf().getBoolean(KEY_USE_SECURE_MODE, false);\r\n    boolean useAuthorization = fs.getConf().getBoolean(NativeAzureFileSystem.KEY_AZURE_AUTHORIZATION, false);\r\n    Assume.assumeTrue(\"Test valid when both SecureMode and Authorization are enabled .. skipping\", useSecureMode && useAuthorization);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInvalidStatusCode",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidStatusCode() throws Throwable\n{\r\n    setupExpectations();\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(INVALID_HTTP_STATUS_CODE_999));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInvalidContentType",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testInvalidContentType() throws Throwable\n{\r\n    setupExpectations();\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"text/plain\"));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMissingContentLength",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMissingContentLength() throws Throwable\n{\r\n    setupExpectations();\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testContentLengthExceedsMax",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testContentLengthExceedsMax() throws Throwable\n{\r\n    setupExpectations();\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"2048\"));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInvalidContentLengthValue",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testInvalidContentLengthValue() throws Throwable\n{\r\n    setupExpectations();\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"20abc48\"));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testValidJSONResponse",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testValidJSONResponse() throws Throwable\n{\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    HttpEntity mockHttpEntity = Mockito.mock(HttpEntity.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponse.getEntity()).thenReturn(mockHttpEntity);\r\n    Mockito.when(mockHttpEntity.getContent()).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8))).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8))).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8)));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMalFormedJSONResponse",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMalFormedJSONResponse() throws Throwable\n{\r\n    expectedEx.expect(WasbAuthorizationException.class);\r\n    expectedEx.expectMessage(\"com.fasterxml.jackson.core.JsonParseException: Unexpected end-of-input in FIELD_NAME\");\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    HttpEntity mockHttpEntity = Mockito.mock(HttpEntity.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponse.getEntity()).thenReturn(mockHttpEntity);\r\n    Mockito.when(mockHttpEntity.getContent()).thenReturn(new ByteArrayInputStream(malformedJsonResponse().getBytes(StandardCharsets.UTF_8)));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFailureCodeJSONResponse",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFailureCodeJSONResponse() throws Throwable\n{\r\n    expectedEx.expect(WasbAuthorizationException.class);\r\n    expectedEx.expectMessage(\"Remote authorization service encountered an error Unauthorized\");\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpResponse mockHttpResponse = Mockito.mock(HttpResponse.class);\r\n    HttpEntity mockHttpEntity = Mockito.mock(HttpEntity.class);\r\n    Mockito.when(mockHttpClient.execute(Mockito.<HttpGet>any())).thenReturn(mockHttpResponse);\r\n    Mockito.when(mockHttpResponse.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponse.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponse.getEntity()).thenReturn(mockHttpEntity);\r\n    Mockito.when(mockHttpEntity.getContent()).thenReturn(new ByteArrayInputStream(failureCodeJsonResponse().getBytes(StandardCharsets.UTF_8)));\r\n    performop(mockHttpClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testWhenOneInstanceIsDown",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testWhenOneInstanceIsDown() throws Throwable\n{\r\n    boolean isAuthorizationCachingEnabled = fs.getConf().getBoolean(CachingAuthorizer.KEY_AUTH_SERVICE_CACHING_ENABLE, false);\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpEntity mockHttpEntity = Mockito.mock(HttpEntity.class);\r\n    HttpResponse mockHttpResponseService1 = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpResponseService1.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_INTERNAL_SERVER_ERROR));\r\n    Mockito.when(mockHttpResponseService1.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponseService1.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponseService1.getEntity()).thenReturn(mockHttpEntity);\r\n    HttpResponse mockHttpResponseService2 = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpResponseService2.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_OK));\r\n    Mockito.when(mockHttpResponseService2.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponseService2.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponseService2.getEntity()).thenReturn(mockHttpEntity);\r\n    HttpResponse mockHttpResponseServiceLocal = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpResponseServiceLocal.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_INTERNAL_SERVER_ERROR));\r\n    Mockito.when(mockHttpResponseServiceLocal.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponseServiceLocal.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponseServiceLocal.getEntity()).thenReturn(mockHttpEntity);\r\n    Mockito.when(mockHttpClient.execute(argThat(new HttpGetForService1()))).thenReturn(mockHttpResponseService1);\r\n    Mockito.when(mockHttpClient.execute(argThat(new HttpGetForService2()))).thenReturn(mockHttpResponseService2);\r\n    Mockito.when(mockHttpClient.execute(argThat(new HttpGetForServiceLocal()))).thenReturn(mockHttpResponseServiceLocal);\r\n    Mockito.when(mockHttpEntity.getContent()).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8))).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8))).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8)));\r\n    performop(mockHttpClient);\r\n    int expectedNumberOfInvocations = isAuthorizationCachingEnabled ? 2 : 3;\r\n    Mockito.verify(mockHttpClient, times(expectedNumberOfInvocations)).execute(Mockito.argThat(new HttpGetForServiceLocal()));\r\n    Mockito.verify(mockHttpClient, times(expectedNumberOfInvocations)).execute(Mockito.argThat(new HttpGetForService2()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testWhenServiceInstancesAreDown",
  "errType" : [ "WasbAuthorizationException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testWhenServiceInstancesAreDown() throws Throwable\n{\r\n    HttpClient mockHttpClient = Mockito.mock(HttpClient.class);\r\n    HttpEntity mockHttpEntity = Mockito.mock(HttpEntity.class);\r\n    HttpResponse mockHttpResponseService1 = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpResponseService1.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_INTERNAL_SERVER_ERROR));\r\n    Mockito.when(mockHttpResponseService1.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponseService1.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponseService1.getEntity()).thenReturn(mockHttpEntity);\r\n    HttpResponse mockHttpResponseService2 = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpResponseService2.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_INTERNAL_SERVER_ERROR));\r\n    Mockito.when(mockHttpResponseService2.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponseService2.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponseService2.getEntity()).thenReturn(mockHttpEntity);\r\n    HttpResponse mockHttpResponseService3 = Mockito.mock(HttpResponse.class);\r\n    Mockito.when(mockHttpResponseService3.getStatusLine()).thenReturn(newStatusLine(HttpStatus.SC_INTERNAL_SERVER_ERROR));\r\n    Mockito.when(mockHttpResponseService3.getFirstHeader(\"Content-Type\")).thenReturn(newHeader(\"Content-Type\", \"application/json\"));\r\n    Mockito.when(mockHttpResponseService3.getFirstHeader(\"Content-Length\")).thenReturn(newHeader(\"Content-Length\", \"1024\"));\r\n    Mockito.when(mockHttpResponseService3.getEntity()).thenReturn(mockHttpEntity);\r\n    Mockito.when(mockHttpClient.execute(argThat(new HttpGetForService1()))).thenReturn(mockHttpResponseService1);\r\n    Mockito.when(mockHttpClient.execute(argThat(new HttpGetForService2()))).thenReturn(mockHttpResponseService2);\r\n    Mockito.when(mockHttpClient.execute(argThat(new HttpGetForServiceLocal()))).thenReturn(mockHttpResponseService3);\r\n    Mockito.when(mockHttpEntity.getContent()).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8))).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8))).thenReturn(new ByteArrayInputStream(validJsonResponse().getBytes(StandardCharsets.UTF_8)));\r\n    try {\r\n        performop(mockHttpClient);\r\n    } catch (WasbAuthorizationException e) {\r\n        e.printStackTrace();\r\n        Mockito.verify(mockHttpClient, atLeast(2)).execute(argThat(new HttpGetForService1()));\r\n        Mockito.verify(mockHttpClient, atLeast(2)).execute(argThat(new HttpGetForService2()));\r\n        Mockito.verify(mockHttpClient, atLeast(3)).execute(argThat(new HttpGetForServiceLocal()));\r\n        Mockito.verify(mockHttpClient, times(7)).execute(Mockito.<HttpGet>any());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setupExpectations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setupExpectations()\n{\r\n    expectedEx.expect(WasbAuthorizationException.class);\r\n    class MatchesPattern extends TypeSafeMatcher<String> {\r\n\r\n        private String pattern;\r\n\r\n        MatchesPattern(String pattern) {\r\n            this.pattern = pattern;\r\n        }\r\n\r\n        @Override\r\n        protected boolean matchesSafely(String item) {\r\n            return item.matches(pattern);\r\n        }\r\n\r\n        @Override\r\n        public void describeTo(Description description) {\r\n            description.appendText(\"matches pattern \").appendValue(pattern);\r\n        }\r\n\r\n        @Override\r\n        protected void describeMismatchSafely(String item, Description mismatchDescription) {\r\n            mismatchDescription.appendText(\"does not match\");\r\n        }\r\n    }\r\n    expectedEx.expectMessage(new MatchesPattern(\"org\\\\.apache\\\\.hadoop\\\\.fs\\\\.azure\\\\.WasbRemoteCallException: \" + \"Encountered error while making remote call to \" + \"http:\\\\/\\\\/localhost1\\\\/,http:\\\\/\\\\/localhost2\\\\/,http:\\\\/\\\\/localhost:8080 retried 6 time\\\\(s\\\\)\\\\.\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "performop",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void performop(HttpClient mockHttpClient) throws Throwable\n{\r\n    Path testPath = new Path(\"/\", \"test.dat\");\r\n    RemoteWasbAuthorizerImpl authorizer = new RemoteWasbAuthorizerImpl();\r\n    authorizer.init(fs.getConf());\r\n    WasbRemoteCallHelper mockWasbRemoteCallHelper = new WasbRemoteCallHelper(RetryUtils.getMultipleLinearRandomRetry(new Configuration(), EMPTY_STRING, true, EMPTY_STRING, \"1000,3,10000,2\"));\r\n    mockWasbRemoteCallHelper.updateHttpClient(mockHttpClient);\r\n    authorizer.updateWasbRemoteCallHelper(mockWasbRemoteCallHelper);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    fs.create(testPath);\r\n    ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n    fs.delete(testPath, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validJsonResponse",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String validJsonResponse()\n{\r\n    return \"{\" + \"\\\"responseCode\\\": 0,\" + \"\\\"authorizationResult\\\": true,\" + \"\\\"responseMessage\\\": \\\"Authorized\\\"\" + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "malformedJsonResponse",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String malformedJsonResponse()\n{\r\n    return \"{\" + \"\\\"responseCode\\\": 0,\" + \"\\\"authorizationResult\\\": true,\" + \"\\\"responseMessage\\\":\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "failureCodeJsonResponse",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String failureCodeJsonResponse()\n{\r\n    return \"{\" + \"\\\"responseCode\\\": 1,\" + \"\\\"authorizationResult\\\": false,\" + \"\\\"responseMessage\\\": \\\"Unauthorized\\\"\" + \"}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "newStatusLine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StatusLine newStatusLine(int statusCode)\n{\r\n    return new StatusLine() {\r\n\r\n        @Override\r\n        public ProtocolVersion getProtocolVersion() {\r\n            return new ProtocolVersion(\"HTTP\", 1, 1);\r\n        }\r\n\r\n        @Override\r\n        public int getStatusCode() {\r\n            return statusCode;\r\n        }\r\n\r\n        @Override\r\n        public String getReasonPhrase() {\r\n            return \"Reason Phrase\";\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "newHeader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Header newHeader(String name, String value)\n{\r\n    return new Header() {\r\n\r\n        @Override\r\n        public String getName() {\r\n            return name;\r\n        }\r\n\r\n        @Override\r\n        public String getValue() {\r\n            return value;\r\n        }\r\n\r\n        @Override\r\n        public HeaderElement[] getElements() throws ParseException {\r\n            return new HeaderElement[0];\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "checkHttpGetMatchHost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean checkHttpGetMatchHost(HttpGet g, String h)\n{\r\n    return g != null && g.getURI().getHost().equals(h);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount = AzureTestUtils.cleanupTestAccount(testAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMode()\n{\r\n    runningInSASMode = AzureBlobStorageTestAccount.createTestConfiguration().getBoolean(AzureNativeFileSystemStore.KEY_USE_SECURE_MODE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateIOStreams",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean validateIOStreams(Path filePath) throws IOException\n{\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    return validateIOStreams(fs, filePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateIOStreams",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean validateIOStreams(FileSystem fs, Path filePath) throws IOException\n{\r\n    OutputStream outputStream = fs.create(filePath);\r\n    outputStream.write(new byte[FILE_SIZE]);\r\n    outputStream.close();\r\n    return (FILE_SIZE == readInputStream(fs, filePath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "readInputStream",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int readInputStream(Path filePath) throws IOException\n{\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    return readInputStream(fs, filePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "readInputStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int readInputStream(FileSystem fs, Path filePath) throws IOException\n{\r\n    InputStream inputStream = fs.open(filePath);\r\n    int count = 0;\r\n    while (inputStream.read() >= 0) {\r\n        count++;\r\n    }\r\n    inputStream.close();\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectUsingKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testConnectUsingKey() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.create();\r\n    assumeNotNull(testAccount);\r\n    assertTrue(validateIOStreams(new Path(\"/wasb_scheme\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectUsingSAS",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testConnectUsingSAS() throws Exception\n{\r\n    Assume.assumeFalse(runningInSASMode);\r\n    testAccount = AzureBlobStorageTestAccount.create(\"\", EnumSet.of(CreateOptions.UseSas, CreateOptions.CreateContainer));\r\n    assumeNotNull(testAccount);\r\n    assertFalse(testAccount.getFileSystem().exists(new Path(\"/IDontExist\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectUsingSASReadonly",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testConnectUsingSASReadonly() throws Exception\n{\r\n    Assume.assumeFalse(runningInSASMode);\r\n    testAccount = AzureBlobStorageTestAccount.create(\"\", EnumSet.of(CreateOptions.UseSas, CreateOptions.CreateContainer, CreateOptions.Readonly));\r\n    assumeNotNull(testAccount);\r\n    final String blobKey = \"blobForReadonly\";\r\n    CloudBlobContainer container = testAccount.getRealContainer();\r\n    CloudBlockBlob blob = container.getBlockBlobReference(blobKey);\r\n    ByteArrayInputStream inputStream = new ByteArrayInputStream(new byte[] { 1, 2, 3 });\r\n    blob.upload(inputStream, 3);\r\n    inputStream.close();\r\n    Path filePath = new Path(\"/\" + blobKey);\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    assertTrue(fs.exists(filePath));\r\n    byte[] obtained = new byte[3];\r\n    DataInputStream obtainedInputStream = fs.open(filePath);\r\n    obtainedInputStream.readFully(obtained);\r\n    obtainedInputStream.close();\r\n    assertEquals(3, obtained[2]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectUsingSecureSAS",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testConnectUsingSecureSAS() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(AzureNativeFileSystemStore.KEY_USE_SECURE_MODE, true);\r\n    testAccount = AzureBlobStorageTestAccount.create(\"\", EnumSet.of(CreateOptions.UseSas), conf);\r\n    assumeNotNull(testAccount);\r\n    NativeAzureFileSystem fs = testAccount.getFileSystem();\r\n    AzureException ex = intercept(AzureException.class, SR.ENUMERATION_ERROR, () -> ContractTestUtils.writeTextFile(fs, new Path(\"/testConnectUsingSecureSAS\"), \"testConnectUsingSecureSAS\", true));\r\n    StorageException cause = getCause(StorageException.class, getCause(NoSuchElementException.class, ex));\r\n    GenericTestUtils.assertExceptionContains(\"The specified container does not exist\", cause);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCause",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "E getCause(Class<E> clazz, Throwable t)\n{\r\n    Throwable e = t.getCause();\r\n    if (e == null) {\r\n        throw new AssertionError(\"No cause\", t);\r\n    }\r\n    if (!clazz.isAssignableFrom(e.getClass())) {\r\n        throw new AssertionError(\"Wrong inner class\", e);\r\n    } else {\r\n        return (E) e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectUsingAnonymous",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testConnectUsingAnonymous() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.createAnonymous(\"testWasb.txt\", FILE_SIZE);\r\n    assumeNotNull(testAccount);\r\n    assertEquals(FILE_SIZE, readInputStream(new Path(\"/testWasb.txt\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectToEmulator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testConnectToEmulator() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.createForEmulator();\r\n    assumeNotNull(testAccount);\r\n    assertTrue(validateIOStreams(new Path(\"/testFile\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectToFullyQualifiedAccountMock",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testConnectToFullyQualifiedAccountMock() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    AzureBlobStorageTestAccount.setMockAccountKey(conf, \"mockAccount.mock.authority.net\");\r\n    AzureNativeFileSystemStore store = new AzureNativeFileSystemStore();\r\n    MockStorageInterface mockStorage = new MockStorageInterface();\r\n    store.setAzureStorageInteractionLayer(mockStorage);\r\n    NativeAzureFileSystem fs = new NativeAzureFileSystem(store);\r\n    fs.initialize(new URI(\"wasb://mockContainer@mockAccount.mock.authority.net\"), conf);\r\n    fs.createNewFile(new Path(\"/x\"));\r\n    assertTrue(mockStorage.getBackingStore().exists(\"http://mockAccount.mock.authority.net/mockContainer/x\"));\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectToRoot",
  "errType" : [ "AzureException", "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testConnectToRoot() throws Exception\n{\r\n    final String blobPrefix = String.format(\"wasbtests-%s-%tQ-blob\", System.getProperty(\"user.name\"), new Date());\r\n    final String inblobName = blobPrefix + \"_In\" + \".txt\";\r\n    final String outblobName = blobPrefix + \"_Out\" + \".txt\";\r\n    testAccount = AzureBlobStorageTestAccount.createRoot(inblobName, FILE_SIZE);\r\n    assumeNotNull(testAccount);\r\n    assertEquals(FILE_SIZE, readInputStream(new Path(PATH_DELIMITER + inblobName)));\r\n    try {\r\n        FileSystem fs = testAccount.getFileSystem();\r\n        Path outputPath = new Path(PATH_DELIMITER + outblobName);\r\n        OutputStream outputStream = fs.create(outputPath);\r\n        fail(\"Expected an AzureException when writing to root folder.\");\r\n        outputStream.write(new byte[FILE_SIZE]);\r\n        outputStream.close();\r\n    } catch (AzureException e) {\r\n        assertTrue(true);\r\n    } catch (Exception e) {\r\n        String errMsg = String.format(\"Expected AzureException but got %s instead.\", e);\r\n        assertTrue(errMsg, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConnectWithThrottling",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConnectWithThrottling() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.createThrottled();\r\n    assertTrue(validateIOStreams(new Path(\"/wasb_scheme\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeSingleByte",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeSingleByte(FileSystem fs, Path testFile, int toWrite) throws Exception\n{\r\n    OutputStream outputStream = fs.create(testFile);\r\n    outputStream.write(toWrite);\r\n    outputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertSingleByteValue",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void assertSingleByteValue(FileSystem fs, Path testFile, int expectedValue) throws Exception\n{\r\n    InputStream inputStream = fs.open(testFile);\r\n    int byteRead = inputStream.read();\r\n    assertTrue(\"File unexpectedly empty: \" + testFile, byteRead >= 0);\r\n    assertTrue(\"File has more than a single byte: \" + testFile, inputStream.read() < 0);\r\n    inputStream.close();\r\n    assertEquals(\"Unxpected content in: \" + testFile, expectedValue, byteRead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultipleContainers",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testMultipleContainers() throws Exception\n{\r\n    AzureBlobStorageTestAccount firstAccount = AzureBlobStorageTestAccount.create(\"first\"), secondAccount = AzureBlobStorageTestAccount.create(\"second\");\r\n    assumeNotNull(firstAccount);\r\n    assumeNotNull(secondAccount);\r\n    try {\r\n        FileSystem firstFs = firstAccount.getFileSystem(), secondFs = secondAccount.getFileSystem();\r\n        Path testFile = new Path(\"/testWasb\");\r\n        assertTrue(validateIOStreams(firstFs, testFile));\r\n        assertTrue(validateIOStreams(secondFs, testFile));\r\n        writeSingleByte(firstFs, testFile, 5);\r\n        writeSingleByte(secondFs, testFile, 7);\r\n        assertSingleByteValue(firstFs, testFile, 5);\r\n        assertSingleByteValue(secondFs, testFile, 7);\r\n    } finally {\r\n        firstAccount.cleanup();\r\n        secondAccount.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDefaultKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultKeyProvider() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    String key = \"testkey\";\r\n    conf.set(SimpleKeyProvider.KEY_ACCOUNT_KEY_PREFIX + account, key);\r\n    String result = AzureNativeFileSystemStore.getAccountKeyFromConfiguration(account, conf);\r\n    assertEquals(key, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCredsFromCredentialProvider",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCredsFromCredentialProvider() throws Exception\n{\r\n    assumeFalse(runningInSASMode);\r\n    String account = \"testacct\";\r\n    String key = \"testkey\";\r\n    final Configuration conf = new Configuration();\r\n    final File file = tempDir.newFile(\"test.jks\");\r\n    final URI jks = ProviderUtils.nestURIForLocalJavaKeyStoreProvider(file.toURI());\r\n    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, jks.toString());\r\n    provisionAccountKey(conf, account, key);\r\n    conf.set(SimpleKeyProvider.KEY_ACCOUNT_KEY_PREFIX + account, key + \"cleartext\");\r\n    String result = AzureNativeFileSystemStore.getAccountKeyFromConfiguration(account, conf);\r\n    assertEquals(\"AccountKey incorrect.\", key, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "provisionAccountKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void provisionAccountKey(final Configuration conf, String account, String key) throws Exception\n{\r\n    final CredentialProvider provider = CredentialProviderFactory.getProviders(conf).get(0);\r\n    provider.createCredentialEntry(SimpleKeyProvider.KEY_ACCOUNT_KEY_PREFIX + account, key.toCharArray());\r\n    provider.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testValidKeyProvider",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testValidKeyProvider() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    String key = \"testkey\";\r\n    conf.set(SimpleKeyProvider.KEY_ACCOUNT_KEY_PREFIX + account, key);\r\n    conf.setClass(\"fs.azure.account.keyprovider.\" + account, SimpleKeyProvider.class, KeyProvider.class);\r\n    String result = AzureNativeFileSystemStore.getAccountKeyFromConfiguration(account, conf);\r\n    assertEquals(key, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInvalidKeyProviderNonexistantClass",
  "errType" : [ "KeyProviderException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testInvalidKeyProviderNonexistantClass() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    conf.set(\"fs.azure.account.keyprovider.\" + account, \"org.apache.Nonexistant.Class\");\r\n    try {\r\n        AzureNativeFileSystemStore.getAccountKeyFromConfiguration(account, conf);\r\n        Assert.fail(\"Nonexistant key provider class should have thrown a \" + \"KeyProviderException\");\r\n    } catch (KeyProviderException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testInvalidKeyProviderWrongClass",
  "errType" : [ "KeyProviderException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testInvalidKeyProviderWrongClass() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    conf.set(\"fs.azure.account.keyprovider.\" + account, \"java.lang.String\");\r\n    try {\r\n        AzureNativeFileSystemStore.getAccountKeyFromConfiguration(account, conf);\r\n        Assert.fail(\"Key provider class that doesn't implement KeyProvider \" + \"should have thrown a KeyProviderException\");\r\n    } catch (KeyProviderException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testNoUriAuthority",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testNoUriAuthority() throws Exception\n{\r\n    String[] wasbAliases = new String[] { \"wasb\", \"wasbs\" };\r\n    for (String defaultScheme : wasbAliases) {\r\n        for (String wantedScheme : wasbAliases) {\r\n            testAccount = AzureBlobStorageTestAccount.createMock();\r\n            Configuration conf = testAccount.getFileSystem().getConf();\r\n            String authority = testAccount.getFileSystem().getUri().getAuthority();\r\n            URI defaultUri = new URI(defaultScheme, authority, null, null, null);\r\n            conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n            conf.addResource(\"azure-test.xml\");\r\n            URI wantedUri = new URI(wantedScheme + \":///random/path\");\r\n            NativeAzureFileSystem obtained = (NativeAzureFileSystem) FileSystem.get(wantedUri, conf);\r\n            assertNotNull(obtained);\r\n            assertEquals(new URI(wantedScheme, authority, null, null, null), obtained.getUri());\r\n            Path qualified = obtained.makeQualified(new Path(wantedUri));\r\n            assertEquals(new URI(wantedScheme, authority, wantedUri.getPath(), null, null), qualified.toUri());\r\n            testAccount.cleanup();\r\n            FileSystem.closeAll();\r\n        }\r\n    }\r\n    testAccount = AzureBlobStorageTestAccount.createMock();\r\n    Configuration conf = testAccount.getFileSystem().getConf();\r\n    conf.set(FS_DEFAULT_NAME_KEY, \"file:///\");\r\n    try {\r\n        FileSystem.get(new URI(\"wasb:///random/path\"), conf);\r\n        fail(\"Should've thrown.\");\r\n    } catch (IllegalArgumentException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testWasbAsDefaultFileSystemHasNoPort",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testWasbAsDefaultFileSystemHasNoPort() throws Exception\n{\r\n    try {\r\n        testAccount = AzureBlobStorageTestAccount.createMock();\r\n        Configuration conf = testAccount.getFileSystem().getConf();\r\n        String authority = testAccount.getFileSystem().getUri().getAuthority();\r\n        URI defaultUri = new URI(\"wasb\", authority, null, null, null);\r\n        conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n        conf.addResource(\"azure-test.xml\");\r\n        FileSystem fs = FileSystem.get(conf);\r\n        assertTrue(fs instanceof NativeAzureFileSystem);\r\n        assertEquals(-1, fs.getUri().getPort());\r\n        AbstractFileSystem afs = FileContext.getFileContext(conf).getDefaultFileSystem();\r\n        assertTrue(afs instanceof Wasb);\r\n        assertEquals(-1, afs.getUri().getPort());\r\n    } finally {\r\n        testAccount.cleanup();\r\n        FileSystem.closeAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAbstractFileSystemImplementationForWasbsScheme",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAbstractFileSystemImplementationForWasbsScheme() throws Exception\n{\r\n    try {\r\n        testAccount = AzureBlobStorageTestAccount.createMock();\r\n        Configuration conf = testAccount.getFileSystem().getConf();\r\n        String authority = testAccount.getFileSystem().getUri().getAuthority();\r\n        URI defaultUri = new URI(\"wasbs\", authority, null, null, null);\r\n        conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n        conf.set(\"fs.AbstractFileSystem.wasbs.impl\", \"org.apache.hadoop.fs.azure.Wasbs\");\r\n        conf.addResource(\"azure-test.xml\");\r\n        FileSystem fs = FileSystem.get(conf);\r\n        assertTrue(fs instanceof NativeAzureFileSystem);\r\n        assertEquals(\"wasbs\", fs.getScheme());\r\n        AbstractFileSystem afs = FileContext.getFileContext(conf).getDefaultFileSystem();\r\n        assertTrue(afs instanceof Wasbs);\r\n        assertEquals(-1, afs.getUri().getPort());\r\n        assertEquals(\"wasbs\", afs.getUri().getScheme());\r\n    } finally {\r\n        testAccount.cleanup();\r\n        FileSystem.closeAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCredentialProviderPathExclusions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCredentialProviderPathExclusions() throws Exception\n{\r\n    String providerPath = \"user:///,jceks://wasb/user/hrt_qa/sqoopdbpasswd.jceks,\" + \"jceks://hdfs@nn1.example.com/my/path/test.jceks\";\r\n    Configuration config = new Configuration();\r\n    config.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, providerPath);\r\n    String newPath = \"user:///,jceks://hdfs@nn1.example.com/my/path/test.jceks\";\r\n    excludeAndTestExpectations(config, newPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testExcludeAllProviderTypesFromConfig",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testExcludeAllProviderTypesFromConfig() throws Exception\n{\r\n    String providerPath = \"jceks://wasb/tmp/test.jceks,\" + \"jceks://wasb@/my/path/test.jceks\";\r\n    Configuration config = new Configuration();\r\n    config.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, providerPath);\r\n    String newPath = null;\r\n    excludeAndTestExpectations(config, newPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "excludeAndTestExpectations",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void excludeAndTestExpectations(Configuration config, String newPath) throws Exception\n{\r\n    Configuration conf = ProviderUtils.excludeIncompatibleCredentialProviders(config, NativeAzureFileSystem.class);\r\n    String effectivePath = conf.get(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, null);\r\n    assertEquals(newPath, effectivePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testUserAgentConfig",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testUserAgentConfig() throws Exception\n{\r\n    try {\r\n        testAccount = AzureBlobStorageTestAccount.createMock();\r\n        Configuration conf = testAccount.getFileSystem().getConf();\r\n        String authority = testAccount.getFileSystem().getUri().getAuthority();\r\n        URI defaultUri = new URI(\"wasbs\", authority, null, null, null);\r\n        conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n        conf.set(\"fs.AbstractFileSystem.wasbs.impl\", \"org.apache.hadoop.fs.azure.Wasbs\");\r\n        conf.set(AzureNativeFileSystemStore.USER_AGENT_ID_KEY, \"TestClient\");\r\n        FileSystem fs = FileSystem.get(conf);\r\n        AbstractFileSystem afs = FileContext.getFileContext(conf).getDefaultFileSystem();\r\n        assertTrue(afs instanceof Wasbs);\r\n        assertEquals(-1, afs.getUri().getPort());\r\n        assertEquals(\"wasbs\", afs.getUri().getScheme());\r\n    } finally {\r\n        testAccount.cleanup();\r\n        FileSystem.closeAll();\r\n    }\r\n    try {\r\n        testAccount = AzureBlobStorageTestAccount.createMock();\r\n        Configuration conf = testAccount.getFileSystem().getConf();\r\n        String authority = testAccount.getFileSystem().getUri().getAuthority();\r\n        URI defaultUri = new URI(\"wasbs\", authority, null, null, null);\r\n        conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n        conf.set(\"fs.AbstractFileSystem.wasbs.impl\", \"org.apache.hadoop.fs.azure.Wasbs\");\r\n        conf.unset(AzureNativeFileSystemStore.USER_AGENT_ID_KEY);\r\n        FileSystem fs = FileSystem.get(conf);\r\n        AbstractFileSystem afs = FileContext.getFileContext(conf).getDefaultFileSystem();\r\n        assertTrue(afs instanceof Wasbs);\r\n        assertEquals(-1, afs.getUri().getPort());\r\n        assertEquals(\"wasbs\", afs.getUri().getScheme());\r\n    } finally {\r\n        testAccount.cleanup();\r\n        FileSystem.closeAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCanonicalServiceName() throws Exception\n{\r\n    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.createMock();\r\n    Configuration conf = testAccount.getFileSystem().getConf();\r\n    String authority = testAccount.getFileSystem().getUri().getAuthority();\r\n    URI defaultUri = new URI(\"wasbs\", authority, null, null, null);\r\n    conf.set(FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n    try {\r\n        FileSystem fs0 = FileSystem.get(conf);\r\n        intercept(IllegalArgumentException.class, \"java.net.UnknownHostException\", () -> fs0.getCanonicalServiceName());\r\n        conf.setBoolean(RETURN_URI_AS_CANONICAL_SERVICE_NAME_PROPERTY_NAME, true);\r\n        FileSystem fs1 = FileSystem.newInstance(defaultUri, conf);\r\n        Assert.assertEquals(\"getCanonicalServiceName() should return URI\", fs1.getUri().toString(), fs1.getCanonicalServiceName());\r\n    } finally {\r\n        testAccount.cleanup();\r\n        FileSystem.closeAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testCustomProviderBinding",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCustomProviderBinding() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    WrappingTokenProvider.enable(conf);\r\n    AbfsConfiguration abfs = new AbfsConfiguration(conf, \"not-a-real-account\");\r\n    CustomTokenProviderAdapter provider = (CustomTokenProviderAdapter) abfs.getTokenProvider();\r\n    assertEquals(\"User agent\", INITED, provider.getUserAgentSuffix());\r\n    ExtensionHelper.bind(provider, new URI(\"abfs://store@user.dfs.core.windows.net\"), conf);\r\n    assertEquals(\"User agent\", BOUND, ExtensionHelper.getUserAgentSuffix(provider, \"\"));\r\n    AzureADToken token = provider.getToken();\r\n    assertEquals(\"Access token propagation\", ACCESS_TOKEN, token.getAccessToken());\r\n    Date expiry = token.getExpiry();\r\n    long time = expiry.getTime();\r\n    assertTrue(\"date wrong: \" + expiry, time <= System.currentTimeMillis());\r\n    provider.close();\r\n    assertEquals(\"User agent\", CLOSED, provider.getUserAgentSuffix());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    Configuration conf = getConfiguration();\r\n    threads = AzureTestUtils.getTestPropertyInt(conf, \"fs.azure.scale.test.list.performance.threads\", NUMBER_OF_THREADS);\r\n    filesPerThread = AzureTestUtils.getTestPropertyInt(conf, \"fs.azure.scale.test.list.performance.files\", NUMBER_OF_FILES_PER_THREAD);\r\n    expectedFileCount = threads * filesPerThread;\r\n    LOG.info(\"Thread = {}, Files per Thread = {}, expected files = {}\", threads, filesPerThread, expectedFileCount);\r\n    conf.set(\"fs.azure.io.retry.max.retries\", \"1\");\r\n    conf.set(\"fs.azure.delete.threads\", \"16\");\r\n    createTestAccount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create(\"itestlistperformance\", EnumSet.of(AzureBlobStorageTestAccount.CreateOptions.CreateContainer), null, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0101_CreateDirectoryWithFiles",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void test_0101_CreateDirectoryWithFiles() throws Exception\n{\r\n    Assume.assumeFalse(\"Test path exists; skipping\", fs.exists(TEST_DIR_PATH));\r\n    ExecutorService executorService = Executors.newFixedThreadPool(threads);\r\n    CloudBlobContainer container = testAccount.getRealContainer();\r\n    final String basePath = (fs.getWorkingDirectory().toUri().getPath() + \"/\" + TEST_DIR_PATH + \"/\").substring(1);\r\n    ArrayList<Callable<Integer>> tasks = new ArrayList<>(threads);\r\n    fs.mkdirs(TEST_DIR_PATH);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    for (int i = 0; i < threads; i++) {\r\n        tasks.add(new Callable<Integer>() {\r\n\r\n            public Integer call() {\r\n                int written = 0;\r\n                for (int j = 0; j < filesPerThread; j++) {\r\n                    String blobName = basePath + UUID.randomUUID().toString();\r\n                    try {\r\n                        CloudBlockBlob blob = container.getBlockBlobReference(blobName);\r\n                        blob.uploadText(\"\");\r\n                        written++;\r\n                    } catch (Exception e) {\r\n                        LOG.error(\"Filed to write {}\", blobName, e);\r\n                        break;\r\n                    }\r\n                }\r\n                LOG.info(\"Thread completed with {} files written\", written);\r\n                return written;\r\n            }\r\n        });\r\n    }\r\n    List<Future<Integer>> futures = executorService.invokeAll(tasks, getTestTimeoutMillis(), TimeUnit.MILLISECONDS);\r\n    long elapsedMs = timer.elapsedTimeMs();\r\n    LOG.info(\"time to create files: {} millis\", elapsedMs);\r\n    for (Future<Integer> future : futures) {\r\n        assertTrue(\"Future timed out\", future.isDone());\r\n        assertEquals(\"Future did not write all files timed out\", filesPerThread, future.get().intValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0200_ListStatusPerformance",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void test_0200_ListStatusPerformance() throws Exception\n{\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    FileStatus[] fileList = fs.listStatus(TEST_DIR_PATH);\r\n    long elapsedMs = timer.elapsedTimeMs();\r\n    LOG.info(String.format(\"files=%1$d, elapsedMs=%2$d\", fileList.length, elapsedMs));\r\n    Map<Path, FileStatus> foundInList = new HashMap<>(expectedFileCount);\r\n    for (FileStatus fileStatus : fileList) {\r\n        foundInList.put(fileStatus.getPath(), fileStatus);\r\n        LOG.info(\"{}: {}\", fileStatus.getPath(), fileStatus.isDirectory() ? \"dir\" : \"file\");\r\n    }\r\n    assertEquals(\"Mismatch between expected files and actual\", expectedFileCount, fileList.length);\r\n    ContractTestUtils.NanoTimer initialStatusCallTimer = new ContractTestUtils.NanoTimer();\r\n    RemoteIterator<LocatedFileStatus> listing = fs.listFiles(TEST_DIR_PATH, true);\r\n    long initialListTime = initialStatusCallTimer.elapsedTimeMs();\r\n    timer = new ContractTestUtils.NanoTimer();\r\n    while (listing.hasNext()) {\r\n        FileStatus fileStatus = listing.next();\r\n        Path path = fileStatus.getPath();\r\n        FileStatus removed = foundInList.remove(path);\r\n        assertNotNull(\"Did not find \" + path + \"{} in the previous listing\", removed);\r\n    }\r\n    elapsedMs = timer.elapsedTimeMs();\r\n    LOG.info(\"time for listFiles() initial call: {} millis;\" + \" time to iterate: {} millis\", initialListTime, elapsedMs);\r\n    assertEquals(\"Not all files from listStatus() were found in listFiles()\", 0, foundInList.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0300_BulkDeletePerformance",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0300_BulkDeletePerformance() throws Exception\n{\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    fs.delete(TEST_DIR_PATH, true);\r\n    long elapsedMs = timer.elapsedTimeMs();\r\n    LOG.info(\"time for delete(): {} millis; {} nanoS per file\", elapsedMs, timer.nanosPerOperation(expectedFileCount));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n    assumeTrue(\"Resilient rename not available\", getFileSystem().hasPathCapability(getContract().getTestPath(), ETAGS_PRESERVED_IN_RENAME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return enableManifestCommitter(prepareTestConfiguration(binding));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "testEtagConsistencyAcrossListAndHead",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testEtagConsistencyAcrossListAndHead() throws Throwable\n{\r\n    describe(\"Etag values must be non-empty and consistent across LIST and HEAD Calls.\");\r\n    final Path path = methodPath();\r\n    final FileSystem fs = getFileSystem();\r\n    ContractTestUtils.touch(fs, path);\r\n    final ManifestStoreOperations operations = createManifestStoreOperations();\r\n    Assertions.assertThat(operations).describedAs(\"Store operations class loaded via Configuration\").isInstanceOf(AbfsManifestStoreOperations.class);\r\n    final FileStatus st = operations.getFileStatus(path);\r\n    final String etag = operations.getEtag(st);\r\n    Assertions.assertThat(etag).describedAs(\"Etag of %s\", st).isNotBlank();\r\n    LOG.info(\"etag of empty file is \\\"{}\\\"\", etag);\r\n    final FileStatus[] statuses = fs.listStatus(path);\r\n    Assertions.assertThat(statuses).describedAs(\"List(%s)\", path).hasSize(1);\r\n    final FileStatus lsStatus = statuses[0];\r\n    Assertions.assertThat(operations.getEtag(lsStatus)).describedAs(\"etag of list status (%s) compared to HEAD value of %s\", lsStatus, st).isEqualTo(etag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "testEtagsOfDifferentDataDifferent",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testEtagsOfDifferentDataDifferent() throws Throwable\n{\r\n    describe(\"Verify that two different blocks of data written have different tags\");\r\n    final Path path = methodPath();\r\n    final FileSystem fs = getFileSystem();\r\n    Path src = new Path(path, \"src\");\r\n    ContractTestUtils.createFile(fs, src, true, \"data1234\".getBytes(StandardCharsets.UTF_8));\r\n    final ManifestStoreOperations operations = createManifestStoreOperations();\r\n    final FileStatus srcStatus = operations.getFileStatus(src);\r\n    final String srcTag = operations.getEtag(srcStatus);\r\n    LOG.info(\"etag of file 1 is \\\"{}\\\"\", srcTag);\r\n    ContractTestUtils.createFile(fs, src, true, \"1234data\".getBytes(StandardCharsets.UTF_8));\r\n    final String tag2 = operations.getEtag(operations.getFileStatus(src));\r\n    LOG.info(\"etag of file 2 is \\\"{}\\\"\", tag2);\r\n    Assertions.assertThat(tag2).describedAs(\"etag of updated file\").isNotEqualTo(srcTag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "testEtagConsistencyAcrossRename",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testEtagConsistencyAcrossRename() throws Throwable\n{\r\n    describe(\"Verify that when a file is renamed, the etag remains unchanged\");\r\n    final Path path = methodPath();\r\n    final FileSystem fs = getFileSystem();\r\n    Path src = new Path(path, \"src\");\r\n    Path dest = new Path(path, \"dest\");\r\n    ContractTestUtils.createFile(fs, src, true, \"sample data\".getBytes(StandardCharsets.UTF_8));\r\n    final ManifestStoreOperations operations = createManifestStoreOperations();\r\n    final FileStatus srcStatus = operations.getFileStatus(src);\r\n    final String srcTag = operations.getEtag(srcStatus);\r\n    LOG.info(\"etag of short file is \\\"{}\\\"\", srcTag);\r\n    Assertions.assertThat(srcTag).describedAs(\"Etag of %s\", srcStatus).isNotBlank();\r\n    operations.commitFile(new FileEntry(src, dest, 0, srcTag));\r\n    FileStatus destStatus = operations.getFileStatus(dest);\r\n    final String destTag = operations.getEtag(destStatus);\r\n    Assertions.assertThat(destTag).describedAs(\"etag of list status (%s) compared to HEAD value of %s\", destStatus, srcStatus).isEqualTo(srcTag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(org.apache.hadoop.conf.Configuration configuration, String accountName)\n{\r\n    boolean throwExceptionAtInit = configuration.getBoolean(MOCK_SASTOKENPROVIDER_FAIL_INIT, false);\r\n    if (throwExceptionAtInit) {\r\n        throw new RuntimeException(\"MockSASTokenProvider initialize exception\");\r\n    }\r\n    this.config = configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getSASToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSASToken(String accountName, String fileSystem, String path, String operation)\n{\r\n    boolean returnEmptySASTokenQuery = this.config.getBoolean(MOCK_SASTOKENPROVIDER_RETURN_EMPTY_SAS_TOKEN, false);\r\n    if (returnEmptySASTokenQuery) {\r\n        return \"\";\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricTags",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMetricTags() throws Exception\n{\r\n    String accountName = getTestAccount().getRealAccount().getBlobEndpoint().getAuthority();\r\n    String containerName = getTestAccount().getRealContainer().getName();\r\n    MetricsRecordBuilder myMetrics = getMyMetrics();\r\n    verify(myMetrics).add(argThat(new TagMatcher(\"accountName\", accountName)));\r\n    verify(myMetrics).add(argThat(new TagMatcher(\"containerName\", containerName)));\r\n    verify(myMetrics).add(argThat(new TagMatcher(\"Context\", \"azureFileSystem\")));\r\n    verify(myMetrics).add(argThat(new TagExistsMatcher(\"wasbFileSystemId\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsOnMkdirList",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMetricsOnMkdirList() throws Exception\n{\r\n    long base = getBaseWebResponses();\r\n    assertTrue(fs.mkdirs(new Path(\"a\")));\r\n    base = assertWebResponsesInRange(base, 1, 18);\r\n    assertEquals(1, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_DIRECTORIES_CREATED));\r\n    assertEquals(1, getFileSystem().listStatus(new Path(\"/\")).length);\r\n    base = assertWebResponsesEquals(base, 1);\r\n    assertNoErrors();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getBandwidthGaugeUpdater",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BandwidthGaugeUpdater getBandwidthGaugeUpdater()\n{\r\n    NativeAzureFileSystem azureFs = (NativeAzureFileSystem) getFileSystem();\r\n    AzureNativeFileSystemStore azureStore = azureFs.getStore();\r\n    return azureStore.getBandwidthGaugeUpdater();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "nonZeroByteArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] nonZeroByteArray(int size)\n{\r\n    byte[] data = new byte[size];\r\n    Arrays.fill(data, (byte) 5);\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsOnFileCreateRead",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void testMetricsOnFileCreateRead() throws Exception\n{\r\n    long base = getBaseWebResponses();\r\n    assertEquals(0, AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation()));\r\n    Path filePath = new Path(\"/metricsTest_webResponses\");\r\n    final int FILE_SIZE = 1000;\r\n    getBandwidthGaugeUpdater().suppressAutoUpdate();\r\n    Date start = new Date();\r\n    OutputStream outputStream = getFileSystem().create(filePath);\r\n    outputStream.write(nonZeroByteArray(FILE_SIZE));\r\n    outputStream.close();\r\n    long uploadDurationMs = new Date().getTime() - start.getTime();\r\n    logOpResponseCount(\"Creating a 1K file\", base);\r\n    base = assertWebResponsesInRange(base, 2, 15);\r\n    getBandwidthGaugeUpdater().triggerUpdate(true);\r\n    long bytesWritten = AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation());\r\n    assertTrue(\"The bytes written in the last second \" + bytesWritten + \" is pretty far from the expected range of around \" + FILE_SIZE + \" bytes plus a little overhead.\", bytesWritten > (FILE_SIZE / 2) && bytesWritten < (FILE_SIZE * 2));\r\n    long totalBytesWritten = AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());\r\n    assertTrue(\"The total bytes written  \" + totalBytesWritten + \" is pretty far from the expected range of around \" + FILE_SIZE + \" bytes plus a little overhead.\", totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));\r\n    long uploadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_RATE);\r\n    LOG.info(\"Upload rate: \" + uploadRate + \" bytes/second.\");\r\n    long expectedRate = (FILE_SIZE * 1000L) / uploadDurationMs;\r\n    assertTrue(\"The upload rate \" + uploadRate + \" is below the expected range of around \" + expectedRate + \" bytes/second that the unit test observed. This should never be\" + \" the case since the test underestimates the rate by looking at \" + \" end-to-end time instead of just block upload time.\", uploadRate >= expectedRate);\r\n    long uploadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_LATENCY);\r\n    LOG.info(\"Upload latency: {}\", uploadLatency);\r\n    long expectedLatency = uploadDurationMs;\r\n    assertTrue(\"The upload latency \" + uploadLatency + \" should be greater than zero now that I've just uploaded a file.\", uploadLatency > 0);\r\n    assertTrue(\"The upload latency \" + uploadLatency + \" is more than the expected range of around \" + expectedLatency + \" milliseconds that the unit test observed. This should never be\" + \" the case since the test overestimates the latency by looking at \" + \" end-to-end time instead of just block upload time.\", uploadLatency <= expectedLatency);\r\n    start = new Date();\r\n    InputStream inputStream = getFileSystem().open(filePath);\r\n    int count = 0;\r\n    while (inputStream.read() >= 0) {\r\n        count++;\r\n    }\r\n    inputStream.close();\r\n    long downloadDurationMs = new Date().getTime() - start.getTime();\r\n    assertEquals(FILE_SIZE, count);\r\n    logOpResponseCount(\"Reading a 1K file\", base);\r\n    base = assertWebResponsesInRange(base, 1, 10);\r\n    getBandwidthGaugeUpdater().triggerUpdate(false);\r\n    long totalBytesRead = AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());\r\n    assertEquals(FILE_SIZE, totalBytesRead);\r\n    long bytesRead = AzureMetricsTestUtil.getCurrentBytesRead(getInstrumentation());\r\n    assertTrue(\"The bytes read in the last second \" + bytesRead + \" is pretty far from the expected range of around \" + FILE_SIZE + \" bytes plus a little overhead.\", bytesRead > (FILE_SIZE / 2) && bytesRead < (FILE_SIZE * 2));\r\n    long downloadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_RATE);\r\n    LOG.info(\"Download rate: \" + downloadRate + \" bytes/second.\");\r\n    expectedRate = (FILE_SIZE * 1000L) / downloadDurationMs;\r\n    assertTrue(\"The download rate \" + downloadRate + \" is below the expected range of around \" + expectedRate + \" bytes/second that the unit test observed. This should never be\" + \" the case since the test underestimates the rate by looking at \" + \" end-to-end time instead of just block download time.\", downloadRate >= expectedRate);\r\n    long downloadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_LATENCY);\r\n    LOG.info(\"Download latency: \" + downloadLatency);\r\n    expectedLatency = downloadDurationMs;\r\n    assertTrue(\"The download latency \" + downloadLatency + \" should be greater than zero now that I've just downloaded a file.\", downloadLatency > 0);\r\n    assertTrue(\"The download latency \" + downloadLatency + \" is more than the expected range of around \" + expectedLatency + \" milliseconds that the unit test observed. This should never be\" + \" the case since the test overestimates the latency by looking at \" + \" end-to-end time instead of just block download time.\", downloadLatency <= expectedLatency);\r\n    assertNoErrors();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsOnBigFileCreateRead",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testMetricsOnBigFileCreateRead() throws Exception\n{\r\n    long base = getBaseWebResponses();\r\n    assertEquals(0, AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation()));\r\n    Path filePath = new Path(\"/metricsTest_webResponses\");\r\n    final int FILE_SIZE = 100 * 1024 * 1024;\r\n    getBandwidthGaugeUpdater().suppressAutoUpdate();\r\n    OutputStream outputStream = getFileSystem().create(filePath);\r\n    outputStream.write(new byte[FILE_SIZE]);\r\n    outputStream.close();\r\n    logOpResponseCount(\"Creating a 100 MB file\", base);\r\n    base = assertWebResponsesInRange(base, 20, 50);\r\n    getBandwidthGaugeUpdater().triggerUpdate(true);\r\n    long totalBytesWritten = AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());\r\n    assertTrue(\"The total bytes written  \" + totalBytesWritten + \" is pretty far from the expected range of around \" + FILE_SIZE + \" bytes plus a little overhead.\", totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));\r\n    long uploadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_RATE);\r\n    LOG.info(\"Upload rate: \" + uploadRate + \" bytes/second.\");\r\n    long uploadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_UPLOAD_LATENCY);\r\n    LOG.info(\"Upload latency: \" + uploadLatency);\r\n    assertTrue(\"The upload latency \" + uploadLatency + \" should be greater than zero now that I've just uploaded a file.\", uploadLatency > 0);\r\n    InputStream inputStream = getFileSystem().open(filePath);\r\n    int count = 0;\r\n    while (inputStream.read() >= 0) {\r\n        count++;\r\n    }\r\n    inputStream.close();\r\n    assertEquals(FILE_SIZE, count);\r\n    logOpResponseCount(\"Reading a 100 MB file\", base);\r\n    base = assertWebResponsesInRange(base, 20, 40);\r\n    getBandwidthGaugeUpdater().triggerUpdate(false);\r\n    long totalBytesRead = AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());\r\n    assertEquals(FILE_SIZE, totalBytesRead);\r\n    long downloadRate = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_RATE);\r\n    LOG.info(\"Download rate: \" + downloadRate + \" bytes/second.\");\r\n    long downloadLatency = AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(), WASB_DOWNLOAD_LATENCY);\r\n    LOG.info(\"Download latency: \" + downloadLatency);\r\n    assertTrue(\"The download latency \" + downloadLatency + \" should be greater than zero now that I've just downloaded a file.\", downloadLatency > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsOnFileRename",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMetricsOnFileRename() throws Exception\n{\r\n    long base = getBaseWebResponses();\r\n    Path originalPath = new Path(\"/metricsTest_RenameStart\");\r\n    Path destinationPath = new Path(\"/metricsTest_RenameFinal\");\r\n    assertEquals(0, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_FILES_CREATED));\r\n    assertTrue(getFileSystem().createNewFile(originalPath));\r\n    logOpResponseCount(\"Creating an empty file\", base);\r\n    base = assertWebResponsesInRange(base, 2, 20);\r\n    assertEquals(1, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_FILES_CREATED));\r\n    assertTrue(((FileSystem) getFileSystem()).rename(originalPath, destinationPath));\r\n    logOpResponseCount(\"Renaming a file\", base);\r\n    base = assertWebResponsesInRange(base, 2, 15);\r\n    assertNoErrors();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsOnFileExistsDelete",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testMetricsOnFileExistsDelete() throws Exception\n{\r\n    long base = getBaseWebResponses();\r\n    Path filePath = new Path(\"/metricsTest_delete\");\r\n    assertFalse(getFileSystem().exists(filePath));\r\n    logOpResponseCount(\"Checking file existence for non-existent file\", base);\r\n    base = assertWebResponsesInRange(base, 1, 5);\r\n    assertTrue(getFileSystem().createNewFile(filePath));\r\n    base = getCurrentWebResponses();\r\n    assertTrue(getFileSystem().exists(filePath));\r\n    logOpResponseCount(\"Checking file existence for existent file\", base);\r\n    base = assertWebResponsesInRange(base, 1, 4);\r\n    assertEquals(0, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_FILES_DELETED));\r\n    assertTrue(getFileSystem().delete(filePath, false));\r\n    logOpResponseCount(\"Deleting a file\", base);\r\n    base = assertWebResponsesInRange(base, 1, 4);\r\n    assertEquals(1, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_FILES_DELETED));\r\n    assertNoErrors();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsOnDirRename",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMetricsOnDirRename() throws Exception\n{\r\n    long base = getBaseWebResponses();\r\n    Path originalDirName = new Path(\"/metricsTestDirectory_RenameStart\");\r\n    Path innerFileName = new Path(originalDirName, \"innerFile\");\r\n    Path destDirName = new Path(\"/metricsTestDirectory_RenameFinal\");\r\n    assertTrue(getFileSystem().mkdirs(originalDirName));\r\n    base = getCurrentWebResponses();\r\n    assertTrue(getFileSystem().createNewFile(innerFileName));\r\n    base = getCurrentWebResponses();\r\n    assertTrue(getFileSystem().rename(originalDirName, destDirName));\r\n    logOpResponseCount(\"Renaming a directory\", base);\r\n    base = assertWebResponsesInRange(base, 1, 20);\r\n    assertNoErrors();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "depth",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int depth(Path path)\n{\r\n    if (path.isRoot()) {\r\n        return 0;\r\n    } else {\r\n        return 1 + depth(path.getParent());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testClientErrorMetrics",
  "errType" : [ "AzureException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testClientErrorMetrics() throws Exception\n{\r\n    String fileName = \"metricsTestFile_ClientError\";\r\n    Path filePath = new Path(\"/\" + fileName);\r\n    final int FILE_SIZE = 100;\r\n    OutputStream outputStream = null;\r\n    String leaseID = null;\r\n    try {\r\n        outputStream = getFileSystem().create(filePath);\r\n        leaseID = getTestAccount().acquireShortLease(fileName);\r\n        try {\r\n            outputStream.write(new byte[FILE_SIZE]);\r\n            outputStream.close();\r\n            assertTrue(\"Should've thrown\", false);\r\n        } catch (AzureException ex) {\r\n            assertTrue(\"Unexpected exception: \" + ex, ex.getMessage().contains(\"lease\"));\r\n        }\r\n        assertEquals(1, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_CLIENT_ERRORS));\r\n        assertEquals(0, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_SERVER_ERRORS));\r\n    } finally {\r\n        if (leaseID != null) {\r\n            getTestAccount().releaseLease(leaseID, fileName);\r\n        }\r\n        IOUtils.closeStream(outputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "logOpResponseCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logOpResponseCount(String opName, long base)\n{\r\n    LOG.info(\"{}  took {} web responses to complete.\", opName, getCurrentWebResponses() - base);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getBaseWebResponses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBaseWebResponses()\n{\r\n    return assertWebResponsesEquals(0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentWebResponses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentWebResponses()\n{\r\n    return AzureMetricsTestUtil.getCurrentWebResponses(getInstrumentation());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "assertWebResponsesEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long assertWebResponsesEquals(long base, long expected)\n{\r\n    assertCounter(WASB_WEB_RESPONSES, base + expected, getMyMetrics());\r\n    return base + expected;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "assertNoErrors",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertNoErrors()\n{\r\n    assertEquals(0, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_CLIENT_ERRORS));\r\n    assertEquals(0, AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_SERVER_ERRORS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "assertWebResponsesInRange",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long assertWebResponsesInRange(long base, long inclusiveLowerLimit, long inclusiveUpperLimit)\n{\r\n    long currentResponses = getCurrentWebResponses();\r\n    long justOperation = currentResponses - base;\r\n    assertTrue(String.format(\"Web responses expected in range [%d, %d], but was %d.\", inclusiveLowerLimit, inclusiveUpperLimit, justOperation), justOperation >= inclusiveLowerLimit && justOperation <= inclusiveUpperLimit);\r\n    return currentResponses;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getMyMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MetricsRecordBuilder getMyMetrics()\n{\r\n    return getMetrics(getInstrumentation());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getInstrumentation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureFileSystemInstrumentation getInstrumentation()\n{\r\n    return getFileSystem().getInstrumentation();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBytesReadFromBufferStatistic",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testBytesReadFromBufferStatistic()\n{\r\n    describe(\"Testing bytesReadFromBuffer statistics value in AbfsInputStream\");\r\n    AbfsInputStreamStatisticsImpl abfsInputStreamStatistics = new AbfsInputStreamStatisticsImpl();\r\n    for (int i = 0; i < OPERATIONS; i++) {\r\n        abfsInputStreamStatistics.bytesReadFromBuffer(1);\r\n    }\r\n    assertEquals(\"Mismatch in bytesReadFromBuffer value\", OPERATIONS, abfsInputStreamStatistics.getBytesReadFromBuffer());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    executorService = Executors.newCachedThreadPool();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    executorService.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyDisablingOfTracker",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyDisablingOfTracker() throws Exception\n{\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, false);\r\n    String latencyDetails = abfsPerfTracker.getClientLatency();\r\n    assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should be empty\").isNull();\r\n    try (AbfsPerfInfo tracker = new AbfsPerfInfo(abfsPerfTracker, \"disablingCaller\", \"disablingCallee\")) {\r\n        AbfsHttpOperation op = new AbfsHttpOperation(url, \"GET\", new ArrayList<>());\r\n        tracker.registerResult(op).registerSuccess(true);\r\n    }\r\n    latencyDetails = abfsPerfTracker.getClientLatency();\r\n    assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should return no record\").isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyTrackingForSingletonLatencyRecords",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyTrackingForSingletonLatencyRecords() throws Exception\n{\r\n    final int numTasks = 100;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, true);\r\n    String latencyDetails = abfsPerfTracker.getClientLatency();\r\n    assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should be empty\").isNull();\r\n    List<Callable<Integer>> tasks = new ArrayList<>();\r\n    AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<>());\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            try (AbfsPerfInfo tracker = new AbfsPerfInfo(abfsPerfTracker, \"oneOperationCaller\", \"oneOperationCallee\")) {\r\n                tracker.registerResult(httpOperation).registerSuccess(true);\r\n                return 0;\r\n            }\r\n        });\r\n    }\r\n    for (Future<Integer> fr : executorService.invokeAll(tasks)) {\r\n        fr.get();\r\n    }\r\n    for (int i = 0; i < numTasks; i++) {\r\n        latencyDetails = abfsPerfTracker.getClientLatency();\r\n        assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should return non-null record\").isNotNull();\r\n        assertThat(latencyDetails).describedAs(\"Latency record should be in the correct format\").containsPattern(\"h=[^ ]* t=[^ ]* a=bogusFilesystemName c=bogusAccountName cr=oneOperationCaller\" + \" ce=oneOperationCallee r=Succeeded l=[0-9]+ s=0 e= ci=[^ ]* ri=[^ ]* bs=0 br=0 m=GET\" + \" u=http%3A%2F%2Fwww.microsoft.com%2FbogusFile\");\r\n    }\r\n    latencyDetails = abfsPerfTracker.getClientLatency();\r\n    assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should return no record\").isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyTrackingForAggregateLatencyRecords",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyTrackingForAggregateLatencyRecords() throws Exception\n{\r\n    final int numTasks = 100;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, true);\r\n    String latencyDetails = abfsPerfTracker.getClientLatency();\r\n    assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should be empty\").isNull();\r\n    List<Callable<Integer>> tasks = new ArrayList<>();\r\n    AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<>());\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            try (AbfsPerfInfo tracker = new AbfsPerfInfo(abfsPerfTracker, \"oneOperationCaller\", \"oneOperationCallee\")) {\r\n                tracker.registerResult(httpOperation).registerSuccess(true).registerAggregates(Instant.now(), TEST_AGGREGATE_COUNT);\r\n                return 0;\r\n            }\r\n        });\r\n    }\r\n    for (Future<Integer> fr : executorService.invokeAll(tasks)) {\r\n        fr.get();\r\n    }\r\n    for (int i = 0; i < numTasks; i++) {\r\n        latencyDetails = abfsPerfTracker.getClientLatency();\r\n        assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should return non-null record\").isNotNull();\r\n        assertThat(latencyDetails).describedAs(\"Latency record should be in the correct format\").containsPattern(\"h=[^ ]* t=[^ ]* a=bogusFilesystemName c=bogusAccountName cr=oneOperationCaller\" + \" ce=oneOperationCallee r=Succeeded l=[0-9]+ ls=[0-9]+ lc=\" + TEST_AGGREGATE_COUNT + \" s=0 e= ci=[^ ]* ri=[^ ]* bs=0 br=0 m=GET u=http%3A%2F%2Fwww.microsoft.com%2FbogusFile\");\r\n    }\r\n    latencyDetails = abfsPerfTracker.getClientLatency();\r\n    assertThat(latencyDetails).describedAs(\"AbfsPerfTracker should return no record\").isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyRecordingSingletonLatencyIsCheapWhenDisabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyRecordingSingletonLatencyIsCheapWhenDisabled() throws Exception\n{\r\n    final double maxLatencyWhenDisabledMs = 1000;\r\n    final double minLatencyWhenDisabledMs = 0;\r\n    final long numTasks = 1000;\r\n    long aggregateLatency = 0;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, false);\r\n    List<Callable<Long>> tasks = new ArrayList<>();\r\n    final AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<>());\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            Instant startRecord = Instant.now();\r\n            try (AbfsPerfInfo tracker = new AbfsPerfInfo(abfsPerfTracker, \"oneOperationCaller\", \"oneOperationCallee\")) {\r\n                tracker.registerResult(httpOperation).registerSuccess(true);\r\n            }\r\n            long latencyRecord = Duration.between(startRecord, Instant.now()).toMillis();\r\n            LOG.debug(\"Spent {} ms in recording latency.\", latencyRecord);\r\n            return latencyRecord;\r\n        });\r\n    }\r\n    for (Future<Long> fr : executorService.invokeAll(tasks)) {\r\n        aggregateLatency += fr.get();\r\n    }\r\n    double averageRecordLatency = aggregateLatency / numTasks;\r\n    assertThat(averageRecordLatency).describedAs(\"Average time for recording singleton latencies should be bounded\").isBetween(minLatencyWhenDisabledMs, maxLatencyWhenDisabledMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyRecordingAggregateLatencyIsCheapWhenDisabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyRecordingAggregateLatencyIsCheapWhenDisabled() throws Exception\n{\r\n    final double maxLatencyWhenDisabledMs = 1000;\r\n    final double minLatencyWhenDisabledMs = 0;\r\n    final long numTasks = 1000;\r\n    long aggregateLatency = 0;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, false);\r\n    List<Callable<Long>> tasks = new ArrayList<>();\r\n    final AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<>());\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            Instant startRecord = Instant.now();\r\n            try (AbfsPerfInfo tracker = new AbfsPerfInfo(abfsPerfTracker, \"oneOperationCaller\", \"oneOperationCallee\")) {\r\n                tracker.registerResult(httpOperation).registerSuccess(true).registerAggregates(startRecord, TEST_AGGREGATE_COUNT);\r\n            }\r\n            long latencyRecord = Duration.between(startRecord, Instant.now()).toMillis();\r\n            LOG.debug(\"Spent {} ms in recording latency.\", latencyRecord);\r\n            return latencyRecord;\r\n        });\r\n    }\r\n    for (Future<Long> fr : executorService.invokeAll(tasks)) {\r\n        aggregateLatency += fr.get();\r\n    }\r\n    double averageRecordLatency = aggregateLatency / numTasks;\r\n    assertThat(averageRecordLatency).describedAs(\"Average time for recording aggregate latencies should be bounded\").isBetween(minLatencyWhenDisabledMs, maxLatencyWhenDisabledMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyGettingLatencyRecordsIsCheapWhenDisabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyGettingLatencyRecordsIsCheapWhenDisabled() throws Exception\n{\r\n    final double maxLatencyWhenDisabledMs = 1000;\r\n    final double minLatencyWhenDisabledMs = 0;\r\n    final long numTasks = 1000;\r\n    long aggregateLatency = 0;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, false);\r\n    List<Callable<Long>> tasks = new ArrayList<>();\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            Instant startGet = Instant.now();\r\n            abfsPerfTracker.getClientLatency();\r\n            long latencyGet = Duration.between(startGet, Instant.now()).toMillis();\r\n            LOG.debug(\"Spent {} ms in retrieving latency record.\", latencyGet);\r\n            return latencyGet;\r\n        });\r\n    }\r\n    for (Future<Long> fr : executorService.invokeAll(tasks)) {\r\n        aggregateLatency += fr.get();\r\n    }\r\n    double averageRecordLatency = aggregateLatency / numTasks;\r\n    assertThat(averageRecordLatency).describedAs(\"Average time for getting latency records should be bounded\").isBetween(minLatencyWhenDisabledMs, maxLatencyWhenDisabledMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyRecordingSingletonLatencyIsCheapWhenEnabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyRecordingSingletonLatencyIsCheapWhenEnabled() throws Exception\n{\r\n    final double maxLatencyWhenDisabledMs = 5000;\r\n    final double minLatencyWhenDisabledMs = 0;\r\n    final long numTasks = 1000;\r\n    long aggregateLatency = 0;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, true);\r\n    List<Callable<Long>> tasks = new ArrayList<>();\r\n    final AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<>());\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            Instant startRecord = Instant.now();\r\n            try (AbfsPerfInfo tracker = new AbfsPerfInfo(abfsPerfTracker, \"oneOperationCaller\", \"oneOperationCallee\")) {\r\n                tracker.registerResult(httpOperation).registerSuccess(true);\r\n            }\r\n            long latencyRecord = Duration.between(startRecord, Instant.now()).toMillis();\r\n            LOG.debug(\"Spent {} ms in recording latency.\", latencyRecord);\r\n            return latencyRecord;\r\n        });\r\n    }\r\n    for (Future<Long> fr : executorService.invokeAll(tasks)) {\r\n        aggregateLatency += fr.get();\r\n    }\r\n    double averageRecordLatency = aggregateLatency / numTasks;\r\n    assertThat(averageRecordLatency).describedAs(\"Average time for recording singleton latencies should be bounded\").isBetween(minLatencyWhenDisabledMs, maxLatencyWhenDisabledMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyRecordingAggregateLatencyIsCheapWhenEnabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyRecordingAggregateLatencyIsCheapWhenEnabled() throws Exception\n{\r\n    final double maxLatencyWhenDisabledMs = 5000;\r\n    final double minLatencyWhenDisabledMs = 0;\r\n    final long numTasks = 1000;\r\n    long aggregateLatency = 0;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, true);\r\n    List<Callable<Long>> tasks = new ArrayList<>();\r\n    final AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<>());\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            Instant startRecord = Instant.now();\r\n            try (AbfsPerfInfo tracker = new AbfsPerfInfo(abfsPerfTracker, \"oneOperationCaller\", \"oneOperationCallee\")) {\r\n                tracker.registerResult(httpOperation).registerSuccess(true).registerAggregates(startRecord, TEST_AGGREGATE_COUNT);\r\n            }\r\n            long latencyRecord = Duration.between(startRecord, Instant.now()).toMillis();\r\n            LOG.debug(\"Spent {} ms in recording latency.\", latencyRecord);\r\n            return latencyRecord;\r\n        });\r\n    }\r\n    for (Future<Long> fr : executorService.invokeAll(tasks)) {\r\n        aggregateLatency += fr.get();\r\n    }\r\n    double averageRecordLatency = aggregateLatency / numTasks;\r\n    assertThat(averageRecordLatency).describedAs(\"Average time for recording aggregate latencies is bounded\").isBetween(minLatencyWhenDisabledMs, maxLatencyWhenDisabledMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyGettingLatencyRecordsIsCheapWhenEnabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyGettingLatencyRecordsIsCheapWhenEnabled() throws Exception\n{\r\n    final double maxLatencyWhenDisabledMs = 5000;\r\n    final double minLatencyWhenDisabledMs = 0;\r\n    final long numTasks = 1000;\r\n    long aggregateLatency = 0;\r\n    AbfsPerfTracker abfsPerfTracker = new AbfsPerfTracker(accountName, filesystemName, true);\r\n    List<Callable<Long>> tasks = new ArrayList<>();\r\n    for (int i = 0; i < numTasks; i++) {\r\n        tasks.add(() -> {\r\n            Instant startRecord = Instant.now();\r\n            abfsPerfTracker.getClientLatency();\r\n            long latencyRecord = Duration.between(startRecord, Instant.now()).toMillis();\r\n            LOG.debug(\"Spent {} ms in recording latency.\", latencyRecord);\r\n            return latencyRecord;\r\n        });\r\n    }\r\n    for (Future<Long> fr : executorService.invokeAll(tasks)) {\r\n        aggregateLatency += fr.get();\r\n    }\r\n    double averageRecordLatency = aggregateLatency / numTasks;\r\n    assertThat(averageRecordLatency).describedAs(\"Average time for getting latency records should be bounded\").isBetween(minLatencyWhenDisabledMs, maxLatencyWhenDisabledMs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyNoExceptionOnInvalidInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyNoExceptionOnInvalidInput() throws Exception\n{\r\n    Instant testInstant = Instant.now();\r\n    AbfsPerfTracker abfsPerfTrackerDisabled = new AbfsPerfTracker(accountName, filesystemName, false);\r\n    AbfsPerfTracker abfsPerfTrackerEnabled = new AbfsPerfTracker(accountName, filesystemName, true);\r\n    final AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<AbfsHttpHeader>());\r\n    verifyNoException(abfsPerfTrackerDisabled);\r\n    verifyNoException(abfsPerfTrackerEnabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyNoException",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyNoException(AbfsPerfTracker abfsPerfTracker) throws Exception\n{\r\n    Instant testInstant = Instant.now();\r\n    final AbfsHttpOperation httpOperation = new AbfsHttpOperation(url, \"GET\", new ArrayList<AbfsHttpHeader>());\r\n    try (AbfsPerfInfo tracker01 = new AbfsPerfInfo(abfsPerfTracker, null, null);\r\n        AbfsPerfInfo tracker02 = new AbfsPerfInfo(abfsPerfTracker, \"test\", null);\r\n        AbfsPerfInfo tracker03 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker04 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker05 = new AbfsPerfInfo(abfsPerfTracker, null, null);\r\n        AbfsPerfInfo tracker06 = new AbfsPerfInfo(abfsPerfTracker, \"test\", null);\r\n        AbfsPerfInfo tracker07 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker08 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker09 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker10 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker11 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker12 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\");\r\n        AbfsPerfInfo tracker13 = new AbfsPerfInfo(abfsPerfTracker, \"test\", \"test\")) {\r\n        tracker01.registerResult(null).registerSuccess(false);\r\n        tracker02.registerResult(null).registerSuccess(false);\r\n        tracker03.registerResult(null).registerSuccess(false);\r\n        tracker04.registerResult(httpOperation).registerSuccess(false);\r\n        tracker05.registerResult(null).registerSuccess(false).registerAggregates(null, 0);\r\n        tracker06.registerResult(null).registerSuccess(false).registerAggregates(null, 0);\r\n        tracker07.registerResult(null).registerSuccess(false).registerAggregates(null, 0);\r\n        tracker08.registerResult(httpOperation).registerSuccess(false).registerAggregates(null, 0);\r\n        tracker09.registerResult(httpOperation).registerSuccess(false).registerAggregates(Instant.now(), 0);\r\n        tracker10.registerResult(httpOperation).registerSuccess(false).registerAggregates(Instant.now(), TEST_AGGREGATE_COUNT);\r\n        tracker11.registerResult(httpOperation).registerSuccess(false).registerAggregates(testInstant, TEST_AGGREGATE_COUNT);\r\n        tracker12.registerResult(httpOperation).registerSuccess(false).registerAggregates(Instant.MAX, TEST_AGGREGATE_COUNT);\r\n        tracker13.registerResult(httpOperation).registerSuccess(false).registerAggregates(Instant.MIN, TEST_AGGREGATE_COUNT);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAPerfTrackerInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsPerfTracker getAPerfTrackerInstance(AbfsConfiguration abfsConfig)\n{\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", abfsConfig.getAccountName(), abfsConfig);\r\n    return tracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return AzureTestConstants.SCALE_TEST_TIMEOUT_MILLIS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    LOG.debug(\"Scale test operation count = {}\", getOperationCount());\r\n    Configuration rawConfiguration = getRawConfiguration();\r\n    assumeScaleTestsEnabled(rawConfiguration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getOperationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOperationCount()\n{\r\n    return getConfiguration().getLong(AzureTestConstants.KEY_OPERATION_COUNT, AzureTestConstants.DEFAULT_OPERATION_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getRandomBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getRandomBytes()\n{\r\n    byte[] buffer = new byte[PageBlobFormatHelpers.PAGE_SIZE - PageBlobFormatHelpers.PAGE_HEADER_SIZE];\r\n    Random rand = new Random();\r\n    rand.nextBytes(buffer);\r\n    return buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBlobPathWithTestName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getBlobPathWithTestName(String parentDir)\n{\r\n    return new Path(parentDir + \"/\" + methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void validate(Path path, byte[] writeBuffer, boolean isEqual) throws IOException\n{\r\n    String blobPath = path.toUri().getPath();\r\n    try (FSDataInputStream inputStream = fs.open(path)) {\r\n        byte[] readBuffer = new byte[PageBlobFormatHelpers.PAGE_SIZE - PageBlobFormatHelpers.PAGE_HEADER_SIZE];\r\n        int numBytesRead = inputStream.read(readBuffer, 0, readBuffer.length);\r\n        if (isEqual) {\r\n            assertArrayEquals(String.format(\"Bytes read do not match bytes written to %1$s\", blobPath), writeBuffer, readBuffer);\r\n        } else {\r\n            assertThat(String.format(\"Bytes read unexpectedly match bytes written to %1$s\", blobPath), readBuffer, IsNot.not(IsEqual.equalTo(writeBuffer)));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isBlockBlobAppendStreamWrapper",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isBlockBlobAppendStreamWrapper(FSDataOutputStream stream)\n{\r\n    return ((SyncableDataOutputStream) ((NativeAzureFileSystem.NativeAzureFsOutputStream) stream.getWrappedStream()).getOutStream()).getOutStream() instanceof BlockBlobAppendStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isPageBlobStreamWrapper",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isPageBlobStreamWrapper(FSDataOutputStream stream)\n{\r\n    return ((SyncableDataOutputStream) stream.getWrappedStream()).getOutStream() instanceof PageBlobOutputStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, PAGE_BLOB_DIR);\r\n    conf.set(AzureNativeFileSystemStore.KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES, BLOCK_BLOB_COMPACTION_DIR);\r\n    return AzureBlobStorageTestAccount.create(\"\", EnumSet.of(AzureBlobStorageTestAccount.CreateOptions.CreateContainer), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPageBlobFlush",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPageBlobFlush() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(PAGE_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        stream.flush();\r\n        SyncableDataOutputStream syncStream = (SyncableDataOutputStream) stream.getWrappedStream();\r\n        PageBlobOutputStream pageBlobStream = (PageBlobOutputStream) syncStream.getOutStream();\r\n        pageBlobStream.waitForLastFlushCompletion();\r\n        validate(path, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPageBlobHFlush",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testPageBlobHFlush() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(PAGE_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isPageBlobStreamWrapper(stream));\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        stream.hflush();\r\n        validate(path, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPageBlobHSync",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testPageBlobHSync() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(PAGE_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isPageBlobStreamWrapper(stream));\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        stream.hsync();\r\n        validate(path, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPageBlobClose",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testPageBlobClose() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(PAGE_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isPageBlobStreamWrapper(stream));\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        stream.close();\r\n        validate(path, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPageBlobCapabilities",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPageBlobCapabilities() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(PAGE_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertHasStreamCapabilities(stream, StreamCapabilities.HFLUSH, StreamCapabilities.HSYNC);\r\n        assertLacksStreamCapabilities(stream, StreamCapabilities.DROPBEHIND, StreamCapabilities.READAHEAD, StreamCapabilities.UNBUFFER);\r\n        stream.write(getRandomBytes());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobFlush",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBlockBlobFlush() throws Exception\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_DIR);\r\n    byte[] buffer = getRandomBytes();\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        for (int i = 0; i < 10; i++) {\r\n            stream.write(buffer);\r\n            stream.flush();\r\n        }\r\n    }\r\n    String blobPath = path.toUri().getPath();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(blobPath.substring(1));\r\n    ArrayList<BlockEntry> blockList = blob.downloadBlockList(BlockListingFilter.COMMITTED, null, null, null);\r\n    assertEquals(1, blockList.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobHFlush",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBlockBlobHFlush() throws Exception\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_DIR);\r\n    byte[] buffer = getRandomBytes();\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        for (int i = 0; i < 10; i++) {\r\n            stream.write(buffer);\r\n            stream.hflush();\r\n        }\r\n    }\r\n    String blobPath = path.toUri().getPath();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(blobPath.substring(1));\r\n    ArrayList<BlockEntry> blockList = blob.downloadBlockList(BlockListingFilter.COMMITTED, null, null, null);\r\n    assertEquals(1, blockList.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobHSync",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBlockBlobHSync() throws Exception\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_DIR);\r\n    byte[] buffer = getRandomBytes();\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        for (int i = 0; i < 10; i++) {\r\n            stream.write(buffer);\r\n            stream.hsync();\r\n        }\r\n    }\r\n    String blobPath = path.toUri().getPath();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(blobPath.substring(1));\r\n    ArrayList<BlockEntry> blockList = blob.downloadBlockList(BlockListingFilter.COMMITTED, null, null, null);\r\n    assertEquals(1, blockList.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobClose",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testBlockBlobClose() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        stream.close();\r\n        validate(path, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobCapabilities",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testBlockBlobCapabilities() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertLacksStreamCapabilities(stream, StreamCapabilities.HFLUSH, StreamCapabilities.HSYNC, StreamCapabilities.DROPBEHIND, StreamCapabilities.READAHEAD, StreamCapabilities.UNBUFFER);\r\n        stream.write(getRandomBytes());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobCompactionFlush",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBlockBlobCompactionFlush() throws Exception\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_COMPACTION_DIR);\r\n    byte[] buffer = getRandomBytes();\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isBlockBlobAppendStreamWrapper(stream));\r\n        for (int i = 0; i < 10; i++) {\r\n            stream.write(buffer);\r\n            stream.flush();\r\n        }\r\n    }\r\n    String blobPath = path.toUri().getPath();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(blobPath.substring(1));\r\n    ArrayList<BlockEntry> blockList = blob.downloadBlockList(BlockListingFilter.COMMITTED, null, null, null);\r\n    assertEquals(1, blockList.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobCompactionHFlush",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBlockBlobCompactionHFlush() throws Exception\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_COMPACTION_DIR);\r\n    byte[] buffer = getRandomBytes();\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isBlockBlobAppendStreamWrapper(stream));\r\n        for (int i = 0; i < 10; i++) {\r\n            stream.write(buffer);\r\n            stream.hflush();\r\n        }\r\n    }\r\n    String blobPath = path.toUri().getPath();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(blobPath.substring(1));\r\n    ArrayList<BlockEntry> blockList = blob.downloadBlockList(BlockListingFilter.COMMITTED, null, null, null);\r\n    assertEquals(10, blockList.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobCompactionHSync",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBlockBlobCompactionHSync() throws Exception\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_COMPACTION_DIR);\r\n    byte[] buffer = getRandomBytes();\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isBlockBlobAppendStreamWrapper(stream));\r\n        for (int i = 0; i < 10; i++) {\r\n            stream.write(buffer);\r\n            stream.hsync();\r\n        }\r\n    }\r\n    String blobPath = path.toUri().getPath();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(blobPath.substring(1));\r\n    ArrayList<BlockEntry> blockList = blob.downloadBlockList(BlockListingFilter.COMMITTED, null, null, null);\r\n    assertEquals(10, blockList.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobCompactionClose",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testBlockBlobCompactionClose() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_COMPACTION_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isBlockBlobAppendStreamWrapper(stream));\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        stream.close();\r\n        validate(path, buffer, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobCompactionCapabilities",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBlockBlobCompactionCapabilities() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_COMPACTION_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertHasStreamCapabilities(stream, StreamCapabilities.HFLUSH, StreamCapabilities.HSYNC);\r\n        assertLacksStreamCapabilities(stream, StreamCapabilities.DROPBEHIND, StreamCapabilities.READAHEAD, StreamCapabilities.UNBUFFER);\r\n        stream.write(getRandomBytes());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPageBlobSmallWrite",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testPageBlobSmallWrite() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(PAGE_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isPageBlobStreamWrapper(stream));\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        validate(path, buffer, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobSmallWrite",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBlockBlobSmallWrite() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        validate(path, buffer, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testBlockBlobCompactionSmallWrite",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testBlockBlobCompactionSmallWrite() throws IOException\n{\r\n    Path path = getBlobPathWithTestName(BLOCK_BLOB_COMPACTION_DIR);\r\n    try (FSDataOutputStream stream = fs.create(path)) {\r\n        assertTrue(isBlockBlobAppendStreamWrapper(stream));\r\n        byte[] buffer = getRandomBytes();\r\n        stream.write(buffer);\r\n        validate(path, buffer, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMaxRequestsAndQueueCapacityDefaults",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMaxRequestsAndQueueCapacityDefaults() throws Exception\n{\r\n    Configuration conf = getRawConfiguration();\r\n    final AzureBlobFileSystem fs = getFileSystem(conf);\r\n    try (FSDataOutputStream out = fs.create(path(TEST_FILE_PATH))) {\r\n        AbfsOutputStream stream = (AbfsOutputStream) out.getWrappedStream();\r\n        int maxConcurrentRequests = getConfiguration().getWriteMaxConcurrentRequestCount();\r\n        if (stream.isAppendBlobStream()) {\r\n            maxConcurrentRequests = 1;\r\n        }\r\n        Assertions.assertThat(stream.getMaxConcurrentRequestCount()).describedAs(\"maxConcurrentRequests should be \" + maxConcurrentRequests).isEqualTo(maxConcurrentRequests);\r\n        Assertions.assertThat(stream.getMaxRequestsThatCanBeQueued()).describedAs(\"maxRequestsToQueue should be \" + getConfiguration().getMaxWriteRequestsToQueue()).isEqualTo(getConfiguration().getMaxWriteRequestsToQueue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMaxRequestsAndQueueCapacity",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMaxRequestsAndQueueCapacity() throws Exception\n{\r\n    Configuration conf = getRawConfiguration();\r\n    int maxConcurrentRequests = 6;\r\n    int maxRequestsToQueue = 10;\r\n    conf.set(ConfigurationKeys.AZURE_WRITE_MAX_CONCURRENT_REQUESTS, \"\" + maxConcurrentRequests);\r\n    conf.set(ConfigurationKeys.AZURE_WRITE_MAX_REQUESTS_TO_QUEUE, \"\" + maxRequestsToQueue);\r\n    final AzureBlobFileSystem fs = getFileSystem(conf);\r\n    try (FSDataOutputStream out = fs.create(path(TEST_FILE_PATH))) {\r\n        AbfsOutputStream stream = (AbfsOutputStream) out.getWrappedStream();\r\n        if (stream.isAppendBlobStream()) {\r\n            maxConcurrentRequests = 1;\r\n        }\r\n        Assertions.assertThat(stream.getMaxConcurrentRequestCount()).describedAs(\"maxConcurrentRequests should be \" + maxConcurrentRequests).isEqualTo(maxConcurrentRequests);\r\n        Assertions.assertThat(stream.getMaxRequestsThatCanBeQueued()).describedAs(\"maxRequestsToQueue should be \" + maxRequestsToQueue).isEqualTo(maxRequestsToQueue);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.createMock();\r\n    fs = testAccount.getFileSystem();\r\n    backingStore = testAccount.getMockStorage().getBackingStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount.cleanup();\r\n    fs = null;\r\n    backingStore = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createEmptyBlobOutOfBand",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createEmptyBlobOutOfBand(String path)\n{\r\n    backingStore.setContent(AzureBlobStorageTestAccount.toMockUri(path), new byte[] { 1, 2 }, new HashMap<String, String>(), false, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testImplicitFolderListed",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testImplicitFolderListed() throws Exception\n{\r\n    createEmptyBlobOutOfBand(\"root/b\");\r\n    FileStatus[] obtained = fs.listStatus(new Path(\"/root/b\"));\r\n    assertNotNull(obtained);\r\n    assertEquals(1, obtained.length);\r\n    assertFalse(obtained[0].isDirectory());\r\n    assertEquals(\"/root/b\", obtained[0].getPath().toUri().getPath());\r\n    obtained = fs.listStatus(new Path(\"/root\"));\r\n    assertNotNull(obtained);\r\n    assertEquals(1, obtained.length);\r\n    assertFalse(obtained[0].isDirectory());\r\n    assertEquals(\"/root/b\", obtained[0].getPath().toUri().getPath());\r\n    FileStatus dirStatus = fs.getFileStatus(new Path(\"/root\"));\r\n    assertNotNull(dirStatus);\r\n    assertTrue(dirStatus.isDirectory());\r\n    assertEquals(\"/root\", dirStatus.getPath().toUri().getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testImplicitFolderDeleted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testImplicitFolderDeleted() throws Exception\n{\r\n    createEmptyBlobOutOfBand(\"root/b\");\r\n    assertTrue(fs.exists(new Path(\"/root\")));\r\n    assertTrue(fs.delete(new Path(\"/root\"), true));\r\n    assertFalse(fs.exists(new Path(\"/root\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileInImplicitFolderDeleted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFileInImplicitFolderDeleted() throws Exception\n{\r\n    createEmptyBlobOutOfBand(\"root/b\");\r\n    assertTrue(fs.exists(new Path(\"/root\")));\r\n    assertTrue(fs.delete(new Path(\"/root/b\"), true));\r\n    assertTrue(fs.exists(new Path(\"/root\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileAndImplicitFolderSameName",
  "errType" : [ "AzureException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFileAndImplicitFolderSameName() throws Exception\n{\r\n    createEmptyBlobOutOfBand(\"root/b\");\r\n    createEmptyBlobOutOfBand(\"root/b/c\");\r\n    FileStatus[] listResult = fs.listStatus(new Path(\"/root/b\"));\r\n    assertEquals(1, listResult.length);\r\n    assertFalse(listResult[0].isDirectory());\r\n    try {\r\n        fs.delete(new Path(\"/root/b/c\"), true);\r\n        assertTrue(\"Should've thrown.\", false);\r\n    } catch (AzureException e) {\r\n        assertEquals(\"File /root/b/c has a parent directory /root/b\" + \" which is also a file. Can't resolve.\", e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreatingDeepFileCreatesExplicitFolder",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCreatingDeepFileCreatesExplicitFolder() throws Exception\n{\r\n    for (DeepCreateTestVariation variation : DeepCreateTestVariation.values()) {\r\n        switch(variation) {\r\n            case File:\r\n                assertTrue(fs.createNewFile(new Path(\"/x/y/z\")));\r\n                break;\r\n            case Folder:\r\n                assertTrue(fs.mkdirs(new Path(\"/x/y/z\")));\r\n                break;\r\n        }\r\n        assertTrue(backingStore.exists(AzureBlobStorageTestAccount.toMockUri(\"x\")));\r\n        assertTrue(backingStore.exists(AzureBlobStorageTestAccount.toMockUri(\"x/y\")));\r\n        fs.delete(new Path(\"/x\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionOnImplicitFolder",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSetPermissionOnImplicitFolder() throws Exception\n{\r\n    createEmptyBlobOutOfBand(\"root/b\");\r\n    FsPermission newPermission = new FsPermission((short) 0600);\r\n    fs.setPermission(new Path(\"/root\"), newPermission);\r\n    FileStatus newStatus = fs.getFileStatus(new Path(\"/root\"));\r\n    assertNotNull(newStatus);\r\n    assertEquals(newPermission, newStatus.getPermission());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetOwnerOnImplicitFolder",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSetOwnerOnImplicitFolder() throws Exception\n{\r\n    createEmptyBlobOutOfBand(\"root/b\");\r\n    fs.setOwner(new Path(\"/root\"), \"newOwner\", null);\r\n    FileStatus newStatus = fs.getFileStatus(new Path(\"/root\"));\r\n    assertNotNull(newStatus);\r\n    assertEquals(\"newOwner\", newStatus.getOwner());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "sleep",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void sleep(long milliseconds)\n{\r\n    try {\r\n        Thread.sleep(milliseconds);\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "fuzzyValidate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void fuzzyValidate(long expected, long actual, double percentage)\n{\r\n    final double lowerBound = Math.max(expected - percentage / 100 * expected, 0);\r\n    final double upperBound = expected + percentage / 100 * expected;\r\n    assertTrue(String.format(\"The actual value %1$d is not within the expected range: \" + \"[%2$.2f, %3$.2f].\", actual, lowerBound, upperBound), actual >= lowerBound && actual <= upperBound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validate(long expected, long actual)\n{\r\n    assertEquals(String.format(\"The actual value %1$d is not the expected value %2$d.\", actual, expected), expected, actual);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "validateLessThanOrEqual",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateLessThanOrEqual(long maxExpected, long actual)\n{\r\n    assertTrue(String.format(\"The actual value %1$d is not less than or equal to the maximum\" + \" expected value %2$d.\", actual, maxExpected), actual < maxExpected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testNoMetricUpdatesThenNoWaiting",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNoMetricUpdatesThenNoWaiting()\n{\r\n    AbfsClientThrottlingAnalyzer analyzer = new AbfsClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    validate(0, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOnlySuccessThenNoWaiting",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testOnlySuccessThenNoWaiting()\n{\r\n    AbfsClientThrottlingAnalyzer analyzer = new AbfsClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    analyzer.addBytesTransferred(8 * MEGABYTE, false);\r\n    validate(0, analyzer.getSleepDuration());\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    validate(0, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOnlyErrorsAndWaiting",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testOnlyErrorsAndWaiting()\n{\r\n    AbfsClientThrottlingAnalyzer analyzer = new AbfsClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    analyzer.addBytesTransferred(4 * MEGABYTE, true);\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    final int expectedSleepDuration1 = 1100;\r\n    validateLessThanOrEqual(expectedSleepDuration1, analyzer.getSleepDuration());\r\n    sleep(10 * ANALYSIS_PERIOD);\r\n    final int expectedSleepDuration2 = 900;\r\n    validateLessThanOrEqual(expectedSleepDuration2, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSuccessAndErrorsAndWaiting",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSuccessAndErrorsAndWaiting()\n{\r\n    AbfsClientThrottlingAnalyzer analyzer = new AbfsClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    analyzer.addBytesTransferred(8 * MEGABYTE, false);\r\n    analyzer.addBytesTransferred(2 * MEGABYTE, true);\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    analyzer.suspendIfNecessary();\r\n    final int expectedElapsedTime = 126;\r\n    fuzzyValidate(expectedElapsedTime, timer.elapsedTimeMs(), MAX_ACCEPTABLE_PERCENT_DIFFERENCE);\r\n    sleep(10 * ANALYSIS_PERIOD);\r\n    final int expectedSleepDuration = 110;\r\n    validateLessThanOrEqual(expectedSleepDuration, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testManySuccessAndErrorsAndWaiting",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testManySuccessAndErrorsAndWaiting()\n{\r\n    AbfsClientThrottlingAnalyzer analyzer = new AbfsClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    final int numberOfRequests = 20;\r\n    for (int i = 0; i < numberOfRequests; i++) {\r\n        analyzer.addBytesTransferred(8 * MEGABYTE, false);\r\n        analyzer.addBytesTransferred(2 * MEGABYTE, true);\r\n    }\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    analyzer.suspendIfNecessary();\r\n    fuzzyValidate(7, timer.elapsedTimeMs(), MAX_ACCEPTABLE_PERCENT_DIFFERENCE);\r\n    sleep(10 * ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "getContainerSASWithFullControl",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String getContainerSASWithFullControl(String accountName, String containerName)\n{\r\n    String sp = \"rcwdl\";\r\n    String sv = AuthenticationVersion.Feb20.toString();\r\n    String sr = \"c\";\r\n    String st = ISO_8601_FORMATTER.format(Instant.now().minus(FIVE_MINUTES));\r\n    String se = ISO_8601_FORMATTER.format(Instant.now().plus(ONE_DAY));\r\n    String signature = computeSignatureForSAS(sp, st, se, sv, \"c\", accountName, containerName, null);\r\n    AbfsUriQueryBuilder qb = new AbfsUriQueryBuilder();\r\n    qb.addQuery(\"sp\", sp);\r\n    qb.addQuery(\"st\", st);\r\n    qb.addQuery(\"se\", se);\r\n    qb.addQuery(\"sv\", sv);\r\n    qb.addQuery(\"sr\", sr);\r\n    qb.addQuery(\"sig\", signature);\r\n    return qb.toString().substring(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "computeSignatureForSAS",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "String computeSignatureForSAS(String sp, String st, String se, String sv, String sr, String accountName, String containerName, String path)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(sp);\r\n    sb.append(\"\\n\");\r\n    sb.append(st);\r\n    sb.append(\"\\n\");\r\n    sb.append(se);\r\n    sb.append(\"\\n\");\r\n    sb.append(\"/blob/\");\r\n    sb.append(accountName);\r\n    sb.append(\"/\");\r\n    sb.append(containerName);\r\n    if (path != null && !sr.equals(\"c\")) {\r\n        sb.append(path);\r\n    }\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(sv);\r\n    sb.append(\"\\n\");\r\n    sb.append(sr);\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    String stringToSign = sb.toString();\r\n    LOG.debug(\"Service SAS stringToSign: \" + stringToSign.replace(\"\\n\", \".\"));\r\n    return computeHmac256(stringToSign);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    backingStore = getTestAccount().getMockStorage().getBackingStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    super.tearDown();\r\n    backingStore = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.createMock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testLinkBlobs",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testLinkBlobs() throws Exception\n{\r\n    Path filePath = new Path(\"/inProgress\");\r\n    FSDataOutputStream outputStream = fs.create(filePath);\r\n    HashMap<String, String> metadata = backingStore.getMetadata(AzureBlobStorageTestAccount.toMockUri(filePath));\r\n    assertNotNull(metadata);\r\n    String linkValue = metadata.get(AzureNativeFileSystemStore.LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY);\r\n    linkValue = URLDecoder.decode(linkValue, \"UTF-8\");\r\n    assertNotNull(linkValue);\r\n    assertTrue(backingStore.exists(AzureBlobStorageTestAccount.toMockUri(linkValue)));\r\n    assertTrue(fs.exists(filePath));\r\n    outputStream.close();\r\n    metadata = backingStore.getMetadata(AzureBlobStorageTestAccount.toMockUri(filePath));\r\n    assertNull(metadata.get(AzureNativeFileSystemStore.LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString(FileStatus[] list)\n{\r\n    String[] asStrings = new String[list.length];\r\n    for (int i = 0; i < list.length; i++) {\r\n        asStrings[i] = list[i].getPath().toString();\r\n    }\r\n    return StringUtils.join(\",\", asStrings);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testNoTempBlobsVisible",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testNoTempBlobsVisible() throws Exception\n{\r\n    Path filePath = new Path(\"/inProgress\");\r\n    FSDataOutputStream outputStream = fs.create(filePath);\r\n    FileStatus[] listOfRoot = fs.listStatus(new Path(\"/\"));\r\n    assertEquals(\"Expected one file listed, instead got: \" + toString(listOfRoot), 1, listOfRoot.length);\r\n    assertEquals(fs.makeQualified(filePath), listOfRoot[0].getPath());\r\n    outputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "selectToString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Iterable<String> selectToString(final Iterable<Throwable> collection)\n{\r\n    return new Iterable<String>() {\r\n\r\n        @Override\r\n        public Iterator<String> iterator() {\r\n            final Iterator<Throwable> exceptionIterator = collection.iterator();\r\n            return new Iterator<String>() {\r\n\r\n                @Override\r\n                public boolean hasNext() {\r\n                    return exceptionIterator.hasNext();\r\n                }\r\n\r\n                @Override\r\n                public String next() {\r\n                    StringWriter stringWriter = new StringWriter();\r\n                    PrintWriter printWriter = new PrintWriter(stringWriter);\r\n                    exceptionIterator.next().printStackTrace(printWriter);\r\n                    printWriter.close();\r\n                    return stringWriter.toString();\r\n                }\r\n\r\n                @Override\r\n                public void remove() {\r\n                    exceptionIterator.remove();\r\n                }\r\n            };\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedOperation",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMultiThreadedOperation() throws Exception\n{\r\n    for (int iter = 0; iter < 10; iter++) {\r\n        final int numThreads = 20;\r\n        Thread[] threads = new Thread[numThreads];\r\n        final ConcurrentLinkedQueue<Throwable> exceptionsEncountered = new ConcurrentLinkedQueue<Throwable>();\r\n        for (int i = 0; i < numThreads; i++) {\r\n            final Path threadLocalFile = new Path(\"/myFile\" + i);\r\n            threads[i] = new Thread(new Runnable() {\r\n\r\n                @Override\r\n                public void run() {\r\n                    try {\r\n                        assertTrue(!fs.exists(threadLocalFile));\r\n                        OutputStream output = fs.create(threadLocalFile);\r\n                        output.write(5);\r\n                        output.close();\r\n                        assertTrue(fs.exists(threadLocalFile));\r\n                        assertTrue(fs.listStatus(new Path(\"/\")).length > 0);\r\n                    } catch (Throwable ex) {\r\n                        exceptionsEncountered.add(ex);\r\n                    }\r\n                }\r\n            });\r\n        }\r\n        for (Thread t : threads) {\r\n            t.start();\r\n        }\r\n        for (Thread t : threads) {\r\n            t.join();\r\n        }\r\n        assertTrue(\"Encountered exceptions: \" + StringUtils.join(\"\\r\\n\", selectToString(exceptionsEncountered)), exceptionsEncountered.isEmpty());\r\n        tearDown();\r\n        setUp();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    root = fs.getUri().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.createMock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreate",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreate() throws Exception\n{\r\n    Path testFile1 = new Path(root + \"/testFile1\");\r\n    assertTrue(fs.createNewFile(testFile1));\r\n    Path testFile2 = new Path(root + \"/testFile2:2\");\r\n    try {\r\n        fs.createNewFile(testFile2);\r\n        fail(\"Should've thrown.\");\r\n    } catch (IOException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRename",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRename() throws Exception\n{\r\n    Path testFile1 = new Path(root + \"/testFile1\");\r\n    assertTrue(fs.createNewFile(testFile1));\r\n    Path testFile2 = new Path(root + \"/testFile2\");\r\n    fs.rename(testFile1, testFile2);\r\n    assertTrue(!fs.exists(testFile1) && fs.exists(testFile2));\r\n    Path testFile3 = new Path(root + \"/testFile3:3\");\r\n    try {\r\n        fs.rename(testFile2, testFile3);\r\n        fail(\"Should've thrown.\");\r\n    } catch (IOException e) {\r\n    }\r\n    assertTrue(fs.exists(testFile2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMkdirs",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMkdirs() throws Exception\n{\r\n    Path testFolder1 = new Path(root + \"/testFolder1\");\r\n    assertTrue(fs.mkdirs(testFolder1));\r\n    Path testFolder2 = new Path(root + \"/testFolder2:2\");\r\n    try {\r\n        assertTrue(fs.mkdirs(testFolder2));\r\n        fail(\"Should've thrown.\");\r\n    } catch (IOException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testWasbFsck",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testWasbFsck() throws Exception\n{\r\n    Path testFolder1 = new Path(root + \"/testFolder1\");\r\n    assertTrue(fs.mkdirs(testFolder1));\r\n    Path testFolder2 = new Path(testFolder1, \"testFolder2\");\r\n    assertTrue(fs.mkdirs(testFolder2));\r\n    Path testFolder3 = new Path(testFolder1, \"testFolder3\");\r\n    assertTrue(fs.mkdirs(testFolder3));\r\n    Path testFile1 = new Path(testFolder2, \"testFile1\");\r\n    assertTrue(fs.createNewFile(testFile1));\r\n    Path testFile2 = new Path(testFolder1, \"testFile2\");\r\n    assertTrue(fs.createNewFile(testFile2));\r\n    assertFalse(runWasbFsck(testFolder1));\r\n    InMemoryBlockBlobStore backingStore = testAccount.getMockStorage().getBackingStore();\r\n    backingStore.setContent(AzureBlobStorageTestAccount.toMockUri(\"testFolder1/testFolder2/test2:2\"), new byte[] { 1, 2 }, new HashMap<String, String>(), false, 0);\r\n    assertTrue(runWasbFsck(testFolder1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "runWasbFsck",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean runWasbFsck(Path p) throws Exception\n{\r\n    WasbFsck fsck = new WasbFsck(fs.getConf());\r\n    fsck.setMockFileSystemForTesting(fs);\r\n    fsck.run(new String[] { p.toString() });\r\n    return fsck.getPathNameWarning();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testNoInitialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoInitialize() throws Exception\n{\r\n    intercept(AssertionError.class, new Callable<FileMetadata>() {\r\n\r\n        @Override\r\n        public FileMetadata call() throws Exception {\r\n            return new AzureNativeFileSystemStore().retrieveMetadata(\"foo\");\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessUnauthorizedPublicContainer",
  "errType" : [ "AzureException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAccessUnauthorizedPublicContainer() throws Exception\n{\r\n    final String container = \"nonExistentContainer\";\r\n    final String account = \"hopefullyNonExistentAccount\";\r\n    Path noAccessPath = new Path(\"wasb://\" + container + \"@\" + account + \"/someFile\");\r\n    NativeAzureFileSystem.suppressRetryPolicy();\r\n    try {\r\n        FileSystem.get(noAccessPath.toUri(), new Configuration()).open(noAccessPath);\r\n        assertTrue(\"Should've thrown.\", false);\r\n    } catch (AzureException ex) {\r\n        GenericTestUtils.assertExceptionContains(String.format(NO_ACCESS_TO_CONTAINER_MSG, account, container), ex);\r\n    } finally {\r\n        NativeAzureFileSystem.resumeRetryPolicy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessContainerWithWrongVersion",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAccessContainerWithWrongVersion() throws Exception\n{\r\n    AzureNativeFileSystemStore store = new AzureNativeFileSystemStore();\r\n    MockStorageInterface mockStorage = new MockStorageInterface();\r\n    store.setAzureStorageInteractionLayer(mockStorage);\r\n    try (FileSystem fs = new NativeAzureFileSystem(store)) {\r\n        Configuration conf = new Configuration();\r\n        AzureBlobStorageTestAccount.setMockAccountKey(conf);\r\n        HashMap<String, String> metadata = new HashMap<String, String>();\r\n        metadata.put(AzureNativeFileSystemStore.VERSION_METADATA_KEY, \"2090-04-05\");\r\n        mockStorage.addPreExistingContainer(AzureBlobStorageTestAccount.getMockContainerUri(), metadata);\r\n        AzureException ex = intercept(AzureException.class, new Callable<FileStatus[]>() {\r\n\r\n            @Override\r\n            public FileStatus[] call() throws Exception {\r\n                fs.initialize(new URI(AzureBlobStorageTestAccount.MOCK_WASB_URI), conf);\r\n                return fs.listStatus(new Path(\"/\"));\r\n            }\r\n        });\r\n        GenericTestUtils.assertExceptionContains(\"unsupported version: 2090-04-05.\", ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "injectTransientError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void injectTransientError(NativeAzureFileSystem fs, final ConnectionRecognizer connectionRecognizer)\n{\r\n    fs.getStore().addTestHookToOperationContext(new TestHookOperationContext() {\r\n\r\n        @Override\r\n        public OperationContext modifyOperationContext(OperationContext original) {\r\n            original.getSendingRequestEventHandler().addListener(new TransientErrorInjector(connectionRecognizer));\r\n            return original;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testTransientErrorOnDelete",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTransientErrorOnDelete() throws Exception\n{\r\n    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.create();\r\n    assumeNotNull(testAccount);\r\n    try {\r\n        NativeAzureFileSystem fs = testAccount.getFileSystem();\r\n        injectTransientError(fs, new ConnectionRecognizer() {\r\n\r\n            @Override\r\n            public boolean isTargetConnection(HttpURLConnection connection) {\r\n                return connection.getRequestMethod().equals(\"DELETE\");\r\n            }\r\n        });\r\n        Path testFile = new Path(\"/a/b\");\r\n        assertTrue(fs.createNewFile(testFile));\r\n        assertTrue(fs.rename(testFile, new Path(\"/x\")));\r\n    } finally {\r\n        testAccount.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeAllThreeFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeAllThreeFile(NativeAzureFileSystem fs, Path testFile) throws IOException\n{\r\n    byte[] buffer = new byte[ALL_THREE_FILE_SIZE];\r\n    Arrays.fill(buffer, (byte) 3);\r\n    try (OutputStream stream = fs.create(testFile)) {\r\n        stream.write(buffer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "readAllThreeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readAllThreeFile(NativeAzureFileSystem fs, Path testFile) throws IOException\n{\r\n    byte[] buffer = new byte[ALL_THREE_FILE_SIZE];\r\n    InputStream inStream = fs.open(testFile);\r\n    assertEquals(buffer.length, inStream.read(buffer, 0, buffer.length));\r\n    inStream.close();\r\n    for (int i = 0; i < buffer.length; i++) {\r\n        assertEquals(3, buffer[i]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testTransientErrorOnCommitBlockList",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTransientErrorOnCommitBlockList() throws Exception\n{\r\n    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.create();\r\n    assumeNotNull(testAccount);\r\n    try {\r\n        NativeAzureFileSystem fs = testAccount.getFileSystem();\r\n        injectTransientError(fs, new ConnectionRecognizer() {\r\n\r\n            @Override\r\n            public boolean isTargetConnection(HttpURLConnection connection) {\r\n                return connection.getRequestMethod().equals(\"PUT\") && connection.getURL().getQuery() != null && connection.getURL().getQuery().contains(\"blocklist\");\r\n            }\r\n        });\r\n        Path testFile = new Path(\"/a/b\");\r\n        writeAllThreeFile(fs, testFile);\r\n        readAllThreeFile(fs, testFile);\r\n    } finally {\r\n        testAccount.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testTransientErrorOnRead",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTransientErrorOnRead() throws Exception\n{\r\n    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.create();\r\n    assumeNotNull(testAccount);\r\n    try {\r\n        NativeAzureFileSystem fs = testAccount.getFileSystem();\r\n        Path testFile = new Path(\"/a/b\");\r\n        writeAllThreeFile(fs, testFile);\r\n        injectTransientError(fs, new ConnectionRecognizer() {\r\n\r\n            @Override\r\n            public boolean isTargetConnection(HttpURLConnection connection) {\r\n                return connection.getRequestMethod().equals(\"GET\");\r\n            }\r\n        });\r\n        readAllThreeFile(fs, testFile);\r\n    } finally {\r\n        testAccount.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    testAccount = AzureBlobStorageTestAccount.createMock();\r\n    fs = testAccount.getFileSystem();\r\n    backingStore = testAccount.getMockStorage().getBackingStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount.cleanup();\r\n    fs = null;\r\n    backingStore = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getExpectedOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getExpectedOwner() throws Exception\n{\r\n    return UserGroupInformation.getCurrentUser().getShortUserName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getExpectedPermissionString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getExpectedPermissionString(String permissionString) throws Exception\n{\r\n    return String.format(\"{\\\"owner\\\":\\\"%s\\\",\\\"group\\\":\\\"%s\\\",\\\"permissions\\\":\\\"%s\\\"}\", getExpectedOwner(), NativeAzureFileSystem.AZURE_DEFAULT_GROUP_DEFAULT, permissionString);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testContainerVersionMetadata",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testContainerVersionMetadata() throws Exception\n{\r\n    fs.createNewFile(new Path(\"/foo\"));\r\n    HashMap<String, String> containerMetadata = backingStore.getContainerMetadata();\r\n    assertNotNull(containerMetadata);\r\n    assertEquals(AzureNativeFileSystemStore.CURRENT_WASB_VERSION, containerMetadata.get(AzureNativeFileSystemStore.VERSION_METADATA_KEY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPreExistingContainerVersionMetadata",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPreExistingContainerVersionMetadata() throws Exception\n{\r\n    FsWithPreExistingContainer fsWithContainer = FsWithPreExistingContainer.create();\r\n    assertFalse(fsWithContainer.getFs().exists(new Path(\"/IDontExist\")));\r\n    assertEquals(0, fsWithContainer.getFs().listStatus(new Path(\"/\")).length);\r\n    assertNull(fsWithContainer.getContainerMetadata());\r\n    fsWithContainer.getFs().mkdirs(new Path(\"/dir\"));\r\n    assertNotNull(fsWithContainer.getContainerMetadata());\r\n    assertEquals(AzureNativeFileSystemStore.CURRENT_WASB_VERSION, fsWithContainer.getContainerMetadata().get(AzureNativeFileSystemStore.VERSION_METADATA_KEY));\r\n    fsWithContainer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFirstContainerVersionMetadata",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFirstContainerVersionMetadata() throws Exception\n{\r\n    HashMap<String, String> containerMetadata = new HashMap<String, String>();\r\n    containerMetadata.put(AzureNativeFileSystemStore.OLD_VERSION_METADATA_KEY, AzureNativeFileSystemStore.FIRST_WASB_VERSION);\r\n    FsWithPreExistingContainer fsWithContainer = FsWithPreExistingContainer.create(containerMetadata);\r\n    assertFalse(fsWithContainer.getFs().exists(new Path(\"/IDontExist\")));\r\n    assertEquals(0, fsWithContainer.getFs().listStatus(new Path(\"/\")).length);\r\n    assertEquals(AzureNativeFileSystemStore.FIRST_WASB_VERSION, fsWithContainer.getContainerMetadata().get(AzureNativeFileSystemStore.OLD_VERSION_METADATA_KEY));\r\n    assertNull(fsWithContainer.getContainerMetadata().get(AzureNativeFileSystemStore.VERSION_METADATA_KEY));\r\n    fsWithContainer.getFs().mkdirs(new Path(\"/dir\"));\r\n    assertEquals(AzureNativeFileSystemStore.CURRENT_WASB_VERSION, fsWithContainer.getContainerMetadata().get(AzureNativeFileSystemStore.VERSION_METADATA_KEY));\r\n    assertNull(fsWithContainer.getContainerMetadata().get(AzureNativeFileSystemStore.OLD_VERSION_METADATA_KEY));\r\n    fsWithContainer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPermissionMetadata",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testPermissionMetadata() throws Exception\n{\r\n    FsPermission justMe = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\r\n    Path selfishFile = new Path(\"/noOneElse\");\r\n    fs.create(selfishFile, justMe, true, 4096, fs.getDefaultReplication(), fs.getDefaultBlockSize(), null).close();\r\n    String mockUri = AzureBlobStorageTestAccount.toMockUri(selfishFile);\r\n    assertNotNull(\"converted URI\", mockUri);\r\n    HashMap<String, String> metadata = backingStore.getMetadata(mockUri);\r\n    assertNotNull(metadata);\r\n    String storedPermission = metadata.get(\"hdi_permission\");\r\n    assertEquals(getExpectedPermissionString(\"rw-------\"), storedPermission);\r\n    FileStatus retrievedStatus = fs.getFileStatus(selfishFile);\r\n    assertNotNull(retrievedStatus);\r\n    assertEquals(justMe, retrievedStatus.getPermission());\r\n    assertEquals(getExpectedOwner(), retrievedStatus.getOwner());\r\n    assertEquals(NativeAzureFileSystem.AZURE_DEFAULT_GROUP_DEFAULT, retrievedStatus.getGroup());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testOldPermissionMetadata",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testOldPermissionMetadata() throws Exception\n{\r\n    Path selfishFile = new Path(\"/noOneElse\");\r\n    HashMap<String, String> metadata = new HashMap<String, String>();\r\n    metadata.put(\"asv_permission\", getExpectedPermissionString(\"rw-------\"));\r\n    backingStore.setContent(AzureBlobStorageTestAccount.toMockUri(selfishFile), new byte[] {}, metadata, false, 0);\r\n    FsPermission justMe = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\r\n    FileStatus retrievedStatus = fs.getFileStatus(selfishFile);\r\n    assertNotNull(retrievedStatus);\r\n    assertEquals(justMe, retrievedStatus.getPermission());\r\n    assertEquals(getExpectedOwner(), retrievedStatus.getOwner());\r\n    assertEquals(NativeAzureFileSystem.AZURE_DEFAULT_GROUP_DEFAULT, retrievedStatus.getGroup());\r\n    FsPermission meAndYou = new FsPermission(FsAction.READ_WRITE, FsAction.READ_WRITE, FsAction.NONE);\r\n    fs.setPermission(selfishFile, meAndYou);\r\n    metadata = backingStore.getMetadata(AzureBlobStorageTestAccount.toMockUri(selfishFile));\r\n    assertNotNull(metadata);\r\n    String storedPermission = metadata.get(\"hdi_permission\");\r\n    assertEquals(getExpectedPermissionString(\"rw-rw----\"), storedPermission);\r\n    assertNull(metadata.get(\"asv_permission\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFolderMetadata",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFolderMetadata() throws Exception\n{\r\n    Path folder = new Path(\"/folder\");\r\n    FsPermission justRead = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\r\n    fs.mkdirs(folder, justRead);\r\n    HashMap<String, String> metadata = backingStore.getMetadata(AzureBlobStorageTestAccount.toMockUri(folder));\r\n    assertNotNull(metadata);\r\n    assertEquals(\"true\", metadata.get(\"hdi_isfolder\"));\r\n    assertEquals(getExpectedPermissionString(\"r--r--r--\"), metadata.get(\"hdi_permission\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.set(NativeAzureFileSystem.KEY_AZURE_AUTHORIZATION, \"true\");\r\n    conf.set(KEY_USE_SECURE_MODE, \"true\");\r\n    conf.set(RemoteWasbAuthorizerImpl.KEY_REMOTE_AUTH_SERVICE_URLS, \"http://localhost/\");\r\n    conf.set(NativeAzureFileSystem.AZURE_CHOWN_USERLIST_PROPERTY_NAME, \"user1 , user2\");\r\n    conf.set(KEY_AUTH_SERVICE_CACHING_ENABLE, \"false\");\r\n    conf.set(NativeAzureFileSystem.AZURE_CHMOD_USERLIST_PROPERTY_NAME, \"user1 , user2\");\r\n    conf.set(NativeAzureFileSystem.AZURE_DAEMON_USERLIST_PROPERTY_NAME, \"hive , hcs , yarn\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create(createConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    boolean useSecureMode = fs.getConf().getBoolean(KEY_USE_SECURE_MODE, false);\r\n    boolean useAuthorization = fs.getConf().getBoolean(NativeAzureFileSystem.KEY_AZURE_AUTHORIZATION, false);\r\n    Assume.assumeTrue(\"Test valid when both SecureMode and Authorization are enabled .. skipping\", useSecureMode && useAuthorization);\r\n    authorizer = new MockWasbAuthorizerImpl(fs);\r\n    authorizer.init(fs.getConf());\r\n    fs.updateWasbAuthorizer(authorizer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "allowRecursiveDelete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void allowRecursiveDelete(NativeAzureFileSystem fs, String path) throws IOException\n{\r\n    int index = path.lastIndexOf('/');\r\n    String parent = (index == 0) ? \"/\" : path.substring(0, index);\r\n    authorizer.deleteAllAuthRules();\r\n    authorizer.addAuthRule(parent, WRITE, getCurrentUserShortName(), true);\r\n    authorizer.addAuthRule((path.endsWith(\"*\") ? path : path + \"*\"), WRITE, getCurrentUserShortName(), true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setExpectedFailureMessage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setExpectedFailureMessage(String operation, Path path)\n{\r\n    expectedEx.expect(WasbAuthorizationException.class);\r\n    expectedEx.expectMessage(String.format(\"%s operation for Path : %s not allowed\", operation, path.makeQualified(fs.getUri(), fs.getWorkingDirectory())));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCurrentUserShortName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCurrentUserShortName() throws IOException\n{\r\n    return UserGroupInformation.getCurrentUser().getShortUserName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreateAccessWithoutCreateIntermediateFoldersCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateAccessWithoutCreateIntermediateFoldersCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreateAccessWithCreateIntermediateFoldersCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCreateAccessWithCreateIntermediateFoldersCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testCreateAccessCheckPositive/1/2/3\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, \"/testCreateAccessCheckPositive\");\r\n        fs.delete(new Path(\"/testCreateAccessCheckPositive\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreateAccessWithOverwriteCheckNegative",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateAccessWithOverwriteCheckNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    setExpectedFailureMessage(\"create\", testPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n        fs.create(testPath, true);\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreateAccessWithOverwriteCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateAccessWithOverwriteCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n        fs.create(testPath, true);\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCreateAccessCheckNegative",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCreateAccessCheckNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testCreateAccessCheckNegative\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    setExpectedFailureMessage(\"create\", testPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, false);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testListAccessCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testListAccessCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testListAccessCheckPositive\");\r\n    Path intermediateFolders = new Path(parentDir, \"1/2/3/\");\r\n    Path testPath = new Path(intermediateFolders, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        fs.listStatus(testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testListAccessCheckNegative",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testListAccessCheckNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testListAccessCheckNegative\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    setExpectedFailureMessage(\"liststatus\", testPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, false);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        fs.listStatus(testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameAccessCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRenameAccessCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testRenameAccessCheckPositive\");\r\n    Path srcPath = new Path(parentDir, \"test1.dat\");\r\n    Path dstPath = new Path(parentDir, \"test2.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        touch(fs, srcPath);\r\n        assertPathExists(fs, \"sourcePath does not exist\", srcPath);\r\n        assertRenameOutcome(fs, srcPath, dstPath, true);\r\n        assertPathExists(fs, \"destPath does not exist\", dstPath);\r\n        assertPathDoesNotExist(fs, \"sourcePath exists after rename!\", srcPath);\r\n    } finally {\r\n        recursiveDelete(parentDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameAccessCheckNegative",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameAccessCheckNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testRenameAccessCheckNegative\");\r\n    Path srcPath = new Path(parentDir, \"test1.dat\");\r\n    Path dstPath = new Path(parentDir, \"test2.dat\");\r\n    setExpectedFailureMessage(\"rename\", srcPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, false);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(srcPath);\r\n        ContractTestUtils.assertPathExists(fs, \"sourcePath does not exist\", srcPath);\r\n        fs.rename(srcPath, dstPath);\r\n        ContractTestUtils.assertPathExists(fs, \"destPath does not exist\", dstPath);\r\n    } finally {\r\n        ContractTestUtils.assertPathExists(fs, \"sourcePath does not exist after rename failure!\", srcPath);\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameAccessCheckNegativeOnDstFolder",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameAccessCheckNegativeOnDstFolder() throws Throwable\n{\r\n    Path parentSrcDir = new Path(\"/testRenameAccessCheckNegativeSrc\");\r\n    Path srcPath = new Path(parentSrcDir, \"test1.dat\");\r\n    Path parentDstDir = new Path(\"/testRenameAccessCheckNegativeDst\");\r\n    Path dstPath = new Path(parentDstDir, \"test2.dat\");\r\n    setExpectedFailureMessage(\"rename\", dstPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDstDir.toString(), WRITE, false);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        touch(fs, srcPath);\r\n        ContractTestUtils.assertPathExists(fs, \"sourcePath does not exist\", srcPath);\r\n        fs.mkdirs(parentDstDir);\r\n        fs.rename(srcPath, dstPath);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"destPath does not exist\", dstPath);\r\n    } finally {\r\n        ContractTestUtils.assertPathExists(fs, \"sourcePath does not exist after rename !\", srcPath);\r\n        recursiveDelete(parentSrcDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameAccessCheckPositiveOnDstFolder",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameAccessCheckPositiveOnDstFolder() throws Throwable\n{\r\n    Path parentSrcDir = new Path(\"/testRenameAccessCheckPositiveSrc\");\r\n    Path srcPath = new Path(parentSrcDir, \"test1.dat\");\r\n    Path parentDstDir = new Path(\"/testRenameAccessCheckPositiveDst\");\r\n    Path dstPath = new Path(parentDstDir, \"test2.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDstDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        touch(fs, srcPath);\r\n        ContractTestUtils.assertPathExists(fs, \"sourcePath does not exist\", srcPath);\r\n        fs.mkdirs(parentDstDir);\r\n        assertRenameOutcome(fs, srcPath, dstPath, true);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"sourcePath does not exist\", srcPath);\r\n        ContractTestUtils.assertPathExists(fs, \"destPath does not exist\", dstPath);\r\n    } finally {\r\n        recursiveDelete(parentSrcDir);\r\n        recursiveDelete(parentDstDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "recursiveDelete",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void recursiveDelete(Path path)\n{\r\n    try {\r\n        allowRecursiveDelete(fs, path.toString());\r\n        fs.delete(path, true);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Failed to delete {}\", path, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenamePositiveWhenDestinationFolderExists",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testRenamePositiveWhenDestinationFolderExists() throws Throwable\n{\r\n    Path parentSrcDir = new Path(\"/testRenamePositiveForFolderSrc\");\r\n    Path srcFilePath = new Path(parentSrcDir, \"test1.dat\");\r\n    Path srcFolderPath = new Path(parentSrcDir, \"testFolder\");\r\n    Path dstDir = new Path(\"/testRenamePositiveForFolderDst\");\r\n    Path finalDstDir = new Path(dstDir, \"testRenamePositiveForFolderSrc\");\r\n    Path dstFilePath = new Path(finalDstDir, \"test1.dat\");\r\n    Path dstFolderPath = new Path(finalDstDir, \"testFolder\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(dstDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/\", READ, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), READ, true);\r\n    authorizer.addAuthRuleForOwner(finalDstDir.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        touch(fs, srcFilePath);\r\n        assertPathExists(fs, \"srcFilePath does not exist\", srcFilePath);\r\n        fs.mkdirs(srcFolderPath);\r\n        assertIsDirectory(fs, srcFolderPath);\r\n        fs.mkdirs(dstDir);\r\n        assertIsDirectory(fs, dstDir);\r\n        assertRenameOutcome(fs, parentSrcDir, dstDir, true);\r\n        assertPathDoesNotExist(fs, \"parentSrcDir exists\", parentSrcDir);\r\n        assertPathDoesNotExist(fs, \"srcFilePath exists\", srcFilePath);\r\n        assertPathDoesNotExist(fs, \"srcFolderPath exists\", srcFolderPath);\r\n        assertPathExists(fs, \"destPath does not exist\", dstDir);\r\n        assertPathExists(fs, \"dstFilePath does not exist\", dstFilePath);\r\n        assertPathExists(fs, \"dstFolderPath does not exist\", dstFolderPath);\r\n    } finally {\r\n        recursiveDelete(parentSrcDir);\r\n        recursiveDelete(dstDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenamePositiveWhenDestinationFolderDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testRenamePositiveWhenDestinationFolderDoesNotExist() throws Throwable\n{\r\n    Path srcParentDir = new Path(\"/testRenamePositiveWhenDestinationFolderDoesNotExist\");\r\n    Path srcDir = new Path(srcParentDir, \"srcDir\");\r\n    Path srcFilePath = new Path(srcDir, \"test1.dat\");\r\n    Path srcSubDirPath = new Path(srcDir, \"testFolder\");\r\n    Path srcSubDirFilePath = new Path(srcSubDirPath, \"test2.dat\");\r\n    Path dstDir = new Path(srcParentDir, \"dstDir\");\r\n    Path dstFilePath = new Path(dstDir, \"test1.dat\");\r\n    Path dstSubDirPath = new Path(dstDir, \"testFolder\");\r\n    Path dstSubDirFilePath = new Path(dstSubDirPath, \"test2.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(srcParentDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(srcDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(srcSubDirPath.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/\", READ, true);\r\n    authorizer.addAuthRuleForOwner(srcParentDir.toString(), READ, true);\r\n    authorizer.addAuthRuleForOwner(srcDir.toString(), READ, true);\r\n    authorizer.addAuthRuleForOwner(srcSubDirPath.toString(), READ, true);\r\n    authorizer.addAuthRuleForOwner(dstDir.toString(), READ, true);\r\n    authorizer.addAuthRuleForOwner(dstSubDirPath.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        touch(fs, srcFilePath);\r\n        assertPathExists(fs, \"srcFilePath does not exist\", srcFilePath);\r\n        fs.mkdirs(srcSubDirPath);\r\n        assertIsDirectory(fs, srcSubDirPath);\r\n        touch(fs, srcSubDirFilePath);\r\n        assertPathExists(fs, \"srcSubDirFilePath does not exist\", srcSubDirFilePath);\r\n        assertRenameOutcome(fs, srcDir, dstDir, true);\r\n        assertPathDoesNotExist(fs, \"srcDir exists\", srcDir);\r\n        assertPathDoesNotExist(fs, \"srcFilePath exists\", srcFilePath);\r\n        assertPathDoesNotExist(fs, \"srcSubDirPath exists\", srcSubDirPath);\r\n        assertPathDoesNotExist(fs, \"srcSubDirFilePath exists\", srcSubDirFilePath);\r\n        assertPathExists(fs, \"destPath does not exist\", dstDir);\r\n        assertPathExists(fs, \"dstFilePath does not exist\", dstFilePath);\r\n        assertPathExists(fs, \"dstSubDirPath does not exist\", dstSubDirPath);\r\n        assertPathExists(fs, \"dstSubDirFilePath does not exist\", dstSubDirFilePath);\r\n    } finally {\r\n        recursiveDelete(srcParentDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameOnNonExistentSource",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRenameOnNonExistentSource() throws Throwable\n{\r\n    Path parentSrcDir = new Path(\"/testRenameOnNonExistentSourceFolderSrc\");\r\n    Path srcPath = new Path(parentSrcDir, \"test1.dat\");\r\n    Path parentDstDir = new Path(\"/testRenameOnNonExistentSourceFolderDst\");\r\n    Path dstPath = new Path(parentDstDir, \"test2.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDstDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/\", READ, true);\r\n    authorizer.addAuthRuleForOwner(parentDstDir.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(parentSrcDir);\r\n        assertIsDirectory(fs, parentSrcDir);\r\n        fs.mkdirs(parentDstDir);\r\n        assertRenameOutcome(fs, srcPath, dstPath, false);\r\n        assertPathDoesNotExist(fs, \"destPath exists!\", dstPath);\r\n    } finally {\r\n        recursiveDelete(parentSrcDir);\r\n        recursiveDelete(parentDstDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameWithStickyBitPositive",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRenameWithStickyBitPositive() throws Throwable\n{\r\n    Path parentSrcDir = new Path(\"/testRenameWithStickyBitPositiveSrc\");\r\n    Path srcPath = new Path(parentSrcDir, \"test1.dat\");\r\n    Path parentDstDir = new Path(\"/testRenameWithStickyBitPositiveDst\");\r\n    Path dstPath = new Path(parentDstDir, \"test2.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDstDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        touch(fs, srcPath);\r\n        assertPathExists(fs, \"sourcePath does not exist\", srcPath);\r\n        fs.mkdirs(parentDstDir);\r\n        assertIsDirectory(fs, parentDstDir);\r\n        fs.setPermission(parentSrcDir, new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        assertRenameOutcome(fs, srcPath, dstPath, true);\r\n        assertPathDoesNotExist(fs, \"sourcePath exists\", srcPath);\r\n        assertPathExists(fs, \"destPath does not exist\", dstPath);\r\n    } finally {\r\n        recursiveDelete(parentSrcDir);\r\n        recursiveDelete(parentDstDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameWithStickyBitNegative",
  "errType" : [ "WasbAuthorizationException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRenameWithStickyBitNegative() throws Throwable\n{\r\n    final Path parentSrcDir = new Path(\"/testRenameWithStickyBitNegativeSrc\");\r\n    final Path srcPath = new Path(parentSrcDir, \"test1.dat\");\r\n    final Path parentDstDir = new Path(\"/testRenameWithStickyBitNegativeDst\");\r\n    final Path dstPath = new Path(parentDstDir, \"test2.dat\");\r\n    expectedEx.expect(WasbAuthorizationException.class);\r\n    expectedEx.expectMessage(String.format(\"Rename operation for %s is not permitted.\" + \" Details : Stickybit check failed.\", srcPath.toString()));\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        touch(fs, srcPath);\r\n        assertPathExists(fs, \"sourcePath does not exist\", srcPath);\r\n        fs.mkdirs(parentDstDir);\r\n        assertIsDirectory(fs, parentDstDir);\r\n        fs.setPermission(parentSrcDir, new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        UserGroupInformation dummyUser = UserGroupInformation.createUserForTesting(\"dummyUser\", new String[] { \"dummygroup\" });\r\n        dummyUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                authorizer.addAuthRule(parentSrcDir.toString(), WRITE, getCurrentUserShortName(), true);\r\n                authorizer.addAuthRule(parentDstDir.toString(), WRITE, getCurrentUserShortName(), true);\r\n                try {\r\n                    fs.rename(srcPath, dstPath);\r\n                } catch (WasbAuthorizationException wae) {\r\n                    assertPathExists(fs, \"sourcePath does not exist\", srcPath);\r\n                    assertPathDoesNotExist(fs, \"destPath exists\", dstPath);\r\n                    throw wae;\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        recursiveDelete(parentSrcDir);\r\n        recursiveDelete(parentDstDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameOnNonExistentSourceWithStickyBit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameOnNonExistentSourceWithStickyBit() throws Throwable\n{\r\n    final Path parentSrcDir = new Path(\"/testRenameOnNonExistentSourceWithStickyBitSrc\");\r\n    final Path srcPath = new Path(parentSrcDir, \"test1.dat\");\r\n    final Path parentDstDir = new Path(\"/testRenameOnNonExistentSourceWithStickyBitDest\");\r\n    final Path dstPath = new Path(parentDstDir, \"test2.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentSrcDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(parentSrcDir);\r\n        assertIsDirectory(fs, parentSrcDir);\r\n        fs.mkdirs(parentDstDir);\r\n        assertIsDirectory(fs, parentDstDir);\r\n        fs.setPermission(parentSrcDir, new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        UserGroupInformation dummyUser = UserGroupInformation.createUserForTesting(\"dummyUser\", new String[] { \"dummygroup\" });\r\n        dummyUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                authorizer.addAuthRule(parentSrcDir.toString(), WRITE, getCurrentUserShortName(), true);\r\n                authorizer.addAuthRule(parentDstDir.toString(), WRITE, getCurrentUserShortName(), true);\r\n                assertRenameOutcome(fs, srcPath, dstPath, false);\r\n                assertPathDoesNotExist(fs, \"destPath exists\", dstPath);\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        recursiveDelete(parentSrcDir);\r\n        recursiveDelete(parentDstDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testReadAccessCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testReadAccessCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testReadAccessCheckPositive\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    FSDataInputStream inputStream = null;\r\n    FSDataOutputStream fso = null;\r\n    try {\r\n        fso = fs.create(testPath);\r\n        String data = \"Hello World\";\r\n        fso.writeBytes(data);\r\n        fso.close();\r\n        inputStream = fs.open(testPath);\r\n        ContractTestUtils.verifyRead(inputStream, data.getBytes(), 0, data.length());\r\n    } finally {\r\n        if (fso != null) {\r\n            fso.close();\r\n        }\r\n        if (inputStream != null) {\r\n            inputStream.close();\r\n        }\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testReadAccessCheckNegative",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReadAccessCheckNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testReadAccessCheckNegative\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    setExpectedFailureMessage(\"read\", testPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, false);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    FSDataInputStream inputStream = null;\r\n    FSDataOutputStream fso = null;\r\n    try {\r\n        fso = fs.create(testPath);\r\n        String data = \"Hello World\";\r\n        fso.writeBytes(data);\r\n        fso.close();\r\n        inputStream = fs.open(testPath);\r\n        ContractTestUtils.verifyRead(inputStream, data.getBytes(), 0, data.length());\r\n    } finally {\r\n        if (fso != null) {\r\n            fso.close();\r\n        }\r\n        if (inputStream != null) {\r\n            inputStream.close();\r\n        }\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileDeleteAccessCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFileDeleteAccessCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"testPath exists after deletion!\", testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileDeleteAccessCheckNegative",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testFileDeleteAccessCheckNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    setExpectedFailureMessage(\"delete\", testPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n        authorizer.deleteAllAuthRules();\r\n        authorizer.addAuthRuleForOwner(\"/\", WRITE, false);\r\n        fs.updateWasbAuthorizer(authorizer);\r\n        fs.delete(testPath, false);\r\n    } finally {\r\n        authorizer.deleteAllAuthRules();\r\n        authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n        fs.updateWasbAuthorizer(authorizer);\r\n        fs.delete(testPath, false);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"testPath exists after deletion!\", testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileDeleteAccessWithIntermediateFoldersCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFileDeleteAccessWithIntermediateFoldersCheckPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testDeleteIntermediateFolder\");\r\n    Path childPath = new Path(parentDir, \"1/2\");\r\n    Path testPath = new Path(childPath, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/testDeleteIntermediateFolder*\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n        fs.delete(parentDir, true);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"testPath exists after deletion!\", parentDir);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteAuthCheckFailureLeavesFilesUndeleted",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testDeleteAuthCheckFailureLeavesFilesUndeleted() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testDeleteAuthCheckFailureLeavesFilesUndeleted\");\r\n    Path childPath1 = new Path(parentDir, \"child1\");\r\n    Path childPath2 = new Path(parentDir, \"child2\");\r\n    Path testPath1 = new Path(childPath1, \"test.dat\");\r\n    Path testPath2 = new Path(childPath2, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/testDeleteAuthCheckFailureLeavesFilesUndeleted*\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath1);\r\n        fs.create(testPath2);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath1 was not created\", testPath1);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath2 was not created\", testPath2);\r\n        authorizer.deleteAllAuthRules();\r\n        authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n        authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, true);\r\n        authorizer.addAuthRuleForOwner(childPath2.toString(), WRITE, true);\r\n        authorizer.addAuthRuleForOwner(childPath1.toString(), WRITE, false);\r\n        assertFalse(fs.delete(parentDir, true));\r\n        ContractTestUtils.assertPathExists(fs, \"child1 is deleted!\", testPath1);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"child2 exists after deletion!\", testPath2);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"child2 exists after deletion!\", childPath2);\r\n        ContractTestUtils.assertPathExists(fs, \"parentDir is deleted!\", parentDir);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleFileDeleteWithStickyBitPositive",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSingleFileDeleteWithStickyBitPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testSingleFileDeleteWithStickyBitPositive\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n        fs.setPermission(parentDir, new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        assertTrue(fs.delete(testPath, true));\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"testPath exists after deletion!\", testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleFileDeleteWithStickyBitNegative",
  "errType" : [ "WasbAuthorizationException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSingleFileDeleteWithStickyBitNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testSingleFileDeleteWithStickyBitNegative\");\r\n    Path testPath = new Path(parentDir, \"test.dat\");\r\n    expectedEx.expect(WasbAuthorizationException.class);\r\n    expectedEx.expectMessage(String.format(\"%s has sticky bit set. File %s cannot be deleted.\", parentDir.toString(), testPath.toString()));\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath was not created\", testPath);\r\n        fs.setPermission(parentDir, new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        UserGroupInformation dummyUser = UserGroupInformation.createUserForTesting(\"dummyUser\", new String[] { \"dummygroup\" });\r\n        dummyUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                try {\r\n                    authorizer.addAuthRule(parentDir.toString(), WRITE, getCurrentUserShortName(), true);\r\n                    fs.delete(testPath, true);\r\n                    return null;\r\n                } catch (WasbAuthorizationException wae) {\r\n                    ContractTestUtils.assertPathExists(fs, \"testPath should not be deleted!\", testPath);\r\n                    throw wae;\r\n                }\r\n            }\r\n        });\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRecursiveDeleteSucceedsWithStickybit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRecursiveDeleteSucceedsWithStickybit() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testRecursiveDeleteSucceedsWithStickybit\");\r\n    Path childDir = new Path(parentDir, \"child\");\r\n    Path testFilePath = new Path(childDir, \"test.dat\");\r\n    Path testFolderPath = new Path(childDir, \"testDirectory\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/testRecursiveDeleteSucceedsWithStickybit*\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testFilePath);\r\n        ContractTestUtils.assertPathExists(fs, \"file was not created\", testFilePath);\r\n        fs.mkdirs(testFolderPath);\r\n        ContractTestUtils.assertPathExists(fs, \"folder was not created\", testFolderPath);\r\n        fs.setPermission(new Path(parentDir, \"child\"), new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        assertTrue(fs.delete(parentDir, true));\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"parentDir exists after deletion!\", parentDir);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRecursiveDeleteFailsWithStickybit",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testRecursiveDeleteFailsWithStickybit() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testRecursiveDeleteFailsWithStickybit\");\r\n    Path childDir = new Path(parentDir, \"child\");\r\n    Path testFilePath = new Path(childDir, \"test.dat\");\r\n    Path testFolderPath = new Path(childDir, \"testDirectory\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/testRecursiveDeleteFailsWithStickybit*\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testFilePath);\r\n        ContractTestUtils.assertPathExists(fs, \"file was not created\", testFilePath);\r\n        fs.mkdirs(testFolderPath);\r\n        ContractTestUtils.assertPathExists(fs, \"folder was not created\", testFolderPath);\r\n        fs.setPermission(new Path(parentDir, \"child\"), new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        UserGroupInformation dummyUser = UserGroupInformation.createUserForTesting(\"dummyUser\", new String[] { \"dummygroup\" });\r\n        dummyUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                authorizer.addAuthRule(\"/\", WRITE, getCurrentUserShortName(), true);\r\n                authorizer.addAuthRule(\"/testRecursiveDeleteFailsWithStickybit*\", WRITE, getCurrentUserShortName(), true);\r\n                assertFalse(fs.delete(parentDir, true));\r\n                return null;\r\n            }\r\n        });\r\n        ContractTestUtils.assertPathExists(fs, \"parentDir is deleted!\", parentDir);\r\n        ContractTestUtils.assertPathExists(fs, \"file is deleted!\", testFilePath);\r\n        ContractTestUtils.assertPathExists(fs, \"folder is deleted!\", testFolderPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteSucceedsForOnlyFilesOwnedByUserWithStickybitSet",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDeleteSucceedsForOnlyFilesOwnedByUserWithStickybitSet() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testDeleteSucceedsForOnlyFilesOwnedByUserWithStickybitSet\");\r\n    Path testFilePath = new Path(parentDir, \"test.dat\");\r\n    Path testFolderPath = new Path(parentDir, \"testDirectory\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/testDeleteSucceedsForOnlyFilesOwnedByUserWithStickybitSet*\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testFilePath);\r\n        ContractTestUtils.assertPathExists(fs, \"file was not created\", testFilePath);\r\n        fs.setPermission(parentDir, new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n        UserGroupInformation dummyUser = UserGroupInformation.createUserForTesting(\"dummyuser\", new String[] { \"dummygroup\" });\r\n        dummyUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                authorizer.addAuthRule(\"/\", WRITE, getCurrentUserShortName(), true);\r\n                authorizer.addAuthRule(\"/testDeleteSucceedsForOnlyFilesOwnedByUserWithStickybitSet*\", WRITE, getCurrentUserShortName(), true);\r\n                fs.create(testFolderPath);\r\n                ContractTestUtils.assertPathExists(fs, \"folder was not created\", testFolderPath);\r\n                assertFalse(fs.delete(parentDir, true));\r\n                ContractTestUtils.assertPathDoesNotExist(fs, \"folder should have been deleted!\", testFolderPath);\r\n                ContractTestUtils.assertPathExists(fs, \"parentDir is deleted!\", parentDir);\r\n                ContractTestUtils.assertPathExists(fs, \"file is deleted!\", testFilePath);\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteSucceedsForParentDirectoryOwnerUserWithStickybit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDeleteSucceedsForParentDirectoryOwnerUserWithStickybit() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testDeleteSucceedsForParentDirectoryOwnerUserWithStickybit\");\r\n    Path testFilePath = new Path(parentDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/testDeleteSucceedsForParentDirectoryOwnerUserWithStickybit*\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(parentDir);\r\n        ContractTestUtils.assertPathExists(fs, \"folder was not created\", parentDir);\r\n        UserGroupInformation dummyUser = UserGroupInformation.createUserForTesting(\"user1\", new String[] { \"dummygroup\" });\r\n        dummyUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                authorizer.addAuthRule(parentDir.toString(), WRITE, getCurrentUserShortName(), true);\r\n                fs.create(testFilePath);\r\n                ContractTestUtils.assertPathExists(fs, \"file was not created\", testFilePath);\r\n                fs.setPermission(parentDir, new FsPermission(STICKYBIT_PERMISSION_CONSTANT));\r\n                return null;\r\n            }\r\n        });\r\n        assertTrue(fs.delete(parentDir, true));\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"parentDir is not deleted!\", parentDir);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"file is not deleted!\", testFilePath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteScenarioForRoot",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDeleteScenarioForRoot() throws Throwable\n{\r\n    Path rootPath = new Path(\"/\");\r\n    Path parentDir = new Path(\"/testDeleteScenarioForRoot\");\r\n    Path childPath1 = new Path(parentDir, \"child1\");\r\n    Path childPath2 = new Path(parentDir, \"child2\");\r\n    Path testPath1 = new Path(childPath1, \"test.dat\");\r\n    Path testPath2 = new Path(childPath2, \"testFolder\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(\"/testDeleteScenarioForRoot*\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(testPath1);\r\n        fs.create(testPath2);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath1 was not created\", testPath1);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath2 was not created\", testPath2);\r\n        assertFalse(fs.delete(rootPath, true));\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"file exists after deletion!\", testPath1);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"folder exists after deletion!\", testPath2);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"parentDir exists after deletion!\", parentDir);\r\n        ContractTestUtils.assertPathExists(fs, \"Root should not have been deleted!\", rootPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testGetFileStatusPositive",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetFileStatusPositive() throws Throwable\n{\r\n    Path testPath = new Path(\"/\");\r\n    authorizer.addAuthRuleForOwner(\"/\", READ, true);\r\n    ContractTestUtils.assertIsDirectory(fs, testPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMkdirsCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMkdirsCheckPositive() throws Throwable\n{\r\n    Path testPath = new Path(\"/testMkdirsAccessCheckPositive/1/2/3\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertIsDirectory(fs, testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, \"/testMkdirsAccessCheckPositive\");\r\n        fs.delete(new Path(\"/testMkdirsAccessCheckPositive\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMkdirsWithExistingHierarchyCheckPositive1",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMkdirsWithExistingHierarchyCheckPositive1() throws Throwable\n{\r\n    Path testPath = new Path(\"/testMkdirsWithExistingHierarchyCheckPositive1\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertIsDirectory(fs, testPath);\r\n        authorizer.deleteAllAuthRules();\r\n        authorizer.addAuthRuleForOwner(testPath.getParent().toString(), READ, true);\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertIsDirectory(fs, testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, testPath.toString());\r\n        fs.delete(testPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMkdirsWithExistingHierarchyCheckPositive2",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testMkdirsWithExistingHierarchyCheckPositive2() throws Throwable\n{\r\n    Path testPath = new Path(\"/testMkdirsWithExistingHierarchyCheckPositive2\");\r\n    Path childPath1 = new Path(testPath, \"1\");\r\n    Path childPath2 = new Path(childPath1, \"2\");\r\n    Path childPath3 = new Path(childPath2, \"3\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(childPath1.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(childPath1);\r\n        ContractTestUtils.assertIsDirectory(fs, childPath1);\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertIsDirectory(fs, testPath);\r\n        fs.mkdirs(childPath1);\r\n        ContractTestUtils.assertIsDirectory(fs, childPath1);\r\n        fs.mkdirs(childPath3);\r\n        ContractTestUtils.assertIsDirectory(fs, childPath3);\r\n    } finally {\r\n        allowRecursiveDelete(fs, testPath.toString());\r\n        fs.delete(testPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMkdirsCheckNegative",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMkdirsCheckNegative() throws Throwable\n{\r\n    Path testPath = new Path(\"/testMkdirsAccessCheckNegative/1/2/3\");\r\n    setExpectedFailureMessage(\"mkdirs\", testPath);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, false);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"testPath was not created\", testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, \"/testMkdirsAccessCheckNegative\");\r\n        fs.delete(new Path(\"/testMkdirsAccessCheckNegative\"), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testListStatusWithTripleSlashCheckPositive",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testListStatusWithTripleSlashCheckPositive() throws Throwable\n{\r\n    Path testPath = new Path(\"/\");\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    Path testPathWithTripleSlash = new Path(\"wasb:///\" + testPath);\r\n    fs.listStatus(testPathWithTripleSlash);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testOwnerPermissionPositive",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testOwnerPermissionPositive() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testOwnerPermissionPositive\");\r\n    Path testPath = new Path(parentDir, \"test.data\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(parentDir);\r\n        ContractTestUtils.assertPathExists(fs, \"parentDir does not exist\", parentDir);\r\n        fs.create(testPath);\r\n        fs.getFileStatus(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"testPath does not exist\", testPath);\r\n        fs.delete(parentDir, true);\r\n        ContractTestUtils.assertPathDoesNotExist(fs, \"testPath does not exist\", testPath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testOwnerPermissionNegative",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testOwnerPermissionNegative() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testOwnerPermissionNegative\");\r\n    Path childDir = new Path(parentDir, \"childDir\");\r\n    setExpectedFailureMessage(\"mkdirs\", childDir);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(parentDir);\r\n        UserGroupInformation ugiSuperUser = UserGroupInformation.createUserForTesting(\"testuser\", new String[] {});\r\n        ugiSuperUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                fs.mkdirs(childDir);\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        allowRecursiveDelete(fs, parentDir.toString());\r\n        fs.delete(parentDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRetrievingOwnerDoesNotFailWhenFileDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRetrievingOwnerDoesNotFailWhenFileDoesNotExist() throws Throwable\n{\r\n    Path testdirectory = new Path(\"/testDirectory123454565\");\r\n    String owner = fs.getOwnerForPath(testdirectory);\r\n    assertEquals(\"\", owner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetOwnerThrowsForUnauthorisedUsers",
  "errType" : [ "WasbAuthorizationException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetOwnerThrowsForUnauthorisedUsers() throws Throwable\n{\r\n    Path testPath = new Path(\"/testSetOwnerNegative\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    final String owner;\r\n    UserGroupInformation unauthorisedUser = UserGroupInformation.createUserForTesting(\"unauthoriseduser\", new String[] { \"group1\" });\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"test path does not exist\", testPath);\r\n        owner = fs.getFileStatus(testPath).getOwner();\r\n        unauthorisedUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                try {\r\n                    fs.setOwner(testPath, \"newowner\", null);\r\n                    fail(\"Failing test because setOwner call was expected to throw\");\r\n                } catch (WasbAuthorizationException wex) {\r\n                    assertOwnerEquals(testPath, owner);\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetOwnerSucceedsForAuthorisedUsers",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSetOwnerSucceedsForAuthorisedUsers() throws Throwable\n{\r\n    Path testPath = new Path(\"/testSetOwnerPositive\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    String newOwner = \"user2\";\r\n    String newGroup = \"newgroup\";\r\n    UserGroupInformation authorisedUser = UserGroupInformation.createUserForTesting(\"user2\", new String[] { \"group1\" });\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"test path does not exist\", testPath);\r\n        String owner = fs.getFileStatus(testPath).getOwner();\r\n        Assume.assumeTrue(\"changing owner requires original and new owner to be different\", !StringUtils.equalsIgnoreCase(owner, newOwner));\r\n        authorisedUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                fs.setOwner(testPath, newOwner, newGroup);\r\n                assertOwnerEquals(testPath, newOwner);\r\n                assertEquals(newGroup, fs.getFileStatus(testPath).getGroup());\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetOwnerSucceedsForAnyUserWhenWildCardIsSpecified",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSetOwnerSucceedsForAnyUserWhenWildCardIsSpecified() throws Throwable\n{\r\n    fs.updateChownAllowedUsers(Collections.singletonList(\"*\"));\r\n    final Path testPath = new Path(\"/testSetOwnerPositiveWildcard\");\r\n    Configuration conf = fs.getConf();\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    final String newOwner = \"newowner\";\r\n    final String newGroup = \"newgroup\";\r\n    UserGroupInformation user = UserGroupInformation.createUserForTesting(\"anyuser\", new String[] { \"group1\" });\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"test path does not exist\", testPath);\r\n        String owner = fs.getFileStatus(testPath).getOwner();\r\n        Assume.assumeTrue(\"changing owner requires original and new owner to be different\", !StringUtils.equalsIgnoreCase(owner, newOwner));\r\n        user.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                fs.setOwner(testPath, newOwner, newGroup);\r\n                assertOwnerEquals(testPath, newOwner);\r\n                assertEquals(newGroup, fs.getFileStatus(testPath).getGroup());\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetOwnerFailsForIllegalSetup",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetOwnerFailsForIllegalSetup() throws Throwable\n{\r\n    fs.updateChownAllowedUsers(Arrays.asList(\"user1\", \"*\"));\r\n    final Path testPath = new Path(\"/testSetOwnerFailsForIllegalSetup\");\r\n    Configuration conf = fs.getConf();\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    UserGroupInformation user = UserGroupInformation.createUserForTesting(\"anyuser\", new String[] { \"group1\" });\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"test path does not exist\", testPath);\r\n        final String owner = fs.getFileStatus(testPath).getOwner();\r\n        user.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                try {\r\n                    fs.setOwner(testPath, \"newowner\", null);\r\n                    fail(\"Failing test because setOwner call was expected to throw\");\r\n                } catch (IllegalArgumentException iex) {\r\n                    assertOwnerEquals(testPath, owner);\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenamePendingAuthorizationCalls",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenamePendingAuthorizationCalls() throws Throwable\n{\r\n    Path testPath = new Path(\"/testRenamePendingAuthorizationCalls\");\r\n    Path srcPath = new Path(testPath, \"srcPath\");\r\n    Path dstPath = new Path(testPath, \"dstPath\");\r\n    Path srcFilePath = new Path(srcPath, \"file.txt\");\r\n    Path dstFilePath = new Path(dstPath, \"file.txt\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.create(srcFilePath);\r\n        String srcKey = fs.pathToKey(srcPath);\r\n        String dstKey = fs.pathToKey(dstPath);\r\n        NativeAzureFileSystem.FolderRenamePending renamePending = new NativeAzureFileSystem.FolderRenamePending(srcKey, dstKey, null, fs);\r\n        renamePending.writeFile(fs);\r\n        fs.getFileStatus(srcPath);\r\n    } catch (FileNotFoundException fnfe) {\r\n        GenericTestUtils.assertExceptionContains(srcPath.toString() + \": No such file or directory.\", fnfe);\r\n        ContractTestUtils.assertPathExists(fs, \"dstFilePath does not exist -- pending rename failed\", dstFilePath);\r\n    } finally {\r\n        allowRecursiveDelete(fs, testPath.toString());\r\n        fs.delete(testPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionThrowsForUnauthorisedUsers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPermissionThrowsForUnauthorisedUsers() throws Throwable\n{\r\n    testSetPermission(\"/testSetPermissionNegative\", null, null, \"unauthorizeduser\", true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionForAuthorisedUsers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPermissionForAuthorisedUsers() throws Throwable\n{\r\n    testSetPermission(\"/testSetPermissionPositive\", null, null, \"user1\", false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionForOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPermissionForOwner() throws Throwable\n{\r\n    testSetPermission(\"/testSetPermissionPositiveOwner\", null, null, null, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionWhenWildCardInAllowedUserList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetPermissionWhenWildCardInAllowedUserList() throws Throwable\n{\r\n    List<String> chmodAllowedUsers = Collections.singletonList(\"*\");\r\n    testSetPermission(\"/testSetPermissionWhenWildCardInAllowedUserList\", chmodAllowedUsers, null, \"testuser\", false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionForInvalidAllowedUserList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetPermissionForInvalidAllowedUserList() throws Throwable\n{\r\n    List<String> chmodAllowedUsers = Arrays.asList(\"*\", \"testuser\");\r\n    testSetPermission(\"/testSetPermissionForInvalidAllowedUserList\", chmodAllowedUsers, null, \"testuser\", true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionForDaemonUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPermissionForDaemonUser() throws Throwable\n{\r\n    testSetPermission(\"/testSetPermissionForDaemonUser\", null, null, \"hive\", false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermissionForInvalidDaemonUserList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetPermissionForInvalidDaemonUserList() throws Throwable\n{\r\n    List<String> daemonUsers = Arrays.asList(\"*\", \"hive\");\r\n    testSetPermission(\"/testSetPermissionForInvalidDaemonUserList\", null, daemonUsers, \"testuser\", true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessWhenPermissionsMatchForAllAndReadWrite",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAccessWhenPermissionsMatchForAllAndReadWrite() throws Throwable\n{\r\n    Configuration conf = fs.getConf();\r\n    fs.setConf(conf);\r\n    final Path testPath = new Path(\"/testAccessWhenPermissionsMatchForAllAndReadWrite\");\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        assertPathExists(fs, \"test path does not exist\", testPath);\r\n        fs.access(testPath, FsAction.ALL);\r\n        fs.access(testPath, FsAction.READ_WRITE);\r\n    } finally {\r\n        recursiveDelete(testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessWhenPermissionsMatchForWriteAndWriteExecute",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAccessWhenPermissionsMatchForWriteAndWriteExecute() throws Throwable\n{\r\n    Configuration conf = fs.getConf();\r\n    fs.setConf(conf);\r\n    final Path testPath = new Path(\"/testAccessWhenPermissionsMatchForWriteAndWriteExecute\");\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        assertPathExists(fs, \"test path does not exist\", testPath);\r\n        fs.access(testPath, FsAction.WRITE);\r\n        fs.access(testPath, FsAction.WRITE_EXECUTE);\r\n    } finally {\r\n        recursiveDelete(testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessWhenPermissionsMatchForReadAndReadExecute",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAccessWhenPermissionsMatchForReadAndReadExecute() throws Throwable\n{\r\n    Configuration conf = fs.getConf();\r\n    fs.setConf(conf);\r\n    final Path testPath = new Path(\"/testAccessWhenPermissionsMatchForReadAndReadExecute\");\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        assertPathExists(fs, \"test path does not exist\", testPath);\r\n        fs.access(testPath, FsAction.READ);\r\n        fs.access(testPath, FsAction.READ_EXECUTE);\r\n    } finally {\r\n        recursiveDelete(testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessWhenPermissionsMatchForExecuteAndNone",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAccessWhenPermissionsMatchForExecuteAndNone() throws Throwable\n{\r\n    Configuration conf = fs.getConf();\r\n    fs.setConf(conf);\r\n    final Path testPath = new Path(\"/testAccessWhenPermissionsMatchForExecuteAndNone\");\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        assertPathExists(fs, \"test path does not exist\", testPath);\r\n        fs.access(testPath, FsAction.EXECUTE);\r\n        fs.access(testPath, FsAction.NONE);\r\n    } finally {\r\n        recursiveDelete(testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessWhenPermissionsDoNotMatch",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAccessWhenPermissionsDoNotMatch() throws Throwable\n{\r\n    Configuration conf = fs.getConf();\r\n    fs.setConf(conf);\r\n    final Path testPath = new Path(\"/testAccessWhenPermissionsDoNotMatch\");\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        assertPathExists(fs, \"test path does not exist\", testPath);\r\n        assertNoAccess(testPath, FsAction.ALL);\r\n        assertNoAccess(testPath, FsAction.WRITE);\r\n        assertNoAccess(testPath, FsAction.WRITE_EXECUTE);\r\n    } finally {\r\n        recursiveDelete(testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessFileDoesNotExist",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAccessFileDoesNotExist() throws Throwable\n{\r\n    expectedEx.expect(FileNotFoundException.class);\r\n    Configuration conf = fs.getConf();\r\n    fs.setConf(conf);\r\n    final Path testPath = new Path(\"/testAccessFileDoesNotExist\");\r\n    authorizer.init(conf);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), READ, true);\r\n    authorizer.addAuthRuleForOwner(testPath.toString(), WRITE, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    assertPathDoesNotExist(fs, \"test path exists\", testPath);\r\n    fs.access(testPath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessFileDoesNotExistWhenNoAccessPermission",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAccessFileDoesNotExistWhenNoAccessPermission() throws Throwable\n{\r\n    expectedEx.expect(FileNotFoundException.class);\r\n    Configuration conf = fs.getConf();\r\n    fs.setConf(conf);\r\n    final Path testPath = new Path(\"/testAccessFileDoesNotExistWhenNoAccessPermission\");\r\n    authorizer.init(conf);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    assertPathDoesNotExist(fs, \"test path exists\", testPath);\r\n    fs.access(testPath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAccessForFileAndIntermediateDirectoryCreated",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAccessForFileAndIntermediateDirectoryCreated() throws Throwable\n{\r\n    Path parentDir = new Path(\"/testAccessDirectory\");\r\n    Path intermediateDir = new Path(parentDir, \"intermediateDir\");\r\n    Path testPath = new Path(intermediateDir, \"test.dat\");\r\n    authorizer.addAuthRuleForOwner(\"/\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString(), WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString() + \"/*\", WRITE, true);\r\n    authorizer.addAuthRuleForOwner(parentDir.toString() + \"/*\", READ, true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    try {\r\n        fs.mkdirs(parentDir);\r\n        fs.create(testPath);\r\n        assertPathExists(fs, \"testPath was not created\", testPath);\r\n        fs.access(parentDir, FsAction.WRITE);\r\n        fs.access(parentDir, FsAction.WRITE_EXECUTE);\r\n        fs.access(intermediateDir, FsAction.ALL);\r\n        fs.access(intermediateDir, FsAction.READ_WRITE);\r\n        fs.access(testPath, FsAction.ALL);\r\n        fs.access(testPath, FsAction.READ_WRITE);\r\n    } finally {\r\n        recursiveDelete(testPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSetPermission",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSetPermission(String path, List<String> chmodAllowedUsers, List<String> daemonUsers, String user, boolean isSetPermissionFailureCase, boolean isInvalidSetup) throws Throwable\n{\r\n    final FsPermission filePermission;\r\n    final Path testPath = new Path(path);\r\n    final FsPermission newPermission = new FsPermission(FULL_PERMISSION_WITH_STICKYBIT);\r\n    authorizer.addAuthRule(\"/\", WRITE, getCurrentUserShortName(), true);\r\n    fs.updateWasbAuthorizer(authorizer);\r\n    if (chmodAllowedUsers != null && !chmodAllowedUsers.isEmpty()) {\r\n        fs.updateChmodAllowedUsers(chmodAllowedUsers);\r\n    }\r\n    if (daemonUsers != null && !daemonUsers.isEmpty()) {\r\n        fs.updateDaemonUsers(daemonUsers);\r\n    }\r\n    UserGroupInformation testUser = (user != null) ? UserGroupInformation.createUserForTesting(user, new String[] { \"testgrp\" }) : null;\r\n    try {\r\n        fs.mkdirs(testPath);\r\n        ContractTestUtils.assertPathExists(fs, \"test path does not exist\", testPath);\r\n        filePermission = fs.getFileStatus(testPath).getPermission();\r\n        if (isSetPermissionFailureCase) {\r\n            executeSetPermissionFailure(testUser, testPath, filePermission, newPermission, isInvalidSetup);\r\n        } else {\r\n            executeSetPermissionSuccess(testUser, testPath, filePermission, newPermission);\r\n        }\r\n    } finally {\r\n        fs.delete(testPath, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "executeSetPermissionFailure",
  "errType" : [ "IllegalArgumentException", "WasbAuthorizationException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void executeSetPermissionFailure(UserGroupInformation testUser, Path testPath, FsPermission oldPermission, FsPermission newPermission, boolean isInvalidSetup) throws Throwable\n{\r\n    testUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            try {\r\n                fs.setPermission(testPath, newPermission);\r\n                fail(\"Failing test because setPermission was expected to throw\");\r\n            } catch (IllegalArgumentException iex) {\r\n                if (!isInvalidSetup) {\r\n                    fail(\"Failing test because IllegalArgumentException\" + \" is not expected to throw\");\r\n                }\r\n                assertPermissionEquals(testPath, oldPermission);\r\n            } catch (WasbAuthorizationException wex) {\r\n                if (isInvalidSetup) {\r\n                    fail(\"Failing test because WasbAuthorizationException\" + \" is not expected to throw\");\r\n                }\r\n                assertPermissionEquals(testPath, oldPermission);\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "executeSetPermissionSuccess",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void executeSetPermissionSuccess(UserGroupInformation testUser, Path testPath, FsPermission oldPermission, FsPermission newPermission) throws Throwable\n{\r\n    if (testUser != null) {\r\n        testUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws Exception {\r\n                fs.setPermission(testPath, newPermission);\r\n                return null;\r\n            }\r\n        });\r\n    } else {\r\n        fs.setPermission(testPath, newPermission);\r\n    }\r\n    assertPermissionEquals(testPath, newPermission);\r\n    assertNotEquals(newPermission, oldPermission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertPermissionEquals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertPermissionEquals(Path path, FsPermission newPermission) throws IOException\n{\r\n    FileStatus status = fs.getFileStatus(path);\r\n    assertEquals(\"Wrong permissions in \" + status, newPermission, status.getPermission());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertOwnerEquals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertOwnerEquals(Path path, String owner) throws IOException\n{\r\n    FileStatus status = fs.getFileStatus(path);\r\n    assertEquals(\"Wrong owner in \" + status, owner, status.getOwner());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertNoAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertNoAccess(final Path path, final FsAction action) throws Exception\n{\r\n    LambdaTestUtils.intercept(AccessControlException.class, new Callable<String>() {\r\n\r\n        @Override\r\n        public String call() throws Exception {\r\n            fs.access(path, action);\r\n            return \"Access granted to \" + path + \" for action \" + action;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendDirShouldFail",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAppendDirShouldFail() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path filePath = path(TEST_FILE_PATH);\r\n    fs.mkdirs(filePath);\r\n    fs.append(filePath, 0).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendWithLength0",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAppendWithLength0() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    try (FSDataOutputStream stream = fs.create(path(TEST_FILE_PATH))) {\r\n        final byte[] b = new byte[1024];\r\n        new Random().nextBytes(b);\r\n        stream.write(b, 1000, 0);\r\n        assertEquals(0, stream.getPos());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendFileAfterDelete",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAppendFileAfterDelete() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path filePath = path(TEST_FILE_PATH);\r\n    ContractTestUtils.touch(fs, filePath);\r\n    fs.delete(filePath, false);\r\n    fs.append(filePath).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendDirectory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAppendDirectory() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path folderPath = path(TEST_FOLDER_PATH);\r\n    fs.mkdirs(folderPath);\r\n    fs.append(folderPath).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testTracingForAppend",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testTracingForAppend() throws IOException\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path testPath = path(TEST_FILE_PATH);\r\n    fs.create(testPath).close();\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.APPEND, false, 0));\r\n    fs.append(testPath, 10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    boolean isHNSEnabled = this.getConfiguration().getBoolean(TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\r\n    Assume.assumeTrue(isHNSEnabled);\r\n    loadConfiguredFileSystem();\r\n    this.getConfiguration().set(ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE, TEST_AUTHZ_CLASS);\r\n    this.getConfiguration().set(ConfigurationKeys.FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, \"SAS\");\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSASTokenProviderInitializeException",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSASTokenProviderInitializeException() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    final AzureBlobFileSystem testFs = new AzureBlobFileSystem();\r\n    Configuration testConfig = this.getConfiguration().getRawConfiguration();\r\n    testConfig.set(ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE, TEST_ERR_AUTHZ_CLASS);\r\n    testConfig.set(MOCK_SASTOKENPROVIDER_FAIL_INIT, \"true\");\r\n    intercept(TokenAccessProviderException.class, () -> {\r\n        testFs.initialize(fs.getUri(), this.getConfiguration().getRawConfiguration());\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSASTokenProviderEmptySASToken",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSASTokenProviderEmptySASToken() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    final AzureBlobFileSystem testFs = new AzureBlobFileSystem();\r\n    Configuration testConfig = this.getConfiguration().getRawConfiguration();\r\n    testConfig.set(ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE, TEST_ERR_AUTHZ_CLASS);\r\n    testConfig.set(MOCK_SASTOKENPROVIDER_RETURN_EMPTY_SAS_TOKEN, \"true\");\r\n    testFs.initialize(fs.getUri(), this.getConfiguration().getRawConfiguration());\r\n    intercept(SASTokenProviderException.class, () -> {\r\n        testFs.create(new org.apache.hadoop.fs.Path(\"/testFile\")).close();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSASTokenProviderNullSASToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSASTokenProviderNullSASToken() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    final AzureBlobFileSystem testFs = new AzureBlobFileSystem();\r\n    Configuration testConfig = this.getConfiguration().getRawConfiguration();\r\n    testConfig.set(ConfigurationKeys.FS_AZURE_SAS_TOKEN_PROVIDER_TYPE, TEST_ERR_AUTHZ_CLASS);\r\n    testFs.initialize(fs.getUri(), this.getConfiguration().getRawConfiguration());\r\n    intercept(SASTokenProviderException.class, () -> {\r\n        testFs.create(new org.apache.hadoop.fs.Path(\"/testFile\")).close();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOpenFileWithInvalidPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testOpenFileWithInvalidPath() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    intercept(IllegalArgumentException.class, () -> {\r\n        fs.open(new Path(\"\")).close();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOpenFileAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOpenFileAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.Open, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOpenFileUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOpenFileUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.Open, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateFileAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreateFileAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.CreatePath, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateFileUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreateFileUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.CreatePath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendFileAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAppendFileAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.Append, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendFileUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAppendFileUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.Append, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.RenamePath, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.RenamePath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteFileAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDeleteFileAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.DeletePath, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteFileUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDeleteFileUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.DeletePath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListStatusAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testListStatusAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.ListPaths, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListStatusUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testListStatusUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.ListPaths, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testMkDirsAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMkDirsAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.Mkdir, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testMkDirsUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMkDirsUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.Mkdir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetFileStatusAuthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetFileStatusAuthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.GetPathStatus, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetFileStatusUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetFileStatusUnauthorized() throws Exception\n{\r\n    runTest(FileSystemOperations.GetPathStatus, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetOwnerUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetOwnerUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.SetOwner, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetPermissionUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.SetPermissions, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testModifyAclEntriesUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.ModifyAclEntries, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRemoveAclEntriesUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.RemoveAclEntries, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAclUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRemoveDefaultAclUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.RemoveDefaultAcl, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRemoveAclUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.RemoveAcl, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetAclUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.SetAcl, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAclStatusAuthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetAclStatusAuthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.GetAcl, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAclStatusUnauthorized",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetAclStatusUnauthorized() throws Exception\n{\r\n    Assume.assumeTrue(getIsNamespaceEnabled(getFileSystem()));\r\n    runTest(FileSystemOperations.GetAcl, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "runTest",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void runTest(FileSystemOperations testOp, boolean expectAbfsAuthorizationException) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path reqPath = new Path(\"requestPath\" + UUID.randomUUID().toString() + (expectAbfsAuthorizationException ? \"unauthorized\" : \"\"));\r\n    getMockSASTokenProvider(fs).setSkipAuthorizationForTestSetup(true);\r\n    if ((testOp != FileSystemOperations.CreatePath) && (testOp != FileSystemOperations.Mkdir)) {\r\n        fs.create(reqPath).close();\r\n        fs.getFileStatus(reqPath);\r\n    }\r\n    getMockSASTokenProvider(fs).setSkipAuthorizationForTestSetup(false);\r\n    if (expectAbfsAuthorizationException) {\r\n        intercept(SASTokenProviderException.class, () -> {\r\n            executeOp(reqPath, fs, testOp);\r\n        });\r\n    } else {\r\n        executeOp(reqPath, fs, testOp);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "executeOp",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void executeOp(Path reqPath, AzureBlobFileSystem fs, FileSystemOperations op) throws IOException, IOException\n{\r\n    switch(op) {\r\n        case ListPaths:\r\n            fs.listStatus(reqPath);\r\n            break;\r\n        case CreatePath:\r\n            fs.create(reqPath).close();\r\n            break;\r\n        case RenamePath:\r\n            fs.rename(reqPath, new Path(\"renameDest\" + UUID.randomUUID().toString()));\r\n            break;\r\n        case GetAcl:\r\n            fs.getAclStatus(reqPath);\r\n            break;\r\n        case GetPathStatus:\r\n            fs.getFileStatus(reqPath);\r\n            break;\r\n        case SetAcl:\r\n            fs.setAcl(reqPath, Arrays.asList(aclEntry(ACCESS, GROUP, BAR, FsAction.ALL)));\r\n            break;\r\n        case SetOwner:\r\n            fs.setOwner(reqPath, TEST_USER, TEST_GROUP);\r\n            break;\r\n        case SetPermissions:\r\n            fs.setPermission(reqPath, new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE));\r\n            break;\r\n        case Append:\r\n            fs.append(reqPath);\r\n            break;\r\n        case ReadFile:\r\n            fs.open(reqPath);\r\n            break;\r\n        case Open:\r\n            fs.open(reqPath);\r\n            break;\r\n        case DeletePath:\r\n            fs.delete(reqPath, false);\r\n            break;\r\n        case Mkdir:\r\n            fs.mkdirs(reqPath, new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE));\r\n            break;\r\n        case RemoveAclEntries:\r\n            fs.removeAclEntries(reqPath, Arrays.asList(aclEntry(ACCESS, GROUP, BAR, FsAction.ALL)));\r\n            break;\r\n        case ModifyAclEntries:\r\n            fs.modifyAclEntries(reqPath, Arrays.asList(aclEntry(ACCESS, GROUP, BAR, FsAction.ALL)));\r\n            break;\r\n        case RemoveAcl:\r\n            fs.removeAcl(reqPath);\r\n            break;\r\n        case RemoveDefaultAcl:\r\n            fs.removeDefaultAcl(reqPath);\r\n            break;\r\n        default:\r\n            throw new IllegalStateException(\"Unexpected value: \" + op);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMockSASTokenProvider",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MockSASTokenProvider getMockSASTokenProvider(AzureBlobFileSystem fs) throws Exception\n{\r\n    return ((MockSASTokenProvider) fs.getAbfsStore().getClient().getSasTokenProvider());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsIteratorWithHasNext",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAbfsIteratorWithHasNext() throws Exception\n{\r\n    Path testDir = createTestDirectory();\r\n    setPageSize(10);\r\n    final List<String> fileNames = createFilesUnderDirectory(testDir);\r\n    ListingSupport listingSupport = Mockito.spy(getFileSystem().getAbfsStore());\r\n    RemoteIterator<FileStatus> fsItr = new AbfsListStatusRemoteIterator(testDir, listingSupport, getTestTracingContext(getFileSystem(), true));\r\n    Assertions.assertThat(fsItr).describedAs(\"RemoteIterator should be instance of \" + \"AbfsListStatusRemoteIterator by default\").isInstanceOf(AbfsListStatusRemoteIterator.class);\r\n    int itrCount = 0;\r\n    while (fsItr.hasNext()) {\r\n        FileStatus fileStatus = fsItr.next();\r\n        verifyIteratorResultContent(fileStatus, fileNames);\r\n        itrCount++;\r\n    }\r\n    verifyIteratorResultCount(itrCount, fileNames);\r\n    int minNumberOfInvocations = TEST_FILES_NUMBER / 10;\r\n    verify(listingSupport, Mockito.atLeast(minNumberOfInvocations)).listStatus(any(Path.class), nullable(String.class), anyList(), anyBoolean(), nullable(String.class), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsIteratorWithoutHasNext",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAbfsIteratorWithoutHasNext() throws Exception\n{\r\n    Path testDir = createTestDirectory();\r\n    setPageSize(10);\r\n    final List<String> fileNames = createFilesUnderDirectory(testDir);\r\n    ListingSupport listingSupport = Mockito.spy(getFileSystem().getAbfsStore());\r\n    RemoteIterator<FileStatus> fsItr = new AbfsListStatusRemoteIterator(testDir, listingSupport, getTestTracingContext(getFileSystem(), true));\r\n    Assertions.assertThat(fsItr).describedAs(\"RemoteIterator should be instance of \" + \"AbfsListStatusRemoteIterator by default\").isInstanceOf(AbfsListStatusRemoteIterator.class);\r\n    int itrCount = 0;\r\n    for (int i = 0; i < TEST_FILES_NUMBER; i++) {\r\n        FileStatus fileStatus = fsItr.next();\r\n        verifyIteratorResultContent(fileStatus, fileNames);\r\n        itrCount++;\r\n    }\r\n    LambdaTestUtils.intercept(NoSuchElementException.class, fsItr::next);\r\n    verifyIteratorResultCount(itrCount, fileNames);\r\n    int minNumberOfInvocations = TEST_FILES_NUMBER / 10;\r\n    verify(listingSupport, Mockito.atLeast(minNumberOfInvocations)).listStatus(any(Path.class), nullable(String.class), anyList(), anyBoolean(), nullable(String.class), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWithAbfsIteratorDisabled",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testWithAbfsIteratorDisabled() throws Exception\n{\r\n    Path testDir = createTestDirectory();\r\n    setPageSize(10);\r\n    disableAbfsIterator();\r\n    final List<String> fileNames = createFilesUnderDirectory(testDir);\r\n    RemoteIterator<FileStatus> fsItr = getFileSystem().listStatusIterator(testDir);\r\n    Assertions.assertThat(fsItr).describedAs(\"RemoteIterator should not be instance of \" + \"AbfsListStatusRemoteIterator when it is disabled\").isNotInstanceOf(AbfsListStatusRemoteIterator.class);\r\n    int itrCount = 0;\r\n    while (fsItr.hasNext()) {\r\n        FileStatus fileStatus = fsItr.next();\r\n        verifyIteratorResultContent(fileStatus, fileNames);\r\n        itrCount++;\r\n    }\r\n    verifyIteratorResultCount(itrCount, fileNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWithAbfsIteratorDisabledWithoutHasNext",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testWithAbfsIteratorDisabledWithoutHasNext() throws Exception\n{\r\n    Path testDir = createTestDirectory();\r\n    setPageSize(10);\r\n    disableAbfsIterator();\r\n    final List<String> fileNames = createFilesUnderDirectory(testDir);\r\n    RemoteIterator<FileStatus> fsItr = getFileSystem().listStatusIterator(testDir);\r\n    Assertions.assertThat(fsItr).describedAs(\"RemoteIterator should not be instance of \" + \"AbfsListStatusRemoteIterator when it is disabled\").isNotInstanceOf(AbfsListStatusRemoteIterator.class);\r\n    int itrCount;\r\n    for (itrCount = 0; itrCount < TEST_FILES_NUMBER; itrCount++) {\r\n        FileStatus fileStatus = fsItr.next();\r\n        verifyIteratorResultContent(fileStatus, fileNames);\r\n    }\r\n    LambdaTestUtils.intercept(NoSuchElementException.class, fsItr::next);\r\n    verifyIteratorResultCount(itrCount, fileNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNextWhenNoMoreElementsPresent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testNextWhenNoMoreElementsPresent() throws Exception\n{\r\n    Path testDir = createTestDirectory();\r\n    setPageSize(10);\r\n    RemoteIterator<FileStatus> fsItr = new AbfsListStatusRemoteIterator(testDir, getFileSystem().getAbfsStore(), getTestTracingContext(getFileSystem(), true));\r\n    fsItr = Mockito.spy(fsItr);\r\n    Mockito.doReturn(false).when(fsItr).hasNext();\r\n    LambdaTestUtils.intercept(NoSuchElementException.class, fsItr::next);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testHasNextForEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testHasNextForEmptyDir() throws Exception\n{\r\n    Path testDir = createTestDirectory();\r\n    setPageSize(10);\r\n    RemoteIterator<FileStatus> fsItr = getFileSystem().listStatusIterator(testDir);\r\n    Assertions.assertThat(fsItr.hasNext()).describedAs(\"hasNext returns false for empty directory\").isFalse();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testHasNextForFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testHasNextForFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFile = path(\"testFile\");\r\n    String testFileName = testFile.toString();\r\n    getFileSystem().create(testFile);\r\n    setPageSize(10);\r\n    RemoteIterator<FileStatus> fsItr = fs.listStatusIterator(testFile);\r\n    Assertions.assertThat(fsItr.hasNext()).describedAs(\"hasNext returns true for file\").isTrue();\r\n    Assertions.assertThat(fsItr.next().getPath().toString()).describedAs(\"next returns the file itself\").endsWith(testFileName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testIOException",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testIOException() throws Exception\n{\r\n    Path testDir = createTestDirectory();\r\n    setPageSize(10);\r\n    getFileSystem().mkdirs(testDir);\r\n    String exceptionMessage = \"test exception\";\r\n    ListingSupport lsSupport = getMockListingSupport(exceptionMessage);\r\n    LambdaTestUtils.intercept(IOException.class, () -> new AbfsListStatusRemoteIterator(testDir, lsSupport, getTestTracingContext(getFileSystem(), true)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNonExistingPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNonExistingPath() throws Exception\n{\r\n    Path nonExistingDir = new Path(\"nonExistingPath\");\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, () -> getFileSystem().listStatusIterator(nonExistingDir));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "verifyIteratorResultContent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyIteratorResultContent(FileStatus fileStatus, List<String> fileNames)\n{\r\n    String pathStr = fileStatus.getPath().toString();\r\n    Assert.assertTrue(String.format(\"Could not remove path %s from filenames %s\", pathStr, fileNames), fileNames.remove(pathStr));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "verifyIteratorResultCount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyIteratorResultCount(int itrCount, List<String> fileNames)\n{\r\n    Assertions.assertThat(itrCount).describedAs(\"Number of iterations should be equal to the files created\").isEqualTo(TEST_FILES_NUMBER);\r\n    Assertions.assertThat(fileNames).describedAs(\"After removing every item found from the iterator, \" + \"there should be no more elements in the fileNames\").hasSize(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMockListingSupport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListingSupport getMockListingSupport(String exceptionMessage)\n{\r\n    return new ListingSupport() {\r\n\r\n        @Override\r\n        public FileStatus[] listStatus(Path path, TracingContext tracingContext) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public FileStatus[] listStatus(Path path, String startFrom, TracingContext tracingContext) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public String listStatus(Path path, String startFrom, List<FileStatus> fileStatuses, boolean fetchAll, String continuation, TracingContext tracingContext) throws IOException {\r\n            throw new IOException(exceptionMessage);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createTestDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path createTestDirectory() throws IOException\n{\r\n    Path testDirectory = path(\"testDirectory\");\r\n    getFileSystem().mkdirs(testDirectory);\r\n    return testDirectory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "disableAbfsIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void disableAbfsIterator() throws IOException\n{\r\n    AzureBlobFileSystemStore abfsStore = getAbfsStore(getFileSystem());\r\n    abfsStore.getAbfsConfiguration().setEnableAbfsListIterator(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setPageSize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setPageSize(int pageSize) throws IOException\n{\r\n    AzureBlobFileSystemStore abfsStore = getAbfsStore(getFileSystem());\r\n    abfsStore.getAbfsConfiguration().setListMaxResults(pageSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFilesUnderDirectory",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<String> createFilesUnderDirectory(Path rootPath) throws ExecutionException, InterruptedException, IOException\n{\r\n    final List<Future<Void>> tasks = new ArrayList<>();\r\n    final List<String> fileNames = Collections.synchronizedList(new ArrayList<>());\r\n    ExecutorService es = Executors.newFixedThreadPool(10);\r\n    try {\r\n        for (int i = 0; i < ITestAbfsListStatusRemoteIterator.TEST_FILES_NUMBER; i++) {\r\n            Path filePath = makeQualified(new Path(rootPath, \"testListPath\" + i));\r\n            tasks.add(es.submit(() -> {\r\n                touch(filePath);\r\n                synchronized (fileNames) {\r\n                    Assert.assertTrue(fileNames.add(filePath.toString()));\r\n                }\r\n                return null;\r\n            }));\r\n        }\r\n        for (Future<Void> task : tasks) {\r\n            task.get();\r\n        }\r\n    } finally {\r\n        es.shutdownNow();\r\n    }\r\n    LOG.debug(fileNames.toString());\r\n    Assertions.assertThat(fileNames).describedAs(\"File creation incorrect or fileNames not added to list\").hasSize(ITestAbfsListStatusRemoteIterator.TEST_FILES_NUMBER);\r\n    return fileNames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConcurrentCreateDeleteFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testConcurrentCreateDeleteFile() throws Exception\n{\r\n    Path testFile = methodPath();\r\n    List<CreateFileTask> tasks = new ArrayList<>(THREAD_COUNT);\r\n    for (int i = 0; i < THREAD_COUNT; i++) {\r\n        tasks.add(new CreateFileTask(fs, testFile));\r\n    }\r\n    ExecutorService es = null;\r\n    try {\r\n        es = Executors.newFixedThreadPool(THREAD_COUNT);\r\n        List<Future<Void>> futures = es.invokeAll(tasks);\r\n        for (Future<Void> future : futures) {\r\n            Assert.assertTrue(future.isDone());\r\n            Assert.assertEquals(null, future.get());\r\n        }\r\n    } finally {\r\n        if (es != null) {\r\n            es.shutdownNow();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConcurrentDeleteFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testConcurrentDeleteFile() throws Exception\n{\r\n    Path testFile = new Path(\"test.dat\");\r\n    fs.create(testFile).close();\r\n    List<DeleteFileTask> tasks = new ArrayList<>(THREAD_COUNT);\r\n    for (int i = 0; i < THREAD_COUNT; i++) {\r\n        tasks.add(new DeleteFileTask(fs, testFile));\r\n    }\r\n    ExecutorService es = null;\r\n    try {\r\n        es = Executors.newFixedThreadPool(THREAD_COUNT);\r\n        List<Future<Boolean>> futures = es.invokeAll(tasks);\r\n        int successCount = 0;\r\n        for (Future<Boolean> future : futures) {\r\n            Assert.assertTrue(future.isDone());\r\n            Boolean success = future.get();\r\n            if (success) {\r\n                successCount++;\r\n            }\r\n        }\r\n        Assert.assertEquals(\"Exactly one delete operation should return true.\", 1, successCount);\r\n    } finally {\r\n        if (es != null) {\r\n            es.shutdownNow();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConcurrentList",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testConcurrentList() throws Exception\n{\r\n    final Path testDir = new Path(\"/tmp/data-loss/11230174258112/_temporary/0/_temporary/attempt_20200624190514_0006_m_0\");\r\n    final Path testFile = new Path(testDir, \"part-00004-15ea87b1-312c-4fdf-1820-95afb3dfc1c3-a010.snappy.parquet\");\r\n    fs.create(testFile).close();\r\n    List<ListTask> tasks = new ArrayList<>(THREAD_COUNT);\r\n    for (int i = 0; i < THREAD_COUNT; i++) {\r\n        tasks.add(new ListTask(fs, testDir));\r\n    }\r\n    ExecutorService es = null;\r\n    try {\r\n        es = Executors.newFixedThreadPool(THREAD_COUNT);\r\n        List<Future<Integer>> futures = es.invokeAll(tasks);\r\n        for (Future<Integer> future : futures) {\r\n            Assert.assertTrue(future.isDone());\r\n            long fileCount = future.get();\r\n            assertEquals(\"The list should always contain 1 file.\", 1, fileCount);\r\n        }\r\n    } finally {\r\n        if (es != null) {\r\n            es.shutdownNow();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"/\", null, 0, SUCCEED }, { \"/\", SORTED_ENTRY_NAMES[1], 1, SUCCEED }, { \"/\", \"/\", -1, FAIL }, { \"/\" + SORTED_ENTRY_NAMES[2], SORTED_ENTRY_NAMES[1], 1, SUCCEED }, { \"/\" + SORTED_ENTRY_NAMES[2], SORTED_ENTRY_NAMES[2], 2, SUCCEED }, { \"/\" + SORTED_ENTRY_NAMES[2], \"/\" + SORTED_ENTRY_NAMES[3], -1, FAIL }, { \"/\" + SORTED_ENTRY_NAMES[2], \"0-non-existent\", 0, SUCCEED }, { \"/\" + SORTED_ENTRY_NAMES[2], \"z-non-existent\", -1, SUCCEED }, { \"/\" + SORTED_ENTRY_NAMES[2], \"A1\", 2, SUCCEED } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListWithRange",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testListWithRange() throws IOException\n{\r\n    try {\r\n        FileStatus[] listResult = store.listStatus(new Path(path), startFrom, getTestTracingContext(fs, true));\r\n        if (!expectedResult) {\r\n            Assert.fail(\"Excepting failure with IllegalArgumentException\");\r\n        }\r\n        verifyFileStatus(listResult, new Path(path), expectedStartIndexInArray);\r\n    } catch (IllegalArgumentException ex) {\r\n        if (expectedResult) {\r\n            Assert.fail(\"Excepting success\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "verifyFileStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyFileStatus(FileStatus[] listResult, Path parentPath, int startIndexInSortedName) throws IOException\n{\r\n    if (startIndexInSortedName == -1) {\r\n        Assert.assertEquals(\"Expected empty FileStatus array\", 0, listResult.length);\r\n        return;\r\n    }\r\n    FileStatus[] allFileStatuses = fs.listStatus(parentPath);\r\n    Assert.assertEquals(\"number of dir/file doesn't match\", SORTED_ENTRY_NAMES.length, allFileStatuses.length);\r\n    int indexInResult = 0;\r\n    for (int index = startIndexInSortedName; index < SORTED_ENTRY_NAMES.length; index++) {\r\n        Assert.assertEquals(\"fileStatus doesn't match\", allFileStatuses[index], listResult[indexInResult++]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "prepareTestFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void prepareTestFiles() throws IOException\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    for (String levelOneFolder : SORTED_ENTRY_NAMES) {\r\n        Path levelOnePath = new Path(\"/\" + levelOneFolder);\r\n        Assert.assertTrue(fs.mkdirs(levelOnePath));\r\n        for (String fileName : SORTED_ENTRY_NAMES) {\r\n            Path filePath = new Path(levelOnePath, fileName);\r\n            ContractTestUtils.touch(fs, filePath);\r\n            ContractTestUtils.assertIsFile(fs, filePath);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testInitializeStats",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testInitializeStats() throws IOException\n{\r\n    describe(\"Testing the counter values after Abfs is initialised\");\r\n    AbfsCounters instrumentation = new AbfsCountersImpl(getFileSystem().getUri());\r\n    for (int i = 0; i < LARGE_OPS; i++) {\r\n        instrumentation.incrementCounter(AbfsStatistic.CALL_GET_DELEGATION_TOKEN, 1);\r\n        instrumentation.incrementCounter(AbfsStatistic.ERROR_IGNORED, 1);\r\n    }\r\n    Map<String, Long> metricMap = instrumentation.toMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_GET_DELEGATION_TOKEN, LARGE_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.ERROR_IGNORED, LARGE_OPS, metricMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, \"/\");\r\n    return AzureBlobStorageTestAccount.create(\"testpagebloboutputstream\", EnumSet.of(AzureBlobStorageTestAccount.CreateOptions.CreateContainer), conf, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testHflush",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testHflush() throws Exception\n{\r\n    Path path = fs.makeQualified(TEST_FILE_PATH);\r\n    FSDataOutputStream os = fs.create(path);\r\n    os.write(1);\r\n    os.hflush();\r\n    fs.delete(path, false);\r\n    os.write(2);\r\n    LambdaTestUtils.intercept(IOException.class, \"The specified blob does not exist\", () -> {\r\n        os.hflush();\r\n    });\r\n    LambdaTestUtils.intercept(IOException.class, \"The specified blob does not exist\", () -> {\r\n        os.close();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testHsync",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testHsync() throws Exception\n{\r\n    Path path = fs.makeQualified(TEST_FILE_PATH);\r\n    FSDataOutputStream os = fs.create(path);\r\n    os.write(1);\r\n    os.hsync();\r\n    fs.delete(path, false);\r\n    os.write(2);\r\n    LambdaTestUtils.intercept(IOException.class, \"The specified blob does not exist\", () -> {\r\n        os.hsync();\r\n    });\r\n    LambdaTestUtils.intercept(IOException.class, \"The specified blob does not exist\", () -> {\r\n        os.close();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "populateAbfsOutputStreamContext",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AbfsOutputStreamContext populateAbfsOutputStreamContext(int writeBufferSize, boolean isFlushEnabled, boolean disableOutputStreamFlush, boolean isAppendBlob, AbfsClient client, String path, TracingContext tracingContext, ExecutorService executorService) throws IOException, IllegalAccessException\n{\r\n    AbfsConfiguration abfsConf = new AbfsConfiguration(new Configuration(), accountName1);\r\n    String blockFactoryName = abfsConf.getRawConfiguration().getTrimmed(DATA_BLOCKS_BUFFER, DATA_BLOCKS_BUFFER_DEFAULT);\r\n    DataBlocks.BlockFactory blockFactory = DataBlocks.createFactory(FS_AZURE_BLOCK_UPLOAD_BUFFER_DIR, abfsConf.getRawConfiguration(), blockFactoryName);\r\n    return new AbfsOutputStreamContext(2).withWriteBufferSize(writeBufferSize).enableFlush(isFlushEnabled).disableOutputStreamFlush(disableOutputStreamFlush).withStreamStatistics(new AbfsOutputStreamStatisticsImpl()).withAppendBlob(isAppendBlob).withWriteMaxConcurrentRequestCount(abfsConf.getWriteMaxConcurrentRequestCount()).withMaxWriteRequestsToQueue(abfsConf.getMaxWriteRequestsToQueue()).withClient(client).withPath(path).withTracingContext(tracingContext).withExecutorService(executorService).withBlockFactory(blockFactory).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyShortWriteRequest",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void verifyShortWriteRequest() throws Exception\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    conf.set(accountKey1, accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.append(anyString(), any(byte[].class), any(AppendRequestParameters.class), any(), any(TracingContext.class))).thenReturn(op);\r\n    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(), isNull(), any(TracingContext.class))).thenReturn(op);\r\n    AbfsOutputStream out = new AbfsOutputStream(populateAbfsOutputStreamContext(BUFFER_SIZE, true, false, false, client, PATH, new TracingContext(abfsConf.getClientCorrelationId(), \"test-fs-id\", FSOperationType.WRITE, abfsConf.getTracingHeaderFormat(), null), createExecutorService(abfsConf)));\r\n    final byte[] b = new byte[WRITE_SIZE];\r\n    new Random().nextBytes(b);\r\n    out.write(b);\r\n    out.hsync();\r\n    final byte[] b1 = new byte[2 * WRITE_SIZE];\r\n    new Random().nextBytes(b1);\r\n    out.write(b1);\r\n    out.flush();\r\n    out.hflush();\r\n    out.hsync();\r\n    AppendRequestParameters firstReqParameters = new AppendRequestParameters(0, 0, WRITE_SIZE, APPEND_MODE, false, null);\r\n    AppendRequestParameters secondReqParameters = new AppendRequestParameters(WRITE_SIZE, 0, 2 * WRITE_SIZE, APPEND_MODE, false, null);\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(firstReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(secondReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(2)).append(eq(PATH), any(byte[].class), any(), any(), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyWriteRequest",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void verifyWriteRequest() throws Exception\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    conf.set(accountKey1, accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\r\n    TracingContext tracingContext = new TracingContext(\"test-corr-id\", \"test-fs-id\", FSOperationType.WRITE, TracingHeaderFormat.ALL_ID_FORMAT, null);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.append(anyString(), any(byte[].class), any(AppendRequestParameters.class), any(), any(TracingContext.class))).thenReturn(op);\r\n    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(), isNull(), any(TracingContext.class))).thenReturn(op);\r\n    AbfsOutputStream out = new AbfsOutputStream(populateAbfsOutputStreamContext(BUFFER_SIZE, true, false, false, client, PATH, tracingContext, createExecutorService(abfsConf)));\r\n    final byte[] b = new byte[WRITE_SIZE];\r\n    new Random().nextBytes(b);\r\n    for (int i = 0; i < 5; i++) {\r\n        out.write(b);\r\n    }\r\n    out.close();\r\n    AppendRequestParameters firstReqParameters = new AppendRequestParameters(0, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    AppendRequestParameters secondReqParameters = new AppendRequestParameters(BUFFER_SIZE, 0, 5 * WRITE_SIZE - BUFFER_SIZE, APPEND_MODE, false, null);\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(firstReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(secondReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(2)).append(eq(PATH), any(byte[].class), any(), any(), any(TracingContext.class));\r\n    ArgumentCaptor<String> acFlushPath = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Long> acFlushPosition = ArgumentCaptor.forClass(Long.class);\r\n    ArgumentCaptor<TracingContext> acTracingContext = ArgumentCaptor.forClass(TracingContext.class);\r\n    ArgumentCaptor<Boolean> acFlushRetainUnCommittedData = ArgumentCaptor.forClass(Boolean.class);\r\n    ArgumentCaptor<Boolean> acFlushClose = ArgumentCaptor.forClass(Boolean.class);\r\n    ArgumentCaptor<String> acFlushSASToken = ArgumentCaptor.forClass(String.class);\r\n    verify(client, times(1)).flush(acFlushPath.capture(), acFlushPosition.capture(), acFlushRetainUnCommittedData.capture(), acFlushClose.capture(), acFlushSASToken.capture(), isNull(), acTracingContext.capture());\r\n    assertThat(Arrays.asList(PATH)).describedAs(\"path\").isEqualTo(acFlushPath.getAllValues());\r\n    assertThat(Arrays.asList(Long.valueOf(5 * WRITE_SIZE))).describedAs(\"position\").isEqualTo(acFlushPosition.getAllValues());\r\n    assertThat(Arrays.asList(false)).describedAs(\"RetainUnCommittedData flag\").isEqualTo(acFlushRetainUnCommittedData.getAllValues());\r\n    assertThat(Arrays.asList(true)).describedAs(\"Close flag\").isEqualTo(acFlushClose.getAllValues());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyWriteRequestOfBufferSizeAndClose",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void verifyWriteRequestOfBufferSizeAndClose() throws Exception\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    conf.set(accountKey1, accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\r\n    TracingContext tracingContext = new TracingContext(abfsConf.getClientCorrelationId(), \"test-fs-id\", FSOperationType.WRITE, abfsConf.getTracingHeaderFormat(), null);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.append(anyString(), any(byte[].class), any(AppendRequestParameters.class), any(), any(TracingContext.class))).thenReturn(op);\r\n    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(), isNull(), any(TracingContext.class))).thenReturn(op);\r\n    when(op.getSasToken()).thenReturn(\"testToken\");\r\n    when(op.getResult()).thenReturn(httpOp);\r\n    AbfsOutputStream out = new AbfsOutputStream(populateAbfsOutputStreamContext(BUFFER_SIZE, true, false, false, client, PATH, tracingContext, createExecutorService(abfsConf)));\r\n    final byte[] b = new byte[BUFFER_SIZE];\r\n    new Random().nextBytes(b);\r\n    for (int i = 0; i < 2; i++) {\r\n        out.write(b);\r\n    }\r\n    out.close();\r\n    AppendRequestParameters firstReqParameters = new AppendRequestParameters(0, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    AppendRequestParameters secondReqParameters = new AppendRequestParameters(BUFFER_SIZE, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(firstReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(secondReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(2)).append(eq(PATH), any(byte[].class), any(), any(), any(TracingContext.class));\r\n    ArgumentCaptor<String> acFlushPath = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Long> acFlushPosition = ArgumentCaptor.forClass(Long.class);\r\n    ArgumentCaptor<TracingContext> acTracingContext = ArgumentCaptor.forClass(TracingContext.class);\r\n    ArgumentCaptor<Boolean> acFlushRetainUnCommittedData = ArgumentCaptor.forClass(Boolean.class);\r\n    ArgumentCaptor<Boolean> acFlushClose = ArgumentCaptor.forClass(Boolean.class);\r\n    ArgumentCaptor<String> acFlushSASToken = ArgumentCaptor.forClass(String.class);\r\n    verify(client, times(1)).flush(acFlushPath.capture(), acFlushPosition.capture(), acFlushRetainUnCommittedData.capture(), acFlushClose.capture(), acFlushSASToken.capture(), isNull(), acTracingContext.capture());\r\n    assertThat(Arrays.asList(PATH)).describedAs(\"path\").isEqualTo(acFlushPath.getAllValues());\r\n    assertThat(Arrays.asList(Long.valueOf(2 * BUFFER_SIZE))).describedAs(\"position\").isEqualTo(acFlushPosition.getAllValues());\r\n    assertThat(Arrays.asList(false)).describedAs(\"RetainUnCommittedData flag\").isEqualTo(acFlushRetainUnCommittedData.getAllValues());\r\n    assertThat(Arrays.asList(true)).describedAs(\"Close flag\").isEqualTo(acFlushClose.getAllValues());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyWriteRequestOfBufferSize",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void verifyWriteRequestOfBufferSize() throws Exception\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    conf.set(accountKey1, accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.append(anyString(), any(byte[].class), any(AppendRequestParameters.class), any(), any(TracingContext.class))).thenReturn(op);\r\n    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(), isNull(), any(TracingContext.class))).thenReturn(op);\r\n    when(op.getSasToken()).thenReturn(\"testToken\");\r\n    when(op.getResult()).thenReturn(httpOp);\r\n    AbfsOutputStream out = new AbfsOutputStream(populateAbfsOutputStreamContext(BUFFER_SIZE, true, false, false, client, PATH, new TracingContext(abfsConf.getClientCorrelationId(), \"test-fs-id\", FSOperationType.WRITE, abfsConf.getTracingHeaderFormat(), null), createExecutorService(abfsConf)));\r\n    final byte[] b = new byte[BUFFER_SIZE];\r\n    new Random().nextBytes(b);\r\n    for (int i = 0; i < 2; i++) {\r\n        out.write(b);\r\n    }\r\n    Thread.sleep(1000);\r\n    AppendRequestParameters firstReqParameters = new AppendRequestParameters(0, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    AppendRequestParameters secondReqParameters = new AppendRequestParameters(BUFFER_SIZE, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(firstReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(secondReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(2)).append(eq(PATH), any(byte[].class), any(), any(), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyWriteRequestOfBufferSizeWithAppendBlob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void verifyWriteRequestOfBufferSizeWithAppendBlob() throws Exception\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    conf.set(accountKey1, accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.append(anyString(), any(byte[].class), any(AppendRequestParameters.class), any(), any(TracingContext.class))).thenReturn(op);\r\n    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(), isNull(), any(TracingContext.class))).thenReturn(op);\r\n    AbfsOutputStream out = new AbfsOutputStream(populateAbfsOutputStreamContext(BUFFER_SIZE, true, false, true, client, PATH, new TracingContext(abfsConf.getClientCorrelationId(), \"test-fs-id\", FSOperationType.OPEN, abfsConf.getTracingHeaderFormat(), null), createExecutorService(abfsConf)));\r\n    final byte[] b = new byte[BUFFER_SIZE];\r\n    new Random().nextBytes(b);\r\n    for (int i = 0; i < 2; i++) {\r\n        out.write(b);\r\n    }\r\n    Thread.sleep(1000);\r\n    AppendRequestParameters firstReqParameters = new AppendRequestParameters(0, 0, BUFFER_SIZE, APPEND_MODE, true, null);\r\n    AppendRequestParameters secondReqParameters = new AppendRequestParameters(BUFFER_SIZE, 0, BUFFER_SIZE, APPEND_MODE, true, null);\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(firstReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(secondReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(2)).append(eq(PATH), any(byte[].class), any(), any(), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyWriteRequestOfBufferSizeAndHFlush",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void verifyWriteRequestOfBufferSizeAndHFlush() throws Exception\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    when(op.getSasToken()).thenReturn(\"\");\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    conf.set(accountKey1, accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\r\n    TracingContext tracingContext = new TracingContext(abfsConf.getClientCorrelationId(), \"test-fs-id\", FSOperationType.WRITE, abfsConf.getTracingHeaderFormat(), null);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.append(anyString(), any(byte[].class), any(AppendRequestParameters.class), any(), any(TracingContext.class))).thenReturn(op);\r\n    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(), isNull(), any(TracingContext.class))).thenReturn(op);\r\n    AbfsOutputStream out = new AbfsOutputStream(populateAbfsOutputStreamContext(BUFFER_SIZE, true, false, false, client, PATH, new TracingContext(abfsConf.getClientCorrelationId(), \"test-fs-id\", FSOperationType.OPEN, abfsConf.getTracingHeaderFormat(), null), createExecutorService(abfsConf)));\r\n    final byte[] b = new byte[BUFFER_SIZE];\r\n    new Random().nextBytes(b);\r\n    for (int i = 0; i < 2; i++) {\r\n        out.write(b);\r\n    }\r\n    out.hflush();\r\n    AppendRequestParameters firstReqParameters = new AppendRequestParameters(0, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    AppendRequestParameters secondReqParameters = new AppendRequestParameters(BUFFER_SIZE, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(firstReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(secondReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(2)).append(eq(PATH), any(byte[].class), any(), any(), any(TracingContext.class));\r\n    ArgumentCaptor<String> acFlushPath = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Long> acFlushPosition = ArgumentCaptor.forClass(Long.class);\r\n    ArgumentCaptor<TracingContext> acTracingContext = ArgumentCaptor.forClass(TracingContext.class);\r\n    ArgumentCaptor<Boolean> acFlushRetainUnCommittedData = ArgumentCaptor.forClass(Boolean.class);\r\n    ArgumentCaptor<Boolean> acFlushClose = ArgumentCaptor.forClass(Boolean.class);\r\n    ArgumentCaptor<String> acFlushSASToken = ArgumentCaptor.forClass(String.class);\r\n    verify(client, times(1)).flush(acFlushPath.capture(), acFlushPosition.capture(), acFlushRetainUnCommittedData.capture(), acFlushClose.capture(), acFlushSASToken.capture(), isNull(), acTracingContext.capture());\r\n    assertThat(Arrays.asList(PATH)).describedAs(\"path\").isEqualTo(acFlushPath.getAllValues());\r\n    assertThat(Arrays.asList(Long.valueOf(2 * BUFFER_SIZE))).describedAs(\"position\").isEqualTo(acFlushPosition.getAllValues());\r\n    assertThat(Arrays.asList(false)).describedAs(\"RetainUnCommittedData flag\").isEqualTo(acFlushRetainUnCommittedData.getAllValues());\r\n    assertThat(Arrays.asList(false)).describedAs(\"Close flag\").isEqualTo(acFlushClose.getAllValues());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyWriteRequestOfBufferSizeAndFlush",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void verifyWriteRequestOfBufferSizeAndFlush() throws Exception\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    conf.set(accountKey1, accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.append(anyString(), any(byte[].class), any(AppendRequestParameters.class), any(), any(TracingContext.class))).thenReturn(op);\r\n    when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(), isNull(), any(TracingContext.class))).thenReturn(op);\r\n    AbfsOutputStream out = new AbfsOutputStream(populateAbfsOutputStreamContext(BUFFER_SIZE, true, false, false, client, PATH, new TracingContext(abfsConf.getClientCorrelationId(), \"test-fs-id\", FSOperationType.WRITE, abfsConf.getTracingHeaderFormat(), null), createExecutorService(abfsConf)));\r\n    final byte[] b = new byte[BUFFER_SIZE];\r\n    new Random().nextBytes(b);\r\n    for (int i = 0; i < 2; i++) {\r\n        out.write(b);\r\n    }\r\n    Thread.sleep(1000);\r\n    out.flush();\r\n    Thread.sleep(1000);\r\n    AppendRequestParameters firstReqParameters = new AppendRequestParameters(0, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    AppendRequestParameters secondReqParameters = new AppendRequestParameters(BUFFER_SIZE, 0, BUFFER_SIZE, APPEND_MODE, false, null);\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(firstReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(1)).append(eq(PATH), any(byte[].class), refEq(secondReqParameters), any(), any(TracingContext.class));\r\n    verify(client, times(2)).append(eq(PATH), any(byte[].class), any(), any(), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createExecutorService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExecutorService createExecutorService(AbfsConfiguration abfsConf)\n{\r\n    ExecutorService executorService = new SemaphoredDelegatingExecutor(BlockingThreadPoolExecutorService.newInstance(abfsConf.getWriteMaxConcurrentRequestCount(), abfsConf.getMaxWriteRequestsToQueue(), 10L, TimeUnit.SECONDS, \"abfs-test-bounded\"), BLOCK_UPLOAD_ACTIVE_BLOCKS_DEFAULT, true);\r\n    return executorService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "testRenameFileBeingAppended",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameFileBeingAppended() throws Throwable\n{\r\n    skip(\"Skipping as renaming an opened file is not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBlobBackCompat",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testBlobBackCompat() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeFalse(\"This test does not support namespace enabled account\", getIsNamespaceEnabled(getFileSystem()));\r\n    String storageConnectionString = getBlobConnectionString();\r\n    CloudStorageAccount storageAccount = CloudStorageAccount.parse(storageConnectionString);\r\n    CloudBlobClient blobClient = storageAccount.createCloudBlobClient();\r\n    CloudBlobContainer container = blobClient.getContainerReference(this.getFileSystemName());\r\n    container.createIfNotExists();\r\n    Path testPath = getUniquePath(\"test\");\r\n    CloudBlockBlob blockBlob = container.getBlockBlobReference(testPath + \"/10/10/10\");\r\n    blockBlob.uploadText(\"\");\r\n    blockBlob = container.getBlockBlobReference(testPath + \"/10/123/3/2/1/3\");\r\n    blockBlob.uploadText(\"\");\r\n    FileStatus[] fileStatuses = fs.listStatus(new Path(String.format(\"/%s/10/\", testPath)));\r\n    assertEquals(2, fileStatuses.length);\r\n    assertEquals(\"10\", fileStatuses[0].getPath().getName());\r\n    assertTrue(fileStatuses[0].isDirectory());\r\n    assertEquals(0, fileStatuses[0].getLen());\r\n    assertEquals(\"123\", fileStatuses[1].getPath().getName());\r\n    assertTrue(fileStatuses[1].isDirectory());\r\n    assertEquals(0, fileStatuses[1].getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getBlobConnectionString",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String getBlobConnectionString()\n{\r\n    String connectionString;\r\n    if (isIPAddress()) {\r\n        connectionString = \"DefaultEndpointsProtocol=http;BlobEndpoint=http://\" + this.getHostName() + \":8880/\" + this.getAccountName().split(\"\\\\.\")[0] + \";AccountName=\" + this.getAccountName().split(\"\\\\.\")[0] + \";AccountKey=\" + this.getAccountKey();\r\n    } else if (this.getConfiguration().isHttpsAlwaysUsed()) {\r\n        connectionString = \"DefaultEndpointsProtocol=https;BlobEndpoint=https://\" + this.getAccountName().replaceFirst(\"\\\\.dfs\\\\.\", \".blob.\") + \";AccountName=\" + this.getAccountName().split(\"\\\\.\")[0] + \";AccountKey=\" + this.getAccountKey();\r\n    } else {\r\n        connectionString = \"DefaultEndpointsProtocol=http;BlobEndpoint=http://\" + this.getAccountName().replaceFirst(\"\\\\.dfs\\\\.\", \".blob.\") + \";AccountName=\" + this.getAccountName().split(\"\\\\.\")[0] + \";AccountKey=\" + this.getAccountKey();\r\n    }\r\n    return connectionString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertConfigMatches",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertConfigMatches(Configuration conf, String key, String expected)\n{\r\n    String v = conf.get(key);\r\n    assertNotNull(\"No value for key \" + key, v);\r\n    assertEquals(\"Wrong value for key \" + key, expected, v);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsFileSystemRegistered",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbfsFileSystemRegistered() throws Throwable\n{\r\n    assertConfigMatches(new Configuration(true), \"fs.abfs.impl\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSecureAbfsFileSystemRegistered",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSecureAbfsFileSystemRegistered() throws Throwable\n{\r\n    assertConfigMatches(new Configuration(true), \"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsFileContextRegistered",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAbfsFileContextRegistered() throws Throwable\n{\r\n    assertConfigMatches(new Configuration(true), \"fs.AbstractFileSystem.abfs.impl\", ABFS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSecureAbfsFileContextRegistered",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSecureAbfsFileContextRegistered() throws Throwable\n{\r\n    assertConfigMatches(new Configuration(true), \"fs.AbstractFileSystem.abfss.impl\", ABFSS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureAzureBlobFileSystemIsDefaultFileSystem",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void ensureAzureBlobFileSystemIsDefaultFileSystem() throws Exception\n{\r\n    Configuration rawConfig = getRawConfiguration();\r\n    AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.get(rawConfig);\r\n    assertNotNull(\"filesystem\", fs);\r\n    if (this.getAuthType() == AuthType.OAuth) {\r\n        Abfss afs = (Abfss) FileContext.getFileContext(rawConfig).getDefaultFileSystem();\r\n        assertNotNull(\"filecontext\", afs);\r\n    } else {\r\n        Abfs afs = (Abfs) FileContext.getFileContext(rawConfig).getDefaultFileSystem();\r\n        assertNotNull(\"filecontext\", afs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureSecureAzureBlobFileSystemIsDefaultFileSystem",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void ensureSecureAzureBlobFileSystemIsDefaultFileSystem() throws Exception\n{\r\n    final String accountName = getAccountName();\r\n    final String fileSystemName = getFileSystemName();\r\n    final URI defaultUri = new URI(FileSystemUriSchemes.ABFS_SECURE_SCHEME, fileSystemName + \"@\" + accountName, null, null, null);\r\n    Configuration rawConfig = getRawConfiguration();\r\n    rawConfig.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n    SecureAzureBlobFileSystem fs = (SecureAzureBlobFileSystem) FileSystem.get(rawConfig);\r\n    assertNotNull(\"filesystem\", fs);\r\n    Abfss afs = (Abfss) FileContext.getFileContext(rawConfig).getDefaultFileSystem();\r\n    assertNotNull(\"filecontext\", afs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsRestOperationExceptionFormat",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testAbfsRestOperationExceptionFormat() throws IOException\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path nonExistedFilePath1 = new Path(\"nonExistedPath1\");\r\n    Path nonExistedFilePath2 = new Path(\"nonExistedPath2\");\r\n    try {\r\n        FileStatus fileStatus = fs.getFileStatus(nonExistedFilePath1);\r\n    } catch (Exception ex) {\r\n        String errorMessage = ex.getLocalizedMessage();\r\n        String[] errorFields = errorMessage.split(\",\");\r\n        Assert.assertEquals(4, errorFields.length);\r\n        Assert.assertEquals(\"Operation failed: \\\"The specified path does not exist.\\\"\", errorFields[0].trim());\r\n        Assert.assertEquals(\"404\", errorFields[1].trim());\r\n        Assert.assertEquals(\"HEAD\", errorFields[2].trim());\r\n        Assert.assertTrue(errorFields[3].trim().startsWith(\"http\"));\r\n    }\r\n    try {\r\n        fs.listFiles(nonExistedFilePath2, false);\r\n    } catch (Exception ex) {\r\n        String errorMessage = ex.getLocalizedMessage();\r\n        String[] errorFields = errorMessage.split(\",\");\r\n        Assertions.assertThat(errorFields).describedAs(\"fields in exception of %s\", ex).hasSize(6);\r\n        Assert.assertEquals(\"Operation failed: \\\"The specified path does not exist.\\\"\", errorFields[0].trim());\r\n        Assert.assertEquals(\"404\", errorFields[1].trim());\r\n        Assert.assertEquals(\"GET\", errorFields[2].trim());\r\n        Assert.assertTrue(errorFields[3].trim().startsWith(\"http\"));\r\n        Assert.assertEquals(\"PathNotFound\", errorFields[4].trim());\r\n        Assert.assertTrue(errorFields[5].contains(\"RequestId\") && errorFields[5].contains(\"Time\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCustomTokenFetchRetryCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCustomTokenFetchRetryCount() throws Exception\n{\r\n    testWithDifferentCustomTokenFetchRetry(0);\r\n    testWithDifferentCustomTokenFetchRetry(3);\r\n    testWithDifferentCustomTokenFetchRetry(5);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWithDifferentCustomTokenFetchRetry",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testWithDifferentCustomTokenFetchRetry(int numOfRetries) throws Exception\n{\r\n    AzureBlobFileSystem fs = this.getFileSystem();\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    String accountName = config.get(\"fs.azure.abfs.account.name\");\r\n    config.set(\"fs.azure.account.auth.type.\" + accountName, \"Custom\");\r\n    config.set(\"fs.azure.account.oauth.provider.type.\" + accountName, \"org.apache.hadoop.fs\" + \".azurebfs.oauth2.RetryTestTokenProvider\");\r\n    config.set(\"fs.azure.custom.token.fetch.retry.count\", Integer.toString(numOfRetries));\r\n    config.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\");\r\n    final AzureBlobFileSystem fs1 = (AzureBlobFileSystem) FileSystem.newInstance(fs.getUri(), config);\r\n    RetryTestTokenProvider.ResetStatusToFirstTokenFetch();\r\n    intercept(Exception.class, () -> {\r\n        fs1.getFileStatus(new Path(\"/\"));\r\n    });\r\n    Assert.assertTrue(\"Number of token fetch retries (\" + RetryTestTokenProvider.reTryCount + \") done, does not match with fs.azure.custom.token.fetch.retry.count configured (\" + numOfRetries + \")\", RetryTestTokenProvider.reTryCount == numOfRetries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAuthFailException",
  "errType" : [ "AbfsRestOperationException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAuthFailException() throws Exception\n{\r\n    Configuration config = new Configuration(getRawConfiguration());\r\n    String accountName = config.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    config.set(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME + DOT + accountName, \"Custom\");\r\n    config.set(FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME + DOT + accountName, RETRY_TEST_TOKEN_PROVIDER);\r\n    config.set(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, \"false\");\r\n    final AzureBlobFileSystem fs = getFileSystem(config);\r\n    try {\r\n        fs.getFileStatus(new Path(\"/\"));\r\n        fail(\"Should fail at auth token fetch call\");\r\n    } catch (AbfsRestOperationException e) {\r\n        String errorDesc = \"Should throw RestOp exception on AAD failure\";\r\n        Assertions.assertThat(e.getStatusCode()).describedAs(\"Incorrect status code. \" + errorDesc).isEqualTo(-1);\r\n        Assertions.assertThat(e.getErrorCode()).describedAs(\"Incorrect error code. \" + errorDesc).isEqualTo(AzureServiceErrorCode.UNKNOWN);\r\n        Assertions.assertThat(e.getErrorMessage()).describedAs(\"Incorrect error message. \" + errorDesc).contains(\"Auth failure: \");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureAzureBlobFileSystemIsInitialized",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void ensureAzureBlobFileSystemIsInitialized() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final String accountName = getAccountName();\r\n    final String filesystem = getFileSystemName();\r\n    String scheme = this.getAuthType() == AuthType.SharedKey ? FileSystemUriSchemes.ABFS_SCHEME : FileSystemUriSchemes.ABFS_SECURE_SCHEME;\r\n    assertEquals(fs.getUri(), new URI(scheme, filesystem + \"@\" + accountName, null, null, null));\r\n    assertNotNull(\"working directory\", fs.getWorkingDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureSecureAzureBlobFileSystemIsInitialized",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void ensureSecureAzureBlobFileSystemIsInitialized() throws Exception\n{\r\n    final String accountName = getAccountName();\r\n    final String filesystem = getFileSystemName();\r\n    final URI defaultUri = new URI(FileSystemUriSchemes.ABFS_SECURE_SCHEME, filesystem + \"@\" + accountName, null, null, null);\r\n    Configuration rawConfig = getRawConfiguration();\r\n    rawConfig.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n    try (SecureAzureBlobFileSystem fs = (SecureAzureBlobFileSystem) FileSystem.newInstance(rawConfig)) {\r\n        assertEquals(fs.getUri(), new URI(FileSystemUriSchemes.ABFS_SECURE_SCHEME, filesystem + \"@\" + accountName, null, null, null));\r\n        assertNotNull(\"working directory\", fs.getWorkingDirectory());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "decodeIdentifier",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StubAbfsTokenIdentifier decodeIdentifier(final Token<?> token) throws IOException\n{\r\n    StubAbfsTokenIdentifier id = (StubAbfsTokenIdentifier) token.decodeIdentifier();\r\n    Preconditions.checkNotNull(id, \"Null decoded identifier\");\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCreated()\n{\r\n    return created;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getUuid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUuid()\n{\r\n    return uuid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(final DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    Text.writeString(out, uri.toString());\r\n    Text.writeString(out, uuid);\r\n    out.writeLong(created);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(final DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    uri = URI.create(Text.readString(in, MAX_TEXT_LENGTH));\r\n    uuid = Text.readString(in, MAX_TEXT_LENGTH);\r\n    created = in.readLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AbfsIDBTokenIdentifier{\");\r\n    sb.append(\"uri=\").append(uri);\r\n    sb.append(\", uuid='\").append(uuid).append('\\'');\r\n    sb.append(\", created='\").append(new Date(created)).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean equals(final Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    if (!super.equals(o)) {\r\n        return false;\r\n    }\r\n    final StubAbfsTokenIdentifier that = (StubAbfsTokenIdentifier) o;\r\n    return created == that.created && uri.equals(that.uri) && uuid.equals(that.uuid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hash(super.hashCode(), uri, uuid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "nameTestThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameTestThread()\n{\r\n    Thread.currentThread().setName(\"JUnit\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "nameThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameThread()\n{\r\n    Thread.currentThread().setName(\"JUnit-\" + methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return AzureTestConstants.AZURE_TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.set(KEY_USE_CONTAINER_SASKEY_FOR_ALL_ACCESS, \"false\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    testPath = path(\"testfile.dat\");\r\n    testFolderPath = path(\"testfolder\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    IOUtils.closeStream(inputStream);\r\n    ContractTestUtils.rm(fs, testPath, true, false);\r\n    ContractTestUtils.rm(fs, testFolderPath, true, false);\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getInputStreamToTest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void getInputStreamToTest(FileSystem fs, Path testPath) throws Throwable\n{\r\n    FSDataOutputStream outputStream = fs.create(testPath);\r\n    String testString = \"This is a test string\";\r\n    outputStream.write(testString.getBytes());\r\n    outputStream.close();\r\n    inputStream = fs.open(testPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedBlockBlobReadScenario",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMultiThreadedBlockBlobReadScenario() throws Throwable\n{\r\n    AzureBlobStorageTestAccount testAccount = createTestAccount();\r\n    NativeAzureFileSystem fs = testAccount.getFileSystem();\r\n    Path base = methodPath();\r\n    Path testFilePath1 = new Path(base, \"test1.dat\");\r\n    Path renamePath = new Path(base, \"test2.dat\");\r\n    getInputStreamToTest(fs, testFilePath1);\r\n    Thread renameThread = new Thread(new RenameThread(fs, testFilePath1, renamePath));\r\n    renameThread.start();\r\n    renameThread.join();\r\n    byte[] readBuffer = new byte[512];\r\n    inputStream.read(readBuffer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadBlockBlobSeekScenario",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultiThreadBlockBlobSeekScenario() throws Throwable\n{\r\n    Path base = methodPath();\r\n    Path testFilePath1 = new Path(base, \"test1.dat\");\r\n    Path renamePath = new Path(base, \"test2.dat\");\r\n    getInputStreamToTest(fs, testFilePath1);\r\n    Thread renameThread = new Thread(new RenameThread(fs, testFilePath1, renamePath));\r\n    renameThread.start();\r\n    renameThread.join();\r\n    inputStream.seek(5);\r\n    inputStream.read();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedPageBlobSetPermissionScenario",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiThreadedPageBlobSetPermissionScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        fs.setPermission(testPath, new FsPermission(FsAction.EXECUTE, FsAction.READ, FsAction.READ));\r\n    }\r\n    fs.setPermission(testPath, new FsPermission(FsAction.EXECUTE, FsAction.READ, FsAction.READ));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedBlockBlobSetPermissionScenario",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiThreadedBlockBlobSetPermissionScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        fs.setPermission(testPath, new FsPermission(FsAction.EXECUTE, FsAction.READ, FsAction.READ));\r\n    }\r\n    fs.setPermission(testPath, new FsPermission(FsAction.EXECUTE, FsAction.READ, FsAction.READ));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedPageBlobOpenScenario",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMultiThreadedPageBlobOpenScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        inputStream = fs.open(testPath);\r\n        inputStream.close();\r\n    }\r\n    inputStream = fs.open(testPath);\r\n    inputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedBlockBlobOpenScenario",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMultiThreadedBlockBlobOpenScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        inputStream = fs.open(testPath);\r\n        inputStream.close();\r\n    }\r\n    inputStream = fs.open(testPath);\r\n    inputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedBlockBlobSetOwnerScenario",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiThreadedBlockBlobSetOwnerScenario() throws Throwable\n{\r\n    createEmptyFile(createTestAccount(), testPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        fs.setOwner(testPath, \"testowner\", \"testgroup\");\r\n    }\r\n    fs.setOwner(testPath, \"testowner\", \"testgroup\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedPageBlobSetOwnerScenario",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiThreadedPageBlobSetOwnerScenario() throws Throwable\n{\r\n    createEmptyFile(getPageBlobTestStorageAccount(), testPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        fs.setOwner(testPath, \"testowner\", \"testgroup\");\r\n    }\r\n    fs.setOwner(testPath, \"testowner\", \"testgroup\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedBlockBlobListStatusScenario",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiThreadedBlockBlobListStatusScenario() throws Throwable\n{\r\n    createTestFolder(createTestAccount(), testFolderPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testFolderPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        fs.listStatus(testFolderPath);\r\n    }\r\n    fs.listStatus(testFolderPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedPageBlobListStatusScenario",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiThreadedPageBlobListStatusScenario() throws Throwable\n{\r\n    createTestFolder(getPageBlobTestStorageAccount(), testFolderPath);\r\n    Thread t = new Thread(new DeleteThread(fs, testFolderPath));\r\n    t.start();\r\n    while (t.isAlive()) {\r\n        fs.listStatus(testFolderPath);\r\n    }\r\n    fs.listStatus(testFolderPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedPageBlobReadScenario",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultiThreadedPageBlobReadScenario() throws Throwable\n{\r\n    bindToTestAccount(getPageBlobTestStorageAccount());\r\n    Path base = methodPath();\r\n    Path testFilePath1 = new Path(base, \"test1.dat\");\r\n    Path renamePath = new Path(base, \"test2.dat\");\r\n    getInputStreamToTest(fs, testFilePath1);\r\n    Thread renameThread = new Thread(new RenameThread(fs, testFilePath1, renamePath));\r\n    renameThread.start();\r\n    renameThread.join();\r\n    byte[] readBuffer = new byte[512];\r\n    inputStream.read(readBuffer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultiThreadedPageBlobSeekScenario",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultiThreadedPageBlobSeekScenario() throws Throwable\n{\r\n    bindToTestAccount(getPageBlobTestStorageAccount());\r\n    Path base = methodPath();\r\n    Path testFilePath1 = new Path(base, \"test1.dat\");\r\n    Path renamePath = new Path(base, \"test2.dat\");\r\n    getInputStreamToTest(fs, testFilePath1);\r\n    Thread renameThread = new Thread(new RenameThread(fs, testFilePath1, renamePath));\r\n    renameThread.start();\r\n    renameThread.join();\r\n    inputStream.seek(5);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testDefaultOAuthTokenFetchRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDefaultOAuthTokenFetchRetryPolicy() throws Exception\n{\r\n    getConfiguration().unset(AZURE_OAUTH_TOKEN_FETCH_RETRY_COUNT);\r\n    getConfiguration().unset(AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF);\r\n    getConfiguration().unset(AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF);\r\n    getConfiguration().unset(AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF);\r\n    String accountName = getConfiguration().get(FS_AZURE_ACCOUNT_NAME);\r\n    AbfsConfiguration abfsConfig = new AbfsConfiguration(getRawConfiguration(), accountName);\r\n    ExponentialRetryPolicy retryPolicy = abfsConfig.getOauthTokenFetchRetryPolicy();\r\n    Assertions.assertThat(retryPolicy.getRetryCount()).describedAs(\"retryCount should be the default value {} as the same \" + \"is not configured\", DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS).isEqualTo(DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_ATTEMPTS);\r\n    Assertions.assertThat(retryPolicy.getMinBackoff()).describedAs(\"minBackOff should be the default value {} as the same is \" + \"not configured\", DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL).isEqualTo(DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF_INTERVAL);\r\n    Assertions.assertThat(retryPolicy.getMaxBackoff()).describedAs(\"maxBackOff should be the default value {} as the same is \" + \"not configured\", DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL).isEqualTo(DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF_INTERVAL);\r\n    Assertions.assertThat(retryPolicy.getDeltaBackoff()).describedAs(\"deltaBackOff should be the default value {} as the same \" + \"is \" + \"not configured\", DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF).isEqualTo(DEFAULT_AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOAuthTokenFetchRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testOAuthTokenFetchRetryPolicy() throws IOException, IllegalAccessException\n{\r\n    getConfiguration().set(AZURE_OAUTH_TOKEN_FETCH_RETRY_COUNT, String.valueOf(TEST_RETRY_COUNT));\r\n    getConfiguration().set(AZURE_OAUTH_TOKEN_FETCH_RETRY_MIN_BACKOFF, String.valueOf(TEST_MIN_BACKOFF));\r\n    getConfiguration().set(AZURE_OAUTH_TOKEN_FETCH_RETRY_MAX_BACKOFF, String.valueOf(TEST_MAX_BACKOFF));\r\n    getConfiguration().set(AZURE_OAUTH_TOKEN_FETCH_RETRY_DELTA_BACKOFF, String.valueOf(TEST_DELTA_BACKOFF));\r\n    String accountName = getConfiguration().get(FS_AZURE_ACCOUNT_NAME);\r\n    AbfsConfiguration abfsConfig = new AbfsConfiguration(getRawConfiguration(), accountName);\r\n    ExponentialRetryPolicy retryPolicy = abfsConfig.getOauthTokenFetchRetryPolicy();\r\n    Assertions.assertThat(retryPolicy.getRetryCount()).describedAs(\"retryCount should be {}\", TEST_RETRY_COUNT).isEqualTo(TEST_RETRY_COUNT);\r\n    Assertions.assertThat(retryPolicy.getMinBackoff()).describedAs(\"minBackOff should be {}\", TEST_MIN_BACKOFF).isEqualTo(TEST_MIN_BACKOFF);\r\n    Assertions.assertThat(retryPolicy.getMaxBackoff()).describedAs(\"maxBackOff should be {}\", TEST_MAX_BACKOFF).isEqualTo(TEST_MAX_BACKOFF);\r\n    Assertions.assertThat(retryPolicy.getDeltaBackoff()).describedAs(\"deltaBackOff should be {}\", TEST_DELTA_BACKOFF).isEqualTo(TEST_DELTA_BACKOFF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getFilesCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getFilesCreated(AzureBlobStorageTestAccount testAccount)\n{\r\n    return testAccount.getLatestMetricValue(WASB_FILES_CREATED, 0).intValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsAcrossFileSystems",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testMetricsAcrossFileSystems() throws Exception\n{\r\n    AzureBlobStorageTestAccount a1, a2, a3;\r\n    a1 = AzureBlobStorageTestAccount.createMock();\r\n    assertFilesCreated(a1, \"a1\", 0);\r\n    a2 = AzureBlobStorageTestAccount.createMock();\r\n    assertFilesCreated(a2, \"a2\", 0);\r\n    a1.getFileSystem().create(new Path(\"/foo\")).close();\r\n    a1.getFileSystem().create(new Path(\"/bar\")).close();\r\n    a2.getFileSystem().create(new Path(\"/baz\")).close();\r\n    assertFilesCreated(a1, \"a1\", 0);\r\n    assertFilesCreated(a2, \"a2\", 0);\r\n    a1.closeFileSystem();\r\n    a2.closeFileSystem();\r\n    assertFilesCreated(a1, \"a1\", 2);\r\n    assertFilesCreated(a2, \"a2\", 1);\r\n    a3 = AzureBlobStorageTestAccount.createMock();\r\n    assertFilesCreated(a3, \"a3\", 0);\r\n    a3.closeFileSystem();\r\n    assertFilesCreated(a3, \"a3\", 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "assertFilesCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertFilesCreated(AzureBlobStorageTestAccount account, String name, int expected)\n{\r\n    assertEquals(\"Files created in account \" + name, expected, getFilesCreated(account));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMetricsSourceNames",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMetricsSourceNames()\n{\r\n    String name1 = NativeAzureFileSystem.newMetricsSourceName();\r\n    String name2 = NativeAzureFileSystem.newMetricsSourceName();\r\n    assertTrue(name1.startsWith(\"AzureFileSystemMetrics\"));\r\n    assertTrue(name2.startsWith(\"AzureFileSystemMetrics\"));\r\n    assertTrue(!name1.equals(name2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testSkipMetricsCollection",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSkipMetricsCollection() throws Exception\n{\r\n    AzureBlobStorageTestAccount a;\r\n    a = AzureBlobStorageTestAccount.createMock();\r\n    a.getFileSystem().getConf().setBoolean(NativeAzureFileSystem.SKIP_AZURE_METRICS_PROPERTY_NAME, true);\r\n    a.getFileSystem().create(new Path(\"/foo\")).close();\r\n    a.closeFileSystem();\r\n    assertFilesCreated(a, \"a\", 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getLongGaugeValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLongGaugeValue(AzureFileSystemInstrumentation instrumentation, String gaugeName)\n{\r\n    return getLongGauge(gaugeName, getMetrics(instrumentation));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getLongCounterValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLongCounterValue(AzureFileSystemInstrumentation instrumentation, String counterName)\n{\r\n    return getLongCounter(counterName, getMetrics(instrumentation));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentBytesWritten(AzureFileSystemInstrumentation instrumentation)\n{\r\n    return getLongGaugeValue(instrumentation, WASB_BYTES_WRITTEN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentBytesRead(AzureFileSystemInstrumentation instrumentation)\n{\r\n    return getLongGaugeValue(instrumentation, WASB_BYTES_READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentTotalBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentTotalBytesWritten(AzureFileSystemInstrumentation instrumentation)\n{\r\n    return getLongCounterValue(instrumentation, WASB_RAW_BYTES_UPLOADED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentTotalBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentTotalBytesRead(AzureFileSystemInstrumentation instrumentation)\n{\r\n    return getLongCounterValue(instrumentation, WASB_RAW_BYTES_DOWNLOADED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentWebResponses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentWebResponses(AzureFileSystemInstrumentation instrumentation)\n{\r\n    return getLongCounter(WASB_WEB_RESPONSES, getMetrics(instrumentation));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return AzureTestConstants.SCALE_TEST_TIMEOUT_MILLIS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n    assumeScaleTestsEnabled(binding.getRawConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsFileSystemContract createContract(Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCopyFromLocalFileSystem",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCopyFromLocalFileSystem() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path localFilePath = new Path(System.getProperty(\"test.build.data\", \"azure_test\"));\r\n    FileSystem localFs = FileSystem.getLocal(new Configuration());\r\n    localFs.delete(localFilePath, true);\r\n    try {\r\n        writeString(localFs, localFilePath, \"Testing\");\r\n        Path dstPath = path(\"copiedFromLocal\");\r\n        assertTrue(FileUtil.copy(localFs, localFilePath, fs, dstPath, false, fs.getConf()));\r\n        assertIsFile(fs, dstPath);\r\n        assertEquals(\"Testing\", readString(fs, dstPath));\r\n        fs.delete(dstPath, true);\r\n    } finally {\r\n        localFs.delete(localFilePath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "readString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String readString(FileSystem fs, Path testFile) throws IOException\n{\r\n    return readString(fs.open(testFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "readString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String readString(FSDataInputStream inputStream) throws IOException\n{\r\n    try (BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream))) {\r\n        final int bufferSize = 1024;\r\n        char[] buffer = new char[bufferSize];\r\n        int count = reader.read(buffer, 0, bufferSize);\r\n        if (count > bufferSize) {\r\n            throw new IOException(\"Exceeded buffer size\");\r\n        }\r\n        return new String(buffer, 0, count);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "writeString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeString(FileSystem fs, Path path, String value) throws IOException\n{\r\n    writeString(fs.create(path, true), value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "writeString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeString(FSDataOutputStream outputStream, String value) throws IOException\n{\r\n    try (BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(outputStream))) {\r\n        writer.write(value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return this.binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, this.isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateDirWithExistingDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateDirWithExistingDir() throws Exception\n{\r\n    Assume.assumeTrue(DEFAULT_FS_AZURE_ENABLE_MKDIR_OVERWRITE || !getIsNamespaceEnabled(getFileSystem()));\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path path = path(\"testFolder\");\r\n    assertMkdirs(fs, path);\r\n    assertMkdirs(fs, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testMkdirExistingDirOverwriteFalse",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMkdirExistingDirOverwriteFalse() throws Exception\n{\r\n    Assume.assumeFalse(\"Ignore test until default overwrite is set to false\", DEFAULT_FS_AZURE_ENABLE_MKDIR_OVERWRITE);\r\n    Assume.assumeTrue(\"Ignore test for Non-HNS accounts\", getIsNamespaceEnabled(getFileSystem()));\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(FS_AZURE_ENABLE_MKDIR_OVERWRITE, Boolean.toString(false));\r\n    AzureBlobFileSystem fs = getFileSystem(config);\r\n    Path path = path(\"testFolder\");\r\n    assertMkdirs(fs, path);\r\n    long timeCreated = fs.getFileStatus(path).getModificationTime();\r\n    assertMkdirs(fs, path);\r\n    assertEquals(\"LMT should not be updated for existing dir\", timeCreated, fs.getFileStatus(path).getModificationTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createDirWithExistingFilename",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createDirWithExistingFilename() throws Exception\n{\r\n    Assume.assumeFalse(\"Ignore test until default overwrite is set to false\", DEFAULT_FS_AZURE_ENABLE_MKDIR_OVERWRITE && getIsNamespaceEnabled(getFileSystem()));\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path path = path(\"testFilePath\");\r\n    fs.create(path).close();\r\n    assertTrue(fs.getFileStatus(path).isFile());\r\n    intercept(FileAlreadyExistsException.class, () -> fs.mkdirs(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreateRoot() throws Exception\n{\r\n    assertMkdirs(getFileSystem(), new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultCreateOverwriteDirTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDefaultCreateOverwriteDirTest() throws Throwable\n{\r\n    testCreateDirOverwrite(true);\r\n    testCreateDirOverwrite(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateDirOverwrite",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCreateDirOverwrite(boolean enableConditionalCreateOverwrite) throws Throwable\n{\r\n    final AzureBlobFileSystem currentFs = getFileSystem();\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(\"fs.azure.enable.conditional.create.overwrite\", Boolean.toString(enableConditionalCreateOverwrite));\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config);\r\n    long totalConnectionMadeBeforeTest = fs.getInstrumentationMap().get(CONNECTIONS_MADE.getStatName());\r\n    int mkdirRequestCount = 0;\r\n    final Path dirPath = new Path(\"/DirPath_\" + UUID.randomUUID().toString());\r\n    fs.mkdirs(dirPath);\r\n    mkdirRequestCount++;\r\n    assertAbfsStatistics(CONNECTIONS_MADE, totalConnectionMadeBeforeTest + mkdirRequestCount, fs.getInstrumentationMap());\r\n    fs.mkdirs(dirPath);\r\n    mkdirRequestCount++;\r\n    assertAbfsStatistics(CONNECTIONS_MADE, totalConnectionMadeBeforeTest + mkdirRequestCount, fs.getInstrumentationMap());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetGetXAttr",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSetGetXAttr() throws Exception\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\r\n    Assume.assumeTrue(getIsNamespaceEnabled(fs));\r\n    byte[] attributeValue1 = fs.getAbfsStore().encodeAttribute(\"hi\");\r\n    byte[] attributeValue2 = fs.getAbfsStore().encodeAttribute(\"你好\");\r\n    String attributeName1 = \"user.asciiAttribute\";\r\n    String attributeName2 = \"user.unicodeAttribute\";\r\n    Path testFile = path(\"setGetXAttr\");\r\n    touch(testFile);\r\n    assertNull(fs.getXAttr(testFile, attributeName1));\r\n    fs.registerListener(new TracingHeaderValidator(conf.getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.SET_ATTR, true, 0));\r\n    fs.setXAttr(testFile, attributeName1, attributeValue1);\r\n    fs.setListenerOperation(FSOperationType.GET_ATTR);\r\n    assertArrayEquals(attributeValue1, fs.getXAttr(testFile, attributeName1));\r\n    fs.registerListener(null);\r\n    fs.setXAttr(testFile, attributeName2, attributeValue2);\r\n    assertArrayEquals(attributeValue1, fs.getXAttr(testFile, attributeName1));\r\n    assertArrayEquals(attributeValue2, fs.getXAttr(testFile, attributeName2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetGetXAttrCreateReplace",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetGetXAttrCreateReplace() throws Exception\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Assume.assumeTrue(getIsNamespaceEnabled(fs));\r\n    byte[] attributeValue = fs.getAbfsStore().encodeAttribute(\"one\");\r\n    String attributeName = \"user.someAttribute\";\r\n    Path testFile = path(\"createReplaceXAttr\");\r\n    touch(testFile);\r\n    fs.setXAttr(testFile, attributeName, attributeValue, CREATE_FLAG);\r\n    assertArrayEquals(attributeValue, fs.getXAttr(testFile, attributeName));\r\n    intercept(IOException.class, () -> fs.setXAttr(testFile, attributeName, attributeValue, CREATE_FLAG));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetGetXAttrReplace",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSetGetXAttrReplace() throws Exception\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Assume.assumeTrue(getIsNamespaceEnabled(fs));\r\n    byte[] attributeValue1 = fs.getAbfsStore().encodeAttribute(\"one\");\r\n    byte[] attributeValue2 = fs.getAbfsStore().encodeAttribute(\"two\");\r\n    String attributeName = \"user.someAttribute\";\r\n    Path testFile = path(\"replaceXAttr\");\r\n    intercept(IOException.class, () -> {\r\n        touch(testFile);\r\n        fs.setXAttr(testFile, attributeName, attributeValue1, REPLACE_FLAG);\r\n    });\r\n    fs.setXAttr(testFile, attributeName, attributeValue1, CREATE_FLAG);\r\n    fs.setXAttr(testFile, attributeName, attributeValue2, REPLACE_FLAG);\r\n    assertArrayEquals(attributeValue2, fs.getXAttr(testFile, attributeName));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeTestFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeTestFile(OutputStream writeStream, long size, long flushInterval) throws IOException\n{\r\n    int bufferSize = (int) Math.min(1000, flushInterval);\r\n    byte[] buffer = new byte[bufferSize];\r\n    Arrays.fill(buffer, (byte) 7);\r\n    int bytesWritten = 0;\r\n    int bytesUnflushed = 0;\r\n    while (bytesWritten < size) {\r\n        int numberToWrite = (int) Math.min(bufferSize, size - bytesWritten);\r\n        writeStream.write(buffer, 0, numberToWrite);\r\n        bytesWritten += numberToWrite;\r\n        bytesUnflushed += numberToWrite;\r\n        if (bytesUnflushed >= flushInterval) {\r\n            writeStream.flush();\r\n            bytesUnflushed = 0;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeTestFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TestResult writeTestFile(NativeAzureFileSystem fs, Path path, long size, long flushInterval) throws IOException\n{\r\n    AzureFileSystemInstrumentation instrumentation = fs.getInstrumentation();\r\n    long initialRequests = instrumentation.getCurrentWebResponses();\r\n    Date start = new Date();\r\n    OutputStream output = fs.create(path);\r\n    writeTestFile(output, size, flushInterval);\r\n    output.close();\r\n    long finalRequests = instrumentation.getCurrentWebResponses();\r\n    return new TestResult(new Date().getTime() - start.getTime(), finalRequests - initialRequests);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeBlockBlobTestFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TestResult writeBlockBlobTestFile(NativeAzureFileSystem fs, long size, long flushInterval) throws IOException\n{\r\n    return writeTestFile(fs, new Path(\"/blockBlob\"), size, flushInterval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writePageBlobTestFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TestResult writePageBlobTestFile(NativeAzureFileSystem fs, long size, long flushInterval) throws IOException\n{\r\n    Path testFile = AzureTestUtils.blobPathForTests(fs, \"writePageBlobTestFile\");\r\n    return writeTestFile(fs, testFile, size, flushInterval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testTenKbFileFrequentFlush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTenKbFileFrequentFlush() throws Exception\n{\r\n    testForSizeAndFlushInterval(getFileSystem(), 10 * 1000, 500);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testForSizeAndFlushInterval",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testForSizeAndFlushInterval(NativeAzureFileSystem fs, final long size, final long flushInterval) throws IOException\n{\r\n    for (int i = 0; i < 5; i++) {\r\n        TestResult pageBlobResults = writePageBlobTestFile(fs, size, flushInterval);\r\n        System.out.printf(\"Page blob upload took %d ms. Total number of requests: %d.\\n\", pageBlobResults.timeTakenInMs, pageBlobResults.totalNumberOfRequests);\r\n        TestResult blockBlobResults = writeBlockBlobTestFile(fs, size, flushInterval);\r\n        System.out.printf(\"Block blob upload took %d ms. Total number of requests: %d.\\n\", blockBlobResults.timeTakenInMs, blockBlobResults.totalNumberOfRequests);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    long size = 10 * 1000 * 1000;\r\n    long flushInterval = 2000;\r\n    if (argv.length > 0) {\r\n        size = Long.parseLong(argv[0]);\r\n    }\r\n    if (argv.length > 1) {\r\n        flushInterval = Long.parseLong(argv[1]);\r\n    }\r\n    testForSizeAndFlushInterval((NativeAzureFileSystem) FileSystem.get(conf), size, flushInterval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPositionedRead",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testPositionedRead() throws IOException\n{\r\n    describe(\"Testing positioned reads in AbfsInputStream\");\r\n    Path dest = path(methodName.getMethodName());\r\n    byte[] data = ContractTestUtils.dataset(TEST_FILE_DATA_SIZE, 'a', 'z');\r\n    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length, TEST_FILE_DATA_SIZE, true);\r\n    int bytesToRead = 10;\r\n    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\r\n        assertTrue(\"unexpected stream type \" + inputStream.getWrappedStream().getClass().getSimpleName(), inputStream.getWrappedStream() instanceof AbfsInputStream);\r\n        byte[] readBuffer = new byte[bytesToRead];\r\n        int readPos = 0;\r\n        Assertions.assertThat(inputStream.read(readPos, readBuffer, 0, bytesToRead)).describedAs(\"AbfsInputStream pread did not read the correct number of bytes\").isEqualTo(bytesToRead);\r\n        Assertions.assertThat(readBuffer).describedAs(\"AbfsInputStream pread did not read correct data\").containsExactly(Arrays.copyOfRange(data, readPos, readPos + bytesToRead));\r\n        Assertions.assertThat(Arrays.copyOfRange(((AbfsInputStream) inputStream.getWrappedStream()).getBuffer(), 0, TEST_FILE_DATA_SIZE)).describedAs(\"AbfsInputStream pread did not read more data into its buffer\").containsExactly(data);\r\n        assertStatistics(inputStream.getIOStatistics(), bytesToRead, 1, 1, TEST_FILE_DATA_SIZE);\r\n        readPos = 50;\r\n        Assertions.assertThat(inputStream.read(readPos, readBuffer, 0, bytesToRead)).describedAs(\"AbfsInputStream pread did not read the correct number of bytes\").isEqualTo(bytesToRead);\r\n        Assertions.assertThat(readBuffer).describedAs(\"AbfsInputStream pread did not read correct data\").containsExactly(Arrays.copyOfRange(data, readPos, readPos + bytesToRead));\r\n        assertStatistics(inputStream.getIOStatistics(), 2 * bytesToRead, 2, 1, TEST_FILE_DATA_SIZE);\r\n        Assertions.assertThat(inputStream.getPos()).describedAs(\"AbfsInputStream positioned reads moved stream position\").isEqualTo(0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertStatistics",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertStatistics(IOStatistics ioStatistics, long expectedBytesRead, long expectedReadOps, long expectedRemoteReadOps, long expectedRemoteReadBytes)\n{\r\n    Assertions.assertThat(ioStatistics.counters().get(StreamStatisticNames.STREAM_READ_BYTES).longValue()).describedAs(\"Mismatch in bytesRead statistics\").isEqualTo(expectedBytesRead);\r\n    Assertions.assertThat(ioStatistics.counters().get(StreamStatisticNames.STREAM_READ_OPERATIONS).longValue()).describedAs(\"Mismatch in readOps statistics\").isEqualTo(expectedReadOps);\r\n    Assertions.assertThat(ioStatistics.counters().get(StreamStatisticNames.REMOTE_READ_OP).longValue()).describedAs(\"Mismatch in remoteReadOps statistics\").isEqualTo(expectedRemoteReadOps);\r\n    Assertions.assertThat(ioStatistics.counters().get(StreamStatisticNames.REMOTE_BYTES_READ).longValue()).describedAs(\"Mismatch in remoteReadBytes statistics\").isEqualTo(expectedRemoteReadBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPositionedReadWithBufferedReadDisabled",
  "errType" : [ "IllegalArgumentException|UnsupportedOperationException|InterruptedException|ExecutionException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testPositionedReadWithBufferedReadDisabled() throws IOException\n{\r\n    describe(\"Testing positioned reads in AbfsInputStream with BufferedReadDisabled\");\r\n    Path dest = path(methodName.getMethodName());\r\n    byte[] data = ContractTestUtils.dataset(TEST_FILE_DATA_SIZE, 'a', 'z');\r\n    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length, TEST_FILE_DATA_SIZE, true);\r\n    FutureDataInputStreamBuilder builder = getFileSystem().openFile(dest);\r\n    builder.opt(ConfigurationKeys.FS_AZURE_BUFFERED_PREAD_DISABLE, true);\r\n    FSDataInputStream inputStream = null;\r\n    try {\r\n        inputStream = builder.build().get();\r\n    } catch (IllegalArgumentException | UnsupportedOperationException | InterruptedException | ExecutionException e) {\r\n        throw new IOException(\"Exception opening \" + dest + \" with FutureDataInputStreamBuilder\", e);\r\n    }\r\n    assertNotNull(\"Null InputStream over \" + dest, inputStream);\r\n    int bytesToRead = 10;\r\n    try {\r\n        AbfsInputStream abfsIs = (AbfsInputStream) inputStream.getWrappedStream();\r\n        byte[] readBuffer = new byte[bytesToRead];\r\n        int readPos = 10;\r\n        Assertions.assertThat(inputStream.read(readPos, readBuffer, 0, bytesToRead)).describedAs(\"AbfsInputStream pread did not read the correct number of bytes\").isEqualTo(bytesToRead);\r\n        Assertions.assertThat(readBuffer).describedAs(\"AbfsInputStream pread did not read correct data\").containsExactly(Arrays.copyOfRange(data, readPos, readPos + bytesToRead));\r\n        assertNull(\"AbfsInputStream pread caused the internal buffer creation\", abfsIs.getBuffer());\r\n        assertStatistics(inputStream.getIOStatistics(), bytesToRead, 1, 1, bytesToRead);\r\n        readPos = 40;\r\n        Assertions.assertThat(inputStream.read(readPos, readBuffer, 0, bytesToRead)).describedAs(\"AbfsInputStream pread did not read the correct number of bytes\").isEqualTo(bytesToRead);\r\n        Assertions.assertThat(readBuffer).describedAs(\"AbfsInputStream pread did not read correct data\").containsExactly(Arrays.copyOfRange(data, readPos, readPos + bytesToRead));\r\n        assertStatistics(inputStream.getIOStatistics(), 2 * bytesToRead, 2, 2, 2 * bytesToRead);\r\n        inputStream.seek(0);\r\n        Assertions.assertThat(inputStream.read(readBuffer)).describedAs(\"AbfsInputStream seek+read did not read the correct number of bytes\").isEqualTo(bytesToRead);\r\n        Assertions.assertThat(Arrays.copyOfRange(((AbfsInputStream) inputStream.getWrappedStream()).getBuffer(), 0, TEST_FILE_DATA_SIZE)).describedAs(\"AbfsInputStream seek+read did not read more data into its buffer\").containsExactly(data);\r\n        assertStatistics(inputStream.getIOStatistics(), 3 * bytesToRead, 3, 3, TEST_FILE_DATA_SIZE + 2 * bytesToRead);\r\n        resetBuffer(abfsIs.getBuffer());\r\n        readPos = 0;\r\n        Assertions.assertThat(inputStream.read(readPos, readBuffer, 0, bytesToRead)).describedAs(\"AbfsInputStream pread did not read the correct number of bytes\").isEqualTo(bytesToRead);\r\n        Assertions.assertThat(readBuffer).describedAs(\"AbfsInputStream pread did not read correct data\").containsExactly(Arrays.copyOfRange(data, readPos, readPos + bytesToRead));\r\n        Assertions.assertThat(Arrays.copyOfRange(((AbfsInputStream) inputStream.getWrappedStream()).getBuffer(), 0, TEST_FILE_DATA_SIZE)).describedAs(\"AbfsInputStream pread read more data into its buffer than expected\").doesNotContain(data);\r\n        assertStatistics(inputStream.getIOStatistics(), 4 * bytesToRead, 4, 4, TEST_FILE_DATA_SIZE + 3 * bytesToRead);\r\n    } finally {\r\n        inputStream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "resetBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetBuffer(byte[] buf)\n{\r\n    for (int i = 0; i < buf.length; i++) {\r\n        buf[i] = (byte) 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAtomicRenameKeyDoesntNPEOnInitializingWithNonDefaultURI",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAtomicRenameKeyDoesntNPEOnInitializingWithNonDefaultURI() throws IOException\n{\r\n    NativeAzureFileSystem azureFs = fs;\r\n    AzureNativeFileSystemStore azureStore = azureFs.getStore();\r\n    Configuration conf = fs.getConf();\r\n    conf.set(HBASE_ROOT_DIR_CONF_STRING, HBASE_ROOT_DIR_VALUE_ON_DIFFERENT_FS);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    azureStore.isAtomicRenameKey(\"anyrandomkey\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureFileCreatedImmediately",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEnsureFileCreatedImmediately() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    FSDataOutputStream out = fs.create(TEST_FILE_PATH);\r\n    try {\r\n        assertIsFile(fs, TEST_FILE_PATH);\r\n    } finally {\r\n        out.close();\r\n    }\r\n    assertIsFile(fs, TEST_FILE_PATH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateNonRecursive",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCreateNonRecursive() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFolderPath = path(TEST_FOLDER_PATH);\r\n    Path testFile = new Path(testFolderPath, TEST_CHILD_FILE);\r\n    try {\r\n        fs.createNonRecursive(testFile, true, 1024, (short) 1, 1024, null);\r\n        fail(\"Should've thrown\");\r\n    } catch (FileNotFoundException expected) {\r\n    }\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.MKDIR, false, 0));\r\n    fs.mkdirs(testFolderPath);\r\n    fs.registerListener(null);\r\n    fs.createNonRecursive(testFile, true, 1024, (short) 1, 1024, null).close();\r\n    assertIsFile(fs, testFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateNonRecursive1",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateNonRecursive1() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFolderPath = path(TEST_FOLDER_PATH);\r\n    Path testFile = new Path(testFolderPath, TEST_CHILD_FILE);\r\n    try {\r\n        fs.createNonRecursive(testFile, FsPermission.getDefault(), EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), 1024, (short) 1, 1024, null);\r\n        fail(\"Should've thrown\");\r\n    } catch (FileNotFoundException expected) {\r\n    }\r\n    fs.mkdirs(testFolderPath);\r\n    fs.createNonRecursive(testFile, true, 1024, (short) 1, 1024, null).close();\r\n    assertIsFile(fs, testFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateNonRecursive2",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCreateNonRecursive2() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFolderPath = path(TEST_FOLDER_PATH);\r\n    Path testFile = new Path(testFolderPath, TEST_CHILD_FILE);\r\n    try {\r\n        fs.createNonRecursive(testFile, FsPermission.getDefault(), false, 1024, (short) 1, 1024, null);\r\n        fail(\"Should've thrown\");\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n    fs.mkdirs(testFolderPath);\r\n    fs.createNonRecursive(testFile, true, 1024, (short) 1, 1024, null).close();\r\n    assertIsFile(fs, testFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteAfterClose",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testWriteAfterClose() throws Throwable\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFolderPath = path(TEST_FOLDER_PATH);\r\n    Path testPath = new Path(testFolderPath, TEST_CHILD_FILE);\r\n    FSDataOutputStream out = fs.create(testPath);\r\n    out.close();\r\n    intercept(IOException.class, () -> out.write('a'));\r\n    intercept(IOException.class, () -> out.write(new byte[] { 'a' }));\r\n    out.flush();\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testTryWithResources",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testTryWithResources() throws Throwable\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFolderPath = path(TEST_FOLDER_PATH);\r\n    Path testPath = new Path(testFolderPath, TEST_CHILD_FILE);\r\n    try (FSDataOutputStream out = fs.create(testPath)) {\r\n        out.write('1');\r\n        out.hsync();\r\n        fs.delete(testPath, false);\r\n        out.write('2');\r\n        out.hsync();\r\n        fail(\"Expected a failure\");\r\n    } catch (FileNotFoundException fnfe) {\r\n        if (!fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(testPath).toString())) {\r\n            Throwable[] suppressed = fnfe.getSuppressed();\r\n            assertEquals(\"suppressed count\", 1, suppressed.length);\r\n            Throwable inner = suppressed[0];\r\n            if (!(inner instanceof IOException)) {\r\n                throw inner;\r\n            }\r\n            GenericTestUtils.assertExceptionContains(fnfe.getMessage(), inner);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFilterFSWriteAfterClose",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFilterFSWriteAfterClose() throws Throwable\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFolderPath = path(TEST_FOLDER_PATH);\r\n    Path testPath = new Path(testFolderPath, TEST_CHILD_FILE);\r\n    FSDataOutputStream out = fs.create(testPath);\r\n    intercept(FileNotFoundException.class, () -> {\r\n        try (FilterOutputStream fos = new FilterOutputStream(out)) {\r\n            fos.write('a');\r\n            fos.flush();\r\n            out.hsync();\r\n            fs.delete(testPath, false);\r\n            throw intercept(FileNotFoundException.class, () -> {\r\n                fos.write('b');\r\n                out.hsync();\r\n                return \"hsync didn't raise an IOE\";\r\n            });\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultCreateOverwriteFileTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDefaultCreateOverwriteFileTest() throws Throwable\n{\r\n    testCreateFileOverwrite(true);\r\n    testCreateFileOverwrite(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateFileOverwrite",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testCreateFileOverwrite(boolean enableConditionalCreateOverwrite) throws Throwable\n{\r\n    final AzureBlobFileSystem currentFs = getFileSystem();\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(\"fs.azure.enable.conditional.create.overwrite\", Boolean.toString(enableConditionalCreateOverwrite));\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config);\r\n    long totalConnectionMadeBeforeTest = fs.getInstrumentationMap().get(CONNECTIONS_MADE.getStatName());\r\n    int createRequestCount = 0;\r\n    final Path nonOverwriteFile = new Path(\"/NonOverwriteTest_FileName_\" + UUID.randomUUID().toString());\r\n    fs.create(nonOverwriteFile, false);\r\n    createRequestCount++;\r\n    assertAbfsStatistics(CONNECTIONS_MADE, totalConnectionMadeBeforeTest + createRequestCount, fs.getInstrumentationMap());\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.CREATE, false, 0));\r\n    intercept(FileAlreadyExistsException.class, () -> fs.create(nonOverwriteFile, false));\r\n    fs.registerListener(null);\r\n    createRequestCount++;\r\n    assertAbfsStatistics(CONNECTIONS_MADE, totalConnectionMadeBeforeTest + createRequestCount, fs.getInstrumentationMap());\r\n    final Path overwriteFilePath = new Path(\"/OverwriteTest_FileName_\" + UUID.randomUUID().toString());\r\n    fs.create(overwriteFilePath, true);\r\n    createRequestCount++;\r\n    assertAbfsStatistics(CONNECTIONS_MADE, totalConnectionMadeBeforeTest + createRequestCount, fs.getInstrumentationMap());\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.CREATE, true, 0));\r\n    fs.create(overwriteFilePath, true);\r\n    fs.registerListener(null);\r\n    if (enableConditionalCreateOverwrite) {\r\n        createRequestCount += 3;\r\n    } else {\r\n        createRequestCount++;\r\n    }\r\n    assertAbfsStatistics(CONNECTIONS_MADE, totalConnectionMadeBeforeTest + createRequestCount, fs.getInstrumentationMap());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNegativeScenariosForCreateOverwriteDisabled",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testNegativeScenariosForCreateOverwriteDisabled() throws Throwable\n{\r\n    final AzureBlobFileSystem currentFs = getFileSystem();\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(\"fs.azure.enable.conditional.create.overwrite\", Boolean.toString(true));\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config);\r\n    AbfsClient mockClient = TestAbfsClient.getMockAbfsClient(fs.getAbfsStore().getClient(), fs.getAbfsStore().getAbfsConfiguration());\r\n    AzureBlobFileSystemStore abfsStore = fs.getAbfsStore();\r\n    abfsStore = setAzureBlobSystemStoreField(abfsStore, \"client\", mockClient);\r\n    boolean isNamespaceEnabled = abfsStore.getIsNamespaceEnabled(getTestTracingContext(fs, false));\r\n    AbfsRestOperation successOp = mock(AbfsRestOperation.class);\r\n    AbfsHttpOperation http200Op = mock(AbfsHttpOperation.class);\r\n    when(http200Op.getStatusCode()).thenReturn(HTTP_OK);\r\n    when(successOp.getResult()).thenReturn(http200Op);\r\n    AbfsRestOperationException conflictResponseEx = getMockAbfsRestOperationException(HTTP_CONFLICT);\r\n    AbfsRestOperationException serverErrorResponseEx = getMockAbfsRestOperationException(HTTP_INTERNAL_ERROR);\r\n    AbfsRestOperationException fileNotFoundResponseEx = getMockAbfsRestOperationException(HTTP_NOT_FOUND);\r\n    AbfsRestOperationException preConditionResponseEx = getMockAbfsRestOperationException(HTTP_PRECON_FAILED);\r\n    doThrow(conflictResponseEx).doThrow(conflictResponseEx).doThrow(conflictResponseEx).doThrow(conflictResponseEx).doThrow(serverErrorResponseEx).when(mockClient).createPath(any(String.class), eq(true), eq(false), isNamespaceEnabled ? any(String.class) : eq(null), isNamespaceEnabled ? any(String.class) : eq(null), any(boolean.class), eq(null), any(TracingContext.class));\r\n    doThrow(fileNotFoundResponseEx).doThrow(serverErrorResponseEx).doReturn(successOp).doReturn(successOp).when(mockClient).getPathStatus(any(String.class), eq(false), any(TracingContext.class));\r\n    doThrow(preConditionResponseEx).doThrow(serverErrorResponseEx).when(mockClient).createPath(any(String.class), eq(true), eq(true), isNamespaceEnabled ? any(String.class) : eq(null), isNamespaceEnabled ? any(String.class) : eq(null), any(boolean.class), eq(null), any(TracingContext.class));\r\n    validateCreateFileException(ConcurrentWriteOperationDetectedException.class, abfsStore);\r\n    validateCreateFileException(AbfsRestOperationException.class, abfsStore);\r\n    validateCreateFileException(ConcurrentWriteOperationDetectedException.class, abfsStore);\r\n    validateCreateFileException(AbfsRestOperationException.class, abfsStore);\r\n    validateCreateFileException(AbfsRestOperationException.class, abfsStore);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setAzureBlobSystemStoreField",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AzureBlobFileSystemStore setAzureBlobSystemStoreField(final AzureBlobFileSystemStore abfsStore, final String fieldName, Object fieldObject) throws Exception\n{\r\n    Field abfsClientField = AzureBlobFileSystemStore.class.getDeclaredField(fieldName);\r\n    abfsClientField.setAccessible(true);\r\n    Field modifiersField = Field.class.getDeclaredField(\"modifiers\");\r\n    modifiersField.setAccessible(true);\r\n    modifiersField.setInt(abfsClientField, abfsClientField.getModifiers() & ~java.lang.reflect.Modifier.FINAL);\r\n    abfsClientField.set(abfsStore, fieldObject);\r\n    return abfsStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateCreateFileException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateCreateFileException(final Class<E> exceptionClass, final AzureBlobFileSystemStore abfsStore) throws Exception\n{\r\n    FsPermission permission = new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL);\r\n    FsPermission umask = new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE);\r\n    Path testPath = new Path(\"testFile\");\r\n    intercept(exceptionClass, () -> abfsStore.createFile(testPath, null, true, permission, umask, getTestTracingContext(getFileSystem(), true)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMockAbfsRestOperationException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsRestOperationException getMockAbfsRestOperationException(int status)\n{\r\n    return new AbfsRestOperationException(status, \"\", \"\", new Exception());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getCustomFileSystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AzureBlobFileSystem getCustomFileSystem(Path infiniteLeaseDirs, int numLeaseThreads) throws Exception\n{\r\n    Configuration conf = getRawConfiguration();\r\n    conf.setBoolean(String.format(\"fs.%s.impl.disable.cache\", getAbfsScheme()), true);\r\n    conf.set(FS_AZURE_INFINITE_LEASE_KEY, infiniteLeaseDirs.toUri().getPath());\r\n    conf.setInt(FS_AZURE_LEASE_THREADS, numLeaseThreads);\r\n    return getFileSystem(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNoInfiniteLease",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNoInfiniteLease() throws IOException\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    fs.mkdirs(testFilePath.getParent());\r\n    try (FSDataOutputStream out = fs.create(testFilePath)) {\r\n        Assert.assertFalse(\"Output stream should not have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n    }\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNoLeaseThreads",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNoLeaseThreads() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 0);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    LambdaTestUtils.intercept(IOException.class, ERR_NO_LEASE_THREADS, () -> {\r\n        try (FSDataOutputStream out = fs.create(testFilePath)) {\r\n        }\r\n        return \"No failure when lease requested with 0 lease threads\";\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOneWriter",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testOneWriter() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    FSDataOutputStream out = fs.create(testFilePath);\r\n    Assert.assertTrue(\"Output stream should have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n    out.close();\r\n    Assert.assertFalse(\"Output stream should not have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSubDir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSubDir() throws Exception\n{\r\n    final Path testFilePath = new Path(new Path(path(methodName.getMethodName()), \"subdir\"), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent().getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent().getParent());\r\n    FSDataOutputStream out = fs.create(testFilePath);\r\n    Assert.assertTrue(\"Output stream should have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n    out.close();\r\n    Assert.assertFalse(\"Output stream should not have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testTwoCreate",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testTwoCreate() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    try (FSDataOutputStream out = fs.create(testFilePath)) {\r\n        LambdaTestUtils.intercept(IOException.class, isHNSEnabled ? ERR_PARALLEL_ACCESS_DETECTED : ERR_NO_LEASE_ID_SPECIFIED, () -> {\r\n            try (FSDataOutputStream out2 = fs.create(testFilePath)) {\r\n            }\r\n            return \"Expected second create on infinite lease dir to fail\";\r\n        });\r\n    }\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "twoWriters",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void twoWriters(AzureBlobFileSystem fs, Path testFilePath, boolean expectException) throws Exception\n{\r\n    try (FSDataOutputStream out = fs.create(testFilePath)) {\r\n        try (FSDataOutputStream out2 = fs.append(testFilePath)) {\r\n            out2.writeInt(2);\r\n            out2.hsync();\r\n        } catch (IOException e) {\r\n            if (expectException) {\r\n                GenericTestUtils.assertExceptionContains(ERR_ACQUIRING_LEASE, e);\r\n            } else {\r\n                throw e;\r\n            }\r\n        }\r\n        out.writeInt(1);\r\n        out.hsync();\r\n    }\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testTwoWritersCreateAppendNoInfiniteLease",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTwoWritersCreateAppendNoInfiniteLease() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    fs.mkdirs(testFilePath.getParent());\r\n    twoWriters(fs, testFilePath, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testTwoWritersCreateAppendWithInfiniteLeaseEnabled",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTwoWritersCreateAppendWithInfiniteLeaseEnabled() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    twoWriters(fs, testFilePath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testLeaseFreedOnClose",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLeaseFreedOnClose() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    FSDataOutputStream out;\r\n    out = fs.create(testFilePath);\r\n    out.write(0);\r\n    Assert.assertTrue(\"Output stream should have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n    out.close();\r\n    Assert.assertFalse(\"Output stream should not have lease after close\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteAfterBreakLease",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testWriteAfterBreakLease() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    FSDataOutputStream out;\r\n    out = fs.create(testFilePath);\r\n    out.write(0);\r\n    out.hsync();\r\n    fs.registerListener(new TracingHeaderValidator(getConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.BREAK_LEASE, false, 0));\r\n    fs.breakLease(testFilePath);\r\n    fs.registerListener(null);\r\n    LambdaTestUtils.intercept(IOException.class, ERR_LEASE_EXPIRED, () -> {\r\n        out.write(1);\r\n        out.hsync();\r\n        return \"Expected exception on write after lease break but got \" + out;\r\n    });\r\n    LambdaTestUtils.intercept(IOException.class, ERR_LEASE_EXPIRED, () -> {\r\n        out.close();\r\n        return \"Expected exception on close after lease break but got \" + out;\r\n    });\r\n    Assert.assertTrue(\"Output stream lease should be freed\", ((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed());\r\n    try (FSDataOutputStream out2 = fs.append(testFilePath)) {\r\n        out2.write(2);\r\n        out2.hsync();\r\n    }\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testLeaseFreedAfterBreak",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLeaseFreedAfterBreak() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    FSDataOutputStream out = fs.create(testFilePath);\r\n    out.write(0);\r\n    fs.breakLease(testFilePath);\r\n    LambdaTestUtils.intercept(IOException.class, ERR_LEASE_EXPIRED, () -> {\r\n        out.close();\r\n        return \"Expected exception on close after lease break but got \" + out;\r\n    });\r\n    Assert.assertTrue(\"Output stream lease should be freed\", ((AbfsOutputStream) out.getWrappedStream()).isLeaseFreed());\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testInfiniteLease",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testInfiniteLease() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    try (FSDataOutputStream out = fs.create(testFilePath)) {\r\n        Assert.assertTrue(\"Output stream should have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n        out.write(0);\r\n    }\r\n    Assert.assertTrue(fs.getAbfsStore().areLeasesFreed());\r\n    try (FSDataOutputStream out = fs.append(testFilePath)) {\r\n        Assert.assertTrue(\"Output stream should have lease\", ((AbfsOutputStream) out.getWrappedStream()).hasLease());\r\n        out.write(1);\r\n    }\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFileSystemClose",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFileSystemClose() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    try (FSDataOutputStream out = fs.create(testFilePath)) {\r\n        out.write(0);\r\n        Assert.assertFalse(\"Store leases should exist\", fs.getAbfsStore().areLeasesFreed());\r\n    }\r\n    fs.close();\r\n    Assert.assertTrue(\"Store leases were not freed\", fs.getAbfsStore().areLeasesFreed());\r\n    LambdaTestUtils.intercept(RejectedExecutionException.class, () -> {\r\n        try (FSDataOutputStream out2 = fs.append(testFilePath)) {\r\n        }\r\n        return \"Expected exception on new append after closed FS\";\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAcquireRetry",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testAcquireRetry() throws Exception\n{\r\n    final Path testFilePath = new Path(path(methodName.getMethodName()), TEST_FILE);\r\n    final AzureBlobFileSystem fs = getCustomFileSystem(testFilePath.getParent(), 1);\r\n    fs.mkdirs(testFilePath.getParent());\r\n    fs.createNewFile(testFilePath);\r\n    TracingContext tracingContext = getTestTracingContext(fs, true);\r\n    Listener listener = new TracingHeaderValidator(getConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.TEST_OP, true, 0);\r\n    tracingContext.setListener(listener);\r\n    AbfsLease lease = new AbfsLease(fs.getAbfsClient(), testFilePath.toUri().getPath(), tracingContext);\r\n    Assert.assertNotNull(\"Did not successfully lease file\", lease.getLeaseID());\r\n    listener.setOperation(FSOperationType.RELEASE_LEASE);\r\n    lease.free();\r\n    lease.getTracingContext().setListener(null);\r\n    Assert.assertEquals(\"Unexpected acquire retry count\", 0, lease.getAcquireRetryCount());\r\n    AbfsClient mockClient = spy(fs.getAbfsClient());\r\n    doThrow(new AbfsLease.LeaseException(\"failed to acquire 1\")).doThrow(new AbfsLease.LeaseException(\"failed to acquire 2\")).doCallRealMethod().when(mockClient).acquireLease(anyString(), anyInt(), any(TracingContext.class));\r\n    lease = new AbfsLease(mockClient, testFilePath.toUri().getPath(), 5, 1, tracingContext);\r\n    Assert.assertNotNull(\"Acquire lease should have retried\", lease.getLeaseID());\r\n    lease.free();\r\n    Assert.assertEquals(\"Unexpected acquire retry count\", 2, lease.getAcquireRetryCount());\r\n    doThrow(new AbfsLease.LeaseException(\"failed to acquire\")).when(mockClient).acquireLease(anyString(), anyInt(), any(TracingContext.class));\r\n    LambdaTestUtils.intercept(AzureBlobFileSystemException.class, () -> {\r\n        new AbfsLease(mockClient, testFilePath.toUri().getPath(), 5, 1, tracingContext);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "prepareTestConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration prepareTestConfiguration(ABFSContractTestBinding contractTestBinding)\n{\r\n    final Configuration conf = contractTestBinding.getRawConfiguration();\r\n    conf.set(OPT_STORE_OPERATIONS_CLASS, AbfsManifestStoreOperations.NAME);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    createTestAccount();\r\n    hugefile = fs.makeQualified(TEST_FILE_PATH);\r\n    try {\r\n        testFileStatus = fs.getFileStatus(TEST_FILE_PATH);\r\n        testFileLength = testFileStatus.getLen();\r\n    } catch (FileNotFoundException e) {\r\n        testFileLength = 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, \"/\");\r\n    return AzureBlobStorageTestAccount.create(\"testpageblobinputstream\", EnumSet.of(AzureBlobStorageTestAccount.CreateOptions.CreateContainer), conf, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestFileAndSetLength",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void createTestFileAndSetLength() throws IOException\n{\r\n    if (fs.exists(TEST_FILE_PATH)) {\r\n        testFileStatus = fs.getFileStatus(TEST_FILE_PATH);\r\n        testFileLength = testFileStatus.getLen();\r\n        LOG.info(\"Reusing test file: {}\", testFileStatus);\r\n        return;\r\n    }\r\n    byte[] buffer = new byte[256];\r\n    for (int i = 0; i < buffer.length; i++) {\r\n        buffer[i] = (byte) i;\r\n    }\r\n    LOG.info(\"Creating test file {} of size: {}\", TEST_FILE_PATH, TEST_FILE_SIZE);\r\n    try (FSDataOutputStream outputStream = fs.create(TEST_FILE_PATH)) {\r\n        int bytesWritten = 0;\r\n        while (bytesWritten < TEST_FILE_SIZE) {\r\n            outputStream.write(buffer);\r\n            bytesWritten += buffer.length;\r\n        }\r\n        LOG.info(\"Closing stream {}\", outputStream);\r\n        outputStream.close();\r\n    }\r\n    testFileLength = fs.getFileStatus(TEST_FILE_PATH).getLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assumeHugeFileExists",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assumeHugeFileExists() throws IOException\n{\r\n    ContractTestUtils.assertPathExists(fs, \"huge file not created\", hugefile);\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    ContractTestUtils.assertIsFile(hugefile, status);\r\n    assertTrue(\"File \" + hugefile + \" is empty\", status.getLen() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0100_CreateHugeFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0100_CreateHugeFile() throws IOException\n{\r\n    createTestFileAndSetLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0200_BasicReadTest",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void test_0200_BasicReadTest() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        byte[] buffer = new byte[3 * MEGABYTE];\r\n        long position = 5 * MEGABYTE;\r\n        inputStream.seek(position);\r\n        int numBytesRead = inputStream.read(buffer, 0, KILOBYTE);\r\n        assertEquals(KILOBYTE, numBytesRead);\r\n        byte[] expected = new byte[3 * MEGABYTE];\r\n        for (int i = 0; i < KILOBYTE; i++) {\r\n            expected[i] = (byte) ((i + position) % 256);\r\n        }\r\n        assertArrayEquals(expected, buffer);\r\n        int len = MEGABYTE;\r\n        int offset = buffer.length - len;\r\n        position = 3 * MEGABYTE;\r\n        inputStream.seek(position);\r\n        numBytesRead = inputStream.read(buffer, offset, len);\r\n        assertEquals(len, numBytesRead);\r\n        for (int i = offset; i < offset + len; i++) {\r\n            expected[i] = (byte) ((i + position) % 256);\r\n        }\r\n        assertArrayEquals(expected, buffer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0201_RandomReadTest",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void test_0201_RandomReadTest() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        final int bufferSize = 4 * KILOBYTE;\r\n        byte[] buffer = new byte[bufferSize];\r\n        long position = 0;\r\n        verifyConsistentReads(inputStream, buffer, position);\r\n        inputStream.seek(0);\r\n        verifyConsistentReads(inputStream, buffer, position);\r\n        int seekPosition = 2 * KILOBYTE;\r\n        inputStream.seek(seekPosition);\r\n        position = seekPosition;\r\n        verifyConsistentReads(inputStream, buffer, position);\r\n        inputStream.seek(0);\r\n        position = 0;\r\n        verifyConsistentReads(inputStream, buffer, position);\r\n        seekPosition = 5 * KILOBYTE;\r\n        inputStream.seek(seekPosition);\r\n        position = seekPosition;\r\n        verifyConsistentReads(inputStream, buffer, position);\r\n        seekPosition = 10 * KILOBYTE;\r\n        inputStream.seek(seekPosition);\r\n        position = seekPosition;\r\n        verifyConsistentReads(inputStream, buffer, position);\r\n        seekPosition = 4100 * KILOBYTE;\r\n        inputStream.seek(seekPosition);\r\n        position = seekPosition;\r\n        verifyConsistentReads(inputStream, buffer, position);\r\n        for (int i = 4 * 1024 * 1023; i < 5000; i++) {\r\n            seekPosition = i;\r\n            inputStream.seek(seekPosition);\r\n            position = seekPosition;\r\n            verifyConsistentReads(inputStream, buffer, position);\r\n        }\r\n        inputStream.seek(0);\r\n        position = 0;\r\n        buffer = new byte[1];\r\n        for (int i = 0; i < 5000; i++) {\r\n            assertEquals(1, inputStream.skip(1));\r\n            position++;\r\n            verifyConsistentReads(inputStream, buffer, position);\r\n            position++;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyConsistentReads",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyConsistentReads(FSDataInputStream inputStream, byte[] buffer, long position) throws IOException\n{\r\n    int size = buffer.length;\r\n    final int numBytesRead = inputStream.read(buffer, 0, size);\r\n    assertEquals(\"Bytes read from stream\", size, numBytesRead);\r\n    byte[] expected = new byte[size];\r\n    for (int i = 0; i < expected.length; i++) {\r\n        expected[i] = (byte) ((position + i) % 256);\r\n    }\r\n    assertArrayEquals(\"Mismatch\", expected, buffer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0301_MarkSupported",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0301_MarkSupported() throws IOException\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        assertTrue(\"mark is not supported\", inputStream.markSupported());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0303_MarkAndResetV1",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void test_0303_MarkAndResetV1() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        inputStream.mark(KILOBYTE - 1);\r\n        byte[] buffer = new byte[KILOBYTE];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        inputStream.reset();\r\n        assertEquals(\"rest -> pos 0\", 0, inputStream.getPos());\r\n        inputStream.mark(8 * KILOBYTE - 1);\r\n        buffer = new byte[8 * KILOBYTE];\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        intercept(IOException.class, \"Resetting to invalid mark\", new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.reset();\r\n                return inputStream;\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0305_SeekToNewSourceV1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_0305_SeekToNewSourceV1() throws IOException\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        assertFalse(inputStream.seekToNewSource(0));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0307_SkipBounds",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void test_0307_SkipBounds() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        long skipped = inputStream.skip(-1);\r\n        assertEquals(0, skipped);\r\n        skipped = inputStream.skip(0);\r\n        assertEquals(0, skipped);\r\n        assertTrue(testFileLength > 0);\r\n        skipped = inputStream.skip(testFileLength);\r\n        assertEquals(testFileLength, skipped);\r\n        intercept(EOFException.class, new Callable<Long>() {\r\n\r\n            @Override\r\n            public Long call() throws Exception {\r\n                return inputStream.skip(1);\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0309_SeekBounds",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void test_0309_SeekBounds() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        inputStream.seek(0);\r\n        assertEquals(0, inputStream.getPos());\r\n        intercept(EOFException.class, FSExceptionMessages.NEGATIVE_SEEK, new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.seek(-1);\r\n                return inputStream;\r\n            }\r\n        });\r\n        assertTrue(\"Test file length only \" + testFileLength, testFileLength > 0);\r\n        inputStream.seek(testFileLength);\r\n        assertEquals(testFileLength, inputStream.getPos());\r\n        intercept(EOFException.class, FSExceptionMessages.CANNOT_SEEK_PAST_EOF, new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.seek(testFileLength + 1);\r\n                return inputStream;\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0311_SeekAndAvailableAndPosition",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void test_0311_SeekAndAvailableAndPosition() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        byte[] expected1 = { 0, 1, 2 };\r\n        byte[] expected2 = { 3, 4, 5 };\r\n        byte[] expected3 = { 1, 2, 3 };\r\n        byte[] expected4 = { 6, 7, 8 };\r\n        byte[] buffer = new byte[3];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected1, buffer);\r\n        assertEquals(buffer.length, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected2, buffer);\r\n        assertEquals(2 * buffer.length, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        int seekPos = 0;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected1, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        seekPos = 1;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected3, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        seekPos = 6;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected4, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0313_SkipAndAvailableAndPosition",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void test_0313_SkipAndAvailableAndPosition() throws IOException\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        byte[] expected1 = { 0, 1, 2 };\r\n        byte[] expected2 = { 3, 4, 5 };\r\n        byte[] expected3 = { 1, 2, 3 };\r\n        byte[] expected4 = { 6, 7, 8 };\r\n        assertEquals(testFileLength, inputStream.available());\r\n        assertEquals(0, inputStream.getPos());\r\n        int n = 3;\r\n        long skipped = inputStream.skip(n);\r\n        assertEquals(skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        assertEquals(skipped, n);\r\n        byte[] buffer = new byte[3];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected2, buffer);\r\n        assertEquals(buffer.length + skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        int seekPos = 1;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected3, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        long currentPosition = inputStream.getPos();\r\n        n = 2;\r\n        skipped = inputStream.skip(n);\r\n        assertEquals(currentPosition + skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        assertEquals(skipped, n);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected4, buffer);\r\n        assertEquals(buffer.length + skipped + currentPosition, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_999_DeleteHugeFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_999_DeleteHugeFiles() throws IOException\n{\r\n    fs.delete(TEST_FILE_PATH, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsThrottlingStatistics",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAbfsThrottlingStatistics() throws IOException\n{\r\n    describe(\"Test to check correct values of read throttle and write \" + \"throttle statistics in Abfs\");\r\n    AbfsCounters statistics = new AbfsCountersImpl(getFileSystem().getUri());\r\n    for (int i = 0; i < LARGE_OPERATIONS; i++) {\r\n        statistics.incrementCounter(AbfsStatistic.READ_THROTTLES, 1);\r\n        statistics.incrementCounter(AbfsStatistic.WRITE_THROTTLES, 1);\r\n    }\r\n    Map<String, Long> metricMap = statistics.toMap();\r\n    assertAbfsStatistics(AbfsStatistic.READ_THROTTLES, LARGE_OPERATIONS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.WRITE_THROTTLES, LARGE_OPERATIONS, metricMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsNetworkDurationTrackers",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAbfsNetworkDurationTrackers() throws IOException, InterruptedException\n{\r\n    describe(\"Test to verify the actual values of DurationTrackers are \" + \"greater than 0.0 while tracking some work.\");\r\n    AbfsCounters abfsCounters = new AbfsCountersImpl(getFileSystem().getUri());\r\n    try (DurationTracker ignoredPatch = abfsCounters.trackDuration(AbfsStatistic.getStatNameFromHttpCall(AbfsHttpConstants.HTTP_METHOD_PATCH));\r\n        DurationTracker ignoredPost = abfsCounters.trackDuration(AbfsStatistic.getStatNameFromHttpCall(AbfsHttpConstants.HTTP_METHOD_POST))) {\r\n        Thread.sleep(10);\r\n        LOG.info(\"Execute some Http requests...\");\r\n    }\r\n    IOStatistics ioStatistics = extractStatistics(abfsCounters);\r\n    for (AbfsStatistic abfsStatistic : HTTP_DURATION_TRACKER_LIST) {\r\n        Assertions.assertThat(lookupMeanStatistic(ioStatistics, abfsStatistic.getStatName() + StoreStatisticNames.SUFFIX_MEAN).mean()).describedAs(\"The DurationTracker Named \" + abfsStatistic.getStatName() + \" Doesn't match the expected value\").isGreaterThan(0.0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsHTTP503ErrorCounter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAbfsHTTP503ErrorCounter() throws IOException\n{\r\n    describe(\"tests to verify the expected value of the HTTP 503 error \" + \"counter is equal to number of times incremented.\");\r\n    AbfsCounters abfsCounters = new AbfsCountersImpl(getFileSystem().getUri());\r\n    for (int i = 0; i < LARGE_OPERATIONS; i++) {\r\n        abfsCounters.incrementCounter(AbfsStatistic.SERVER_UNAVAILABLE, 1);\r\n    }\r\n    Map<String, Long> metricsMap = abfsCounters.toMap();\r\n    assertAbfsStatistics(AbfsStatistic.SERVER_UNAVAILABLE, LARGE_OPERATIONS, metricsMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "tesCrc64Compute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tesCrc64Compute()\n{\r\n    CRC64 crc64 = new CRC64();\r\n    final String[] testStr = { \"#$\", \"dir_2_ac83abee\", \"dir_42_976df1f5\" };\r\n    final String[] expected = { \"f91f7e6a837dbfa8\", \"203f9fefc38ae97b\", \"cc0d56eafe58a855\" };\r\n    for (int i = 0; i < testStr.length; i++) {\r\n        Assert.assertEquals(expected[i], Long.toHexString(crc64.compute(testStr[i].getBytes())));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWithCPK",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testReadWithCPK() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(true);\r\n    String fileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, fileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    int length = FILE_SIZE;\r\n    byte[] buffer = new byte[length];\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    final AbfsRestOperation op = abfsClient.getPathStatus(fileName, false, tracingContext);\r\n    final String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n    AbfsRestOperation abfsRestOperation = abfsClient.read(fileName, 0, buffer, 0, length, eTag, null, tracingContext);\r\n    assertCPKHeaders(abfsRestOperation, true);\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_ENCRYPTION_KEY_SHA256, getCPKSha(fs));\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_SERVER_ENCRYPTED, \"true\");\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_REQUEST_SERVER_ENCRYPTED, \"\");\r\n    Configuration conf = fs.getConf();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"different-1234567890123456789012\");\r\n    try (AzureBlobFileSystem fs2 = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n        FSDataInputStream iStream = fs2.open(new Path(fileName))) {\r\n        int len = 8 * ONE_MB;\r\n        byte[] b = new byte[len];\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            iStream.read(b, 0, len);\r\n        });\r\n    }\r\n    conf.unset(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n    try (AzureBlobFileSystem fs3 = (AzureBlobFileSystem) FileSystem.get(conf);\r\n        FSDataInputStream iStream = fs3.open(new Path(fileName))) {\r\n        int len = 8 * ONE_MB;\r\n        byte[] b = new byte[len];\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            iStream.read(b, 0, len);\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testReadWithoutCPK() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(false);\r\n    String fileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, fileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    int length = INT_512;\r\n    byte[] buffer = new byte[length * 4];\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    final AbfsRestOperation op = abfsClient.getPathStatus(fileName, false, tracingContext);\r\n    final String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n    AbfsRestOperation abfsRestOperation = abfsClient.read(fileName, 0, buffer, 0, length, eTag, null, tracingContext);\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_ENCRYPTION_KEY_SHA256, getCPKSha(fs));\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_SERVER_ENCRYPTED, \"true\");\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_REQUEST_SERVER_ENCRYPTED, \"\");\r\n    Configuration conf = fs.getConf();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"12345678901234567890123456789012\");\r\n    try (AzureBlobFileSystem fs2 = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n        AbfsClient abfsClient2 = fs2.getAbfsClient()) {\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            abfsClient2.read(fileName, 0, buffer, 0, length, eTag, null, getTestTracingContext(fs, false));\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendWithCPK",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testAppendWithCPK() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(true);\r\n    final String fileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, fileName, FILE_SIZE);\r\n    AppendRequestParameters appendRequestParameters = new AppendRequestParameters(0, 0, 5, Mode.APPEND_MODE, false, null);\r\n    byte[] buffer = getRandomBytesArray(5);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    AbfsRestOperation abfsRestOperation = abfsClient.append(fileName, buffer, appendRequestParameters, null, getTestTracingContext(fs, false));\r\n    assertCPKHeaders(abfsRestOperation, true);\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_ENCRYPTION_KEY_SHA256, getCPKSha(fs));\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_SERVER_ENCRYPTED, \"\");\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_REQUEST_SERVER_ENCRYPTED, \"true\");\r\n    Configuration conf = fs.getConf();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"different-1234567890123456789012\");\r\n    try (AzureBlobFileSystem fs2 = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n        AbfsClient abfsClient2 = fs2.getAbfsClient()) {\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            abfsClient2.append(fileName, buffer, appendRequestParameters, null, getTestTracingContext(fs, false));\r\n        });\r\n    }\r\n    conf.unset(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n    try (AzureBlobFileSystem fs3 = (AzureBlobFileSystem) FileSystem.get(conf);\r\n        AbfsClient abfsClient3 = fs3.getAbfsClient()) {\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            abfsClient3.append(fileName, buffer, appendRequestParameters, null, getTestTracingContext(fs, false));\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAppendWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testAppendWithoutCPK() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(false);\r\n    final String fileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, fileName, FILE_SIZE);\r\n    AppendRequestParameters appendRequestParameters = new AppendRequestParameters(0, 0, 5, Mode.APPEND_MODE, false, null);\r\n    byte[] buffer = getRandomBytesArray(5);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    AbfsRestOperation abfsRestOperation = abfsClient.append(fileName, buffer, appendRequestParameters, null, getTestTracingContext(fs, false));\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_ENCRYPTION_KEY_SHA256, \"\");\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_SERVER_ENCRYPTED, \"\");\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_REQUEST_SERVER_ENCRYPTED, \"true\");\r\n    Configuration conf = fs.getConf();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"12345678901234567890123456789012\");\r\n    try (AzureBlobFileSystem fs2 = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n        AbfsClient abfsClient2 = fs2.getAbfsClient()) {\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            abfsClient2.append(fileName, buffer, appendRequestParameters, null, getTestTracingContext(fs, false));\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetGetXAttr",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetGetXAttr() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(true);\r\n    final String fileName = path(methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, fileName, FILE_SIZE);\r\n    String valSent = \"testValue\";\r\n    String attrName = \"testXAttr\";\r\n    fs.setXAttr(new Path(fileName), attrName, valSent.getBytes(StandardCharsets.UTF_8), EnumSet.of(XAttrSetFlag.CREATE));\r\n    byte[] valBytes = fs.getXAttr(new Path(fileName), attrName);\r\n    String valRecieved = new String(valBytes);\r\n    assertEquals(valSent, valRecieved);\r\n    valSent = \"new value\";\r\n    fs.setXAttr(new Path(fileName), attrName, valSent.getBytes(StandardCharsets.UTF_8), EnumSet.of(XAttrSetFlag.REPLACE));\r\n    valBytes = fs.getXAttr(new Path(fileName), attrName);\r\n    valRecieved = new String(valBytes);\r\n    assertEquals(valSent, valRecieved);\r\n    LambdaTestUtils.intercept(IOException.class, () -> {\r\n        getAbfs(false).getXAttr(new Path(fileName), attrName);\r\n    });\r\n    LambdaTestUtils.intercept(IOException.class, () -> {\r\n        getSameFSWithWrongCPK(fs).getXAttr(new Path(fileName), attrName);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCopyBetweenAccounts",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testCopyBetweenAccounts() throws Exception\n{\r\n    String accountName = getRawConfiguration().get(FS_AZURE_TEST_CPK_ENABLED_SECONDARY_ACCOUNT);\r\n    String accountKey = getRawConfiguration().get(FS_AZURE_TEST_CPK_ENABLED_SECONDARY_ACCOUNT_KEY);\r\n    Assume.assumeTrue(accountName != null && !accountName.isEmpty());\r\n    Assume.assumeTrue(accountKey != null && !accountKey.isEmpty());\r\n    String fileSystemName = \"cpkfs\";\r\n    AzureBlobFileSystem fs1 = getAbfs(true);\r\n    int fileSize = FILE_SIZE_FOR_COPY_BETWEEN_ACCOUNTS;\r\n    byte[] fileContent = getRandomBytesArray(fileSize);\r\n    Path testFilePath = createFileWithContent(fs1, String.format(\"fs1-file%s.txt\", UUID.randomUUID()), fileContent);\r\n    Configuration conf = new Configuration();\r\n    conf.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    conf.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, true);\r\n    conf.unset(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    conf.set(FS_AZURE_ABFS_ACCOUNT_NAME, accountName);\r\n    conf.set(FS_AZURE_ACCOUNT_KEY + \".\" + accountName, accountKey);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"123456789012345678901234567890ab\");\r\n    conf.set(\"fs.defaultFS\", \"abfs://\" + fileSystemName + \"@\" + accountName);\r\n    AzureBlobFileSystem fs2 = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n    Path fs2DestFilePath = new Path(String.format(\"fs2-dest-file%s.txt\", UUID.randomUUID()));\r\n    FSDataOutputStream ops = fs2.create(fs2DestFilePath);\r\n    try (FSDataInputStream iStream = fs1.open(testFilePath)) {\r\n        long totalBytesRead = 0;\r\n        do {\r\n            int length = 8 * ONE_MB;\r\n            byte[] buffer = new byte[length];\r\n            int bytesRead = iStream.read(buffer, 0, length);\r\n            totalBytesRead += bytesRead;\r\n            ops.write(buffer);\r\n        } while (totalBytesRead < fileContent.length);\r\n        ops.close();\r\n    }\r\n    conf.unset(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"different-1234567890123456789012\");\r\n    try (AzureBlobFileSystem fs3 = (AzureBlobFileSystem) FileSystem.get(conf);\r\n        FSDataInputStream iStream = fs3.open(fs2DestFilePath)) {\r\n        int length = 8 * ONE_MB;\r\n        byte[] buffer = new byte[length];\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            iStream.read(buffer, 0, length);\r\n        });\r\n    }\r\n    conf.unset(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n    try (AzureBlobFileSystem fs4 = (AzureBlobFileSystem) FileSystem.get(conf);\r\n        FSDataInputStream iStream = fs4.open(fs2DestFilePath)) {\r\n        int length = 8 * ONE_MB;\r\n        byte[] buffer = new byte[length];\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            iStream.read(buffer, 0, length);\r\n        });\r\n    }\r\n    try (FSDataInputStream iStream = fs2.open(fs2DestFilePath)) {\r\n        long totalBytesRead = 0;\r\n        int pos = 0;\r\n        do {\r\n            int length = 8 * ONE_MB;\r\n            byte[] buffer = new byte[length];\r\n            int bytesRead = iStream.read(buffer, 0, length);\r\n            totalBytesRead += bytesRead;\r\n            for (int i = 0; i < bytesRead; i++) {\r\n                assertEquals(fileContent[pos + i], buffer[i]);\r\n            }\r\n            pos = pos + bytesRead;\r\n        } while (totalBytesRead < fileContent.length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListPathWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testListPathWithCPK() throws Exception\n{\r\n    testListPath(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListPathWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testListPathWithoutCPK() throws Exception\n{\r\n    testListPath(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListPath",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testListPath(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final Path testPath = path(\"/\" + methodName.getMethodName());\r\n    String testDirName = testPath.toString();\r\n    fs.mkdirs(testPath);\r\n    createFileAndGetContent(fs, testDirName + \"/aaa\", FILE_SIZE);\r\n    createFileAndGetContent(fs, testDirName + \"/bbb\", FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    AbfsRestOperation abfsRestOperation = abfsClient.listPath(testDirName, false, INT_50, null, getTestTracingContext(fs, false));\r\n    assertListstatus(fs, abfsRestOperation, testPath);\r\n    Configuration conf = fs.getConf();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"different-1234567890123456789012\");\r\n    AzureBlobFileSystem fs2 = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n    AbfsClient abfsClient2 = fs2.getAbfsClient();\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    abfsRestOperation = abfsClient2.listPath(testDirName, false, INT_50, null, tracingContext);\r\n    assertListstatus(fs, abfsRestOperation, testPath);\r\n    if (isWithCPK) {\r\n        conf.unset(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n        AzureBlobFileSystem fs3 = (AzureBlobFileSystem) FileSystem.get(conf);\r\n        AbfsClient abfsClient3 = fs3.getAbfsClient();\r\n        abfsRestOperation = abfsClient3.listPath(testDirName, false, INT_50, null, tracingContext);\r\n        assertListstatus(fs, abfsRestOperation, testPath);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertListstatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void assertListstatus(AzureBlobFileSystem fs, AbfsRestOperation abfsRestOperation, Path testPath) throws IOException\n{\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertNoCPKResponseHeadersPresent(abfsRestOperation);\r\n    FileStatus[] listStatuses = fs.listStatus(testPath);\r\n    Assertions.assertThat(listStatuses.length).describedAs(\"listStatuses should have 2 entries\").isEqualTo(2);\r\n    listStatuses = getSameFSWithWrongCPK(fs).listStatus(testPath);\r\n    Assertions.assertThat(listStatuses.length).describedAs(\"listStatuses should have 2 entries\").isEqualTo(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreatePathWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreatePathWithCPK() throws Exception\n{\r\n    testCreatePath(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreatePathWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCreatePathWithoutCPK() throws Exception\n{\r\n    testCreatePath(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreatePath",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCreatePath(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    FsPermission permission = new FsPermission(FsAction.EXECUTE, FsAction.EXECUTE, FsAction.EXECUTE);\r\n    FsPermission umask = new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE);\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    boolean isNamespaceEnabled = fs.getIsNamespaceEnabled(tracingContext);\r\n    AbfsRestOperation abfsRestOperation = abfsClient.createPath(testFileName, true, true, isNamespaceEnabled ? getOctalNotation(permission) : null, isNamespaceEnabled ? getOctalNotation(umask) : null, false, null, tracingContext);\r\n    assertCPKHeaders(abfsRestOperation, isWithCPK);\r\n    assertResponseHeader(abfsRestOperation, isWithCPK, X_MS_ENCRYPTION_KEY_SHA256, getCPKSha(fs));\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_SERVER_ENCRYPTED, \"\");\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_REQUEST_SERVER_ENCRYPTED, \"true\");\r\n    FileStatus[] listStatuses = fs.listStatus(new Path(testFileName));\r\n    Assertions.assertThat(listStatuses.length).describedAs(\"listStatuses should have 1 entry\").isEqualTo(1);\r\n    listStatuses = getSameFSWithWrongCPK(fs).listStatus(new Path(testFileName));\r\n    Assertions.assertThat(listStatuses.length).describedAs(\"listStatuses should have 1 entry\").isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenamePathWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenamePathWithCPK() throws Exception\n{\r\n    testRenamePath(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenamePathWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenamePathWithoutCPK() throws Exception\n{\r\n    testRenamePath(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenamePath",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenamePath(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    FileStatus fileStatusBeforeRename = fs.getFileStatus(new Path(testFileName));\r\n    String newName = \"/newName\";\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    AbfsRestOperation abfsRestOperation = abfsClient.renamePath(testFileName, newName, null, getTestTracingContext(fs, false), null).getLeft();\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertNoCPKResponseHeadersPresent(abfsRestOperation);\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, (() -> fs.getFileStatus(new Path(testFileName))));\r\n    FileStatus fileStatusAfterRename = fs.getFileStatus(new Path(newName));\r\n    Assertions.assertThat(fileStatusAfterRename.getLen()).describedAs(\"File size has to be same before and after rename\").isEqualTo(fileStatusBeforeRename.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFlushWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFlushWithCPK() throws Exception\n{\r\n    testFlush(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFlushWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFlushWithoutCPK() throws Exception\n{\r\n    testFlush(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFlush",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testFlush(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    fs.create(new Path(testFileName)).close();\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    String expectedCPKSha = getCPKSha(fs);\r\n    byte[] fileContent = getRandomBytesArray(FILE_SIZE);\r\n    Path testFilePath = new Path(testFileName + \"1\");\r\n    try (FSDataOutputStream oStream = fs.create(testFilePath)) {\r\n        oStream.write(fileContent);\r\n    }\r\n    Configuration conf = fs.getConf();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, \"different-1234567890123456789012\");\r\n    try (AzureBlobFileSystem fs2 = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n        AbfsClient abfsClient2 = fs2.getAbfsClient()) {\r\n        LambdaTestUtils.intercept(IOException.class, () -> {\r\n            abfsClient2.flush(testFileName, 0, false, false, null, null, getTestTracingContext(fs, false));\r\n        });\r\n    }\r\n    if (isWithCPK) {\r\n        conf.unset(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n        try (AzureBlobFileSystem fs3 = (AzureBlobFileSystem) FileSystem.get(conf);\r\n            AbfsClient abfsClient3 = fs3.getAbfsClient()) {\r\n            LambdaTestUtils.intercept(IOException.class, () -> {\r\n                abfsClient3.flush(testFileName, 0, false, false, null, null, getTestTracingContext(fs, false));\r\n            });\r\n        }\r\n    }\r\n    AbfsRestOperation abfsRestOperation = abfsClient.flush(testFileName, 0, false, false, null, null, getTestTracingContext(fs, false));\r\n    assertCPKHeaders(abfsRestOperation, isWithCPK);\r\n    assertResponseHeader(abfsRestOperation, isWithCPK, X_MS_ENCRYPTION_KEY_SHA256, expectedCPKSha);\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_SERVER_ENCRYPTED, \"\");\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_REQUEST_SERVER_ENCRYPTED, isWithCPK + \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPathPropertiesWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPathPropertiesWithCPK() throws Exception\n{\r\n    testSetPathProperties(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPathPropertiesWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPathPropertiesWithoutCPK() throws Exception\n{\r\n    testSetPathProperties(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPathProperties",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetPathProperties(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    final Hashtable<String, String> properties = new Hashtable<>();\r\n    properties.put(\"key\", \"val\");\r\n    AbfsRestOperation abfsRestOperation = abfsClient.setPathProperties(testFileName, convertXmsPropertiesToCommaSeparatedString(properties), getTestTracingContext(fs, false));\r\n    assertCPKHeaders(abfsRestOperation, isWithCPK);\r\n    assertResponseHeader(abfsRestOperation, isWithCPK, X_MS_ENCRYPTION_KEY_SHA256, getCPKSha(fs));\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_SERVER_ENCRYPTED, \"\");\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_REQUEST_SERVER_ENCRYPTED, \"true\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetPathStatusFileWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetPathStatusFileWithCPK() throws Exception\n{\r\n    testGetPathStatusFile(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetPathStatusFileWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetPathStatusFileWithoutCPK() throws Exception\n{\r\n    testGetPathStatusFile(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetPathStatusFile",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testGetPathStatusFile(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    AbfsRestOperation abfsRestOperation = abfsClient.getPathStatus(testFileName, false, tracingContext);\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertResponseHeader(abfsRestOperation, isWithCPK, X_MS_ENCRYPTION_KEY_SHA256, getCPKSha(fs));\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_SERVER_ENCRYPTED, \"true\");\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_REQUEST_SERVER_ENCRYPTED, \"\");\r\n    abfsRestOperation = abfsClient.getPathStatus(testFileName, true, tracingContext);\r\n    assertCPKHeaders(abfsRestOperation, isWithCPK);\r\n    assertResponseHeader(abfsRestOperation, isWithCPK, X_MS_ENCRYPTION_KEY_SHA256, getCPKSha(fs));\r\n    assertResponseHeader(abfsRestOperation, true, X_MS_SERVER_ENCRYPTED, \"true\");\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_REQUEST_SERVER_ENCRYPTED, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeletePathWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDeletePathWithCPK() throws Exception\n{\r\n    testDeletePath(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeletePathWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDeletePathWithoutCPK() throws Exception\n{\r\n    testDeletePath(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeletePath",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDeletePath(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    FileStatus[] listStatuses = fs.listStatus(new Path(testFileName));\r\n    Assertions.assertThat(listStatuses.length).describedAs(\"listStatuses should have 1 entry\").isEqualTo(1);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    AbfsRestOperation abfsRestOperation = abfsClient.deletePath(testFileName, false, null, getTestTracingContext(fs, false));\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertNoCPKResponseHeadersPresent(abfsRestOperation);\r\n    Assertions.assertThatThrownBy(() -> fs.listStatus(new Path(testFileName))).isInstanceOf(FileNotFoundException.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPermissionWithCPK() throws Exception\n{\r\n    testSetPermission(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetPermissionWithoutCPK() throws Exception\n{\r\n    testSetPermission(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermission",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetPermission(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    Assume.assumeTrue(fs.getIsNamespaceEnabled(getTestTracingContext(fs, false)));\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    FsPermission permission = new FsPermission(FsAction.EXECUTE, FsAction.EXECUTE, FsAction.EXECUTE);\r\n    AbfsRestOperation abfsRestOperation = abfsClient.setPermission(testFileName, permission.toString(), getTestTracingContext(fs, false));\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertNoCPKResponseHeadersPresent(abfsRestOperation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetAclWithCPK() throws Exception\n{\r\n    testSetAcl(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetAclWithoutCPK() throws Exception\n{\r\n    testSetAcl(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAcl",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetAcl(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    Assume.assumeTrue(fs.getIsNamespaceEnabled(tracingContext));\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL));\r\n    final Map<String, String> aclEntries = AbfsAclHelper.deserializeAclSpec(AclEntry.aclSpecToString(aclSpec));\r\n    AbfsRestOperation abfsRestOperation = abfsClient.setAcl(testFileName, AbfsAclHelper.serializeAclSpec(aclEntries), tracingContext);\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertNoCPKResponseHeadersPresent(abfsRestOperation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAclWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetAclWithCPK() throws Exception\n{\r\n    testGetAcl(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAclWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetAclWithoutCPK() throws Exception\n{\r\n    testGetAcl(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAcl",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetAcl(final boolean isWithCPK) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    Assume.assumeTrue(fs.getIsNamespaceEnabled(tracingContext));\r\n    createFileAndGetContent(fs, testFileName, FILE_SIZE);\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    AbfsRestOperation abfsRestOperation = abfsClient.getAclStatus(testFileName, tracingContext);\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertNoCPKResponseHeadersPresent(abfsRestOperation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccessWithCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCheckAccessWithCPK() throws Exception\n{\r\n    testCheckAccess(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccessWithoutCPK",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCheckAccessWithoutCPK() throws Exception\n{\r\n    testCheckAccess(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccess",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCheckAccess(final boolean isWithCPK) throws Exception\n{\r\n    boolean isHNSEnabled = getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\r\n    Assume.assumeTrue(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT + \" is false\", isHNSEnabled);\r\n    Assume.assumeTrue(\"AuthType has to be OAuth\", getAuthType() == AuthType.OAuth);\r\n    final AzureBlobFileSystem fs = getAbfs(isWithCPK);\r\n    final String testFileName = path(\"/\" + methodName.getMethodName()).toString();\r\n    fs.create(new Path(testFileName)).close();\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    AbfsRestOperation abfsRestOperation = abfsClient.checkAccess(testFileName, \"rwx\", getTestTracingContext(fs, false));\r\n    assertCPKHeaders(abfsRestOperation, false);\r\n    assertNoCPKResponseHeadersPresent(abfsRestOperation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFileAndGetContent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] createFileAndGetContent(AzureBlobFileSystem fs, String fileName, int fileSize) throws IOException\n{\r\n    byte[] fileContent = getRandomBytesArray(fileSize);\r\n    Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n    ContractTestUtils.verifyFileContents(fs, testFilePath, fileContent);\r\n    return fileContent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertCPKHeaders",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertCPKHeaders(AbfsRestOperation abfsRestOperation, boolean isCPKHeaderExpected)\n{\r\n    assertHeader(abfsRestOperation, X_MS_ENCRYPTION_KEY, isCPKHeaderExpected);\r\n    assertHeader(abfsRestOperation, X_MS_ENCRYPTION_KEY_SHA256, isCPKHeaderExpected);\r\n    assertHeader(abfsRestOperation, X_MS_ENCRYPTION_ALGORITHM, isCPKHeaderExpected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertNoCPKResponseHeadersPresent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertNoCPKResponseHeadersPresent(AbfsRestOperation abfsRestOperation)\n{\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_SERVER_ENCRYPTED, \"\");\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_REQUEST_SERVER_ENCRYPTED, \"\");\r\n    assertResponseHeader(abfsRestOperation, false, X_MS_ENCRYPTION_KEY_SHA256, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertResponseHeader",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertResponseHeader(AbfsRestOperation abfsRestOperation, boolean isHeaderExpected, String headerName, String expectedValue)\n{\r\n    final AbfsHttpOperation result = abfsRestOperation.getResult();\r\n    final String value = result.getResponseHeader(headerName);\r\n    if (isHeaderExpected) {\r\n        Assertions.assertThat(value).isEqualTo(expectedValue);\r\n    } else {\r\n        Assertions.assertThat(value).isNull();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertHeader",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertHeader(AbfsRestOperation abfsRestOperation, String headerName, boolean isCPKHeaderExpected)\n{\r\n    assertTrue(abfsRestOperation != null);\r\n    Optional<AbfsHttpHeader> header = abfsRestOperation.getRequestHeaders().stream().filter(abfsHttpHeader -> abfsHttpHeader.getName().equalsIgnoreCase(headerName)).findFirst();\r\n    String desc;\r\n    if (isCPKHeaderExpected) {\r\n        desc = \"CPK header \" + headerName + \" is expected, but the same is absent.\";\r\n    } else {\r\n        desc = \"CPK header \" + headerName + \" is not expected, but the same is present.\";\r\n    }\r\n    Assertions.assertThat(header.isPresent()).describedAs(desc).isEqualTo(isCPKHeaderExpected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getSHA256Hash",
  "errType" : [ "NoSuchAlgorithmException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] getSHA256Hash(String key) throws IOException\n{\r\n    try {\r\n        final MessageDigest digester = MessageDigest.getInstance(\"SHA-256\");\r\n        return digester.digest(key.getBytes(StandardCharsets.UTF_8));\r\n    } catch (NoSuchAlgorithmException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getCPKSha",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getCPKSha(final AzureBlobFileSystem abfs) throws IOException\n{\r\n    Configuration conf = abfs.getConf();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    String encryptionKey = conf.get(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n    if (encryptionKey == null || encryptionKey.isEmpty()) {\r\n        return \"\";\r\n    }\r\n    return getBase64EncodedString(getSHA256Hash(encryptionKey));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getBase64EncodedString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBase64EncodedString(byte[] bytes)\n{\r\n    return java.util.Base64.getEncoder().encodeToString(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFileWithContent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path createFileWithContent(FileSystem fs, String fileName, byte[] fileContent) throws IOException\n{\r\n    Path testFilePath = new Path(fileName);\r\n    try (FSDataOutputStream oStream = fs.create(testFilePath)) {\r\n        oStream.write(fileContent);\r\n        oStream.flush();\r\n    }\r\n    return testFilePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "convertXmsPropertiesToCommaSeparatedString",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String, String> properties) throws CharacterCodingException\n{\r\n    StringBuilder commaSeparatedProperties = new StringBuilder();\r\n    final CharsetEncoder encoder = Charset.forName(XMS_PROPERTIES_ENCODING).newEncoder();\r\n    for (Map.Entry<String, String> propertyEntry : properties.entrySet()) {\r\n        String key = propertyEntry.getKey();\r\n        String value = propertyEntry.getValue();\r\n        Boolean canEncodeValue = encoder.canEncode(value);\r\n        if (!canEncodeValue) {\r\n            throw new CharacterCodingException();\r\n        }\r\n        String encodedPropertyValue = Base64.encode(encoder.encode(CharBuffer.wrap(value)).array());\r\n        commaSeparatedProperties.append(key).append(AbfsHttpConstants.EQUAL).append(encodedPropertyValue);\r\n        commaSeparatedProperties.append(AbfsHttpConstants.COMMA);\r\n    }\r\n    if (commaSeparatedProperties.length() != 0) {\r\n        commaSeparatedProperties.deleteCharAt(commaSeparatedProperties.length() - 1);\r\n    }\r\n    return commaSeparatedProperties.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getOctalNotation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getOctalNotation(FsPermission fsPermission)\n{\r\n    Preconditions.checkNotNull(fsPermission, \"fsPermission\");\r\n    return String.format(AbfsHttpConstants.PERMISSION_FORMAT, fsPermission.toOctal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRandomBytesArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getRandomBytesArray(int length)\n{\r\n    final byte[] b = new byte[length];\r\n    new Random().nextBytes(b);\r\n    return b;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobFileSystem getAbfs(boolean withCPK) throws IOException\n{\r\n    return getAbfs(withCPK, \"12345678901234567890123456789012\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AzureBlobFileSystem getAbfs(boolean withCPK, String cpk) throws IOException\n{\r\n    Configuration conf = getRawConfiguration();\r\n    if (withCPK) {\r\n        conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + getAccountName(), cpk);\r\n    } else {\r\n        conf.unset(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + getAccountName());\r\n    }\r\n    return (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getSameFSWithWrongCPK",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "AzureBlobFileSystem getSameFSWithWrongCPK(final AzureBlobFileSystem fs) throws IOException\n{\r\n    AbfsConfiguration abfsConf = fs.getAbfsStore().getAbfsConfiguration();\r\n    Configuration conf = abfsConf.getRawConfiguration();\r\n    String accountName = conf.get(FS_AZURE_ABFS_ACCOUNT_NAME);\r\n    String cpk = conf.get(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName);\r\n    if (cpk == null || cpk.isEmpty()) {\r\n        cpk = \"01234567890123456789012345678912\";\r\n    }\r\n    cpk = \"different-\" + cpk;\r\n    String differentCpk = cpk.substring(0, ENCRYPTION_KEY_LEN - 1);\r\n    conf.set(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY + \".\" + accountName, differentCpk);\r\n    conf.set(\"fs.defaultFS\", \"abfs://\" + getFileSystemName() + \"@\" + accountName);\r\n    AzureBlobFileSystem sameFSWithDifferentCPK = (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n    return sameFSWithDifferentCPK;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setTestUserFs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setTestUserFs() throws Exception\n{\r\n    if (this.testUserFs != null) {\r\n        return;\r\n    }\r\n    checkIfConfigIsSet(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT + \".\" + getAccountName());\r\n    Configuration conf = getRawConfiguration();\r\n    setTestFsConf(FS_AZURE_BLOB_FS_CLIENT_ID, FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID);\r\n    setTestFsConf(FS_AZURE_BLOB_FS_CLIENT_SECRET, FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET);\r\n    conf.set(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.OAuth.name());\r\n    conf.set(FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME + \".\" + getAccountName(), ClientCredsTokenProvider.class.getName());\r\n    conf.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, false);\r\n    this.testUserFs = FileSystem.newInstance(getRawConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setTestFsConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setTestFsConf(final String fsConfKey, final String testFsConfKey)\n{\r\n    final String confKeyWithAccountName = fsConfKey + \".\" + getAccountName();\r\n    final String confValue = getConfiguration().getString(testFsConfKey, \"\");\r\n    getRawConfiguration().set(confKeyWithAccountName, confValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccessWithNullPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCheckAccessWithNullPath() throws IOException\n{\r\n    superUserFs.access(null, FsAction.READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccessForFileWithNullFsAction",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCheckAccessForFileWithNullFsAction() throws Exception\n{\r\n    Assume.assumeTrue(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT + \" is false\", isHNSEnabled);\r\n    Assume.assumeTrue(FS_AZURE_ENABLE_CHECK_ACCESS + \" is false\", isCheckAccessEnabled);\r\n    superUserFs.access(new Path(\"test.txt\"), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccessForNonExistentFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCheckAccessForNonExistentFile() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path nonExistentFile = setupTestDirectoryAndUserAccess(\"/nonExistentFile1.txt\", FsAction.ALL);\r\n    superUserFs.delete(nonExistentFile, true);\r\n    testUserFs.access(nonExistentFile, FsAction.READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWhenCheckAccessConfigIsOff",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testWhenCheckAccessConfigIsOff() throws Exception\n{\r\n    Assume.assumeTrue(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT + \" is false\", isHNSEnabled);\r\n    Configuration conf = getRawConfiguration();\r\n    conf.setBoolean(FS_AZURE_ENABLE_CHECK_ACCESS, false);\r\n    FileSystem fs = FileSystem.newInstance(conf);\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test1.txt\", FsAction.NONE);\r\n    fs.access(testFilePath, FsAction.EXECUTE);\r\n    fs.access(testFilePath, FsAction.READ);\r\n    fs.access(testFilePath, FsAction.WRITE);\r\n    fs.access(testFilePath, FsAction.READ_EXECUTE);\r\n    fs.access(testFilePath, FsAction.WRITE_EXECUTE);\r\n    fs.access(testFilePath, FsAction.READ_WRITE);\r\n    fs.access(testFilePath, FsAction.ALL);\r\n    testFilePath = setupTestDirectoryAndUserAccess(\"/test1.txt\", FsAction.ALL);\r\n    fs.access(testFilePath, FsAction.EXECUTE);\r\n    fs.access(testFilePath, FsAction.READ);\r\n    fs.access(testFilePath, FsAction.WRITE);\r\n    fs.access(testFilePath, FsAction.READ_EXECUTE);\r\n    fs.access(testFilePath, FsAction.WRITE_EXECUTE);\r\n    fs.access(testFilePath, FsAction.READ_WRITE);\r\n    fs.access(testFilePath, FsAction.ALL);\r\n    fs.access(testFilePath, null);\r\n    Path nonExistentFile = setupTestDirectoryAndUserAccess(\"/nonExistentFile2\" + \".txt\", FsAction.NONE);\r\n    superUserFs.delete(nonExistentFile, true);\r\n    fs.access(nonExistentFile, FsAction.READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccessForAccountWithoutNS",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testCheckAccessForAccountWithoutNS() throws Exception\n{\r\n    Assume.assumeFalse(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT + \" is true\", getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, true));\r\n    Assume.assumeTrue(FS_AZURE_ENABLE_CHECK_ACCESS + \" is false\", isCheckAccessEnabled);\r\n    checkIfConfigIsSet(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID);\r\n    checkIfConfigIsSet(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET);\r\n    checkIfConfigIsSet(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID);\r\n    setTestUserFs();\r\n    intercept(AccessControlException.class, \"\\\"This request is not authorized to perform this operation using \" + \"this permission.\\\", 403\", () -> testUserFs.access(new Path(\"/\"), FsAction.READ));\r\n    AzureBlobFileSystemStore mockAbfsStore = Mockito.mock(AzureBlobFileSystemStore.class);\r\n    Mockito.when(mockAbfsStore.getIsNamespaceEnabled(getTestTracingContext(getFileSystem(), false))).thenReturn(true);\r\n    Field abfsStoreField = AzureBlobFileSystem.class.getDeclaredField(\"abfsStore\");\r\n    abfsStoreField.setAccessible(true);\r\n    abfsStoreField.set(testUserFs, mockAbfsStore);\r\n    testUserFs.access(new Path(\"/\"), FsAction.READ);\r\n    superUserFs.access(new Path(\"/\"), FsAction.READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFsActionNONE",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFsActionNONE() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test2.txt\", FsAction.NONE);\r\n    assertInaccessible(testFilePath, FsAction.EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ);\r\n    assertInaccessible(testFilePath, FsAction.WRITE);\r\n    assertInaccessible(testFilePath, FsAction.READ_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.WRITE_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ_WRITE);\r\n    assertInaccessible(testFilePath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFsActionEXECUTE",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFsActionEXECUTE() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test3.txt\", FsAction.EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ);\r\n    assertInaccessible(testFilePath, FsAction.WRITE);\r\n    assertInaccessible(testFilePath, FsAction.READ_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.WRITE_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ_WRITE);\r\n    assertInaccessible(testFilePath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFsActionREAD",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFsActionREAD() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test4.txt\", FsAction.READ);\r\n    assertAccessible(testFilePath, FsAction.READ);\r\n    assertInaccessible(testFilePath, FsAction.EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.WRITE);\r\n    assertInaccessible(testFilePath, FsAction.READ_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.WRITE_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ_WRITE);\r\n    assertInaccessible(testFilePath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFsActionWRITE",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFsActionWRITE() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test5.txt\", FsAction.WRITE);\r\n    assertAccessible(testFilePath, FsAction.WRITE);\r\n    assertInaccessible(testFilePath, FsAction.EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ);\r\n    assertInaccessible(testFilePath, FsAction.READ_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.WRITE_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ_WRITE);\r\n    assertInaccessible(testFilePath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFsActionREADEXECUTE",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFsActionREADEXECUTE() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test6.txt\", FsAction.READ_EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.READ);\r\n    assertAccessible(testFilePath, FsAction.READ_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.WRITE);\r\n    assertInaccessible(testFilePath, FsAction.WRITE_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ_WRITE);\r\n    assertInaccessible(testFilePath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFsActionWRITEEXECUTE",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFsActionWRITEEXECUTE() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test7.txt\", FsAction.WRITE_EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.WRITE);\r\n    assertAccessible(testFilePath, FsAction.WRITE_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ);\r\n    assertInaccessible(testFilePath, FsAction.READ_EXECUTE);\r\n    assertInaccessible(testFilePath, FsAction.READ_WRITE);\r\n    assertInaccessible(testFilePath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFsActionALL",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFsActionALL() throws Exception\n{\r\n    checkPrerequisites();\r\n    Path testFilePath = setupTestDirectoryAndUserAccess(\"/test8.txt\", FsAction.ALL);\r\n    assertAccessible(testFilePath, FsAction.EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.WRITE);\r\n    assertAccessible(testFilePath, FsAction.WRITE_EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.READ);\r\n    assertAccessible(testFilePath, FsAction.READ_EXECUTE);\r\n    assertAccessible(testFilePath, FsAction.READ_WRITE);\r\n    assertAccessible(testFilePath, FsAction.ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkPrerequisites",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkPrerequisites() throws Exception\n{\r\n    setTestUserFs();\r\n    Assume.assumeTrue(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT + \" is false\", isHNSEnabled);\r\n    Assume.assumeTrue(FS_AZURE_ENABLE_CHECK_ACCESS + \" is false\", isCheckAccessEnabled);\r\n    checkIfConfigIsSet(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_ID);\r\n    checkIfConfigIsSet(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_CLIENT_SECRET);\r\n    checkIfConfigIsSet(FS_AZURE_BLOB_FS_CHECKACCESS_TEST_USER_GUID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkIfConfigIsSet",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkIfConfigIsSet(String configKey)\n{\r\n    AbfsConfiguration conf = getConfiguration();\r\n    String value = conf.get(configKey);\r\n    Assume.assumeTrue(configKey + \" config is mandatory for the test to run\", value != null && value.trim().length() > 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertAccessible",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertAccessible(Path testFilePath, FsAction fsAction) throws IOException\n{\r\n    assertTrue(\"Should have been given access  \" + fsAction + \" on \" + testFilePath, isAccessible(testUserFs, testFilePath, fsAction));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertInaccessible",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertInaccessible(Path testFilePath, FsAction fsAction) throws IOException\n{\r\n    assertFalse(\"Should have been denied access  \" + fsAction + \" on \" + testFilePath, isAccessible(testUserFs, testFilePath, fsAction));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setExecuteAccessForParentDirs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setExecuteAccessForParentDirs(Path dir) throws IOException\n{\r\n    dir = dir.getParent();\r\n    while (dir != null) {\r\n        modifyAcl(dir, testUserGuid, FsAction.EXECUTE);\r\n        dir = dir.getParent();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "modifyAcl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void modifyAcl(Path file, String uid, FsAction fsAction) throws IOException\n{\r\n    List<AclEntry> aclSpec = Lists.newArrayList(AclTestHelpers.aclEntry(AclEntryScope.ACCESS, AclEntryType.USER, uid, fsAction));\r\n    this.superUserFs.modifyAclEntries(file, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setupTestDirectoryAndUserAccess",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path setupTestDirectoryAndUserAccess(String testFileName, FsAction fsAction) throws Exception\n{\r\n    Path testPath = path(TEST_FOLDER_PATH);\r\n    Path file = new Path(testPath + testFileName);\r\n    file = this.superUserFs.makeQualified(file);\r\n    this.superUserFs.delete(file, true);\r\n    this.superUserFs.create(file);\r\n    modifyAcl(file, testUserGuid, fsAction);\r\n    setExecuteAccessForParentDirs(file);\r\n    return file;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isAccessible",
  "errType" : [ "AccessControlException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isAccessible(FileSystem fs, Path path, FsAction fsAction) throws IOException\n{\r\n    try {\r\n        fs.access(path, fsAction);\r\n    } catch (AccessControlException ace) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMaskingAndEncoding",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testMaskingAndEncoding() throws MalformedURLException, UnsupportedEncodingException\n{\r\n    testIfMaskAndEncodeSuccessful(\"Where sig is the only query param\", \"http://www.testurl.net?sig=abcd\", \"http://www.testurl.net?sig=XXXXX\");\r\n    testIfMaskAndEncodeSuccessful(\"Where oid is the only query param\", \"http://www.testurl.net?saoid=abcdef\", \"http://www.testurl.net?saoid=abcXXX\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig is the first query param, oid is last\", \"http://www.testurl.net?sig=abcd&abc=xyz&saoid=pqrs456\", \"http://www.testurl.net?sig=XXXXX&abc=xyz&saoid=pqrsXXX\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig/oid are neither first nor last query param\", \"http://www.testurl.net?lmn=abc&sig=abcd&suoid=mnop789&abc=xyz\", \"http://www.testurl.net?lmn=abc&sig=XXXXX&suoid=mnopXXX&abc=xyz\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig is the last query param, oid is first\", \"http://www.testurl.net?skoid=pqrs123&abc=xyz&sig=abcd\", \"http://www.testurl.net?skoid=pqrsXXX&abc=xyz&sig=XXXXX\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig/oid query param are not present\", \"http://www.testurl.net?abc=xyz\", \"http://www.testurl.net?abc=xyz\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig/oid query param are not present but mysig and myoid\", \"http://www.testurl.net?abc=xyz&mysig=qwerty&mysaoid=uvw\", \"http://www.testurl.net?abc=xyz&mysig=qwerty&mysaoid=uvw\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig/oid query param is not present but sigmy and oidmy\", \"http://www.testurl.net?abc=xyz&sigmy=qwerty&skoidmy=uvw\", \"http://www.testurl.net?abc=xyz&sigmy=qwerty&skoidmy=uvw\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig/oid query param is not present but values sig and oid\", \"http://www.testurl.net?abc=xyz&mnop=sig&pqr=saoid\", \"http://www.testurl.net?abc=xyz&mnop=sig&pqr=saoid\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig/oid query param is not present but a value ends with sig/oid\", \"http://www.testurl.net?abc=xyzsaoid&mnop=abcsig\", \"http://www.testurl.net?abc=xyzsaoid&mnop=abcsig\");\r\n    testIfMaskAndEncodeSuccessful(\"Where sig/oid query param is not present but a value starts with sig/oid\", \"http://www.testurl.net?abc=saoidxyz&mnop=sigabc\", \"http://www.testurl.net?abc=saoidxyz&mnop=sigabc\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testUrlWithNullValues",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUrlWithNullValues() throws MalformedURLException, UnsupportedEncodingException\n{\r\n    testIfMaskAndEncodeSuccessful(\"Where param to be masked has null value\", \"http://www.testurl.net?abc=xyz&saoid=&mnop=abcsig\", \"http://www.testurl.net?abc=xyz&saoid=&mnop=abcsig\");\r\n    testIfMaskAndEncodeSuccessful(\"Where visible param has null value\", \"http://www.testurl.net?abc=xyz&pqr=&mnop=abcd\", \"http://www.testurl.net?abc=xyz&pqr=&mnop=abcd\");\r\n    testIfMaskAndEncodeSuccessful(\"Where last param has null value\", \"http://www.testurl.net?abc=xyz&pqr=&mnop=\", \"http://www.testurl.net?abc=xyz&pqr=&mnop=\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testIfMaskAndEncodeSuccessful",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testIfMaskAndEncodeSuccessful(final String scenario, final String url, final String expectedMaskedUrl) throws UnsupportedEncodingException, MalformedURLException\n{\r\n    Assertions.assertThat(UriUtils.getMaskedUrl(new URL(url))).describedAs(url + \" (\" + scenario + \") after masking should be: \" + expectedMaskedUrl).isEqualTo(expectedMaskedUrl);\r\n    final String expectedMaskedEncodedUrl = URLEncoder.encode(expectedMaskedUrl, \"UTF-8\");\r\n    Assertions.assertThat(UriUtils.encodedUrlStr(expectedMaskedUrl)).describedAs(url + \" (\" + scenario + \") after masking and encoding should \" + \"be: \" + expectedMaskedEncodedUrl).isEqualTo(expectedMaskedEncodedUrl);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    fs = getTestAccount().getFileSystem();\r\n    assertEquals(0, MAX_BYTES % PAGE_SIZE);\r\n    randomData = new byte[PAGE_SIZE * MAX_PAGES];\r\n    rand.nextBytes(randomData);\r\n    blobPath = blobPath(\"ITestReadAndSeekPageBlobAfterWrite\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    deleteQuietly(fs, blobPath, true);\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testIsPageBlobFileName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testIsPageBlobFileName()\n{\r\n    AzureNativeFileSystemStore store = ((NativeAzureFileSystem) fs).getStore();\r\n    String[] a = blobPath.toUri().getPath().split(\"/\");\r\n    String key2 = a[1] + \"/\";\r\n    assertTrue(\"Not a page blob: \" + blobPath, store.isPageBlobKey(key2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testReadAfterWriteRandomData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReadAfterWriteRandomData() throws IOException\n{\r\n    final int pds = PAGE_DATA_SIZE;\r\n    int[] dataSizes = { 0, 1, 2, 3, pds - 1, pds, pds + 1, pds + 2, pds + 3, (2 * pds) - 1, (2 * pds), (2 * pds) + 1, (2 * pds) + 2, (2 * pds) + 3, (10 * pds) - 1, (10 * pds), (10 * pds) + 1, (10 * pds) + 2, (10 * pds) + 3, MAX_BYTES };\r\n    for (int i : dataSizes) {\r\n        testReadAfterWriteRandomData(i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testReadAfterWriteRandomData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReadAfterWriteRandomData(int size) throws IOException\n{\r\n    writeRandomData(size);\r\n    readRandomDataAndVerify(size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "readRandomDataAndVerify",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readRandomDataAndVerify(int size) throws AzureException, IOException\n{\r\n    byte[] b = new byte[size];\r\n    FSDataInputStream stream = fs.open(blobPath);\r\n    int bytesRead = stream.read(b);\r\n    stream.close();\r\n    assertEquals(bytesRead, size);\r\n    assertTrue(comparePrefix(randomData, b, size));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "comparePrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean comparePrefix(byte[] a, byte[] b, int size)\n{\r\n    if (a.length < size || b.length < size) {\r\n        return false;\r\n    }\r\n    for (int i = 0; i < size; i++) {\r\n        if (a[i] != b[i]) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeRandomData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeRandomData(int size) throws IOException\n{\r\n    OutputStream output = fs.create(blobPath);\r\n    output.write(randomData, 0, size);\r\n    output.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testPageBlobSeekAndReadAfterWrite",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testPageBlobSeekAndReadAfterWrite() throws IOException\n{\r\n    writeRandomData(PAGE_SIZE * MAX_PAGES);\r\n    int recordSize = 100;\r\n    byte[] b = new byte[recordSize];\r\n    try (FSDataInputStream stream = fs.open(blobPath)) {\r\n        int seekPosition = 5 * PAGE_SIZE + 250;\r\n        stream.seek(seekPosition);\r\n        int bytesRead = stream.read(b);\r\n        verifyReadRandomData(b, bytesRead, seekPosition, recordSize);\r\n        seekPosition = 10 * PAGE_SIZE + 250;\r\n        stream.seek(seekPosition);\r\n        recordSize = 1000;\r\n        b = new byte[recordSize];\r\n        bytesRead = stream.read(b);\r\n        verifyReadRandomData(b, bytesRead, seekPosition, recordSize);\r\n        recordSize = 100;\r\n        seekPosition = PAGE_SIZE * MAX_PAGES - recordSize;\r\n        stream.seek(seekPosition);\r\n        b = new byte[recordSize];\r\n        bytesRead = stream.read(b);\r\n        verifyReadRandomData(b, bytesRead, seekPosition, recordSize);\r\n        recordSize = 100;\r\n        seekPosition = PAGE_SIZE * MAX_PAGES - recordSize + 50;\r\n        stream.seek(seekPosition);\r\n        b = new byte[recordSize];\r\n        bytesRead = stream.read(b);\r\n        assertEquals(50, bytesRead);\r\n        byte[] tail = Arrays.copyOfRange(randomData, seekPosition, randomData.length);\r\n        assertTrue(comparePrefix(tail, b, 50));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyReadRandomData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyReadRandomData(byte[] b, int bytesRead, int seekPosition, int recordSize)\n{\r\n    byte[] originalRecordData = Arrays.copyOfRange(randomData, seekPosition, seekPosition + recordSize + 1);\r\n    assertEquals(recordSize, bytesRead);\r\n    assertTrue(comparePrefix(originalRecordData, b, recordSize));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testManySmallWritesWithHFlush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testManySmallWritesWithHFlush() throws IOException\n{\r\n    writeAndReadOneFile(50, 100, 20);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeAndReadOneFile",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void writeAndReadOneFile(int numWrites, int recordLength, int syncInterval) throws IOException\n{\r\n    final long MINIMUM_EXPECTED_TIME = 20;\r\n    LOG.info(\"Writing \" + numWrites * recordLength + \" bytes to \" + blobPath.getName());\r\n    FSDataOutputStream output = fs.create(blobPath);\r\n    int writesSinceHFlush = 0;\r\n    try {\r\n        output.flush();\r\n        output.hflush();\r\n        for (int i = 0; i < numWrites; i++) {\r\n            output.write(randomData, i * recordLength, recordLength);\r\n            writesSinceHFlush++;\r\n            output.flush();\r\n            if ((i % syncInterval) == 0) {\r\n                output.hflush();\r\n                writesSinceHFlush = 0;\r\n            }\r\n        }\r\n    } finally {\r\n        long start = Time.monotonicNow();\r\n        output.close();\r\n        long end = Time.monotonicNow();\r\n        LOG.debug(\"close duration = \" + (end - start) + \" msec.\");\r\n        if (writesSinceHFlush > 0) {\r\n            assertTrue(String.format(\"close duration with >= 1 pending write is %d, less than minimum expected of %d\", end - start, MINIMUM_EXPECTED_TIME), end - start >= MINIMUM_EXPECTED_TIME);\r\n        }\r\n    }\r\n    FSDataInputStream stream = fs.open(blobPath);\r\n    int SIZE = numWrites * recordLength;\r\n    byte[] b = new byte[SIZE];\r\n    try {\r\n        stream.seek(0);\r\n        stream.read(b, 0, SIZE);\r\n        verifyReadRandomData(b, SIZE, 0, SIZE);\r\n    } finally {\r\n        stream.close();\r\n    }\r\n    fs.delete(blobPath, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testLargeFileStress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLargeFileStress() throws IOException\n{\r\n    int numWrites = 32;\r\n    int recordSize = 1024 * 1024;\r\n    int syncInterval = 10;\r\n    int repetitions = 1;\r\n    for (int i = 0; i < repetitions; i++) {\r\n        writeAndReadOneFile(numWrites, recordSize, syncInterval);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFileSizeExtension",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFileSizeExtension() throws IOException\n{\r\n    final int writeSize = 1024 * 1024;\r\n    final int numWrites = 129;\r\n    final byte dataByte = 5;\r\n    byte[] data = new byte[writeSize];\r\n    Arrays.fill(data, dataByte);\r\n    try (FSDataOutputStream output = fs.create(blobPath)) {\r\n        for (int i = 0; i < numWrites; i++) {\r\n            output.write(data);\r\n            output.hflush();\r\n            LOG.debug(\"total writes = \" + (i + 1));\r\n        }\r\n    }\r\n    assertTrue(numWrites * writeSize > PageBlobOutputStream.PAGE_BLOB_MIN_SIZE);\r\n    FileStatus[] status = fs.listStatus(blobPath);\r\n    assertEquals(\"File size hasn't changed \" + status, numWrites * writeSize, status[0].getLen());\r\n    LOG.debug(\"Total bytes written to \" + blobPath + \" = \" + status[0].getLen());\r\n    fs.delete(blobPath, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return enableManifestCommitter(prepareTestConfiguration(binding));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "suitename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suitename()\n{\r\n    return \"ITestAbfsManifestCommitProtocol\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFinalize",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFinalize() throws Exception\n{\r\n    Configuration rawConfig = this.getRawConfiguration();\r\n    rawConfig.setBoolean(DISABLE_ABFS_CACHE_KEY, true);\r\n    rawConfig.setBoolean(DISABLE_ABFSS_CACHE_KEY, true);\r\n    AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.get(rawConfig);\r\n    WeakReference<Object> ref = new WeakReference<Object>(fs);\r\n    fs = null;\r\n    int i = 0;\r\n    int maxTries = 1000;\r\n    while (ref.get() != null && i < maxTries) {\r\n        System.gc();\r\n        System.runFinalization();\r\n        i++;\r\n    }\r\n    Assert.assertTrue(\"testFinalizer didn't get cleaned up within maxTries\", ref.get() == null);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "initializeMac",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initializeMac()\n{\r\n    try {\r\n        hmacSha256 = Mac.getInstance(\"HmacSHA256\");\r\n        hmacSha256.init(new SecretKeySpec(key, \"HmacSHA256\"));\r\n    } catch (final Exception e) {\r\n        throw new IllegalArgumentException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "computeHmac256",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String computeHmac256(final String stringToSign)\n{\r\n    byte[] utf8Bytes;\r\n    try {\r\n        utf8Bytes = stringToSign.getBytes(StandardCharsets.UTF_8.toString());\r\n    } catch (final UnsupportedEncodingException e) {\r\n        throw new IllegalArgumentException(e);\r\n    }\r\n    byte[] hmac;\r\n    synchronized (this) {\r\n        hmac = hmacSha256.doFinal(utf8Bytes);\r\n    }\r\n    return Base64.encode(hmac);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setup();\r\n    getFileSystem().getConf().set(IOSTATISTICS_LOGGING_LEVEL, IOSTATISTICS_LOGGING_LEVEL_INFO);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testInitialStatsValues",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testInitialStatsValues() throws IOException\n{\r\n    describe(\"Testing the initial values of Abfs counters\");\r\n    AbfsCounters abfsCounters = new AbfsCountersImpl(getFileSystem().getUri());\r\n    IOStatistics ioStatistics = abfsCounters.getIOStatistics();\r\n    for (Map.Entry<String, Long> entry : ioStatistics.counters().entrySet()) {\r\n        checkInitialValue(entry.getKey(), entry.getValue(), 0);\r\n    }\r\n    for (Map.Entry<String, Long> entry : ioStatistics.gauges().entrySet()) {\r\n        checkInitialValue(entry.getKey(), entry.getValue(), 0);\r\n    }\r\n    for (Map.Entry<String, Long> entry : ioStatistics.maximums().entrySet()) {\r\n        checkInitialValue(entry.getKey(), entry.getValue(), -1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateStatistics",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testCreateStatistics() throws IOException\n{\r\n    describe(\"Testing counter values got by creating directories and files in\" + \" Abfs\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path createFilePath = path(getMethodName());\r\n    Path createDirectoryPath = path(getMethodName() + \"Dir\");\r\n    fs.mkdirs(createDirectoryPath);\r\n    fs.createNonRecursive(createFilePath, FsPermission.getDefault(), false, 1024, (short) 1, 1024, null).close();\r\n    Map<String, Long> metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_CREATE, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_CREATE_NON_RECURSIVE, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.FILES_CREATED, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.DIRECTORIES_CREATED, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_MKDIRS, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_GET_FILE_STATUS, 2, metricMap);\r\n    fs.initialize(fs.getUri(), fs.getConf());\r\n    for (int i = 0; i < NUMBER_OF_OPS; i++) {\r\n        fs.mkdirs(path(getMethodName() + \"Dir\" + i));\r\n        fs.createNonRecursive(path(getMethodName() + i), FsPermission.getDefault(), false, 1024, (short) 1, 1024, null).close();\r\n    }\r\n    metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_CREATE, NUMBER_OF_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_CREATE_NON_RECURSIVE, NUMBER_OF_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.FILES_CREATED, NUMBER_OF_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.DIRECTORIES_CREATED, NUMBER_OF_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_MKDIRS, NUMBER_OF_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_GET_FILE_STATUS, 1 + NUMBER_OF_OPS, metricMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteStatistics",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testDeleteStatistics() throws IOException\n{\r\n    describe(\"Testing counter values got by deleting directory and files \" + \"in Abfs\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path createDirectoryPath = path(\"/\");\r\n    Path createFilePath = path(getMethodName());\r\n    fs.mkdirs(createDirectoryPath);\r\n    fs.create(path(createDirectoryPath + getMethodName())).close();\r\n    fs.delete(createDirectoryPath, true);\r\n    Map<String, Long> metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_DELETE, 2, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.FILES_DELETED, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_LIST_STATUS, 1, metricMap);\r\n    fs.mkdirs(createDirectoryPath);\r\n    fs.create(createFilePath).close();\r\n    fs.delete(createDirectoryPath, true);\r\n    metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.DIRECTORIES_DELETED, 1, metricMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOpenAppendRenameExists",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testOpenAppendRenameExists() throws IOException\n{\r\n    describe(\"Testing counter values on calling open, append and rename and \" + \"exists methods on Abfs\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path createFilePath = path(getMethodName());\r\n    Path destCreateFilePath = path(getMethodName() + \"New\");\r\n    fs.create(createFilePath).close();\r\n    fs.open(createFilePath).close();\r\n    fs.append(createFilePath).close();\r\n    assertTrue(fs.rename(createFilePath, destCreateFilePath));\r\n    Map<String, Long> metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_OPEN, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_APPEND, 1, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_RENAME, 1, metricMap);\r\n    assertTrue(String.format(\"File with name %s should exist\", destCreateFilePath), fs.exists(destCreateFilePath));\r\n    assertFalse(String.format(\"File with name %s should not exist\", createFilePath), fs.exists(createFilePath));\r\n    metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_EXIST, 2, metricMap);\r\n    fs.initialize(fs.getUri(), fs.getConf());\r\n    fs.create(destCreateFilePath).close();\r\n    for (int i = 0; i < NUMBER_OF_OPS; i++) {\r\n        fs.open(destCreateFilePath);\r\n        fs.append(destCreateFilePath).close();\r\n    }\r\n    metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_OPEN, NUMBER_OF_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_APPEND, NUMBER_OF_OPS, metricMap);\r\n    for (int i = 0; i < NUMBER_OF_OPS; i++) {\r\n        assertTrue(fs.rename(destCreateFilePath, createFilePath));\r\n        assertTrue(fs.rename(createFilePath, destCreateFilePath));\r\n        assertTrue(String.format(\"File with name %s should exist\", destCreateFilePath), fs.exists(destCreateFilePath));\r\n        assertFalse(String.format(\"File with name %s should not exist\", createFilePath), fs.exists(createFilePath));\r\n    }\r\n    metricMap = fs.getInstrumentationMap();\r\n    assertAbfsStatistics(AbfsStatistic.CALL_RENAME, 2 * NUMBER_OF_OPS, metricMap);\r\n    assertAbfsStatistics(AbfsStatistic.CALL_EXIST, 2 * NUMBER_OF_OPS, metricMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkInitialValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkInitialValue(String statName, long statValue, long expectedInitialValue)\n{\r\n    assertEquals(\"Mismatch in \" + statName, expectedInitialValue, statValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPurgeBufferManagerForParallelStreams",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testPurgeBufferManagerForParallelStreams() throws Exception\n{\r\n    describe(\"Testing purging of buffers from ReadBufferManager for \" + \"parallel input streams\");\r\n    final int numBuffers = 16;\r\n    final LinkedList<Integer> freeList = new LinkedList<>();\r\n    for (int i = 0; i < numBuffers; i++) {\r\n        freeList.add(i);\r\n    }\r\n    ExecutorService executorService = Executors.newFixedThreadPool(4);\r\n    AzureBlobFileSystem fs = getABFSWithReadAheadConfig();\r\n    try {\r\n        for (int i = 0; i < 4; i++) {\r\n            final String fileName = methodName.getMethodName() + i;\r\n            executorService.submit((Callable<Void>) () -> {\r\n                byte[] fileContent = getRandomBytesArray(ONE_MB);\r\n                Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n                try (FSDataInputStream iStream = fs.open(testFilePath)) {\r\n                    iStream.read();\r\n                }\r\n                return null;\r\n            });\r\n        }\r\n    } finally {\r\n        executorService.shutdown();\r\n    }\r\n    ReadBufferManager bufferManager = ReadBufferManager.getBufferManager();\r\n    assertListEmpty(\"CompletedList\", bufferManager.getCompletedReadListCopy());\r\n    assertListEmpty(\"InProgressList\", bufferManager.getInProgressCopiedList());\r\n    assertListEmpty(\"ReadAheadQueue\", bufferManager.getReadAheadQueueCopy());\r\n    Assertions.assertThat(bufferManager.getFreeListCopy()).describedAs(\"After closing all streams free list contents should match with \" + freeList).hasSize(numBuffers).containsExactlyInAnyOrderElementsOf(freeList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertListEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertListEmpty(String listName, List<ReadBuffer> list)\n{\r\n    Assertions.assertThat(list).describedAs(\"After closing all streams %s should be empty\", listName).hasSize(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPurgeBufferManagerForSequentialStream",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testPurgeBufferManagerForSequentialStream() throws Exception\n{\r\n    describe(\"Testing purging of buffers in ReadBufferManager for \" + \"sequential input streams\");\r\n    AzureBlobFileSystem fs = getABFSWithReadAheadConfig();\r\n    final String fileName = methodName.getMethodName();\r\n    byte[] fileContent = getRandomBytesArray(ONE_MB);\r\n    Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n    AbfsInputStream iStream1 = null;\r\n    try {\r\n        iStream1 = (AbfsInputStream) fs.open(testFilePath).getWrappedStream();\r\n        iStream1.read();\r\n    } finally {\r\n        IOUtils.closeStream(iStream1);\r\n    }\r\n    ReadBufferManager bufferManager = ReadBufferManager.getBufferManager();\r\n    AbfsInputStream iStream2 = null;\r\n    try {\r\n        iStream2 = (AbfsInputStream) fs.open(testFilePath).getWrappedStream();\r\n        iStream2.read();\r\n        assertListDoesnotContainBuffersForIstream(bufferManager.getInProgressCopiedList(), iStream1);\r\n        assertListDoesnotContainBuffersForIstream(bufferManager.getCompletedReadListCopy(), iStream1);\r\n        assertListDoesnotContainBuffersForIstream(bufferManager.getReadAheadQueueCopy(), iStream1);\r\n    } finally {\r\n        IOUtils.closeStream(iStream2);\r\n    }\r\n    assertListDoesnotContainBuffersForIstream(bufferManager.getInProgressCopiedList(), iStream2);\r\n    assertListDoesnotContainBuffersForIstream(bufferManager.getCompletedReadListCopy(), iStream2);\r\n    assertListDoesnotContainBuffersForIstream(bufferManager.getReadAheadQueueCopy(), iStream2);\r\n    assertListEmpty(\"CompletedList\", bufferManager.getCompletedReadListCopy());\r\n    assertListEmpty(\"InProgressList\", bufferManager.getInProgressCopiedList());\r\n    assertListEmpty(\"ReadAheadQueue\", bufferManager.getReadAheadQueueCopy());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertListDoesnotContainBuffersForIstream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertListDoesnotContainBuffersForIstream(List<ReadBuffer> list, AbfsInputStream inputStream)\n{\r\n    for (ReadBuffer buffer : list) {\r\n        Assertions.assertThat(buffer.getStream()).describedAs(\"Buffers associated with closed input streams shouldn't be present\").isNotEqualTo(inputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getABFSWithReadAheadConfig",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AzureBlobFileSystem getABFSWithReadAheadConfig() throws Exception\n{\r\n    Configuration conf = getRawConfiguration();\r\n    conf.setLong(FS_AZURE_READ_AHEAD_QUEUE_DEPTH, 8);\r\n    conf.setInt(AZURE_READ_BUFFER_SIZE, MIN_BUFFER_SIZE);\r\n    conf.setInt(FS_AZURE_READ_AHEAD_BLOCK_SIZE, MIN_BUFFER_SIZE);\r\n    return (AzureBlobFileSystem) FileSystem.newInstance(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRandomBytesArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getRandomBytesArray(int length)\n{\r\n    final byte[] b = new byte[length];\r\n    new Random().nextBytes(b);\r\n    return b;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createFileWithContent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createFileWithContent(FileSystem fs, String fileName, byte[] fileContent) throws IOException\n{\r\n    Path testFilePath = path(fileName);\r\n    try (FSDataOutputStream oStream = fs.create(testFilePath)) {\r\n        oStream.write(fileContent);\r\n        oStream.flush();\r\n    }\r\n    return testFilePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    fs = AzureBlobStorageTestAccount.createMock().getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMoveFileUnderParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMoveFileUnderParent() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameFileToSelf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameFileToSelf() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameChildDirForbidden",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameChildDirForbidden() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMoveDirUnderParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMoveDirUnderParent() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameDirToSelf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameDirToSelf() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetTrileanForBoolean",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetTrileanForBoolean()\n{\r\n    assertThat(Trilean.getTrilean(true)).describedAs(\"getTrilean should return Trilean.TRUE when true is passed\").isEqualTo(Trilean.TRUE);\r\n    assertThat(Trilean.getTrilean(false)).describedAs(\"getTrilean should return Trilean.FALSE when false is passed\").isEqualTo(Trilean.FALSE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetTrileanForString",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetTrileanForString()\n{\r\n    assertThat(Trilean.getTrilean(TRUE_STR.toLowerCase())).describedAs(\"getTrilean should return Trilean.TRUE when true is passed\").isEqualTo(Trilean.TRUE);\r\n    assertThat(Trilean.getTrilean(TRUE_STR.toUpperCase())).describedAs(\"getTrilean should return Trilean.TRUE when TRUE is passed\").isEqualTo(Trilean.TRUE);\r\n    assertThat(Trilean.getTrilean(FALSE_STR.toLowerCase())).describedAs(\"getTrilean should return Trilean.FALSE when false is passed\").isEqualTo(Trilean.FALSE);\r\n    assertThat(Trilean.getTrilean(FALSE_STR.toUpperCase())).describedAs(\"getTrilean should return Trilean.FALSE when FALSE is passed\").isEqualTo(Trilean.FALSE);\r\n    testInvalidString(null);\r\n    testInvalidString(\" \");\r\n    testInvalidString(\"invalid\");\r\n    testInvalidString(\"truee\");\r\n    testInvalidString(\"falsee\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testInvalidString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInvalidString(String invalidString)\n{\r\n    assertThat(Trilean.getTrilean(invalidString)).describedAs(\"getTrilean should return Trilean.UNKNOWN for anything not true/false\").isEqualTo(Trilean.UNKNOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testToBoolean",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testToBoolean() throws TrileanConversionException\n{\r\n    assertThat(Trilean.TRUE.toBoolean()).describedAs(\"toBoolean should return true for Trilean.TRUE\").isTrue();\r\n    assertThat(Trilean.FALSE.toBoolean()).describedAs(\"toBoolean should return false for Trilean.FALSE\").isFalse();\r\n    assertThat(catchThrowable(() -> Trilean.UNKNOWN.toBoolean())).describedAs(\"toBoolean on Trilean.UNKNOWN results in TrileanConversionException\").isInstanceOf(TrileanConversionException.class).describedAs(\"Exception message should be: catchThrowable(()->Trilean.UNKNOWN\" + \".toBoolean())\").hasMessage(\"Cannot convert Trilean.UNKNOWN to boolean\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addRecord",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRecord(MetricsRecord record)\n{\r\n    allMetrics.add(record);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getMockContainerUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMockContainerUri()\n{\r\n    return String.format(\"http://%s/%s\", AzureBlobStorageTestAccount.MOCK_ACCOUNT_NAME, AzureBlobStorageTestAccount.MOCK_CONTAINER_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toMockUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toMockUri(String path)\n{\r\n    return String.format(\"http://%s/%s/%s\", AzureBlobStorageTestAccount.MOCK_ACCOUNT_NAME, AzureBlobStorageTestAccount.MOCK_CONTAINER_NAME, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toMockUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toMockUri(Path path)\n{\r\n    return toMockUri(path.toUri().getRawPath().substring(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "pageBlobPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path pageBlobPath()\n{\r\n    return new Path(\"/\" + DEFAULT_PAGE_BLOB_DIRECTORY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "pageBlobPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path pageBlobPath(String fileName)\n{\r\n    return new Path(pageBlobPath(), fileName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getLatestMetricValue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Number getLatestMetricValue(String metricName, Number defaultValue) throws IndexOutOfBoundsException\n{\r\n    boolean found = false;\r\n    Number ret = null;\r\n    for (MetricsRecord currentRecord : allMetrics) {\r\n        if (wasGeneratedByMe(currentRecord)) {\r\n            for (AbstractMetric currentMetric : currentRecord.metrics()) {\r\n                if (currentMetric.name().equalsIgnoreCase(metricName)) {\r\n                    found = true;\r\n                    ret = currentMetric.value();\r\n                    break;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (!found) {\r\n        if (defaultValue != null) {\r\n            return defaultValue;\r\n        }\r\n        throw new IndexOutOfBoundsException(metricName);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "wasGeneratedByMe",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean wasGeneratedByMe(MetricsRecord currentRecord)\n{\r\n    Assert.assertNotNull(\"null filesystem\", fs);\r\n    Assert.assertNotNull(\"null filesystemn instance ID\", fs.getInstrumentation().getFileSystemInstanceId());\r\n    String myFsId = fs.getInstrumentation().getFileSystemInstanceId().toString();\r\n    for (MetricsTag currentTag : currentRecord.tags()) {\r\n        if (currentTag.name().equalsIgnoreCase(\"wasbFileSystemId\")) {\r\n            return currentTag.value().equals(myFsId);\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBlobReference",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CloudBlockBlob getBlobReference(String blobKey) throws Exception\n{\r\n    return container.getBlockBlobReference(String.format(blobKey));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "acquireShortLease",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String acquireShortLease(String blobKey) throws Exception\n{\r\n    return getBlobReference(blobKey).acquireLease(60, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "releaseLease",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void releaseLease(String leaseID, String blobKey) throws Exception\n{\r\n    AccessCondition accessCondition = new AccessCondition();\r\n    accessCondition.setLeaseID(leaseID);\r\n    getBlobReference(blobKey).releaseLease(accessCondition);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "saveMetricsConfigFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void saveMetricsConfigFile() throws IOException\n{\r\n    if (!metricsConfigSaved) {\r\n        String testFilename = TestMetricsConfig.getTestFilename(\"hadoop-metrics2-azure-file-system\");\r\n        File dest = new File(testFilename).getCanonicalFile();\r\n        dest.getParentFile().mkdirs();\r\n        new org.apache.hadoop.metrics2.impl.ConfigBuilder().add(\"azure-file-system.sink.azuretestcollector.class\", StandardCollector.class.getName()).save(testFilename);\r\n        metricsConfigSaved = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createMock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createMock() throws Exception\n{\r\n    return createMock(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createMock",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AzureBlobStorageTestAccount createMock(Configuration conf) throws Exception\n{\r\n    saveMetricsConfigFile();\r\n    configurePageBlobDir(conf);\r\n    configureAtomicRenameDir(conf);\r\n    AzureNativeFileSystemStore store = new AzureNativeFileSystemStore();\r\n    MockStorageInterface mockStorage = new MockStorageInterface();\r\n    store.setAzureStorageInteractionLayer(mockStorage);\r\n    NativeAzureFileSystem fs = new NativeAzureFileSystem(store);\r\n    setMockAccountKey(conf);\r\n    configureSecureModeTestSettings(conf);\r\n    fs.initialize(new URI(MOCK_WASB_URI), conf);\r\n    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs, mockStorage);\r\n    return testAcct;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "configurePageBlobDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void configurePageBlobDir(Configuration conf)\n{\r\n    if (conf.get(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES) == null) {\r\n        conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, \"/\" + DEFAULT_PAGE_BLOB_DIRECTORY);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "configureAtomicRenameDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void configureAtomicRenameDir(Configuration conf)\n{\r\n    if (conf.get(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES) == null) {\r\n        conf.set(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES, DEFAULT_ATOMIC_RENAME_DIRECTORIES);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createForEmulator",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AzureBlobStorageTestAccount createForEmulator() throws Exception\n{\r\n    saveMetricsConfigFile();\r\n    NativeAzureFileSystem fs = null;\r\n    CloudBlobContainer container = null;\r\n    Configuration conf = createTestConfiguration();\r\n    if (!conf.getBoolean(USE_EMULATOR_PROPERTY_NAME, false)) {\r\n        LOG.warn(\"Skipping emulator Azure test because configuration \" + \"doesn't indicate that it's running.\");\r\n        return null;\r\n    }\r\n    CloudStorageAccount account = CloudStorageAccount.getDevelopmentStorageAccount();\r\n    fs = new NativeAzureFileSystem();\r\n    String containerName = String.format(\"wasbtests-%s-%tQ\", System.getProperty(\"user.name\"), new Date());\r\n    container = account.createCloudBlobClient().getContainerReference(containerName);\r\n    container.create();\r\n    URI accountUri = createAccountUri(DEFAULT_STORAGE_EMULATOR_ACCOUNT_NAME, containerName);\r\n    configureSecureModeTestSettings(conf);\r\n    fs.initialize(accountUri, conf);\r\n    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs, account, container);\r\n    return testAcct;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createOutOfBandStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createOutOfBandStore(int uploadBlockSize, int downloadBlockSize) throws Exception\n{\r\n    return createOutOfBandStore(uploadBlockSize, downloadBlockSize, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createOutOfBandStore",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "AzureBlobStorageTestAccount createOutOfBandStore(int uploadBlockSize, int downloadBlockSize, boolean enableSecureMode) throws Exception\n{\r\n    saveMetricsConfigFile();\r\n    CloudBlobContainer container = null;\r\n    Configuration conf = createTestConfiguration();\r\n    CloudStorageAccount account = createTestAccount(conf);\r\n    if (null == account) {\r\n        return null;\r\n    }\r\n    String containerName = String.format(\"wasbtests-%s-%tQ\", System.getProperty(\"user.name\"), new Date());\r\n    container = account.createCloudBlobClient().getContainerReference(containerName);\r\n    container.create();\r\n    String accountName = verifyWasbAccountNameInConfig(conf);\r\n    conf.setBoolean(KEY_DISABLE_THROTTLING, true);\r\n    conf.setBoolean(KEY_READ_TOLERATE_CONCURRENT_APPEND, true);\r\n    conf.setBoolean(KEY_USE_SECURE_MODE, enableSecureMode);\r\n    configureSecureModeTestSettings(conf);\r\n    URI accountUri = createAccountUri(accountName, containerName);\r\n    AzureFileSystemMetricsSystem.fileSystemStarted();\r\n    String sourceName = NativeAzureFileSystem.newMetricsSourceName();\r\n    String sourceDesc = \"Azure Storage Volume File System metrics\";\r\n    AzureFileSystemInstrumentation instrumentation = new AzureFileSystemInstrumentation(conf);\r\n    AzureFileSystemMetricsSystem.registerSource(sourceName, sourceDesc, instrumentation);\r\n    AzureNativeFileSystemStore testStorage = new AzureNativeFileSystemStore();\r\n    testStorage.initialize(accountUri, conf, instrumentation);\r\n    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(testStorage, account, container);\r\n    return testAcct;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setMockAccountKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMockAccountKey(Configuration conf)\n{\r\n    setMockAccountKey(conf, MOCK_ACCOUNT_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "configureSecureModeTestSettings",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configureSecureModeTestSettings(Configuration conf)\n{\r\n    conf.set(KEY_USE_LOCAL_SAS_KEY_MODE, \"true\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setMockAccountKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMockAccountKey(Configuration conf, String accountName)\n{\r\n    conf.set(ACCOUNT_KEY_PROPERTY_NAME + accountName, Base64.encode(new byte[] { 1, 2, 3 }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createAccountUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI createAccountUri(String accountName) throws URISyntaxException\n{\r\n    return new URI(WASB_SCHEME + \":\" + PATH_DELIMITER + PATH_DELIMITER + accountName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createAccountUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI createAccountUri(String accountName, String containerName) throws URISyntaxException\n{\r\n    return new URI(WASB_SCHEME + \":\" + PATH_DELIMITER + PATH_DELIMITER + containerName + WASB_AUTHORITY_DELIMITER + accountName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount create() throws Exception\n{\r\n    return create(\"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount create(String containerNameSuffix) throws Exception\n{\r\n    return create(containerNameSuffix, EnumSet.of(CreateOptions.CreateContainer));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createThrottled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createThrottled() throws Exception\n{\r\n    return create(\"\", EnumSet.of(CreateOptions.useThrottling, CreateOptions.CreateContainer));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount create(Configuration conf) throws Exception\n{\r\n    return create(\"\", EnumSet.of(CreateOptions.CreateContainer), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createStorageAccount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CloudStorageAccount createStorageAccount(String accountName, Configuration conf, boolean allowAnonymous) throws URISyntaxException, KeyProviderException\n{\r\n    String accountKey = AzureNativeFileSystemStore.getAccountKeyFromConfiguration(accountName, conf);\r\n    final StorageCredentials credentials;\r\n    if (accountKey == null) {\r\n        if (allowAnonymous) {\r\n            credentials = StorageCredentialsAnonymous.ANONYMOUS;\r\n        } else {\r\n            LOG.warn(\"Skipping live Azure test because of missing key for\" + \" account '\" + accountName + \"'.\");\r\n            return null;\r\n        }\r\n    } else {\r\n        credentials = new StorageCredentialsAccountAndKey(accountName.split(\"\\\\.\")[0], accountKey);\r\n    }\r\n    return new CloudStorageAccount(credentials);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createTestConfiguration()\n{\r\n    return createTestConfiguration(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createTestConfiguration(Configuration conf)\n{\r\n    if (conf == null) {\r\n        conf = new Configuration();\r\n    }\r\n    conf.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CloudStorageAccount createTestAccount() throws URISyntaxException, KeyProviderException\n{\r\n    return createTestAccount(createTestConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CloudStorageAccount createTestAccount(Configuration conf) throws URISyntaxException, KeyProviderException\n{\r\n    AzureTestUtils.assumeNamespaceDisabled(conf);\r\n    String testAccountName = verifyWasbAccountNameInConfig(conf);\r\n    if (testAccountName == null) {\r\n        LOG.warn(\"Skipping live Azure test because of missing test account\");\r\n        return null;\r\n    }\r\n    return createStorageAccount(testAccountName, conf, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount create(String containerNameSuffix, EnumSet<CreateOptions> createOptions) throws Exception\n{\r\n    return create(containerNameSuffix, createOptions, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount create(String containerNameSuffix, EnumSet<CreateOptions> createOptions, Configuration initialConfiguration) throws Exception\n{\r\n    return create(containerNameSuffix, createOptions, initialConfiguration, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "AzureBlobStorageTestAccount create(String containerNameSuffix, EnumSet<CreateOptions> createOptions, Configuration initialConfiguration, boolean useContainerSuffixAsContainerName) throws Exception\n{\r\n    saveMetricsConfigFile();\r\n    NativeAzureFileSystem fs = null;\r\n    CloudBlobContainer container = null;\r\n    Configuration conf = createTestConfiguration(initialConfiguration);\r\n    configurePageBlobDir(conf);\r\n    configureAtomicRenameDir(conf);\r\n    CloudStorageAccount account = createTestAccount(conf);\r\n    if (account == null) {\r\n        return null;\r\n    }\r\n    fs = new NativeAzureFileSystem();\r\n    String containerName = useContainerSuffixAsContainerName ? containerNameSuffix : String.format(\"wasbtests-%s-%s%s\", System.getProperty(\"user.name\"), UUID.randomUUID().toString(), containerNameSuffix);\r\n    container = account.createCloudBlobClient().getContainerReference(containerName);\r\n    if (createOptions.contains(CreateOptions.CreateContainer)) {\r\n        container.createIfNotExists();\r\n    }\r\n    String accountName = verifyWasbAccountNameInConfig(conf);\r\n    if (createOptions.contains(CreateOptions.UseSas)) {\r\n        String sas = generateSAS(container, createOptions.contains(CreateOptions.Readonly));\r\n        if (!createOptions.contains(CreateOptions.CreateContainer)) {\r\n            container.delete();\r\n        }\r\n        if (!conf.getBoolean(AzureNativeFileSystemStore.KEY_USE_SECURE_MODE, false)) {\r\n            conf.set(ACCOUNT_KEY_PROPERTY_NAME + accountName, \"\");\r\n        }\r\n        conf.set(SAS_PROPERTY_NAME + containerName + \".\" + accountName, sas);\r\n    }\r\n    if (createOptions.contains(CreateOptions.useThrottling)) {\r\n        conf.setBoolean(KEY_DISABLE_THROTTLING, false);\r\n    } else {\r\n        conf.setBoolean(KEY_DISABLE_THROTTLING, true);\r\n    }\r\n    configureSecureModeTestSettings(conf);\r\n    URI accountUri = createAccountUri(accountName, containerName);\r\n    fs.initialize(accountUri, conf);\r\n    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs, account, container, useContainerSuffixAsContainerName);\r\n    return testAcct;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "generateContainerName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateContainerName() throws Exception\n{\r\n    String containerName = String.format(\"wasbtests-%s-%tQ\", System.getProperty(\"user.name\"), new Date());\r\n    return containerName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "generateSAS",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String generateSAS(CloudBlobContainer container, boolean readonly) throws Exception\n{\r\n    container.createIfNotExists();\r\n    SharedAccessBlobPolicy sasPolicy = new SharedAccessBlobPolicy();\r\n    GregorianCalendar calendar = new GregorianCalendar(TimeZone.getTimeZone(\"UTC\"));\r\n    calendar.setTime(new Date());\r\n    sasPolicy.setSharedAccessStartTime(calendar.getTime());\r\n    calendar.add(Calendar.HOUR, 10);\r\n    sasPolicy.setSharedAccessExpiryTime(calendar.getTime());\r\n    if (readonly) {\r\n        sasPolicy.setPermissions(EnumSet.of(SharedAccessBlobPermissions.READ, SharedAccessBlobPermissions.LIST));\r\n    } else {\r\n        sasPolicy.setPermissions(EnumSet.of(SharedAccessBlobPermissions.READ, SharedAccessBlobPermissions.WRITE, SharedAccessBlobPermissions.LIST));\r\n    }\r\n    BlobContainerPermissions containerPermissions = new BlobContainerPermissions();\r\n    containerPermissions.setPublicAccess(BlobContainerPublicAccessType.OFF);\r\n    container.uploadPermissions(containerPermissions);\r\n    String sas = container.generateSharedAccessSignature(sasPolicy, null);\r\n    Thread.sleep(1500);\r\n    return sas;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "primePublicContainer",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void primePublicContainer(CloudBlobClient blobClient, String accountName, String containerName, String blobName, int fileSize) throws Exception\n{\r\n    CloudBlobContainer container = blobClient.getContainerReference(containerName);\r\n    container.createIfNotExists();\r\n    SharedAccessBlobPolicy sasPolicy = new SharedAccessBlobPolicy();\r\n    sasPolicy.setPermissions(EnumSet.of(SharedAccessBlobPermissions.READ, SharedAccessBlobPermissions.WRITE, SharedAccessBlobPermissions.LIST, SharedAccessBlobPermissions.DELETE));\r\n    BlobContainerPermissions containerPermissions = new BlobContainerPermissions();\r\n    containerPermissions.setPublicAccess(BlobContainerPublicAccessType.CONTAINER);\r\n    containerPermissions.getSharedAccessPolicies().put(\"testwasbpolicy\", sasPolicy);\r\n    container.uploadPermissions(containerPermissions);\r\n    CloudBlockBlob blob = container.getBlockBlobReference(blobName);\r\n    BlobOutputStream outputStream = blob.openOutputStream();\r\n    outputStream.write(new byte[fileSize]);\r\n    outputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createAnonymous",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "AzureBlobStorageTestAccount createAnonymous(final String blobName, final int fileSize) throws Exception\n{\r\n    NativeAzureFileSystem fs = null;\r\n    CloudBlobContainer container = null;\r\n    Configuration conf = createTestConfiguration(), noTestAccountConf = new Configuration();\r\n    CloudStorageAccount account = createTestAccount(conf);\r\n    if (account == null) {\r\n        return null;\r\n    }\r\n    CloudBlobClient blobClient = account.createCloudBlobClient();\r\n    String accountName = verifyWasbAccountNameInConfig(conf);\r\n    configureSecureModeTestSettings(conf);\r\n    String containerName = generateContainerName();\r\n    primePublicContainer(blobClient, accountName, containerName, blobName, fileSize);\r\n    container = blobClient.getContainerReference(containerName);\r\n    if (null == container || !container.exists()) {\r\n        final String errMsg = String.format(\"Container '%s' expected but not found while creating SAS account.\");\r\n        throw new Exception(errMsg);\r\n    }\r\n    URI accountUri = createAccountUri(accountName, containerName);\r\n    fs = new NativeAzureFileSystem();\r\n    fs.initialize(accountUri, noTestAccountConf);\r\n    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs, account, container);\r\n    return testAcct;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "primeRootContainer",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "CloudBlockBlob primeRootContainer(CloudBlobClient blobClient, String accountName, String blobName, int fileSize) throws Exception\n{\r\n    CloudBlobContainer container = blobClient.getContainerReference(\"https://\" + accountName + \"/\" + \"$root\");\r\n    container.createIfNotExists();\r\n    CloudBlockBlob blob = container.getBlockBlobReference(blobName);\r\n    BlobOutputStream outputStream = blob.openOutputStream();\r\n    outputStream.write(new byte[fileSize]);\r\n    outputStream.close();\r\n    return blob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createRoot",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AzureBlobStorageTestAccount createRoot(final String blobName, final int fileSize) throws Exception\n{\r\n    NativeAzureFileSystem fs = null;\r\n    CloudBlobContainer container = null;\r\n    Configuration conf = createTestConfiguration();\r\n    CloudStorageAccount account = createTestAccount(conf);\r\n    if (account == null) {\r\n        return null;\r\n    }\r\n    CloudBlobClient blobClient = account.createCloudBlobClient();\r\n    String accountName = verifyWasbAccountNameInConfig(conf);\r\n    configureSecureModeTestSettings(conf);\r\n    CloudBlockBlob blobRoot = primeRootContainer(blobClient, accountName, blobName, fileSize);\r\n    container = blobClient.getContainerReference(AZURE_ROOT_CONTAINER);\r\n    if (null == container || !container.exists()) {\r\n        final String errMsg = String.format(\"Container '%s' expected but not found while creating SAS account.\");\r\n        throw new Exception(errMsg);\r\n    }\r\n    URI accountUri = createAccountUri(accountName);\r\n    fs = new NativeAzureFileSystem();\r\n    fs.initialize(accountUri, conf);\r\n    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs, account, blobRoot);\r\n    return testAcct;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "closeFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeFileSystem() throws Exception\n{\r\n    if (fs != null) {\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    if (fs != null) {\r\n        fs.close();\r\n        fs = null;\r\n    }\r\n    if (!skipContainerDelete && container != null) {\r\n        container.deleteIfExists();\r\n        container = null;\r\n    }\r\n    if (blob != null) {\r\n        blob.delete();\r\n        blob = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws Exception\n{\r\n    cleanup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NativeAzureFileSystem getFileSystem()\n{\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureNativeFileSystemStore getStore()\n{\r\n    return this.storage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getRealContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CloudBlobContainer getRealContainer()\n{\r\n    return container;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getRealAccount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CloudStorageAccount getRealAccount()\n{\r\n    return account;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getMockStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MockStorageInterface getMockStorage()\n{\r\n    return mockStorage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setPageBlobDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPageBlobDirectory(String directory)\n{\r\n    this.pageBlobDirectory = directory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPageBlobDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPageBlobDirectory()\n{\r\n    return pageBlobDirectory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testAnonymouseCredentialExceptionMessage",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testAnonymouseCredentialExceptionMessage() throws Throwable\n{\r\n    Configuration conf = AzureBlobStorageTestAccount.createTestConfiguration();\r\n    CloudStorageAccount account = AzureBlobStorageTestAccount.createTestAccount(conf);\r\n    AzureTestUtils.assume(\"No test account\", account != null);\r\n    String testStorageAccount = verifyWasbAccountNameInConfig(conf);\r\n    conf = new Configuration();\r\n    conf.set(\"fs.AbstractFileSystem.wasb.impl\", \"org.apache.hadoop.fs.azure.Wasb\");\r\n    conf.set(\"fs.azure.skip.metrics\", \"true\");\r\n    String testContainer = UUID.randomUUID().toString();\r\n    String wasbUri = String.format(\"wasb://%s@%s\", testContainer, testStorageAccount);\r\n    try (NativeAzureFileSystem filesystem = new NativeAzureFileSystem()) {\r\n        filesystem.initialize(new URI(wasbUri), conf);\r\n        fail(\"Expected an exception, got \" + filesystem);\r\n    } catch (Exception ex) {\r\n        Throwable innerException = ex.getCause();\r\n        while (innerException != null && !(innerException instanceof AzureException)) {\r\n            innerException = innerException.getCause();\r\n        }\r\n        if (innerException != null) {\r\n            GenericTestUtils.assertExceptionContains(String.format(NO_ACCESS_TO_CONTAINER_MSG, testStorageAccount, testContainer), ex);\r\n        } else {\r\n            fail(\"No inner azure exception\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(AzureNativeFileSystemStore.KEY_INPUT_STREAM_VERSION, 1);\r\n    accountUsingInputStreamV1 = AzureBlobStorageTestAccount.create(\"testblockblobinputstream\", EnumSet.of(AzureBlobStorageTestAccount.CreateOptions.CreateContainer), conf, true);\r\n    accountUsingInputStreamV2 = AzureBlobStorageTestAccount.create(\"testblockblobinputstream\", EnumSet.noneOf(AzureBlobStorageTestAccount.CreateOptions.class), null, true);\r\n    assumeNotNull(accountUsingInputStreamV1);\r\n    assumeNotNull(accountUsingInputStreamV2);\r\n    hugefile = fs.makeQualified(TEST_FILE_PATH);\r\n    try {\r\n        testFileStatus = fs.getFileStatus(TEST_FILE_PATH);\r\n        testFileLength = testFileStatus.getLen();\r\n    } catch (FileNotFoundException e) {\r\n        testFileLength = 0;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(AzureNativeFileSystemStore.KEY_INPUT_STREAM_VERSION, 1);\r\n    accountUsingInputStreamV1 = AzureBlobStorageTestAccount.create(\"testblockblobinputstream\", EnumSet.of(AzureBlobStorageTestAccount.CreateOptions.CreateContainer), conf, true);\r\n    accountUsingInputStreamV2 = AzureBlobStorageTestAccount.create(\"testblockblobinputstream\", EnumSet.noneOf(AzureBlobStorageTestAccount.CreateOptions.class), null, true);\r\n    assumeNotNull(accountUsingInputStreamV1);\r\n    assumeNotNull(accountUsingInputStreamV2);\r\n    return accountUsingInputStreamV1;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestFileAndSetLength",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void createTestFileAndSetLength() throws IOException\n{\r\n    FileSystem fs = accountUsingInputStreamV1.getFileSystem();\r\n    if (fs.exists(TEST_FILE_PATH)) {\r\n        testFileStatus = fs.getFileStatus(TEST_FILE_PATH);\r\n        testFileLength = testFileStatus.getLen();\r\n        LOG.info(\"Reusing test file: {}\", testFileStatus);\r\n        return;\r\n    }\r\n    int sizeOfAlphabet = ('z' - 'a' + 1);\r\n    byte[] buffer = new byte[26 * KILOBYTE];\r\n    char character = 'a';\r\n    for (int i = 0; i < buffer.length; i++) {\r\n        buffer[i] = (byte) character;\r\n        character = (character == 'z') ? 'a' : (char) ((int) character + 1);\r\n    }\r\n    LOG.info(\"Creating test file {} of size: {}\", TEST_FILE_PATH, TEST_FILE_SIZE);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    try (FSDataOutputStream outputStream = fs.create(TEST_FILE_PATH)) {\r\n        int bytesWritten = 0;\r\n        while (bytesWritten < TEST_FILE_SIZE) {\r\n            outputStream.write(buffer);\r\n            bytesWritten += buffer.length;\r\n        }\r\n        LOG.info(\"Closing stream {}\", outputStream);\r\n        ContractTestUtils.NanoTimer closeTimer = new ContractTestUtils.NanoTimer();\r\n        outputStream.close();\r\n        closeTimer.end(\"time to close() output stream\");\r\n    }\r\n    timer.end(\"time to write %d KB\", TEST_FILE_SIZE / 1024);\r\n    testFileLength = fs.getFileStatus(TEST_FILE_PATH).getLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assumeHugeFileExists",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assumeHugeFileExists() throws IOException\n{\r\n    ContractTestUtils.assertPathExists(fs, \"huge file not created\", hugefile);\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    ContractTestUtils.assertIsFile(hugefile, status);\r\n    assertTrue(\"File \" + hugefile + \" is empty\", status.getLen() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toMbps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double toMbps(long bytes, long milliseconds)\n{\r\n    return bytes / 1000.0 * 8 / milliseconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0100_CreateHugeFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0100_CreateHugeFile() throws IOException\n{\r\n    createTestFileAndSetLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0200_BasicReadTest",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void test_0200_BasicReadTest() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStreamV1 = accountUsingInputStreamV1.getFileSystem().open(TEST_FILE_PATH);\r\n        FSDataInputStream inputStreamV2 = accountUsingInputStreamV2.getFileSystem().open(TEST_FILE_PATH)) {\r\n        byte[] bufferV1 = new byte[3 * MEGABYTE];\r\n        byte[] bufferV2 = new byte[bufferV1.length];\r\n        inputStreamV1.seek(5 * MEGABYTE);\r\n        int numBytesReadV1 = inputStreamV1.read(bufferV1, 0, KILOBYTE);\r\n        assertEquals(KILOBYTE, numBytesReadV1);\r\n        inputStreamV2.seek(5 * MEGABYTE);\r\n        int numBytesReadV2 = inputStreamV2.read(bufferV2, 0, KILOBYTE);\r\n        assertEquals(KILOBYTE, numBytesReadV2);\r\n        assertArrayEquals(bufferV1, bufferV2);\r\n        int len = MEGABYTE;\r\n        int offset = bufferV1.length - len;\r\n        inputStreamV1.seek(3 * MEGABYTE);\r\n        numBytesReadV1 = inputStreamV1.read(bufferV1, offset, len);\r\n        assertEquals(len, numBytesReadV1);\r\n        inputStreamV2.seek(3 * MEGABYTE);\r\n        numBytesReadV2 = inputStreamV2.read(bufferV2, offset, len);\r\n        assertEquals(len, numBytesReadV2);\r\n        assertArrayEquals(bufferV1, bufferV2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0201_RandomReadTest",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void test_0201_RandomReadTest() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStreamV1 = accountUsingInputStreamV1.getFileSystem().open(TEST_FILE_PATH);\r\n        FSDataInputStream inputStreamV2 = accountUsingInputStreamV2.getFileSystem().open(TEST_FILE_PATH)) {\r\n        final int bufferSize = 4 * KILOBYTE;\r\n        byte[] bufferV1 = new byte[bufferSize];\r\n        byte[] bufferV2 = new byte[bufferV1.length];\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        inputStreamV1.seek(0);\r\n        inputStreamV2.seek(0);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        int seekPosition = 2 * KILOBYTE;\r\n        inputStreamV1.seek(seekPosition);\r\n        inputStreamV2.seek(seekPosition);\r\n        inputStreamV1.seek(0);\r\n        inputStreamV2.seek(0);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        seekPosition = 5 * KILOBYTE;\r\n        inputStreamV1.seek(seekPosition);\r\n        inputStreamV2.seek(seekPosition);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        seekPosition = 10 * KILOBYTE;\r\n        inputStreamV1.seek(seekPosition);\r\n        inputStreamV2.seek(seekPosition);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        seekPosition = 4100 * KILOBYTE;\r\n        inputStreamV1.seek(seekPosition);\r\n        inputStreamV2.seek(seekPosition);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyConsistentReads",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyConsistentReads(FSDataInputStream inputStreamV1, FSDataInputStream inputStreamV2, byte[] bufferV1, byte[] bufferV2) throws IOException\n{\r\n    int size = bufferV1.length;\r\n    final int numBytesReadV1 = inputStreamV1.read(bufferV1, 0, size);\r\n    assertEquals(\"Bytes read from V1 stream\", size, numBytesReadV1);\r\n    final int numBytesReadV2 = inputStreamV2.read(bufferV2, 0, size);\r\n    assertEquals(\"Bytes read from V2 stream\", size, numBytesReadV2);\r\n    assertArrayEquals(\"Mismatch in read data\", bufferV1, bufferV2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_202_PosReadTest",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void test_202_PosReadTest() throws Exception\n{\r\n    assumeHugeFileExists();\r\n    FutureDataInputStreamBuilder builder = accountUsingInputStreamV2.getFileSystem().openFile(TEST_FILE_PATH);\r\n    builder.opt(AzureNativeFileSystemStore.FS_AZURE_BLOCK_BLOB_BUFFERED_PREAD_DISABLE, true);\r\n    try (FSDataInputStream inputStreamV1 = accountUsingInputStreamV1.getFileSystem().open(TEST_FILE_PATH);\r\n        FSDataInputStream inputStreamV2 = accountUsingInputStreamV2.getFileSystem().open(TEST_FILE_PATH);\r\n        FSDataInputStream inputStreamV2NoBuffer = builder.build().get()) {\r\n        final int bufferSize = 4 * KILOBYTE;\r\n        byte[] bufferV1 = new byte[bufferSize];\r\n        byte[] bufferV2 = new byte[bufferSize];\r\n        byte[] bufferV2NoBuffer = new byte[bufferSize];\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, inputStreamV2NoBuffer, 0, bufferV1, bufferV2, bufferV2NoBuffer);\r\n        int pos = 2 * KILOBYTE;\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, inputStreamV2NoBuffer, pos, bufferV1, bufferV2, bufferV2NoBuffer);\r\n        pos = 10 * KILOBYTE;\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, inputStreamV2NoBuffer, pos, bufferV1, bufferV2, bufferV2NoBuffer);\r\n        pos = 4100 * KILOBYTE;\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, inputStreamV2NoBuffer, pos, bufferV1, bufferV2, bufferV2NoBuffer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyConsistentReads",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void verifyConsistentReads(FSDataInputStream inputStreamV1, FSDataInputStream inputStreamV2, FSDataInputStream inputStreamV2NoBuffer, int pos, byte[] bufferV1, byte[] bufferV2, byte[] bufferV2NoBuffer) throws IOException\n{\r\n    int size = bufferV1.length;\r\n    int numBytesReadV1 = inputStreamV1.read(pos, bufferV1, 0, size);\r\n    assertEquals(\"Bytes read from V1 stream\", size, numBytesReadV1);\r\n    int numBytesReadV2 = inputStreamV2.read(pos, bufferV2, 0, size);\r\n    assertEquals(\"Bytes read from V2 stream\", size, numBytesReadV2);\r\n    int numBytesReadV2NoBuffer = inputStreamV2NoBuffer.read(pos, bufferV2NoBuffer, 0, size);\r\n    assertEquals(\"Bytes read from V2 stream (buffered pread disabled)\", size, numBytesReadV2NoBuffer);\r\n    assertArrayEquals(\"Mismatch in read data\", bufferV1, bufferV2);\r\n    assertArrayEquals(\"Mismatch in read data\", bufferV2, bufferV2NoBuffer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0301_MarkSupportedV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0301_MarkSupportedV1() throws IOException\n{\r\n    validateMarkSupported(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0302_MarkSupportedV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0302_MarkSupportedV2() throws IOException\n{\r\n    validateMarkSupported(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateMarkSupported",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateMarkSupported(FileSystem fs) throws IOException\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        assertTrue(\"mark is not supported\", inputStream.markSupported());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0303_MarkAndResetV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0303_MarkAndResetV1() throws Exception\n{\r\n    validateMarkAndReset(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0304_MarkAndResetV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0304_MarkAndResetV2() throws Exception\n{\r\n    validateMarkAndReset(accountUsingInputStreamV2.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateMarkAndReset",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void validateMarkAndReset(FileSystem fs) throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        inputStream.mark(KILOBYTE - 1);\r\n        byte[] buffer = new byte[KILOBYTE];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        inputStream.reset();\r\n        assertEquals(\"rest -> pos 0\", 0, inputStream.getPos());\r\n        inputStream.mark(8 * KILOBYTE - 1);\r\n        buffer = new byte[8 * KILOBYTE];\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        intercept(IOException.class, \"Resetting to invalid mark\", new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.reset();\r\n                return inputStream;\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0305_SeekToNewSourceV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0305_SeekToNewSourceV1() throws IOException\n{\r\n    validateSeekToNewSource(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0306_SeekToNewSourceV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0306_SeekToNewSourceV2() throws IOException\n{\r\n    validateSeekToNewSource(accountUsingInputStreamV2.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateSeekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateSeekToNewSource(FileSystem fs) throws IOException\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        assertFalse(inputStream.seekToNewSource(0));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0307_SkipBoundsV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0307_SkipBoundsV1() throws Exception\n{\r\n    validateSkipBounds(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0308_SkipBoundsV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0308_SkipBoundsV2() throws Exception\n{\r\n    validateSkipBounds(accountUsingInputStreamV2.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateSkipBounds",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void validateSkipBounds(FileSystem fs) throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        NanoTimer timer = new NanoTimer();\r\n        long skipped = inputStream.skip(-1);\r\n        assertEquals(0, skipped);\r\n        skipped = inputStream.skip(0);\r\n        assertEquals(0, skipped);\r\n        assertTrue(testFileLength > 0);\r\n        skipped = inputStream.skip(testFileLength);\r\n        assertEquals(testFileLength, skipped);\r\n        intercept(EOFException.class, new Callable<Long>() {\r\n\r\n            @Override\r\n            public Long call() throws Exception {\r\n                return inputStream.skip(1);\r\n            }\r\n        });\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        assertTrue(String.format(\"There should not be any network I/O (elapsedTimeMs=%1$d).\", elapsedTimeMs), elapsedTimeMs < 20);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0309_SeekBoundsV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0309_SeekBoundsV1() throws Exception\n{\r\n    validateSeekBounds(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0310_SeekBoundsV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0310_SeekBoundsV2() throws Exception\n{\r\n    validateSeekBounds(accountUsingInputStreamV2.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateSeekBounds",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void validateSeekBounds(FileSystem fs) throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        NanoTimer timer = new NanoTimer();\r\n        inputStream.seek(0);\r\n        assertEquals(0, inputStream.getPos());\r\n        intercept(EOFException.class, FSExceptionMessages.NEGATIVE_SEEK, new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.seek(-1);\r\n                return inputStream;\r\n            }\r\n        });\r\n        assertTrue(\"Test file length only \" + testFileLength, testFileLength > 0);\r\n        inputStream.seek(testFileLength);\r\n        assertEquals(testFileLength, inputStream.getPos());\r\n        intercept(EOFException.class, FSExceptionMessages.CANNOT_SEEK_PAST_EOF, new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.seek(testFileLength + 1);\r\n                return inputStream;\r\n            }\r\n        });\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        assertTrue(String.format(\"There should not be any network I/O (elapsedTimeMs=%1$d).\", elapsedTimeMs), elapsedTimeMs < 20);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0311_SeekAndAvailableAndPositionV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0311_SeekAndAvailableAndPositionV1() throws Exception\n{\r\n    validateSeekAndAvailableAndPosition(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0312_SeekAndAvailableAndPositionV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0312_SeekAndAvailableAndPositionV2() throws Exception\n{\r\n    validateSeekAndAvailableAndPosition(accountUsingInputStreamV2.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateSeekAndAvailableAndPosition",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void validateSeekAndAvailableAndPosition(FileSystem fs) throws Exception\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        byte[] expected1 = { (byte) 'a', (byte) 'b', (byte) 'c' };\r\n        byte[] expected2 = { (byte) 'd', (byte) 'e', (byte) 'f' };\r\n        byte[] expected3 = { (byte) 'b', (byte) 'c', (byte) 'd' };\r\n        byte[] expected4 = { (byte) 'g', (byte) 'h', (byte) 'i' };\r\n        byte[] buffer = new byte[3];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected1, buffer);\r\n        assertEquals(buffer.length, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected2, buffer);\r\n        assertEquals(2 * buffer.length, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        int seekPos = 0;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected1, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        seekPos = 1;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected3, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        seekPos = 6;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected4, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0313_SkipAndAvailableAndPositionV1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0313_SkipAndAvailableAndPositionV1() throws IOException\n{\r\n    validateSkipAndAvailableAndPosition(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0314_SkipAndAvailableAndPositionV2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_0314_SkipAndAvailableAndPositionV2() throws IOException\n{\r\n    validateSkipAndAvailableAndPosition(accountUsingInputStreamV1.getFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateSkipAndAvailableAndPosition",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void validateSkipAndAvailableAndPosition(FileSystem fs) throws IOException\n{\r\n    assumeHugeFileExists();\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        byte[] expected1 = { (byte) 'a', (byte) 'b', (byte) 'c' };\r\n        byte[] expected2 = { (byte) 'd', (byte) 'e', (byte) 'f' };\r\n        byte[] expected3 = { (byte) 'b', (byte) 'c', (byte) 'd' };\r\n        byte[] expected4 = { (byte) 'g', (byte) 'h', (byte) 'i' };\r\n        assertEquals(testFileLength, inputStream.available());\r\n        assertEquals(0, inputStream.getPos());\r\n        int n = 3;\r\n        long skipped = inputStream.skip(n);\r\n        assertEquals(skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        assertEquals(skipped, n);\r\n        byte[] buffer = new byte[3];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected2, buffer);\r\n        assertEquals(buffer.length + skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        int seekPos = 1;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected3, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        long currentPosition = inputStream.getPos();\r\n        n = 2;\r\n        skipped = inputStream.skip(n);\r\n        assertEquals(currentPosition + skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        assertEquals(skipped, n);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected4, buffer);\r\n        assertEquals(buffer.length + skipped + currentPosition, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0315_SequentialReadPerformance",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_0315_SequentialReadPerformance() throws IOException\n{\r\n    assumeHugeFileExists();\r\n    final int maxAttempts = 10;\r\n    final double maxAcceptableRatio = 1.01;\r\n    double v1ElapsedMs = 0, v2ElapsedMs = 0;\r\n    double ratio = Double.MAX_VALUE;\r\n    for (int i = 0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {\r\n        v1ElapsedMs = sequentialRead(1, accountUsingInputStreamV1.getFileSystem(), false);\r\n        v2ElapsedMs = sequentialRead(2, accountUsingInputStreamV2.getFileSystem(), false);\r\n        ratio = v2ElapsedMs / v1ElapsedMs;\r\n        LOG.info(String.format(\"v1ElapsedMs=%1$d, v2ElapsedMs=%2$d, ratio=%3$.2f\", (long) v1ElapsedMs, (long) v2ElapsedMs, ratio));\r\n    }\r\n    assertTrue(String.format(\"Performance of version 2 is not acceptable: v1ElapsedMs=%1$d,\" + \" v2ElapsedMs=%2$d, ratio=%3$.2f\", (long) v1ElapsedMs, (long) v2ElapsedMs, ratio), ratio < maxAcceptableRatio);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0316_SequentialReadAfterReverseSeekPerformanceV2",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_0316_SequentialReadAfterReverseSeekPerformanceV2() throws IOException\n{\r\n    assumeHugeFileExists();\r\n    final int maxAttempts = 10;\r\n    final double maxAcceptableRatio = 1.01;\r\n    double beforeSeekElapsedMs = 0, afterSeekElapsedMs = 0;\r\n    double ratio = Double.MAX_VALUE;\r\n    for (int i = 0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {\r\n        beforeSeekElapsedMs = sequentialRead(2, accountUsingInputStreamV2.getFileSystem(), false);\r\n        afterSeekElapsedMs = sequentialRead(2, accountUsingInputStreamV2.getFileSystem(), true);\r\n        ratio = afterSeekElapsedMs / beforeSeekElapsedMs;\r\n        LOG.info(String.format(\"beforeSeekElapsedMs=%1$d, afterSeekElapsedMs=%2$d, ratio=%3$.2f\", (long) beforeSeekElapsedMs, (long) afterSeekElapsedMs, ratio));\r\n    }\r\n    assertTrue(String.format(\"Performance of version 2 after reverse seek is not acceptable:\" + \" beforeSeekElapsedMs=%1$d, afterSeekElapsedMs=%2$d,\" + \" ratio=%3$.2f\", (long) beforeSeekElapsedMs, (long) afterSeekElapsedMs, ratio), ratio < maxAcceptableRatio);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "sequentialRead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "long sequentialRead(int version, FileSystem fs, boolean afterReverseSeek) throws IOException\n{\r\n    byte[] buffer = new byte[16 * KILOBYTE];\r\n    long totalBytesRead = 0;\r\n    long bytesRead = 0;\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        if (afterReverseSeek) {\r\n            while (bytesRead > 0 && totalBytesRead < 4 * MEGABYTE) {\r\n                bytesRead = inputStream.read(buffer);\r\n                totalBytesRead += bytesRead;\r\n            }\r\n            totalBytesRead = 0;\r\n            inputStream.seek(0);\r\n        }\r\n        NanoTimer timer = new NanoTimer();\r\n        while ((bytesRead = inputStream.read(buffer)) > 0) {\r\n            totalBytesRead += bytesRead;\r\n        }\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        LOG.info(String.format(\"v%1$d: bytesRead=%2$d, elapsedMs=%3$d, Mbps=%4$.2f,\" + \" afterReverseSeek=%5$s\", version, totalBytesRead, elapsedTimeMs, toMbps(totalBytesRead, elapsedTimeMs), afterReverseSeek));\r\n        assertEquals(testFileLength, totalBytesRead);\r\n        inputStream.close();\r\n        return elapsedTimeMs;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_0317_RandomReadPerformance",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_0317_RandomReadPerformance() throws IOException\n{\r\n    assumeHugeFileExists();\r\n    final int maxAttempts = 10;\r\n    final double maxAcceptableRatio = 0.10;\r\n    double v1ElapsedMs = 0, v2ElapsedMs = 0;\r\n    double ratio = Double.MAX_VALUE;\r\n    for (int i = 0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {\r\n        v1ElapsedMs = randomRead(1, accountUsingInputStreamV1.getFileSystem());\r\n        v2ElapsedMs = randomRead(2, accountUsingInputStreamV2.getFileSystem());\r\n        ratio = v2ElapsedMs / v1ElapsedMs;\r\n        LOG.info(String.format(\"v1ElapsedMs=%1$d, v2ElapsedMs=%2$d, ratio=%3$.2f\", (long) v1ElapsedMs, (long) v2ElapsedMs, ratio));\r\n    }\r\n    assertTrue(String.format(\"Performance of version 2 is not acceptable: v1ElapsedMs=%1$d,\" + \" v2ElapsedMs=%2$d, ratio=%3$.2f\", (long) v1ElapsedMs, (long) v2ElapsedMs, ratio), ratio < maxAcceptableRatio);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "randomRead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "long randomRead(int version, FileSystem fs) throws IOException\n{\r\n    assumeHugeFileExists();\r\n    final int minBytesToRead = 2 * MEGABYTE;\r\n    Random random = new Random();\r\n    byte[] buffer = new byte[8 * KILOBYTE];\r\n    long totalBytesRead = 0;\r\n    long bytesRead = 0;\r\n    try (FSDataInputStream inputStream = fs.open(TEST_FILE_PATH)) {\r\n        NanoTimer timer = new NanoTimer();\r\n        do {\r\n            bytesRead = inputStream.read(buffer);\r\n            totalBytesRead += bytesRead;\r\n            inputStream.seek(random.nextInt((int) (testFileLength - buffer.length)));\r\n        } while (bytesRead > 0 && totalBytesRead < minBytesToRead);\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        inputStream.close();\r\n        LOG.info(String.format(\"v%1$d: totalBytesRead=%2$d, elapsedTimeMs=%3$d, Mbps=%4$.2f\", version, totalBytesRead, elapsedTimeMs, toMbps(totalBytesRead, elapsedTimeMs)));\r\n        assertTrue(minBytesToRead <= totalBytesRead);\r\n        return elapsedTimeMs;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_999_DeleteHugeFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void test_999_DeleteHugeFiles() throws IOException\n{\r\n    try {\r\n        NanoTimer timer = new NanoTimer();\r\n        NativeAzureFileSystem fs = getFileSystem();\r\n        fs.delete(TEST_FILE_PATH, false);\r\n        timer.end(\"time to delete %s\", TEST_FILE_PATH);\r\n    } finally {\r\n        AzureTestUtils.cleanupTestAccount(accountUsingInputStreamV1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureFileIsRenamed",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testEnsureFileIsRenamed() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path src = path(\"testEnsureFileIsRenamed-src\");\r\n    touch(src);\r\n    Path dest = path(\"testEnsureFileIsRenamed-dest\");\r\n    fs.delete(dest, true);\r\n    assertRenameOutcome(fs, src, dest, true);\r\n    assertIsFile(fs, dest);\r\n    assertPathDoesNotExist(fs, \"expected renamed\", src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameWithPreExistingDestination",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRenameWithPreExistingDestination() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path src = path(\"renameSrc\");\r\n    touch(src);\r\n    Path dest = path(\"renameDest\");\r\n    touch(dest);\r\n    assertRenameOutcome(fs, src, dest, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameFileUnderDir",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRenameFileUnderDir() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path sourceDir = path(\"/testSrc\");\r\n    assertMkdirs(fs, sourceDir);\r\n    String filename = \"file1\";\r\n    Path file1 = new Path(sourceDir, filename);\r\n    touch(file1);\r\n    Path destDir = path(\"/testDst\");\r\n    assertRenameOutcome(fs, sourceDir, destDir, true);\r\n    FileStatus[] fileStatus = fs.listStatus(destDir);\r\n    assertNotNull(\"Null file status\", fileStatus);\r\n    FileStatus status = fileStatus[0];\r\n    assertEquals(\"Wrong filename in \" + status, filename, status.getPath().getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameDirectory",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRenameDirectory() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testDir = path(\"testDir\");\r\n    fs.mkdirs(testDir);\r\n    Path test1 = new Path(testDir + \"/test1\");\r\n    fs.mkdirs(test1);\r\n    fs.mkdirs(new Path(testDir + \"/test1/test2\"));\r\n    fs.mkdirs(new Path(testDir + \"/test1/test2/test3\"));\r\n    assertRenameOutcome(fs, test1, new Path(testDir + \"/test10\"), true);\r\n    assertPathDoesNotExist(fs, \"rename source dir\", test1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameFirstLevelDirectory",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRenameFirstLevelDirectory() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final List<Future<Void>> tasks = new ArrayList<>();\r\n    ExecutorService es = Executors.newFixedThreadPool(10);\r\n    Path source = path(\"/test\");\r\n    for (int i = 0; i < 1000; i++) {\r\n        final Path fileName = new Path(source + \"/\" + i);\r\n        Callable<Void> callable = new Callable<Void>() {\r\n\r\n            @Override\r\n            public Void call() throws Exception {\r\n                touch(fileName);\r\n                return null;\r\n            }\r\n        };\r\n        tasks.add(es.submit(callable));\r\n    }\r\n    for (Future<Void> task : tasks) {\r\n        task.get();\r\n    }\r\n    es.shutdownNow();\r\n    Path dest = path(\"/renamedDir\");\r\n    assertRenameOutcome(fs, source, dest, true);\r\n    FileStatus[] files = fs.listStatus(dest);\r\n    assertEquals(\"Wrong number of files in listing\", 1000, files.length);\r\n    assertPathDoesNotExist(fs, \"rename source dir\", source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameRoot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRenameRoot() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    assertRenameOutcome(fs, new Path(\"/\"), new Path(\"/testRenameRoot\"), false);\r\n    assertRenameOutcome(fs, new Path(fs.getUri().toString() + \"/\"), new Path(fs.getUri().toString() + \"/s\"), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testPosixRenameDirectory",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testPosixRenameDirectory() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Path testDir2 = path(\"testDir2\");\r\n    fs.mkdirs(new Path(testDir2 + \"/test1/test2/test3\"));\r\n    fs.mkdirs(new Path(testDir2 + \"/test4\"));\r\n    Assert.assertTrue(fs.rename(new Path(testDir2 + \"/test1/test2/test3\"), new Path(testDir2 + \"/test4\")));\r\n    assertPathExists(fs, \"This path should exist\", testDir2);\r\n    assertPathExists(fs, \"This path should exist\", new Path(testDir2 + \"/test1/test2\"));\r\n    assertPathExists(fs, \"This path should exist\", new Path(testDir2 + \"/test4\"));\r\n    assertPathExists(fs, \"This path should exist\", new Path(testDir2 + \"/test4/test3\"));\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", new Path(testDir2 + \"/test1/test2/test3\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "sleep",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void sleep(long milliseconds)\n{\r\n    try {\r\n        Thread.sleep(milliseconds);\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "fuzzyValidate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void fuzzyValidate(long expected, long actual, double percentage)\n{\r\n    final double lowerBound = Math.max(expected - percentage / 100 * expected, 0);\r\n    final double upperBound = expected + percentage / 100 * expected;\r\n    assertTrue(String.format(\"The actual value %1$d is not within the expected range: \" + \"[%2$.2f, %3$.2f].\", actual, lowerBound, upperBound), actual >= lowerBound && actual <= upperBound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validate(long expected, long actual)\n{\r\n    assertEquals(String.format(\"The actual value %1$d is not the expected value %2$d.\", actual, expected), expected, actual);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateLessThanOrEqual",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateLessThanOrEqual(long maxExpected, long actual)\n{\r\n    assertTrue(String.format(\"The actual value %1$d is not less than or equal to the maximum\" + \" expected value %2$d.\", actual, maxExpected), actual < maxExpected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testNoMetricUpdatesThenNoWaiting",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNoMetricUpdatesThenNoWaiting()\n{\r\n    ClientThrottlingAnalyzer analyzer = new ClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    validate(0, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testOnlySuccessThenNoWaiting",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testOnlySuccessThenNoWaiting()\n{\r\n    ClientThrottlingAnalyzer analyzer = new ClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    analyzer.addBytesTransferred(8 * MEGABYTE, false);\r\n    validate(0, analyzer.getSleepDuration());\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    validate(0, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testOnlyErrorsAndWaiting",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testOnlyErrorsAndWaiting()\n{\r\n    ClientThrottlingAnalyzer analyzer = new ClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    analyzer.addBytesTransferred(4 * MEGABYTE, true);\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    final int expectedSleepDuration1 = 1100;\r\n    validateLessThanOrEqual(expectedSleepDuration1, analyzer.getSleepDuration());\r\n    sleep(10 * ANALYSIS_PERIOD);\r\n    final int expectedSleepDuration2 = 900;\r\n    validateLessThanOrEqual(expectedSleepDuration2, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSuccessAndErrorsAndWaiting",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSuccessAndErrorsAndWaiting()\n{\r\n    ClientThrottlingAnalyzer analyzer = new ClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    analyzer.addBytesTransferred(8 * MEGABYTE, false);\r\n    analyzer.addBytesTransferred(2 * MEGABYTE, true);\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    NanoTimer timer = new NanoTimer();\r\n    analyzer.suspendIfNecessary();\r\n    final int expectedElapsedTime = 126;\r\n    fuzzyValidate(expectedElapsedTime, timer.elapsedTimeMs(), MAX_ACCEPTABLE_PERCENT_DIFFERENCE);\r\n    sleep(10 * ANALYSIS_PERIOD);\r\n    final int expectedSleepDuration = 110;\r\n    validateLessThanOrEqual(expectedSleepDuration, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testManySuccessAndErrorsAndWaiting",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testManySuccessAndErrorsAndWaiting()\n{\r\n    ClientThrottlingAnalyzer analyzer = new ClientThrottlingAnalyzer(\"test\", ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n    final int numberOfRequests = 20;\r\n    for (int i = 0; i < numberOfRequests; i++) {\r\n        analyzer.addBytesTransferred(8 * MEGABYTE, false);\r\n        analyzer.addBytesTransferred(2 * MEGABYTE, true);\r\n    }\r\n    sleep(ANALYSIS_PERIOD_PLUS_10_PERCENT);\r\n    NanoTimer timer = new NanoTimer();\r\n    analyzer.suspendIfNecessary();\r\n    fuzzyValidate(7, timer.elapsedTimeMs(), MAX_ACCEPTABLE_PERCENT_DIFFERENCE);\r\n    sleep(10 * ANALYSIS_PERIOD);\r\n    validate(0, analyzer.getSleepDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    AzureBlobStorageTestAccount account = createTestAccount();\r\n    assumeNotNull(\"test account\", account);\r\n    bindToTestAccount(account);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    describe(\"closing test account and filesystem\");\r\n    testAccount = cleanupTestAccount(testAccount);\r\n    IOUtils.closeStream(fs);\r\n    fs = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AzureBlobStorageTestAccount.createTestConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestAccount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureBlobStorageTestAccount getTestAccount()\n{\r\n    return testAccount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NativeAzureFileSystem getFileSystem()\n{\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return getFileSystem().getConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "bindToTestAccount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void bindToTestAccount(AzureBlobStorageTestAccount account)\n{\r\n    cleanupTestAccount(testAccount);\r\n    IOUtils.closeStream(fs);\r\n    testAccount = account;\r\n    if (testAccount != null) {\r\n        fs = testAccount.getFileSystem();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "blobPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path blobPath(String filepath) throws IOException\n{\r\n    return blobPathForTests(getFileSystem(), filepath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "path",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path path(String filepath) throws IOException\n{\r\n    return pathForTests(getFileSystem(), filepath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "methodPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path methodPath() throws IOException\n{\r\n    return path(methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "methodBlobPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path methodBlobPath() throws IOException\n{\r\n    return blobPath(methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "describe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void describe(String text, Object... args)\n{\r\n    LOG.info(\"\\n\\n{}: {}\\n\", methodName.getMethodName(), String.format(text, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "createTestFileSystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "NativeAzureFileSystem createTestFileSystem(Configuration conf) throws IOException\n{\r\n    String fsname = conf.getTrimmed(TEST_FS_WASB_NAME, \"\");\r\n    boolean liveTest = !StringUtils.isEmpty(fsname);\r\n    URI testURI = null;\r\n    if (liveTest) {\r\n        testURI = URI.create(fsname);\r\n        liveTest = testURI.getScheme().equals(WASB_SCHEME);\r\n    }\r\n    if (!liveTest) {\r\n        throw new AssumptionViolatedException(\"No test filesystem in \" + TEST_FS_WASB_NAME);\r\n    }\r\n    NativeAzureFileSystem fs1 = new NativeAzureFileSystem();\r\n    fs1.initialize(testURI, conf);\r\n    return fs1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "createTestFileContext",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileContext createTestFileContext(Configuration conf) throws IOException\n{\r\n    String fsname = conf.getTrimmed(TEST_FS_WASB_NAME, \"\");\r\n    boolean liveTest = !StringUtils.isEmpty(fsname);\r\n    URI testURI = null;\r\n    if (liveTest) {\r\n        testURI = URI.create(fsname);\r\n        liveTest = testURI.getScheme().equals(WASB_SCHEME);\r\n    }\r\n    if (!liveTest) {\r\n        throw new AssumptionViolatedException(\"No test filesystem in \" + TEST_FS_WASB_NAME);\r\n    }\r\n    FileContext fc = FileContext.getFileContext(testURI, conf);\r\n    return fc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getTestPropertyLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTestPropertyLong(Configuration conf, String key, long defVal)\n{\r\n    return Long.valueOf(getTestProperty(conf, key, Long.toString(defVal)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getTestPropertyBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTestPropertyBytes(Configuration conf, String key, String defVal)\n{\r\n    return org.apache.hadoop.util.StringUtils.TraditionalBinaryPrefix.string2long(getTestProperty(conf, key, defVal));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getTestPropertyInt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTestPropertyInt(Configuration conf, String key, int defVal)\n{\r\n    return (int) getTestPropertyLong(conf, key, defVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getTestPropertyBool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTestPropertyBool(Configuration conf, String key, boolean defVal)\n{\r\n    return Boolean.valueOf(getTestProperty(conf, key, Boolean.toString(defVal)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getTestProperty",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getTestProperty(Configuration conf, String key, String defVal)\n{\r\n    String confVal = conf != null ? conf.getTrimmed(key, defVal) : defVal;\r\n    String propval = System.getProperty(key);\r\n    return StringUtils.isNotEmpty(propval) && !UNSET_PROPERTY.equals(propval) ? propval : confVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "verifyExceptionClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Exception verifyExceptionClass(Class clazz, Exception ex) throws Exception\n{\r\n    if (!(ex.getClass().equals(clazz))) {\r\n        throw ex;\r\n    }\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "disableFilesystemCaching",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableFilesystemCaching(Configuration conf)\n{\r\n    conf.setBoolean(\"fs.wasb.impl.disable.cache\", true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "createTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path createTestPath(Path defVal)\n{\r\n    String testUniqueForkId = System.getProperty(AzureTestConstants.TEST_UNIQUE_FORK_ID);\r\n    return testUniqueForkId == null ? defVal : new Path(\"/\" + testUniqueForkId, \"test\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "blobPathForTests",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path blobPathForTests(FileSystem fs, String filename)\n{\r\n    String testUniqueForkId = System.getProperty(AzureTestConstants.TEST_UNIQUE_FORK_ID);\r\n    return fs.makeQualified(new Path(PAGE_BLOB_DIR, testUniqueForkId == null ? filename : (testUniqueForkId + \"/\" + filename)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "pathForTests",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path pathForTests(FileSystem fs, String filename)\n{\r\n    String testUniqueForkId = System.getProperty(AzureTestConstants.TEST_UNIQUE_FORK_ID);\r\n    return fs.makeQualified(new Path(testUniqueForkId == null ? (\"/test/\" + filename) : (testUniqueForkId + \"/\" + filename)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getForkID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getForkID()\n{\r\n    return System.getProperty(AzureTestConstants.TEST_UNIQUE_FORK_ID, \"fork-1\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "isParallelExecution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isParallelExecution()\n{\r\n    return Boolean.getBoolean(KEY_PARALLEL_TEST_EXECUTION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "assertInstanceOf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertInstanceOf(Class<?> expectedClass, Object obj)\n{\r\n    Assert.assertTrue(String.format(\"Expected instance of class %s, but is %s.\", expectedClass, obj.getClass()), expectedClass.isAssignableFrom(obj.getClass()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "buildClassListString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String buildClassListString(List<T> classes)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    for (int i = 0; i < classes.size(); ++i) {\r\n        if (i > 0) {\r\n            sb.append(',');\r\n        }\r\n        sb.append(classes.get(i).getName());\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "assertOptionEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertOptionEquals(Configuration conf, String key, String expected)\n{\r\n    assertEquals(\"Value of \" + key, expected, conf.get(key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "assume",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assume(String message, boolean condition)\n{\r\n    if (!condition) {\r\n        LOG.warn(message);\r\n    }\r\n    Assume.assumeTrue(message, condition);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getLongGaugeValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLongGaugeValue(NativeAzureFileSystem fs, String gaugeName)\n{\r\n    return getLongGauge(gaugeName, getMetrics(fs.getInstrumentation()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getLongCounterValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLongCounterValue(NativeAzureFileSystem fs, String counterName)\n{\r\n    return getLongCounter(counterName, getMetrics(fs.getInstrumentation()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "deleteQuietly",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteQuietly(FileSystem fs, Path path, boolean recursive) throws IOException\n{\r\n    if (fs != null && path != null) {\r\n        try {\r\n            fs.delete(path, recursive);\r\n        } catch (IOException e) {\r\n            LOG.warn(\"When deleting {}\", path, e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount cleanup(AzureBlobStorageTestAccount testAccount) throws Exception\n{\r\n    if (testAccount != null) {\r\n        testAccount.cleanup();\r\n        testAccount = null;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "cleanupTestAccount",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureBlobStorageTestAccount cleanupTestAccount(AzureBlobStorageTestAccount testAccount)\n{\r\n    if (testAccount != null) {\r\n        try {\r\n            testAccount.cleanup();\r\n        } catch (Exception e) {\r\n            LOG.error(\"While cleaning up test account: \", e);\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "assumeScaleTestsEnabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assumeScaleTestsEnabled(Configuration conf)\n{\r\n    boolean enabled = getTestPropertyBool(conf, KEY_SCALE_TESTS_ENABLED, DEFAULT_SCALE_TESTS_ENABLED);\r\n    assume(\"Scale test disabled: to enable set property \" + KEY_SCALE_TESTS_ENABLED, enabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "verifyWasbAccountNameInConfig",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String verifyWasbAccountNameInConfig(Configuration conf)\n{\r\n    String accountName = conf.get(ACCOUNT_NAME_PROPERTY_NAME);\r\n    if (accountName == null) {\r\n        accountName = conf.get(WASB_TEST_ACCOUNT_NAME_WITH_DOMAIN);\r\n    }\r\n    assumeTrue(\"Account for WASB is missing or it is not in correct format\", accountName != null && !accountName.endsWith(WASB_ACCOUNT_NAME_DOMAIN_SUFFIX_REGEX));\r\n    return accountName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "writeStringToFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeStringToFile(FileSystem fs, Path path, String value) throws IOException\n{\r\n    FSDataOutputStream outputStream = fs.create(path, true);\r\n    writeStringToStream(outputStream, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "writeStringToStream",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeStringToStream(FSDataOutputStream outputStream, String value) throws IOException\n{\r\n    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(outputStream));\r\n    writer.write(value);\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "readStringFromFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String readStringFromFile(FileSystem fs, Path testFile) throws IOException\n{\r\n    FSDataInputStream inputStream = fs.open(testFile);\r\n    String ret = readStringFromStream(inputStream);\r\n    inputStream.close();\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "readStringFromStream",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String readStringFromStream(FSDataInputStream inputStream) throws IOException\n{\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream));\r\n    final int BUFFER_SIZE = 1024;\r\n    char[] buffer = new char[BUFFER_SIZE];\r\n    int count = reader.read(buffer, 0, BUFFER_SIZE);\r\n    if (count > BUFFER_SIZE) {\r\n        throw new IOException(\"Exceeded buffer size\");\r\n    }\r\n    inputStream.close();\r\n    return new String(buffer, 0, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "assumeNamespaceDisabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assumeNamespaceDisabled(Configuration conf)\n{\r\n    Assume.assumeFalse(\"Hierarchical namespace is enabled for test account.\", conf.getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "setupCluster",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupCluster() throws Exception\n{\r\n    resetUGI();\r\n    cluster = new KerberizedAbfsCluster();\r\n    cluster.init(new Configuration());\r\n    cluster.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardownCluster() throws Exception\n{\r\n    resetUGI();\r\n    ServiceOperations.stopQuietly(LOG, cluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Configuration conf = getRawConfiguration();\r\n    cluster.bindConfToCluster(conf);\r\n    conf.setBoolean(HADOOP_SECURITY_TOKEN_SERVICE_USE_IP, false);\r\n    resetUGI();\r\n    UserGroupInformation.setConfiguration(conf);\r\n    aliceUser = cluster.createAliceUser();\r\n    assertSecurityEnabled();\r\n    UserGroupInformation.setLoginUser(aliceUser);\r\n    StubDelegationTokenManager.useStubDTManager(conf);\r\n    FileSystem.closeAllForUGI(UserGroupInformation.getLoginUser());\r\n    super.setup();\r\n    assertNotNull(\"No StubDelegationTokenManager created in filesystem init\", getStubDTManager());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getStubDTManager",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StubDelegationTokenManager getStubDTManager() throws IOException\n{\r\n    return (StubDelegationTokenManager) getDelegationTokenManager().getTokenManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    FileSystem.closeAllForUGI(UserGroupInformation.getLoginUser());\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "assertSecurityEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertSecurityEnabled()\n{\r\n    assertTrue(\"Security is needed for this test\", UserGroupInformation.isSecurityEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "resetUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetUGI()\n{\r\n    UserGroupInformation.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "mkTokens",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Credentials mkTokens(final FileSystem fs) throws IOException\n{\r\n    Credentials cred = new Credentials();\r\n    fs.addDelegationTokens(\"rm/rm1@EXAMPLE.COM\", cred);\r\n    return cred;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testTokenManagerBinding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testTokenManagerBinding() throws Throwable\n{\r\n    StubDelegationTokenManager instance = getStubDTManager();\r\n    assertNotNull(\"No StubDelegationTokenManager created in filesystem init\", instance);\r\n    assertTrue(\"token manager not initialized: \" + instance, instance.isInitialized());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testCanonicalization",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCanonicalization() throws Throwable\n{\r\n    String service = getCanonicalServiceName();\r\n    assertNotNull(\"No canonical service name from filesystem \" + getFileSystem(), service);\r\n    assertEquals(\"canonical URI and service name mismatch\", getFilesystemURI(), new URI(service));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getFilesystemURI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI getFilesystemURI() throws IOException\n{\r\n    return getFileSystem().getUri();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCanonicalServiceName() throws IOException\n{\r\n    return getFileSystem().getCanonicalServiceName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testDefaultCanonicalization",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultCanonicalization() throws Throwable\n{\r\n    FileSystem fs = getFileSystem();\r\n    clearTokenServiceName();\r\n    assertEquals(\"canonicalServiceName is not the default\", getDefaultServiceName(fs), getCanonicalServiceName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getDefaultServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDefaultServiceName(final FileSystem fs)\n{\r\n    return SecurityUtil.buildDTServiceName(fs.getUri(), 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "clearTokenServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearTokenServiceName() throws IOException\n{\r\n    getStubDTManager().setCanonicalServiceName(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testRequestToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRequestToken() throws Throwable\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Credentials credentials = mkTokens(fs);\r\n    assertEquals(\"Number of collected tokens\", 1, credentials.numberOfTokens());\r\n    verifyCredentialsContainsToken(credentials, fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testRequestTokenDefault",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRequestTokenDefault() throws Throwable\n{\r\n    clearTokenServiceName();\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    assertEquals(\"canonicalServiceName is not the default\", getDefaultServiceName(fs), fs.getCanonicalServiceName());\r\n    Credentials credentials = mkTokens(fs);\r\n    assertEquals(\"Number of collected tokens\", 1, credentials.numberOfTokens());\r\n    verifyCredentialsContainsToken(credentials, getDefaultServiceName(fs), getFilesystemURI().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "verifyCredentialsContainsToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyCredentialsContainsToken(final Credentials credentials, FileSystem fs) throws IOException\n{\r\n    verifyCredentialsContainsToken(credentials, fs.getCanonicalServiceName(), fs.getUri().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "verifyCredentialsContainsToken",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "StubAbfsTokenIdentifier verifyCredentialsContainsToken(final Credentials credentials, final String serviceName, final String tokenService) throws IOException\n{\r\n    Token<? extends TokenIdentifier> token = credentials.getToken(new Text(serviceName));\r\n    assertEquals(\"Token Kind in \" + token, StubAbfsTokenIdentifier.TOKEN_KIND, token.getKind());\r\n    assertEquals(\"Token Service Kind in \" + token, tokenService, token.getService().toString());\r\n    StubAbfsTokenIdentifier abfsId = (StubAbfsTokenIdentifier) token.decodeIdentifier();\r\n    LOG.info(\"Created token {}\", abfsId);\r\n    assertEquals(\"token URI in \" + abfsId, tokenService, abfsId.getUri().toString());\r\n    return abfsId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testJobsCollectTokens",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testJobsCollectTokens() throws Throwable\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Credentials credentials = new Credentials();\r\n    Path root = fs.makeQualified(new Path(\"/\"));\r\n    Path[] paths = { root };\r\n    Configuration conf = fs.getConf();\r\n    TokenCache.obtainTokensForNamenodes(credentials, paths, conf);\r\n    verifyCredentialsContainsToken(credentials, fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "dtutil",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String dtutil(final int expected, final Configuration conf, final String... args) throws Exception\n{\r\n    final ByteArrayOutputStream dtUtilContent = new ByteArrayOutputStream();\r\n    DtUtilShell dt = new DtUtilShell();\r\n    dt.setOut(new PrintStream(dtUtilContent));\r\n    dtUtilContent.reset();\r\n    int r = doAs(aliceUser, () -> ToolRunner.run(conf, dt, args));\r\n    String s = dtUtilContent.toString();\r\n    LOG.info(\"\\n{}\", s);\r\n    assertEquals(\"Exit code from command dtutil \" + StringUtils.join(\" \", args) + \" with output \" + s, expected, r);\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testDTUtilShell",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDTUtilShell() throws Throwable\n{\r\n    File tokenfile = cluster.createTempTokenFile();\r\n    String tfs = tokenfile.toString();\r\n    String fsURI = getFileSystem().getUri().toString();\r\n    dtutil(0, getRawConfiguration(), \"get\", fsURI, \"-format\", \"protobuf\", tfs);\r\n    assertTrue(\"not created: \" + tokenfile, tokenfile.exists());\r\n    assertTrue(\"File is empty \" + tokenfile, tokenfile.length() > 0);\r\n    assertTrue(\"File only contains header \" + tokenfile, tokenfile.length() > 6);\r\n    String printed = dtutil(0, getRawConfiguration(), \"print\", tfs);\r\n    assertTrue(\"no \" + fsURI + \" in \" + printed, printed.contains(fsURI));\r\n    assertTrue(\"no \" + StubAbfsTokenIdentifier.ID + \" in \" + printed, printed.contains(StubAbfsTokenIdentifier.ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testBaseDTLifecycle",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testBaseDTLifecycle() throws Throwable\n{\r\n    Configuration conf = new Configuration(getRawConfiguration());\r\n    ClassicDelegationTokenManager.useClassicDTManager(conf);\r\n    try (FileSystem fs = FileSystem.newInstance(getFilesystemURI(), conf)) {\r\n        Credentials credentials = mkTokens(fs);\r\n        assertEquals(\"Number of collected tokens\", 1, credentials.numberOfTokens());\r\n        verifyCredentialsContainsToken(credentials, fs.getCanonicalServiceName(), ClassicDelegationTokenManager.UNSET);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "getDelegationSAS",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "String getDelegationSAS(String accountName, String containerName, String path, String operation, String saoid, String suoid, String scid)\n{\r\n    final String sv = AuthenticationVersion.Feb20.toString();\r\n    final String st = ISO_8601_FORMATTER.format(Instant.now().minus(FIVE_MINUTES));\r\n    final String se = ISO_8601_FORMATTER.format(Instant.now().plus(ONE_DAY));\r\n    String sr = \"b\";\r\n    String sdd = null;\r\n    String sp;\r\n    switch(operation) {\r\n        case SASTokenProvider.CREATE_FILE_OPERATION:\r\n        case SASTokenProvider.CREATE_DIRECTORY_OPERATION:\r\n        case SASTokenProvider.WRITE_OPERATION:\r\n        case SASTokenProvider.SET_PROPERTIES_OPERATION:\r\n            sp = \"w\";\r\n            break;\r\n        case SASTokenProvider.DELETE_OPERATION:\r\n            sp = \"d\";\r\n            break;\r\n        case SASTokenProvider.DELETE_RECURSIVE_OPERATION:\r\n            sp = \"d\";\r\n            sr = \"d\";\r\n            sdd = Integer.toString(StringUtils.countMatches(path, \"/\"));\r\n            break;\r\n        case SASTokenProvider.CHECK_ACCESS_OPERATION:\r\n        case SASTokenProvider.GET_ACL_OPERATION:\r\n        case SASTokenProvider.GET_STATUS_OPERATION:\r\n            sp = \"e\";\r\n            break;\r\n        case SASTokenProvider.LIST_OPERATION:\r\n            sp = \"l\";\r\n            break;\r\n        case SASTokenProvider.GET_PROPERTIES_OPERATION:\r\n        case SASTokenProvider.READ_OPERATION:\r\n            sp = \"r\";\r\n            break;\r\n        case SASTokenProvider.RENAME_DESTINATION_OPERATION:\r\n        case SASTokenProvider.RENAME_SOURCE_OPERATION:\r\n            sp = \"m\";\r\n            break;\r\n        case SASTokenProvider.SET_ACL_OPERATION:\r\n        case SASTokenProvider.SET_PERMISSION_OPERATION:\r\n            sp = \"p\";\r\n            break;\r\n        case SASTokenProvider.SET_OWNER_OPERATION:\r\n            sp = \"o\";\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(operation);\r\n    }\r\n    String signature = computeSignatureForSAS(sp, st, se, sv, sr, accountName, containerName, path, saoid, suoid, scid);\r\n    AbfsUriQueryBuilder qb = new AbfsUriQueryBuilder();\r\n    qb.addQuery(\"skoid\", skoid);\r\n    qb.addQuery(\"sktid\", sktid);\r\n    qb.addQuery(\"skt\", skt);\r\n    qb.addQuery(\"ske\", ske);\r\n    qb.addQuery(\"sks\", sks);\r\n    qb.addQuery(\"skv\", skv);\r\n    if (saoid != null) {\r\n        qb.addQuery(\"saoid\", saoid);\r\n    }\r\n    if (suoid != null) {\r\n        qb.addQuery(\"suoid\", suoid);\r\n    }\r\n    if (scid != null) {\r\n        qb.addQuery(\"scid\", scid);\r\n    }\r\n    qb.addQuery(\"sp\", sp);\r\n    qb.addQuery(\"st\", st);\r\n    qb.addQuery(\"se\", se);\r\n    qb.addQuery(\"sv\", sv);\r\n    qb.addQuery(\"sr\", sr);\r\n    if (sdd != null) {\r\n        qb.addQuery(\"sdd\", sdd);\r\n    }\r\n    qb.addQuery(\"sig\", signature);\r\n    return qb.toString().substring(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "computeSignatureForSAS",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "String computeSignatureForSAS(String sp, String st, String se, String sv, String sr, String accountName, String containerName, String path, String saoid, String suoid, String scid)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(sp);\r\n    sb.append(\"\\n\");\r\n    sb.append(st);\r\n    sb.append(\"\\n\");\r\n    sb.append(se);\r\n    sb.append(\"\\n\");\r\n    sb.append(\"/blob/\");\r\n    sb.append(accountName);\r\n    sb.append(\"/\");\r\n    sb.append(containerName);\r\n    if (path != null && !sr.equals(\"c\")) {\r\n        sb.append(path);\r\n    }\r\n    sb.append(\"\\n\");\r\n    sb.append(skoid);\r\n    sb.append(\"\\n\");\r\n    sb.append(sktid);\r\n    sb.append(\"\\n\");\r\n    sb.append(skt);\r\n    sb.append(\"\\n\");\r\n    sb.append(ske);\r\n    sb.append(\"\\n\");\r\n    sb.append(sks);\r\n    sb.append(\"\\n\");\r\n    sb.append(skv);\r\n    sb.append(\"\\n\");\r\n    if (saoid != null) {\r\n        sb.append(saoid);\r\n    }\r\n    sb.append(\"\\n\");\r\n    if (suoid != null) {\r\n        sb.append(suoid);\r\n    }\r\n    sb.append(\"\\n\");\r\n    if (scid != null) {\r\n        sb.append(scid);\r\n    }\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(sv);\r\n    sb.append(\"\\n\");\r\n    sb.append(sr);\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    sb.append(\"\\n\");\r\n    String stringToSign = sb.toString();\r\n    LOG.debug(\"Delegation SAS stringToSign: \" + stringToSign.replace(\"\\n\", \".\"));\r\n    return computeHmac256(stringToSign);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "callTracingHeaderValidator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void callTracingHeaderValidator(String tracingContextHeader, TracingHeaderFormat format)\n{\r\n    this.format = format;\r\n    validateTracingHeader(tracingContextHeader);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "getClone",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TracingHeaderValidator getClone()\n{\r\n    TracingHeaderValidator tracingHeaderValidator = new TracingHeaderValidator(clientCorrelationId, fileSystemId, operation, needsPrimaryRequestId, retryNum, streamID);\r\n    tracingHeaderValidator.primaryRequestId = primaryRequestId;\r\n    return tracingHeaderValidator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "validateTracingHeader",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateTracingHeader(String tracingContextHeader)\n{\r\n    String[] idList = tracingContextHeader.split(\":\");\r\n    validateBasicFormat(idList);\r\n    if (format != TracingHeaderFormat.ALL_ID_FORMAT) {\r\n        return;\r\n    }\r\n    if (!primaryRequestId.isEmpty() && !idList[3].isEmpty()) {\r\n        Assertions.assertThat(idList[3]).describedAs(\"PrimaryReqID should be common for these requests\").isEqualTo(primaryRequestId);\r\n    }\r\n    if (!streamID.isEmpty()) {\r\n        Assertions.assertThat(idList[4]).describedAs(\"Stream id should be common for these requests\").isEqualTo(streamID);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "validateBasicFormat",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void validateBasicFormat(String[] idList)\n{\r\n    if (format == TracingHeaderFormat.ALL_ID_FORMAT) {\r\n        Assertions.assertThat(idList).describedAs(\"header should have 7 elements\").hasSize(7);\r\n    } else if (format == TracingHeaderFormat.TWO_ID_FORMAT) {\r\n        Assertions.assertThat(idList).describedAs(\"header should have 2 elements\").hasSize(2);\r\n    } else {\r\n        Assertions.assertThat(idList).describedAs(\"header should have 1 element\").hasSize(1);\r\n        Assertions.assertThat(idList[0]).describedAs(\"Client request ID is a guid\").matches(GUID_PATTERN);\r\n        return;\r\n    }\r\n    if (clientCorrelationId.matches(\"[a-zA-Z0-9-]*\")) {\r\n        Assertions.assertThat(idList[0]).describedAs(\"Correlation ID should match config\").isEqualTo(clientCorrelationId);\r\n    } else {\r\n        Assertions.assertThat(idList[0]).describedAs(\"Invalid config should be replaced with empty string\").isEmpty();\r\n    }\r\n    Assertions.assertThat(idList[1]).describedAs(\"Client request ID is a guid\").matches(GUID_PATTERN);\r\n    if (format != TracingHeaderFormat.ALL_ID_FORMAT) {\r\n        return;\r\n    }\r\n    Assertions.assertThat(idList[2]).describedAs(\"Filesystem ID incorrect\").isEqualTo(fileSystemId);\r\n    if (needsPrimaryRequestId && !operation.equals(FSOperationType.READ)) {\r\n        Assertions.assertThat(idList[3]).describedAs(\"should have primaryReqId\").isNotEmpty();\r\n    }\r\n    Assertions.assertThat(idList[5]).describedAs(\"Operation name incorrect\").isEqualTo(operation.toString());\r\n    int retryCount = Integer.parseInt(idList[6]);\r\n    Assertions.assertThat(retryCount).describedAs(\"Retry was required due to issue on server side\").isEqualTo(retryNum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setOperation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOperation(FSOperationType operation)\n{\r\n    this.operation = operation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "updatePrimaryRequestID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updatePrimaryRequestID(String primaryRequestId)\n{\r\n    this.primaryRequestId = primaryRequestId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<String> getKeys()\n{\r\n    return new ArrayList<String>(blobs.keySet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "listBlobs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Iterable<ListBlobEntry> listBlobs(String prefix, boolean includeMetadata)\n{\r\n    ArrayList<ListBlobEntry> list = new ArrayList<ListBlobEntry>();\r\n    for (Map.Entry<String, Entry> entry : blobs.entrySet()) {\r\n        if (entry.getKey().startsWith(prefix)) {\r\n            list.add(new ListBlobEntry(entry.getKey(), includeMetadata ? new HashMap<String, String>(entry.getValue().metadata) : null, entry.getValue().content.length, entry.getValue().isPageBlob));\r\n        }\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getContent(String key)\n{\r\n    return blobs.get(key).content;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setContent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setContent(String key, byte[] value, HashMap<String, String> metadata, boolean isPageBlob, long length)\n{\r\n    blobs.put(key, new Entry(value, (HashMap<String, String>) metadata.clone(), isPageBlob, length));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setMetadata",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMetadata(String key, HashMap<String, String> metadata)\n{\r\n    blobs.get(key).metadata = (HashMap<String, String>) metadata.clone();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "uploadBlockBlob",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "OutputStream uploadBlockBlob(final String key, final HashMap<String, String> metadata)\n{\r\n    setContent(key, new byte[0], metadata, false, 0);\r\n    return new ByteArrayOutputStream() {\r\n\r\n        @Override\r\n        public void flush() throws IOException {\r\n            super.flush();\r\n            byte[] tempBytes = toByteArray();\r\n            setContent(key, tempBytes, metadata, false, tempBytes.length);\r\n        }\r\n\r\n        @Override\r\n        public void close() throws IOException {\r\n            super.close();\r\n            byte[] tempBytes = toByteArray();\r\n            setContent(key, tempBytes, metadata, false, tempBytes.length);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "uploadPageBlob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "OutputStream uploadPageBlob(final String key, final HashMap<String, String> metadata, final long length)\n{\r\n    setContent(key, new byte[0], metadata, true, length);\r\n    return new ByteArrayOutputStream() {\r\n\r\n        @Override\r\n        public void flush() throws IOException {\r\n            super.flush();\r\n            setContent(key, toByteArray(), metadata, true, length);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "copy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void copy(String sourceKey, String destKey)\n{\r\n    blobs.put(destKey, blobs.get(sourceKey));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void delete(String key)\n{\r\n    blobs.remove(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "exists",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean exists(String key)\n{\r\n    return blobs.containsKey(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getMetadata",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HashMap<String, String> getMetadata(String key)\n{\r\n    Entry entry = requireNonNull(blobs.get(key), \"entry for \" + key);\r\n    return (HashMap<String, String>) requireNonNull(entry.metadata, \"metadata for \" + key).clone();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerMetadata",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HashMap<String, String> getContainerMetadata()\n{\r\n    return containerMetadata;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setContainerMetadata",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setContainerMetadata(HashMap<String, String> metadata)\n{\r\n    containerMetadata = metadata;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBackingStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InMemoryBlockBlobStore getBackingStore()\n{\r\n    return backingStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addPreExistingContainer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addPreExistingContainer(String uri, HashMap<String, String> metadata)\n{\r\n    preExistingContainers.add(new PreExistingContainer(uri, metadata));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setRetryPolicyFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRetryPolicyFactory(final RetryPolicyFactory retryPolicyFactory)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setTimeoutInMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimeoutInMs(int timeoutInMs)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void createBlobClient(CloudStorageAccount account)\n{\r\n    backingStore = new InMemoryBlockBlobStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void createBlobClient(URI baseUri)\n{\r\n    backingStore = new InMemoryBlockBlobStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createBlobClient(URI baseUri, StorageCredentials credentials)\n{\r\n    this.baseUriString = baseUri.toString();\r\n    backingStore = new InMemoryBlockBlobStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageCredentials getCredentials()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "convertUriToDecodedString",
  "errType" : [ "DecoderException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String convertUriToDecodedString(URI uri)\n{\r\n    try {\r\n        return codec.decode(uri.toString());\r\n    } catch (DecoderException e) {\r\n        throw new AssertionError(\"Failed to decode URI: \" + uri.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "convertKeyToEncodedUri",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "URI convertKeyToEncodedUri(String key)\n{\r\n    try {\r\n        Path p = new Path(key);\r\n        URI unEncodedURI = p.toUri();\r\n        return new URIBuilder().setPath(unEncodedURI.getPath()).setScheme(unEncodedURI.getScheme()).build();\r\n    } catch (URISyntaxException e) {\r\n        int i = e.getIndex();\r\n        String details;\r\n        if (i >= 0) {\r\n            details = \" -- \\\"\" + e.getInput().charAt(i) + \"\\\"\";\r\n        } else {\r\n            details = \"\";\r\n        }\r\n        throw new AssertionError(\"Failed to encode key: \" + key + \":  \" + e + details);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerReference",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "CloudBlobContainerWrapper getContainerReference(String name) throws URISyntaxException, StorageException\n{\r\n    String fullUri;\r\n    URIBuilder builder = new URIBuilder(baseUriString);\r\n    String path = builder.getPath() == null ? \"\" : builder.getPath() + \"/\";\r\n    fullUri = builder.setPath(path + name).toString();\r\n    MockCloudBlobContainerWrapper container = new MockCloudBlobContainerWrapper(fullUri, name);\r\n    for (PreExistingContainer existing : preExistingContainers) {\r\n        if (fullUri.equalsIgnoreCase(existing.containerUri)) {\r\n            container.created = true;\r\n            backingStore.setContainerMetadata(existing.containerMetadata);\r\n            break;\r\n        }\r\n    }\r\n    return container;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "nameThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameThread()\n{\r\n    Thread.currentThread().setName(\"JUnit-\" + methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    nameThread();\r\n    testAccount = AzureBlobStorageTestAccount.createForEmulator();\r\n    if (testAccount != null) {\r\n        fs = testAccount.getFileSystem();\r\n    }\r\n    assumeNotNull(fs);\r\n    basePath = fs.makeQualified(AzureTestUtils.createTestPath(new Path(\"ITestNativeAzureFileSystemContractEmulator\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    super.tearDown();\r\n    testAccount = AzureTestUtils.cleanup(testAccount);\r\n    fs = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testRmNonEmptyRootDirNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRmNonEmptyRootDirNonRecursive() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsFileSystemContract createContract(Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testUnbufferOnClosedFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUnbufferOnClosedFile() throws IOException\n{\r\n    describe(\"unbuffer a file before a read\");\r\n    FSDataInputStream stream = null;\r\n    try {\r\n        stream = getFileSystem().open(getFile());\r\n        validateFullFileContents(stream);\r\n    } finally {\r\n        if (stream != null) {\r\n            stream.close();\r\n        }\r\n    }\r\n    if (stream != null) {\r\n        stream.unbuffer();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testRenameFileBeingAppended",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRenameFileBeingAppended() throws Throwable\n{\r\n    skip(\"Skipping as renaming an opened file is not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    binding.setup();\r\n    fs = binding.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    try {\r\n        super.tearDown();\r\n    } finally {\r\n        binding.teardown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testListOnFolderWithNoChildren",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testListOnFolderWithNoChildren() throws IOException\n{\r\n    assertTrue(fs.mkdirs(path(\"testListStatus/c/1\")));\r\n    FileStatus[] paths;\r\n    paths = fs.listStatus(path(\"testListStatus\"));\r\n    assertEquals(1, paths.length);\r\n    paths = fs.listStatus(path(\"testListStatus/c\"));\r\n    assertEquals(1, paths.length);\r\n    fs.delete(path(\"testListStatus/c/1\"), true);\r\n    paths = fs.listStatus(path(\"testListStatus/c\"));\r\n    assertEquals(0, paths.length);\r\n    assertTrue(fs.delete(path(\"testListStatus\"), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testListOnfileAndFolder",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testListOnfileAndFolder() throws IOException\n{\r\n    Path folderPath = path(\"testListStatus/folder\");\r\n    Path filePath = path(\"testListStatus/file\");\r\n    assertTrue(fs.mkdirs(folderPath));\r\n    ContractTestUtils.touch(fs, filePath);\r\n    FileStatus[] listFolderStatus;\r\n    listFolderStatus = fs.listStatus(path(\"testListStatus\"));\r\n    assertEquals(filePath, listFolderStatus[0].getPath());\r\n    FileStatus[] listFileStatus = fs.listStatus(filePath);\r\n    assertEquals(filePath, listFileStatus[0].getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testMkdirsWithUmask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMkdirsWithUmask() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOnlyOneServerCallIsMadeWhenTheConfIsTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOnlyOneServerCallIsMadeWhenTheConfIsTrue() throws Exception\n{\r\n    testNumBackendCalls(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMultipleServerCallsAreMadeWhenTheConfIsFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMultipleServerCallsAreMadeWhenTheConfIsFalse() throws Exception\n{\r\n    testNumBackendCalls(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testNumBackendCalls",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testNumBackendCalls(boolean optimizeFooterRead) throws Exception\n{\r\n    for (int i = 1; i <= 4; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead, fileSize);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        int length = AbfsInputStream.FOOTER_SIZE;\r\n        try (FSDataInputStream iStream = fs.open(testFilePath)) {\r\n            byte[] buffer = new byte[length];\r\n            Map<String, Long> metricMap = getInstrumentationMap(fs);\r\n            long requestsMadeBeforeTest = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n            iStream.seek(fileSize - 8);\r\n            iStream.read(buffer, 0, length);\r\n            iStream.seek(fileSize - (TEN * ONE_KB));\r\n            iStream.read(buffer, 0, length);\r\n            iStream.seek(fileSize - (TWENTY * ONE_KB));\r\n            iStream.read(buffer, 0, length);\r\n            metricMap = getInstrumentationMap(fs);\r\n            long requestsMadeAfterTest = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n            if (optimizeFooterRead) {\r\n                assertEquals(1, requestsMadeAfterTest - requestsMadeBeforeTest);\r\n            } else {\r\n                assertEquals(3, requestsMadeAfterTest - requestsMadeBeforeTest);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeginAndReadWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeginAndReadWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(true, SeekTo.BEGIN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeginAndReadWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeginAndReadWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(false, SeekTo.BEGIN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeforeFooterAndReadWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeforeFooterAndReadWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(true, SeekTo.BEFORE_FOOTER_START);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToBeforeFooterAndReadWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToBeforeFooterAndReadWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(false, SeekTo.BEFORE_FOOTER_START);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToFooterAndReadWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToFooterAndReadWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(true, SeekTo.AT_FOOTER_START);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToFooterAndReadWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToFooterAndReadWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(false, SeekTo.AT_FOOTER_START);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToAfterFooterAndReadWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToAfterFooterAndReadWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(true, SeekTo.AFTER_FOOTER_START);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToToAfterFooterAndReadWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToToAfterFooterAndReadWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(false, SeekTo.AFTER_FOOTER_START);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToEndAndReadWithConfTrue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToEndAndReadWithConfTrue() throws Exception\n{\r\n    testSeekAndReadWithConf(true, SeekTo.END);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekToEndAndReadWithConfFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSeekToEndAndReadWithConfFalse() throws Exception\n{\r\n    testSeekAndReadWithConf(false, SeekTo.END);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSeekAndReadWithConf",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSeekAndReadWithConf(boolean optimizeFooterRead, SeekTo seekTo) throws Exception\n{\r\n    for (int i = 2; i <= 6; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(optimizeFooterRead, fileSize);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        seekReadAndTest(fs, testFilePath, seekPos(seekTo, fileSize), HUNDRED, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int seekPos(SeekTo seekTo, int fileSize)\n{\r\n    if (seekTo == SeekTo.BEGIN) {\r\n        return 0;\r\n    }\r\n    if (seekTo == SeekTo.BEFORE_FOOTER_START) {\r\n        return fileSize - AbfsInputStream.FOOTER_SIZE - 1;\r\n    }\r\n    if (seekTo == SeekTo.AT_FOOTER_START) {\r\n        return fileSize - AbfsInputStream.FOOTER_SIZE;\r\n    }\r\n    if (seekTo == SeekTo.END) {\r\n        return fileSize - 1;\r\n    }\r\n    return fileSize - AbfsInputStream.FOOTER_SIZE + 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekReadAndTest",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void seekReadAndTest(final FileSystem fs, final Path testFilePath, final int seekPos, final int length, final byte[] fileContent) throws IOException, NoSuchFieldException, IllegalAccessException\n{\r\n    AbfsConfiguration conf = getAbfsStore(fs).getAbfsConfiguration();\r\n    long actualContentLength = fileContent.length;\r\n    try (FSDataInputStream iStream = fs.open(testFilePath)) {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        long bufferSize = abfsInputStream.getBufferSize();\r\n        seek(iStream, seekPos);\r\n        byte[] buffer = new byte[length];\r\n        long bytesRead = iStream.read(buffer, 0, length);\r\n        long footerStart = max(0, actualContentLength - AbfsInputStream.FOOTER_SIZE);\r\n        boolean optimizationOn = conf.optimizeFooterRead() && seekPos >= footerStart;\r\n        long actualLength = length;\r\n        if (seekPos + length > actualContentLength) {\r\n            long delta = seekPos + length - actualContentLength;\r\n            actualLength = length - delta;\r\n        }\r\n        long expectedLimit;\r\n        long expectedBCurson;\r\n        long expectedFCursor;\r\n        if (optimizationOn) {\r\n            if (actualContentLength <= bufferSize) {\r\n                expectedLimit = actualContentLength;\r\n                expectedBCurson = seekPos + actualLength;\r\n            } else {\r\n                expectedLimit = bufferSize;\r\n                long lastBlockStart = max(0, actualContentLength - bufferSize);\r\n                expectedBCurson = seekPos - lastBlockStart + actualLength;\r\n            }\r\n            expectedFCursor = actualContentLength;\r\n        } else {\r\n            if (seekPos + bufferSize < actualContentLength) {\r\n                expectedLimit = bufferSize;\r\n                expectedFCursor = bufferSize;\r\n            } else {\r\n                expectedLimit = actualContentLength - seekPos;\r\n                expectedFCursor = min(seekPos + bufferSize, actualContentLength);\r\n            }\r\n            expectedBCurson = actualLength;\r\n        }\r\n        assertEquals(expectedFCursor, abfsInputStream.getFCursor());\r\n        assertEquals(expectedFCursor, abfsInputStream.getFCursorAfterLastRead());\r\n        assertEquals(expectedLimit, abfsInputStream.getLimit());\r\n        assertEquals(expectedBCurson, abfsInputStream.getBCursor());\r\n        assertEquals(actualLength, bytesRead);\r\n        assertContentReadCorrectly(fileContent, seekPos, (int) actualLength, buffer, testFilePath);\r\n        int from = seekPos;\r\n        if (optimizationOn) {\r\n            from = (int) max(0, actualContentLength - bufferSize);\r\n        }\r\n        assertContentReadCorrectly(fileContent, from, (int) abfsInputStream.getLimit(), abfsInputStream.getBuffer(), testFilePath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPartialReadWithNoData",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPartialReadWithNoData() throws Exception\n{\r\n    for (int i = 2; i <= 6; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(true, fileSize);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        testPartialReadWithNoData(fs, testFilePath, fileSize - AbfsInputStream.FOOTER_SIZE, AbfsInputStream.FOOTER_SIZE, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPartialReadWithNoData",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testPartialReadWithNoData(final FileSystem fs, final Path testFilePath, final int seekPos, final int length, final byte[] fileContent) throws IOException, NoSuchFieldException, IllegalAccessException\n{\r\n    FSDataInputStream iStream = fs.open(testFilePath);\r\n    try {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        abfsInputStream = spy(abfsInputStream);\r\n        doReturn(10).doReturn(10).doCallRealMethod().when(abfsInputStream).readRemote(anyLong(), any(), anyInt(), anyInt(), any(TracingContext.class));\r\n        iStream = new FSDataInputStream(abfsInputStream);\r\n        seek(iStream, seekPos);\r\n        byte[] buffer = new byte[length];\r\n        int bytesRead = iStream.read(buffer, 0, length);\r\n        assertEquals(length, bytesRead);\r\n        assertContentReadCorrectly(fileContent, seekPos, length, buffer, testFilePath);\r\n        assertEquals(fileContent.length, abfsInputStream.getFCursor());\r\n        assertEquals(length, abfsInputStream.getBCursor());\r\n        assertTrue(abfsInputStream.getLimit() >= length);\r\n    } finally {\r\n        iStream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPartialReadWithSomeDat",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPartialReadWithSomeDat() throws Exception\n{\r\n    for (int i = 3; i <= 6; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(true, fileSize);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        testPartialReadWithSomeDat(fs, testFilePath, fileSize - AbfsInputStream.FOOTER_SIZE, AbfsInputStream.FOOTER_SIZE, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testPartialReadWithSomeDat",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testPartialReadWithSomeDat(final FileSystem fs, final Path testFilePath, final int seekPos, final int length, final byte[] fileContent) throws IOException, NoSuchFieldException, IllegalAccessException\n{\r\n    FSDataInputStream iStream = fs.open(testFilePath);\r\n    try {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        abfsInputStream = spy(abfsInputStream);\r\n        int someDataLength = 2;\r\n        int secondReturnSize = min(fileContent.length, abfsInputStream.getBufferSize()) - 10 - someDataLength;\r\n        doReturn(10).doReturn(secondReturnSize).doCallRealMethod().when(abfsInputStream).readRemote(anyLong(), any(), anyInt(), anyInt(), any(TracingContext.class));\r\n        iStream = new FSDataInputStream(abfsInputStream);\r\n        seek(iStream, seekPos);\r\n        byte[] buffer = new byte[length];\r\n        int bytesRead = iStream.read(buffer, 0, length);\r\n        assertEquals(length, bytesRead);\r\n        assertEquals(fileContent.length, abfsInputStream.getFCursor());\r\n        assertEquals(someDataLength, abfsInputStream.getBCursor());\r\n        assertEquals(someDataLength, abfsInputStream.getLimit());\r\n    } finally {\r\n        iStream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AzureBlobFileSystem getFileSystem(boolean optimizeFooterRead, int fileSize) throws IOException\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    getAbfsStore(fs).getAbfsConfiguration().setOptimizeFooterRead(optimizeFooterRead);\r\n    if (fileSize <= getAbfsStore(fs).getAbfsConfiguration().getReadBufferSize()) {\r\n        getAbfsStore(fs).getAbfsConfiguration().setReadSmallFilesCompletely(false);\r\n    }\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    Configuration conf = fs.getConf();\r\n    conf.setInt(NativeAzureFileSystem.AZURE_RENAME_THREADS, renameThreads);\r\n    conf.setInt(NativeAzureFileSystem.AZURE_DELETE_THREADS, deleteThreads);\r\n    conf.setBoolean(AzureNativeFileSystemStore.KEY_ENABLE_FLAT_LISTING, true);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    logs = LogCapturer.captureLogs(new Log4JLogger(org.apache.log4j.Logger.getRootLogger()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createFolder",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createFolder(FileSystem fs, String root) throws Exception\n{\r\n    fs.mkdirs(new Path(root));\r\n    for (int i = 0; i < this.iterations; i++) {\r\n        fs.mkdirs(new Path(root + \"/\" + i));\r\n        fs.createNewFile(new Path(root + \"/\" + i + \"/fileToRename\"));\r\n        fs.createNewFile(new Path(root + \"/\" + i + \"/file/to/rename\"));\r\n        fs.createNewFile(new Path(root + \"/\" + i + \"/file+to%rename\"));\r\n        fs.createNewFile(new Path(root + \"/fileToRename\" + i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateRenameFolder",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void validateRenameFolder(FileSystem fs, String source, String dest) throws Exception\n{\r\n    createFolder(fs, source);\r\n    Path sourceFolder = new Path(source);\r\n    Path destFolder = new Path(dest);\r\n    assertTrue(fs.rename(sourceFolder, destFolder));\r\n    assertTrue(fs.exists(destFolder));\r\n    for (int i = 0; i < this.iterations; i++) {\r\n        assertTrue(fs.exists(new Path(dest + \"/\" + i)));\r\n        assertTrue(fs.exists(new Path(dest + \"/\" + i + \"/fileToRename\")));\r\n        assertTrue(fs.exists(new Path(dest + \"/\" + i + \"/file/to/rename\")));\r\n        assertTrue(fs.exists(new Path(dest + \"/\" + i + \"/file+to%rename\")));\r\n        assertTrue(fs.exists(new Path(dest + \"/fileToRename\" + i)));\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i)));\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i + \"/fileToRename\")));\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i + \"/file/to/rename\")));\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i + \"/file+to%rename\")));\r\n        assertFalse(fs.exists(new Path(source + \"/fileToRename\" + i)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameSmallFolderWithThreads",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRenameSmallFolderWithThreads() throws Exception\n{\r\n    validateRenameFolder(fs, \"root\", \"rootnew\");\r\n    int expectedThreadsCreated = Math.min(7, renameThreads);\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"ms with threads: \" + expectedThreadsCreated);\r\n    for (int i = 0; i < expectedThreadsCreated; i++) {\r\n        assertInLog(content, \"AzureBlobRenameThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n    }\r\n    if (expectedThreadsCreated < renameThreads) {\r\n        for (int i = expectedThreadsCreated; i < renameThreads; i++) {\r\n            assertNotInLog(content, \"AzureBlobRenameThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameLargeFolderWithThreads",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRenameLargeFolderWithThreads() throws Exception\n{\r\n    this.iterations = 10;\r\n    validateRenameFolder(fs, \"root\", \"rootnew\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"ms with threads: \" + renameThreads);\r\n    for (int i = 0; i < renameThreads; i++) {\r\n        assertInLog(content, \"AzureBlobRenameThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameLargeFolderDisableThreads",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRenameLargeFolderDisableThreads() throws Exception\n{\r\n    Configuration conf = fs.getConf();\r\n    conf.setInt(NativeAzureFileSystem.AZURE_RENAME_THREADS, 0);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    this.iterations = 10;\r\n    validateRenameFolder(fs, \"root\", \"rootnew\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Disabling threads for Rename operation as thread count 0\");\r\n    for (int i = 0; i < renameThreads; i++) {\r\n        String term = \"AzureBlobRenameThread-\" + Thread.currentThread().getName() + \"-\" + i;\r\n        assertNotInLog(content, term);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertInLog",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertInLog(String content, String term)\n{\r\n    assertTrue(\"Empty log\", !content.isEmpty());\r\n    if (!content.contains(term)) {\r\n        String message = \"No \" + term + \" found in logs\";\r\n        LOG.error(message);\r\n        System.err.println(content);\r\n        fail(message);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertNotInLog",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertNotInLog(String content, String term)\n{\r\n    assertTrue(\"Empty log\", !content.isEmpty());\r\n    if (content.contains(term)) {\r\n        String message = term + \" found in logs\";\r\n        LOG.error(message);\r\n        System.err.println(content);\r\n        fail(message);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameSmallFolderDisableThreadsDisableFlatListing",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRenameSmallFolderDisableThreadsDisableFlatListing() throws Exception\n{\r\n    Configuration conf = fs.getConf();\r\n    conf = fs.getConf();\r\n    conf.setInt(NativeAzureFileSystem.AZURE_RENAME_THREADS, 1);\r\n    conf.setBoolean(AzureNativeFileSystemStore.KEY_ENABLE_FLAT_LISTING, false);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    validateRenameFolder(fs, \"root\", \"rootnew\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Disabling threads for Rename operation as thread count 1\");\r\n    for (int i = 0; i < renameThreads; i++) {\r\n        assertNotInLog(content, \"AzureBlobRenameThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateDeleteFolder",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void validateDeleteFolder(FileSystem fs, String source) throws Exception\n{\r\n    createFolder(fs, \"root\");\r\n    Path sourceFolder = new Path(source);\r\n    assertTrue(fs.delete(sourceFolder, true));\r\n    assertFalse(fs.exists(sourceFolder));\r\n    for (int i = 0; i < this.iterations; i++) {\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i)));\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i + \"/fileToRename\")));\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i + \"/file/to/rename\")));\r\n        assertFalse(fs.exists(new Path(source + \"/\" + i + \"/file+to%rename\")));\r\n        assertFalse(fs.exists(new Path(source + \"/fileToRename\" + i)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteSmallFolderWithThreads",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDeleteSmallFolderWithThreads() throws Exception\n{\r\n    validateDeleteFolder(fs, \"root\");\r\n    int expectedThreadsCreated = Math.min(7, deleteThreads);\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"ms with threads: \" + expectedThreadsCreated);\r\n    for (int i = 0; i < expectedThreadsCreated; i++) {\r\n        assertInLog(content, \"AzureBlobDeleteThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n    }\r\n    if (expectedThreadsCreated < deleteThreads) {\r\n        for (int i = expectedThreadsCreated; i < deleteThreads; i++) {\r\n            assertNotInLog(content, \"AzureBlobDeleteThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteLargeFolderWithThreads",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDeleteLargeFolderWithThreads() throws Exception\n{\r\n    this.iterations = 10;\r\n    validateDeleteFolder(fs, \"root\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"ms with threads: \" + deleteThreads);\r\n    for (int i = 0; i < deleteThreads; i++) {\r\n        assertInLog(content, \"AzureBlobDeleteThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteLargeFolderDisableThreads",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDeleteLargeFolderDisableThreads() throws Exception\n{\r\n    Configuration conf = fs.getConf();\r\n    conf.setInt(NativeAzureFileSystem.AZURE_DELETE_THREADS, 0);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    this.iterations = 10;\r\n    validateDeleteFolder(fs, \"root\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Disabling threads for Delete operation as thread count 0\");\r\n    for (int i = 0; i < deleteThreads; i++) {\r\n        assertNotInLog(content, \"AzureBlobDeleteThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteSmallFolderDisableThreadsDisableFlatListing",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDeleteSmallFolderDisableThreadsDisableFlatListing() throws Exception\n{\r\n    Configuration conf = fs.getConf();\r\n    conf.setInt(NativeAzureFileSystem.AZURE_DELETE_THREADS, 1);\r\n    conf.setBoolean(AzureNativeFileSystemStore.KEY_ENABLE_FLAT_LISTING, false);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    validateDeleteFolder(fs, \"root\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Disabling threads for Delete operation as thread count 1\");\r\n    for (int i = 0; i < deleteThreads; i++) {\r\n        assertNotInLog(content, \"AzureBlobDeleteThread-\" + Thread.currentThread().getName() + \"-\" + i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteThreadPoolExceptionFailure",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDeleteThreadPoolExceptionFailure() throws Exception\n{\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(mockFs.getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS));\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenThrow(new Exception());\r\n    Mockito.when(mockFs.getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS)).thenReturn(mockThreadPoolExecutor);\r\n    validateDeleteFolder(mockFs, \"root\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Failed to create thread pool with threads\");\r\n    assertInLog(content, \"Serializing the Delete operation\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteThreadPoolExecuteFailure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDeleteThreadPoolExecuteFailure() throws Exception\n{\r\n    ThreadPoolExecutor mockThreadExecutor = Mockito.mock(ThreadPoolExecutor.class);\r\n    Mockito.doThrow(new RejectedExecutionException()).when(mockThreadExecutor).execute(Mockito.any(Runnable.class));\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(mockFs.getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS));\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenReturn(mockThreadExecutor);\r\n    Mockito.when(mockFs.getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS)).thenReturn(mockThreadPoolExecutor);\r\n    validateDeleteFolder(mockFs, \"root\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Rejected execution of thread for Delete operation on blob\");\r\n    assertInLog(content, \"Serializing the Delete operation\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteThreadPoolExecuteSingleThreadFailure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDeleteThreadPoolExecuteSingleThreadFailure() throws Exception\n{\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(mockFs.getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS));\r\n    Mockito.when(mockFs.getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS)).thenReturn(mockThreadPoolExecutor);\r\n    ThreadPoolExecutor mockThreadExecutor = Mockito.spy(mockThreadPoolExecutor.getThreadPool(7));\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenReturn(mockThreadExecutor);\r\n    Mockito.doCallRealMethod().doThrow(new RejectedExecutionException()).when(mockThreadExecutor).execute(Mockito.any(Runnable.class));\r\n    validateDeleteFolder(mockFs, \"root\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Using thread pool for Delete operation with threads 7\");\r\n    assertInLog(content, \"6 threads not used for Delete operation on blob\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteThreadPoolTerminationFailure",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testDeleteThreadPoolTerminationFailure() throws Exception\n{\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(((NativeAzureFileSystem) fs).getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS));\r\n    ThreadPoolExecutor mockThreadExecutor = Mockito.mock(ThreadPoolExecutor.class);\r\n    Mockito.doNothing().when(mockThreadExecutor).execute(Mockito.any(Runnable.class));\r\n    Mockito.when(mockThreadExecutor.awaitTermination(Long.MAX_VALUE, TimeUnit.DAYS)).thenThrow(new InterruptedException());\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenReturn(mockThreadExecutor);\r\n    Mockito.when(mockFs.getThreadPoolExecutor(deleteThreads, \"AzureBlobDeleteThread\", \"Delete\", path, NativeAzureFileSystem.AZURE_DELETE_THREADS)).thenReturn(mockThreadPoolExecutor);\r\n    createFolder(mockFs, \"root\");\r\n    Path sourceFolder = new Path(\"root\");\r\n    boolean exception = false;\r\n    try {\r\n        mockFs.delete(sourceFolder, true);\r\n    } catch (IOException e) {\r\n        exception = true;\r\n    }\r\n    assertTrue(exception);\r\n    assertTrue(mockFs.exists(sourceFolder));\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Using thread pool for Delete operation with threads\");\r\n    assertInLog(content, \"Threads got interrupted Delete blob operation\");\r\n    assertInLog(content, \"Delete failed as operation on subfolders and files failed.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRecursiveDirectoryDeleteWhenChildDirectoryDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecursiveDirectoryDeleteWhenChildDirectoryDeleted() throws Exception\n{\r\n    testRecusiveDirectoryDelete(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRecursiveDirectoryDeleteWhenDeletingChildFileReturnsFalse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecursiveDirectoryDeleteWhenDeletingChildFileReturnsFalse() throws Exception\n{\r\n    testRecusiveDirectoryDelete(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRecusiveDirectoryDelete",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRecusiveDirectoryDelete(boolean useDir) throws Exception\n{\r\n    String childPathToBeDeletedByExternalAgent = (useDir) ? \"root/0\" : \"root/0/fileToRename\";\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(childPathToBeDeletedByExternalAgent)));\r\n    Answer<Boolean> answer = new Answer<Boolean>() {\r\n\r\n        public Boolean answer(InvocationOnMock invocation) throws Throwable {\r\n            String path = (String) invocation.getArguments()[0];\r\n            boolean isDir = (boolean) invocation.getArguments()[1];\r\n            boolean realResult = fs.deleteFile(path, isDir);\r\n            assertTrue(realResult);\r\n            boolean fakeResult = false;\r\n            return fakeResult;\r\n        }\r\n    };\r\n    Mockito.when(mockFs.deleteFile(path, useDir)).thenAnswer(answer);\r\n    createFolder(mockFs, \"root\");\r\n    Path sourceFolder = new Path(\"root\");\r\n    assertTrue(mockFs.delete(sourceFolder, true));\r\n    assertFalse(mockFs.exists(sourceFolder));\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Using thread pool for Delete operation with threads\");\r\n    assertInLog(content, String.format(\"Attempt to delete non-existent %s %s\", useDir ? \"directory\" : \"file\", path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteSingleDeleteException",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDeleteSingleDeleteException() throws Exception\n{\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root/0\")));\r\n    Mockito.doThrow(new IOException()).when(mockFs).deleteFile(path, true);\r\n    createFolder(mockFs, \"root\");\r\n    Path sourceFolder = new Path(\"root\");\r\n    boolean exception = false;\r\n    try {\r\n        mockFs.delete(sourceFolder, true);\r\n    } catch (IOException e) {\r\n        exception = true;\r\n    }\r\n    assertTrue(exception);\r\n    assertTrue(mockFs.exists(sourceFolder));\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Using thread pool for Delete operation with threads\");\r\n    assertInLog(content, \"Encountered Exception for Delete operation for file \" + path);\r\n    assertInLog(content, \"Terminating execution of Delete operation now as some other thread already got exception or operation failed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameThreadPoolExceptionFailure",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRenameThreadPoolExceptionFailure() throws Exception\n{\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(((NativeAzureFileSystem) fs).getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS));\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenThrow(new Exception());\r\n    Mockito.doReturn(mockThreadPoolExecutor).when(mockFs).getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS);\r\n    validateRenameFolder(mockFs, \"root\", \"rootnew\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Failed to create thread pool with threads\");\r\n    assertInLog(content, \"Serializing the Rename operation\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameThreadPoolExecuteFailure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameThreadPoolExecuteFailure() throws Exception\n{\r\n    ThreadPoolExecutor mockThreadExecutor = Mockito.mock(ThreadPoolExecutor.class);\r\n    Mockito.doThrow(new RejectedExecutionException()).when(mockThreadExecutor).execute(Mockito.any(Runnable.class));\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(mockFs.getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS));\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenReturn(mockThreadExecutor);\r\n    Mockito.when(mockFs.getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS)).thenReturn(mockThreadPoolExecutor);\r\n    validateRenameFolder(mockFs, \"root\", \"rootnew\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Rejected execution of thread for Rename operation on blob\");\r\n    assertInLog(content, \"Serializing the Rename operation\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameThreadPoolExecuteSingleThreadFailure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRenameThreadPoolExecuteSingleThreadFailure() throws Exception\n{\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(mockFs.getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS));\r\n    Mockito.when(mockFs.getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS)).thenReturn(mockThreadPoolExecutor);\r\n    ThreadPoolExecutor mockThreadExecutor = Mockito.spy(mockThreadPoolExecutor.getThreadPool(7));\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenReturn(mockThreadExecutor);\r\n    Mockito.doCallRealMethod().doThrow(new RejectedExecutionException()).when(mockThreadExecutor).execute(Mockito.any(Runnable.class));\r\n    validateRenameFolder(mockFs, \"root\", \"rootnew\");\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Using thread pool for Rename operation with threads 7\");\r\n    assertInLog(content, \"6 threads not used for Rename operation on blob\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameThreadPoolTerminationFailure",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testRenameThreadPoolTerminationFailure() throws Exception\n{\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root\")));\r\n    AzureFileSystemThreadPoolExecutor mockThreadPoolExecutor = Mockito.spy(mockFs.getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS));\r\n    Mockito.when(mockFs.getThreadPoolExecutor(renameThreads, \"AzureBlobRenameThread\", \"Rename\", path, NativeAzureFileSystem.AZURE_RENAME_THREADS)).thenReturn(mockThreadPoolExecutor);\r\n    ThreadPoolExecutor mockThreadExecutor = Mockito.mock(ThreadPoolExecutor.class);\r\n    Mockito.doNothing().when(mockThreadExecutor).execute(Mockito.any(Runnable.class));\r\n    Mockito.when(mockThreadExecutor.awaitTermination(Long.MAX_VALUE, TimeUnit.DAYS)).thenThrow(new InterruptedException());\r\n    Mockito.when(mockThreadPoolExecutor.getThreadPool(7)).thenReturn(mockThreadExecutor);\r\n    createFolder(mockFs, \"root\");\r\n    Path sourceFolder = new Path(\"root\");\r\n    Path destFolder = new Path(\"rootnew\");\r\n    boolean exception = false;\r\n    try {\r\n        mockFs.rename(sourceFolder, destFolder);\r\n    } catch (IOException e) {\r\n        exception = true;\r\n    }\r\n    assertTrue(exception);\r\n    assertTrue(mockFs.exists(sourceFolder));\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Using thread pool for Rename operation with threads\");\r\n    assertInLog(content, \"Threads got interrupted Rename blob operation\");\r\n    assertInLog(content, \"Rename failed as operation on subfolders and files failed.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameSingleRenameException",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testRenameSingleRenameException() throws Exception\n{\r\n    Path sourceFolder = new Path(\"root\");\r\n    Path destFolder = new Path(\"rootnew\");\r\n    NativeAzureFileSystem mockFs = Mockito.spy((NativeAzureFileSystem) fs);\r\n    createFolder(mockFs, \"root\");\r\n    String srcKey = mockFs.pathToKey(mockFs.makeAbsolute(sourceFolder));\r\n    String dstKey = mockFs.pathToKey(mockFs.makeAbsolute(destFolder));\r\n    FolderRenamePending mockRenameFs = Mockito.spy(mockFs.prepareAtomicFolderRename(srcKey, dstKey));\r\n    Mockito.when(mockFs.prepareAtomicFolderRename(srcKey, dstKey)).thenReturn(mockRenameFs);\r\n    String path = mockFs.pathToKey(mockFs.makeAbsolute(new Path(\"root/0\")));\r\n    Mockito.doThrow(new IOException()).when(mockRenameFs).renameFile(Mockito.any(FileMetadata.class));\r\n    boolean exception = false;\r\n    try {\r\n        mockFs.rename(sourceFolder, destFolder);\r\n    } catch (IOException e) {\r\n        exception = true;\r\n    }\r\n    assertTrue(exception);\r\n    assertTrue(mockFs.exists(sourceFolder));\r\n    String content = logs.getOutput();\r\n    assertInLog(content, \"Using thread pool for Rename operation with threads\");\r\n    assertInLog(content, \"Encountered Exception for Rename operation for file \" + path);\r\n    assertInLog(content, \"Terminating execution of Rename operation now as some other thread already got exception or operation failed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return enableManifestCommitter(prepareTestConfiguration(binding));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "shouldDeleteTestRootAtEndOfTestRun",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldDeleteTestRootAtEndOfTestRun()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "validateTaskAttemptManifest",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateTaskAttemptManifest(String attemptId, List<Path> files, TaskManifest manifest) throws IOException\n{\r\n    super.validateTaskAttemptManifest(attemptId, files, manifest);\r\n    final List<FileEntry> commit = manifest.getFilesToCommit();\r\n    final ManifestStoreOperations operations = getStoreOperations();\r\n    for (FileEntry entry : commit) {\r\n        Assertions.assertThat(entry.getEtag()).describedAs(\"Etag of %s\", entry).isNotEmpty();\r\n        final FileStatus sourceStatus = operations.getFileStatus(entry.getSourcePath());\r\n        final String etag = ManifestCommitterSupport.getEtag(sourceStatus);\r\n        Assertions.assertThat(etag).describedAs(\"Etag of %s\", sourceStatus).isEqualTo(entry.getEtag());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testContinuationTokenHavingEqualSign",
  "errType" : [ "AbfsRestOperationException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testContinuationTokenHavingEqualSign() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    AbfsClient abfsClient = fs.getAbfsClient();\r\n    try {\r\n        AbfsRestOperation op = abfsClient.listPath(\"/\", true, LIST_MAX_RESULTS, \"===========\", getTestTracingContext(fs, true));\r\n        Assert.assertTrue(false);\r\n    } catch (AbfsRestOperationException ex) {\r\n        Assert.assertEquals(\"InvalidQueryParameterValue\", ex.getErrorCode().getErrorCode());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testUnknownHost",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUnknownHost() throws Exception\n{\r\n    AbfsConfiguration conf = this.getConfiguration();\r\n    String accountName = this.getAccountName();\r\n    String fakeAccountName = \"fake\" + UUID.randomUUID() + accountName.substring(accountName.indexOf(\".\"));\r\n    String fsDefaultFS = conf.get(FS_DEFAULT_NAME_KEY);\r\n    conf.set(FS_DEFAULT_NAME_KEY, fsDefaultFS.replace(accountName, fakeAccountName));\r\n    conf.set(FS_AZURE_ACCOUNT_KEY + \".\" + fakeAccountName, this.getAccountKey());\r\n    intercept(AbfsRestOperationException.class, \"UnknownHostException: \" + fakeAccountName, () -> FileSystem.get(conf.getRawConfiguration()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListPathWithValidListMaxResultsValues",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testListPathWithValidListMaxResultsValues() throws IOException, ExecutionException, InterruptedException\n{\r\n    final int fileCount = 10;\r\n    final Path directory = getUniquePath(\"testWithValidListMaxResultsValues\");\r\n    createDirectoryWithNFiles(directory, fileCount);\r\n    final int[] testData = { fileCount + 100, fileCount + 1, fileCount, fileCount - 1, 1 };\r\n    for (int i = 0; i < testData.length; i++) {\r\n        int listMaxResults = testData[i];\r\n        setListMaxResults(listMaxResults);\r\n        int expectedListResultsSize = listMaxResults > fileCount ? fileCount : listMaxResults;\r\n        Assertions.assertThat(listPath(directory.toString())).describedAs(\"AbfsClient.listPath result should contain %d items when \" + \"listMaxResults is %d and directory contains %d items\", expectedListResultsSize, listMaxResults, fileCount).hasSize(expectedListResultsSize);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListPathWithValueGreaterThanServerMaximum",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testListPathWithValueGreaterThanServerMaximum() throws IOException, ExecutionException, InterruptedException\n{\r\n    setListMaxResults(LIST_MAX_RESULTS_SERVER + 100);\r\n    final Path directory = getUniquePath(\"testWithValueGreaterThanServerMaximum\");\r\n    createDirectoryWithNFiles(directory, LIST_MAX_RESULTS_SERVER + 200);\r\n    Assertions.assertThat(listPath(directory.toString())).describedAs(\"AbfsClient.listPath result will contain a maximum of %d items \" + \"even if listMaxResults >= %d or directory \" + \"contains more than %d items\", LIST_MAX_RESULTS_SERVER, LIST_MAX_RESULTS_SERVER, LIST_MAX_RESULTS_SERVER).hasSize(LIST_MAX_RESULTS_SERVER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListPathWithInvalidListMaxResultsValues",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListPathWithInvalidListMaxResultsValues() throws Exception\n{\r\n    for (int i = -1; i < 1; i++) {\r\n        setListMaxResults(i);\r\n        intercept(AbfsRestOperationException.class, \"Operation failed: \\\"One of \" + \"the query parameters specified in the request URI is outside\" + \" \" + \"the permissible range.\", () -> listPath(\"directory\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "listPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ListResultEntrySchema> listPath(String directory) throws IOException\n{\r\n    return getFileSystem().getAbfsClient().listPath(directory, false, getListMaxResults(), null, getTestTracingContext(getFileSystem(), true)).getResult().getListResultSchema().paths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getListMaxResults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getListMaxResults() throws IOException\n{\r\n    return getFileSystem().getAbfsStore().getAbfsConfiguration().getListMaxResults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setListMaxResults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setListMaxResults(int listMaxResults) throws IOException\n{\r\n    getFileSystem().getAbfsStore().getAbfsConfiguration().setListMaxResults(listMaxResults);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createDirectoryWithNFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createDirectoryWithNFiles(Path directory, int n) throws ExecutionException, InterruptedException\n{\r\n    final List<Future<Void>> tasks = new ArrayList<>();\r\n    ExecutorService es = Executors.newFixedThreadPool(10);\r\n    for (int i = 0; i < n; i++) {\r\n        final Path fileName = new Path(\"/\" + directory + \"/test\" + i);\r\n        tasks.add(es.submit(() -> {\r\n            touch(fileName);\r\n            return null;\r\n        }));\r\n    }\r\n    for (Future<Void> task : tasks) {\r\n        task.get();\r\n    }\r\n    es.shutdownNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init(Configuration conf)\n{\r\n    cache.init(conf);\r\n    authRules = new HashMap<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addAuthRuleForOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addAuthRuleForOwner(String wasbAbsolutePath, String accessType, boolean access)\n{\r\n    addAuthRule(wasbAbsolutePath, accessType, \"owner\", access);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addAuthRule",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addAuthRule(String wasbAbsolutePath, String accessType, String user, boolean access)\n{\r\n    wasbAbsolutePath = qualifiedPrefixUrl + wasbAbsolutePath;\r\n    AuthorizationComponent component = wasbAbsolutePath.endsWith(\"*\") ? new AuthorizationComponent(\"^\" + wasbAbsolutePath.replace(\"*\", \".*\"), accessType, user) : new AuthorizationComponent(wasbAbsolutePath, accessType, user);\r\n    this.authRules.put(component, access);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "authorize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean authorize(String wasbAbsolutePath, String accessType, String owner) throws WasbAuthorizationException\n{\r\n    if (wasbAbsolutePath.endsWith(NativeAzureFileSystem.FolderRenamePending.SUFFIX)) {\r\n        return true;\r\n    }\r\n    CachedAuthorizerEntry cacheKey = new CachedAuthorizerEntry(wasbAbsolutePath, accessType, owner);\r\n    Boolean cacheresult = cache.get(cacheKey);\r\n    if (cacheresult != null) {\r\n        return cacheresult;\r\n    }\r\n    boolean authorizeresult = authorizeInternal(wasbAbsolutePath, accessType, owner);\r\n    cache.put(cacheKey, authorizeresult);\r\n    return authorizeresult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "authorizeInternal",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean authorizeInternal(String wasbAbsolutePath, String accessType, String owner) throws WasbAuthorizationException\n{\r\n    String currentUserShortName = \"\";\r\n    try {\r\n        UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n        currentUserShortName = ugi.getShortUserName();\r\n    } catch (Exception e) {\r\n    }\r\n    if (StringUtils.equalsIgnoreCase(wasbAbsolutePath, qualifiedPrefixUrl + \"/\")) {\r\n        owner = currentUserShortName;\r\n    }\r\n    AuthorizationComponent component = new AuthorizationComponent(wasbAbsolutePath, accessType, currentUserShortName);\r\n    return processRules(authRules, component, owner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "processRules",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "boolean processRules(Map<AuthorizationComponent, Boolean> authRules, AuthorizationComponent component, String owner)\n{\r\n    if (authRules.containsKey(component)) {\r\n        return authRules.get(component);\r\n    } else {\r\n        for (Map.Entry<AuthorizationComponent, Boolean> entry : authRules.entrySet()) {\r\n            AuthorizationComponent key = entry.getKey();\r\n            String keyPath = key.getWasbAbsolutePath();\r\n            String keyAccess = key.getAccessType();\r\n            String keyUser = key.getUser();\r\n            boolean foundMatchingOwnerRule = keyPath.equals(component.getWasbAbsolutePath()) && keyAccess.equals(component.getAccessType()) && keyUser.equalsIgnoreCase(\"owner\") && owner.equals(component.getUser());\r\n            boolean foundMatchingPatternRule = keyPath.endsWith(\"*\") && Pattern.matches(keyPath, component.getWasbAbsolutePath()) && keyAccess.equals(component.getAccessType()) && keyUser.equalsIgnoreCase(component.getUser());\r\n            boolean foundMatchingPatternOwnerRule = keyPath.endsWith(\"*\") && Pattern.matches(keyPath, component.getWasbAbsolutePath()) && keyAccess.equals(component.getAccessType()) && keyUser.equalsIgnoreCase(\"owner\") && owner.equals(component.getUser());\r\n            if (foundMatchingOwnerRule || foundMatchingPatternRule || foundMatchingPatternOwnerRule) {\r\n                return entry.getValue();\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "deleteAllAuthRules",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteAllAuthRules()\n{\r\n    authRules.clear();\r\n    cache.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDaemonServiceSettingIdentity",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDaemonServiceSettingIdentity() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    assertEquals(\"Identity should not change for default config\", DAEMON, identityTransformer.transformUserOrGroupForSetRequest(DAEMON));\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, SERVICE_PRINCIPAL_ID);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, \"a,b,c,d\");\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"Identity should not change when substitution list doesn't contain daemon\", DAEMON, identityTransformer.transformUserOrGroupForSetRequest(DAEMON));\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, DAEMON + \",a,b,c,d\");\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"Identity should be replaced to servicePrincipalId\", SERVICE_PRINCIPAL_ID, identityTransformer.transformUserOrGroupForSetRequest(DAEMON));\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, ASTERISK);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"Identity should be replaced to servicePrincipalId\", SERVICE_PRINCIPAL_ID, identityTransformer.transformUserOrGroupForSetRequest(DAEMON));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFullyQualifiedNameSettingIdentity",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFullyQualifiedNameSettingIdentity() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    assertEquals(\"short name should not be converted to full name by default\", SHORT_NAME, identityTransformer.transformUserOrGroupForSetRequest(SHORT_NAME));\r\n    resetIdentityConfig(config);\r\n    config.setBoolean(FS_AZURE_FILE_OWNER_ENABLE_SHORTNAME, true);\r\n    config.set(FS_AZURE_FILE_OWNER_DOMAINNAME, DOMAIN);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"short name should be converted to full name\", FULLY_QUALIFIED_NAME, identityTransformer.transformUserOrGroupForSetRequest(SHORT_NAME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNoOpForSettingOidAsIdentity",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testNoOpForSettingOidAsIdentity() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    config.setBoolean(FS_AZURE_FILE_OWNER_ENABLE_SHORTNAME, true);\r\n    config.set(FS_AZURE_FILE_OWNER_DOMAINNAME, DOMAIN);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, UUID.randomUUID().toString());\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, \"a,b,c,d\");\r\n    IdentityTransformer identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    final String principalId = UUID.randomUUID().toString();\r\n    assertEquals(\"Identity should not be changed when owner is already a principal id \", principalId, identityTransformer.transformUserOrGroupForSetRequest(principalId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNoOpWhenSettingSuperUserAsdentity",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNoOpWhenSettingSuperUserAsdentity() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    config.setBoolean(FS_AZURE_FILE_OWNER_ENABLE_SHORTNAME, true);\r\n    config.set(FS_AZURE_FILE_OWNER_DOMAINNAME, DOMAIN);\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    assertEquals(\"Identity should not be changed because it is not in substitution list\", SUPER_USER, identityTransformer.transformUserOrGroupForSetRequest(SUPER_USER));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testIdentityReplacementForSuperUserGetRequest",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testIdentityReplacementForSuperUserGetRequest() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    assertEquals(\"$superuser should be replaced with local user by default\", localUser, identityTransformer.transformIdentityForGetRequest(SUPER_USER, true, localUser));\r\n    config.setBoolean(FS_AZURE_SKIP_SUPER_USER_REPLACEMENT, true);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"$superuser should not be replaced\", SUPER_USER, identityTransformer.transformIdentityForGetRequest(SUPER_USER, true, localUser));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testIdentityReplacementForDaemonServiceGetRequest",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testIdentityReplacementForDaemonServiceGetRequest() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    assertEquals(\"By default servicePrincipalId should not be converted for GetFileStatus(), listFileStatus(), getAcl()\", SERVICE_PRINCIPAL_ID, identityTransformer.transformIdentityForGetRequest(SERVICE_PRINCIPAL_ID, true, localUser));\r\n    resetIdentityConfig(config);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, \"a,b,c,d\");\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"servicePrincipalId should not be replaced if local daemon user is not in substitution list\", SERVICE_PRINCIPAL_ID, identityTransformer.transformIdentityForGetRequest(SERVICE_PRINCIPAL_ID, true, localUser));\r\n    resetIdentityConfig(config);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, localUser + \",a,b,c,d\");\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, UUID.randomUUID().toString());\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"servicePrincipalId should not be replaced if it is not equal to the SPN set in config\", SERVICE_PRINCIPAL_ID, identityTransformer.transformIdentityForGetRequest(SERVICE_PRINCIPAL_ID, true, localUser));\r\n    resetIdentityConfig(config);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, localUser + \",a,b,c,d\");\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, SERVICE_PRINCIPAL_ID);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"servicePrincipalId should be transformed to local use\", localUser, identityTransformer.transformIdentityForGetRequest(SERVICE_PRINCIPAL_ID, true, localUser));\r\n    resetIdentityConfig(config);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, ASTERISK);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, UUID.randomUUID().toString());\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"servicePrincipalId should not be replaced if it is not equal to the SPN set in config\", SERVICE_PRINCIPAL_ID, identityTransformer.transformIdentityForGetRequest(SERVICE_PRINCIPAL_ID, true, localUser));\r\n    resetIdentityConfig(config);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, ASTERISK);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, SERVICE_PRINCIPAL_ID);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"servicePrincipalId should be transformed to local user\", localUser, identityTransformer.transformIdentityForGetRequest(SERVICE_PRINCIPAL_ID, true, localUser));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testIdentityReplacementForKinitUserGetRequest",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testIdentityReplacementForKinitUserGetRequest() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    assertEquals(\"full name should not be transformed if shortname is not enabled\", FULLY_QUALIFIED_NAME, identityTransformer.transformIdentityForGetRequest(FULLY_QUALIFIED_NAME, true, localUser));\r\n    config.setBoolean(FS_AZURE_FILE_OWNER_ENABLE_SHORTNAME, true);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    assertEquals(\"should convert the full owner name to shortname \", SHORT_NAME, identityTransformer.transformIdentityForGetRequest(FULLY_QUALIFIED_NAME, true, localUser));\r\n    assertEquals(\"group name should not be converted to shortname \", FULLY_QUALIFIED_NAME, identityTransformer.transformIdentityForGetRequest(FULLY_QUALIFIED_NAME, false, localGroup));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "transformAclEntriesForSetRequest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void transformAclEntriesForSetRequest() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    List<AclEntry> aclEntriesToBeTransformed = Lists.newArrayList(aclEntry(ACCESS, USER, DAEMON, ALL), aclEntry(ACCESS, USER, FULLY_QUALIFIED_NAME, ALL), aclEntry(DEFAULT, USER, SUPER_USER, ALL), aclEntry(DEFAULT, USER, SERVICE_PRINCIPAL_ID, ALL), aclEntry(DEFAULT, USER, SHORT_NAME, ALL), aclEntry(DEFAULT, GROUP, DAEMON, ALL), aclEntry(DEFAULT, GROUP, SHORT_NAME, ALL), aclEntry(DEFAULT, OTHER, ALL), aclEntry(DEFAULT, MASK, ALL));\r\n    List<AclEntry> aclEntries = Lists.newArrayList(aclEntriesToBeTransformed);\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    identityTransformer.transformAclEntriesForSetRequest(aclEntries);\r\n    checkAclEntriesList(aclEntriesToBeTransformed, aclEntries);\r\n    resetIdentityConfig(config);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, DAEMON + \",a,b,c,d\");\r\n    config.setBoolean(FS_AZURE_FILE_OWNER_ENABLE_SHORTNAME, true);\r\n    config.set(FS_AZURE_FILE_OWNER_DOMAINNAME, DOMAIN);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, SERVICE_PRINCIPAL_ID);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    identityTransformer.transformAclEntriesForSetRequest(aclEntries);\r\n    List<AclEntry> expectedAclEntries = Lists.newArrayList(aclEntry(ACCESS, USER, SERVICE_PRINCIPAL_ID, ALL), aclEntry(ACCESS, USER, FULLY_QUALIFIED_NAME, ALL), aclEntry(DEFAULT, USER, SUPER_USER, ALL), aclEntry(DEFAULT, USER, SERVICE_PRINCIPAL_ID, ALL), aclEntry(DEFAULT, USER, FULLY_QUALIFIED_NAME, ALL), aclEntry(DEFAULT, GROUP, SERVICE_PRINCIPAL_ID, ALL), aclEntry(DEFAULT, GROUP, SHORT_NAME, ALL), aclEntry(DEFAULT, OTHER, ALL), aclEntry(DEFAULT, MASK, ALL));\r\n    checkAclEntriesList(aclEntries, expectedAclEntries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "transformAclEntriesForGetRequest",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void transformAclEntriesForGetRequest() throws IOException\n{\r\n    Configuration config = this.getRawConfiguration();\r\n    resetIdentityConfig(config);\r\n    List<AclEntry> aclEntriesToBeTransformed = Lists.newArrayList(aclEntry(ACCESS, USER, FULLY_QUALIFIED_NAME, ALL), aclEntry(DEFAULT, USER, SUPER_USER, ALL), aclEntry(DEFAULT, USER, SERVICE_PRINCIPAL_ID, ALL), aclEntry(DEFAULT, USER, SHORT_NAME, ALL), aclEntry(DEFAULT, GROUP, SHORT_NAME, ALL), aclEntry(DEFAULT, OTHER, ALL), aclEntry(DEFAULT, MASK, ALL));\r\n    List<AclEntry> aclEntries = Lists.newArrayList(aclEntriesToBeTransformed);\r\n    IdentityTransformer identityTransformer = getTransformerWithDefaultIdentityConfig(config);\r\n    identityTransformer.transformAclEntriesForGetRequest(aclEntries, localUser, localGroup);\r\n    checkAclEntriesList(aclEntriesToBeTransformed, aclEntries);\r\n    resetIdentityConfig(config);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP_LIST, localUser + \",a,b,c,d\");\r\n    config.setBoolean(FS_AZURE_FILE_OWNER_ENABLE_SHORTNAME, true);\r\n    config.set(FS_AZURE_FILE_OWNER_DOMAINNAME, DOMAIN);\r\n    config.set(FS_AZURE_OVERRIDE_OWNER_SP, SERVICE_PRINCIPAL_ID);\r\n    identityTransformer = getTransformerWithCustomizedIdentityConfig(config);\r\n    aclEntries = Lists.newArrayList(aclEntriesToBeTransformed);\r\n    identityTransformer.transformAclEntriesForGetRequest(aclEntries, localUser, localGroup);\r\n    List<AclEntry> expectedAclEntries = Lists.newArrayList(aclEntry(ACCESS, USER, SHORT_NAME, ALL), aclEntry(DEFAULT, USER, localUser, ALL), aclEntry(DEFAULT, USER, localUser, ALL), aclEntry(DEFAULT, USER, SHORT_NAME, ALL), aclEntry(DEFAULT, GROUP, SHORT_NAME, ALL), aclEntry(DEFAULT, OTHER, ALL), aclEntry(DEFAULT, MASK, ALL));\r\n    checkAclEntriesList(aclEntries, expectedAclEntries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "resetIdentityConfig",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void resetIdentityConfig(Configuration config)\n{\r\n    config.unset(FS_AZURE_FILE_OWNER_ENABLE_SHORTNAME);\r\n    config.unset(FS_AZURE_FILE_OWNER_DOMAINNAME);\r\n    config.unset(FS_AZURE_OVERRIDE_OWNER_SP);\r\n    config.unset(FS_AZURE_OVERRIDE_OWNER_SP_LIST);\r\n    config.unset(FS_AZURE_SKIP_SUPER_USER_REPLACEMENT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTransformerWithDefaultIdentityConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IdentityTransformer getTransformerWithDefaultIdentityConfig(Configuration config) throws IOException\n{\r\n    resetIdentityConfig(config);\r\n    return new IdentityTransformer(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTransformerWithCustomizedIdentityConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IdentityTransformer getTransformerWithCustomizedIdentityConfig(Configuration config) throws IOException\n{\r\n    return new IdentityTransformer(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkAclEntriesList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkAclEntriesList(List<AclEntry> aclEntries, List<AclEntry> expected)\n{\r\n    assertTrue(\"list size not equals\", aclEntries.size() == expected.size());\r\n    for (int i = 0; i < aclEntries.size(); i++) {\r\n        assertEquals(\"Identity doesn't match\", expected.get(i).getName(), aclEntries.get(i).getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AbfsCommitTestHelper.prepareTestConfiguration(binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "requireRenameResilience",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean requireRenameResilience()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "filesToCreate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int filesToCreate()\n{\r\n    return FILES_TO_CREATE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n    binding.getFileSystem().delete(binding.getTestPath(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initialize(Configuration configuration, String accountName) throws IOException\n{\r\n    String appID = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_ID);\r\n    String appSecret = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SECRET);\r\n    String sktid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_TENANT_ID);\r\n    String skoid = configuration.get(TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID);\r\n    String skt = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().minus(SASGenerator.FIVE_MINUTES));\r\n    String ske = SASGenerator.ISO_8601_FORMATTER.format(Instant.now().plus(SASGenerator.ONE_DAY));\r\n    String skv = SASGenerator.AuthenticationVersion.Dec19.toString();\r\n    byte[] key = getUserDelegationKey(accountName, appID, appSecret, sktid, skt, ske, skv);\r\n    generator = new DelegationSASGenerator(key, skoid, sktid, skt, ske, skv);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getAuthorizationHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getAuthorizationHeader(String accountName, String appID, String appSecret, String sktid) throws IOException\n{\r\n    String authEndPoint = String.format(\"https://login.microsoftonline.com/%s/oauth2/v2.0/token\", sktid);\r\n    ClientCredsTokenProvider provider = new ClientCredsTokenProvider(authEndPoint, appID, appSecret);\r\n    return \"Bearer \" + provider.getToken().getAccessToken();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getUserDelegationKey",
  "errType" : [ "MalformedURLException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "byte[] getUserDelegationKey(String accountName, String appID, String appSecret, String sktid, String skt, String ske, String skv) throws IOException\n{\r\n    String method = \"POST\";\r\n    String account = accountName.substring(0, accountName.indexOf(AbfsHttpConstants.DOT));\r\n    final StringBuilder sb = new StringBuilder(128);\r\n    sb.append(\"https://\");\r\n    sb.append(account);\r\n    sb.append(\".blob.core.windows.net/?restype=service&comp=userdelegationkey\");\r\n    URL url;\r\n    try {\r\n        url = new URL(sb.toString());\r\n    } catch (MalformedURLException ex) {\r\n        throw new InvalidUriException(sb.toString());\r\n    }\r\n    List<AbfsHttpHeader> requestHeaders = new ArrayList<AbfsHttpHeader>();\r\n    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_VERSION, skv));\r\n    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.CONTENT_TYPE, \"application/x-www-form-urlencoded\"));\r\n    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.AUTHORIZATION, getAuthorizationHeader(account, appID, appSecret, sktid)));\r\n    final StringBuilder requestBody = new StringBuilder(512);\r\n    requestBody.append(\"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?><KeyInfo><Start>\");\r\n    requestBody.append(skt);\r\n    requestBody.append(\"</Start><Expiry>\");\r\n    requestBody.append(ske);\r\n    requestBody.append(\"</Expiry></KeyInfo>\");\r\n    AbfsHttpOperation op = new AbfsHttpOperation(url, method, requestHeaders);\r\n    byte[] requestBuffer = requestBody.toString().getBytes(StandardCharsets.UTF_8.toString());\r\n    op.sendRequest(requestBuffer, 0, requestBuffer.length);\r\n    byte[] responseBuffer = new byte[4 * 1024];\r\n    op.processResponse(responseBuffer, 0, responseBuffer.length);\r\n    String responseBody = new String(responseBuffer, 0, (int) op.getBytesReceived(), StandardCharsets.UTF_8);\r\n    int beginIndex = responseBody.indexOf(\"<Value>\") + \"<Value>\".length();\r\n    int endIndex = responseBody.indexOf(\"</Value>\");\r\n    String value = responseBody.substring(beginIndex, endIndex);\r\n    return Base64.decode(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getSASToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getSASToken(String accountName, String fileSystem, String path, String operation) throws IOException, AccessControlException\n{\r\n    String saoid = null;\r\n    String suoid = null;\r\n    if (path == null || !path.endsWith(NO_AGENT_PATH)) {\r\n        saoid = (operation == SASTokenProvider.CHECK_ACCESS_OPERATION) ? null : TEST_OWNER;\r\n        suoid = (operation == SASTokenProvider.CHECK_ACCESS_OPERATION) ? TEST_OWNER : null;\r\n    }\r\n    return generator.getDelegationSAS(accountName, fileSystem, path, operation, saoid, suoid, CORRELATION_ID);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getKdc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniKdc getKdc()\n{\r\n    return kdc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getKeytab",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "File getKeytab()\n{\r\n    return keytab;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getKeytabPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getKeytabPath()\n{\r\n    return keytab.getAbsolutePath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "createBobUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation createBobUser() throws IOException\n{\r\n    return loginUserFromKeytabAndReturnUGI(bobPrincipal, keytab.getAbsolutePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "createAliceUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation createAliceUser() throws IOException\n{\r\n    return loginUserFromKeytabAndReturnUGI(alicePrincipal, keytab.getAbsolutePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getWorkDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "File getWorkDir()\n{\r\n    return workDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getKrbInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getKrbInstance()\n{\r\n    return krbInstance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getLoginUsername",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLoginUsername()\n{\r\n    return loginUsername;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getLoginPrincipal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLoginPrincipal()\n{\r\n    return loginPrincipal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "withRealm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String withRealm(String user)\n{\r\n    return user + \"@EXAMPLE.COM\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    patchConfigAtInit(conf);\r\n    super.serviceInit(conf);\r\n    Properties kdcConf = MiniKdc.createConf();\r\n    workDir = GenericTestUtils.getTestDir(\"kerberos\");\r\n    workDir.mkdirs();\r\n    kdc = new MiniKdc(kdcConf, workDir);\r\n    krbInstance = LOCALHOST_NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    kdc.start();\r\n    keytab = new File(workDir, \"keytab.bin\");\r\n    loginUsername = UserGroupInformation.getLoginUser().getShortUserName();\r\n    loginPrincipal = loginUsername + \"/\" + krbInstance;\r\n    alicePrincipal = ALICE + \"/\" + krbInstance;\r\n    bobPrincipal = BOB + \"/\" + krbInstance;\r\n    kdc.createPrincipal(keytab, alicePrincipal, bobPrincipal, \"HTTP/\" + krbInstance, HTTP_LOCALHOST, loginPrincipal);\r\n    final File keystoresDir = new File(workDir, \"ssl\");\r\n    keystoresDir.mkdirs();\r\n    sslConfDir = KeyStoreTestUtil.getClasspathDir(this.getClass());\r\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir.getAbsolutePath(), sslConfDir, getConfig(), false);\r\n    clientSSLConfigFileName = KeyStoreTestUtil.getClientSSLConfigFileName();\r\n    serverSSLConfigFileName = KeyStoreTestUtil.getServerSSLConfigFileName();\r\n    String kerberosRule = \"RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//\\nDEFAULT\";\r\n    KerberosName.setRules(kerberosRule);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    super.serviceStop();\r\n    kdc.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "patchConfigAtInit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void patchConfigAtInit(final Configuration conf)\n{\r\n    int timeout = (int) Duration.ofHours(1).toMillis();\r\n    conf.setInt(\"jvm.pause.info-threshold.ms\", timeout);\r\n    conf.setInt(\"jvm.pause.warn-threshold.ms\", timeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "resetUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetUGI()\n{\r\n    UserGroupInformation.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "userOnHost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String userOnHost(final String shortname)\n{\r\n    return shortname + \"/\" + krbInstance + \"@\" + getRealm();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getRealm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRealm()\n{\r\n    return kdc.getRealm();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "loginUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void loginUser(final String user) throws IOException\n{\r\n    UserGroupInformation.loginUserFromKeytab(user, getKeytabPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "loginPrincipal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void loginPrincipal() throws IOException\n{\r\n    loginUser(getLoginPrincipal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "assertSecurityEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertSecurityEnabled()\n{\r\n    assertTrue(\"Security is needed for this test\", UserGroupInformation.isSecurityEnabled());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "closeUserFileSystems",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeUserFileSystems(UserGroupInformation ugi) throws IOException\n{\r\n    if (ugi != null) {\r\n        FileSystem.closeAllForUGI(ugi);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "bindConfToCluster",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void bindConfToCluster(Configuration conf)\n{\r\n    conf.set(HADOOP_SECURITY_AUTHENTICATION, UserGroupInformation.AuthenticationMethod.KERBEROS.name());\r\n    conf.set(CommonConfigurationKeys.HADOOP_USER_GROUP_STATIC_OVERRIDES, \"alice,alice\");\r\n    conf.set(YarnConfiguration.RM_PRINCIPAL, BOB);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "newURI",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI newURI(String uri)\n{\r\n    try {\r\n        return new URI(uri);\r\n    } catch (URISyntaxException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "createTempTokenFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File createTempTokenFile() throws IOException\n{\r\n    File tokenfile = File.createTempFile(\"tokens\", \".bin\", getWorkDir());\r\n    tokenfile.delete();\r\n    return tokenfile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setClassField",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "T setClassField(Class<T> type, final T obj, final String fieldName, Object fieldObject) throws Exception\n{\r\n    Field field = type.getDeclaredField(fieldName);\r\n    field.setAccessible(true);\r\n    Field modifiersField = Field.class.getDeclaredField(\"modifiers\");\r\n    modifiersField.setAccessible(true);\r\n    modifiersField.setInt(field, field.getModifiers() & ~Modifier.FINAL);\r\n    field.set(obj, fieldObject);\r\n    return obj;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteHeavyBytesToFileAcrossThreads",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testWriteHeavyBytesToFileAcrossThreads() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Path testFile = path(methodName.getMethodName());\r\n    final FSDataOutputStream stream = fs.create(testFile);\r\n    ExecutorService es = Executors.newFixedThreadPool(TEN);\r\n    int testWriteBufferSize = 2 * TEN * ONE_THOUSAND * BASE_SIZE;\r\n    final byte[] b = new byte[testWriteBufferSize];\r\n    new Random().nextBytes(b);\r\n    List<Future<Void>> tasks = new ArrayList<>();\r\n    int operationCount = DEFAULT_WRITE_TIMES;\r\n    for (int i = 0; i < operationCount; i++) {\r\n        Callable<Void> callable = new Callable<Void>() {\r\n\r\n            @Override\r\n            public Void call() throws Exception {\r\n                stream.write(b);\r\n                return null;\r\n            }\r\n        };\r\n        tasks.add(es.submit(callable));\r\n    }\r\n    for (Future<Void> task : tasks) {\r\n        task.get();\r\n    }\r\n    tasks.clear();\r\n    stream.close();\r\n    es.shutdownNow();\r\n    FileStatus fileStatus = fs.getFileStatus(testFile);\r\n    assertEquals(testWriteBufferSize * operationCount, fileStatus.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWriteHeavyBytesToFileWithStatistics",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testReadWriteHeavyBytesToFileWithStatistics() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final FileSystem.Statistics abfsStatistics;\r\n    final Path testFile = path(methodName.getMethodName());\r\n    int testBufferSize;\r\n    final byte[] sourceData;\r\n    try (FSDataOutputStream stream = fs.create(testFile)) {\r\n        abfsStatistics = fs.getFsStatistics();\r\n        abfsStatistics.reset();\r\n        testBufferSize = 5 * TEN * ONE_THOUSAND * BASE_SIZE;\r\n        sourceData = new byte[testBufferSize];\r\n        new Random().nextBytes(sourceData);\r\n        stream.write(sourceData);\r\n    }\r\n    final byte[] remoteData = new byte[testBufferSize];\r\n    int bytesRead;\r\n    try (FSDataInputStream inputStream = fs.open(testFile, 4 * ONE_MB)) {\r\n        bytesRead = inputStream.read(remoteData);\r\n    }\r\n    String stats = abfsStatistics.toString();\r\n    assertEquals(\"Bytes read in \" + stats, remoteData.length, abfsStatistics.getBytesRead());\r\n    assertEquals(\"bytes written in \" + stats, sourceData.length, abfsStatistics.getBytesWritten());\r\n    assertEquals(\"bytesRead from read() call\", testBufferSize, bytesRead);\r\n    assertArrayEquals(\"round tripped data\", sourceData, remoteData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return AzureTestConstants.SCALE_TEST_TIMEOUT_MILLIS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n    requireScaleTestsEnabled();\r\n    if (getClusterBinding() == null) {\r\n        clusterBinding = demandCreateClusterBinding();\r\n    }\r\n    assertNotNull(\"cluster is not bound\", getClusterBinding());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "teardownClusters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownClusters() throws IOException\n{\r\n    terminateCluster(clusterBinding);\r\n    clusterBinding = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AbfsCommitTestHelper.prepareTestConfiguration(binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ClusterBinding createCluster(final JobConf conf) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Creating YARN MiniCluster\")) {\r\n        conf.setBoolean(JHAdminConfig.MR_HISTORY_CLEANER_ENABLE, false);\r\n        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd-HH.mm.ss.SS\"));\r\n        String clusterName = \"yarn-\" + timestamp;\r\n        MiniMRYarnCluster yarnCluster = new MiniMRYarnCluster(clusterName, NO_OF_NODEMANAGERS);\r\n        yarnCluster.init(conf);\r\n        yarnCluster.start();\r\n        return new ClusterBinding(clusterName, yarnCluster);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "terminateCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void terminateCluster(ClusterBinding cluster)\n{\r\n    if (cluster != null) {\r\n        cluster.terminate();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "getClusterBinding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClusterBinding getClusterBinding()\n{\r\n    return clusterBinding;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "getYarn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MiniMRYarnCluster getYarn()\n{\r\n    return getClusterBinding().getYarn();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "demandCreateClusterBinding",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClusterBinding demandCreateClusterBinding() throws Exception\n{\r\n    return createCluster(new JobConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "newJobConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobConf newJobConf() throws IOException\n{\r\n    JobConf jobConf = new JobConf(getYarn().getConfig());\r\n    jobConf.addResource(getConfiguration());\r\n    applyCustomConfigOptions(jobConf);\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "patchConfigurationForCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration patchConfigurationForCommitter(final Configuration jobConf)\n{\r\n    enableManifestCommitter(jobConf);\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "applyCustomConfigOptions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void applyCustomConfigOptions(JobConf jobConf) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "requireScaleTestsEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireScaleTestsEnabled()\n{\r\n    assumeScaleTestsEnabled(getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { \"OptmON_FlushCloseTest_EmptyFile_BufferSizeWrite\", true, false, 0, TEST_BUFFER_SIZE, 1, false }, { \"OptmON_FlushCloseTest_NonEmptyFile_BufferSizeWrite\", true, false, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 1, false }, { \"OptmON_CloseTest_EmptyFile_BufferSizeWrite\", true, true, 0, TEST_BUFFER_SIZE, 1, false }, { \"OptmON_CloseTest_NonEmptyFile_BufferSizeWrite\", true, true, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 1, false }, { \"OptmOFF_FlushCloseTest_EmptyFile_BufferSizeWrite\", false, false, 0, TEST_BUFFER_SIZE, 1, false }, { \"OptmOFF_FlushCloseTest_NonEmptyFile_BufferSizeWrite\", false, false, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 1, false }, { \"OptmOFF_CloseTest_EmptyFile_BufferSizeWrite\", false, true, 0, TEST_BUFFER_SIZE, 1, false }, { \"OptmOFF_CloseTest_NonEmptyFile_BufferSizeWrite\", false, true, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 1, false }, { \"OptmON_FlushCloseTest_EmptyFile_LessThanBufferSizeWrite\", true, false, 0, Math.abs(HALF_TEST_BUFFER_SIZE), 1, true }, { \"OptmON_FlushCloseTest_NonEmptyFile_LessThanBufferSizeWrite\", true, false, 2 * TEST_BUFFER_SIZE, Math.abs(HALF_TEST_BUFFER_SIZE), 1, true }, { \"OptmON_CloseTest_EmptyFile_LessThanBufferSizeWrite\", true, true, 0, Math.abs(HALF_TEST_BUFFER_SIZE), 1, true }, { \"OptmON_CloseTest_NonEmptyFile_LessThanBufferSizeWrite\", true, true, 2 * TEST_BUFFER_SIZE, Math.abs(HALF_TEST_BUFFER_SIZE), 1, true }, { \"OptmOFF_FlushCloseTest_EmptyFile_LessThanBufferSizeWrite\", false, false, 0, Math.abs(HALF_TEST_BUFFER_SIZE), 1, false }, { \"OptmOFF_FlushCloseTest_NonEmptyFile_LessThanBufferSizeWrite\", false, false, 2 * TEST_BUFFER_SIZE, Math.abs(HALF_TEST_BUFFER_SIZE), 1, false }, { \"OptmOFF_CloseTest_EmptyFile_LessThanBufferSizeWrite\", false, true, 0, Math.abs(HALF_TEST_BUFFER_SIZE), 1, false }, { \"OptmOFF_CloseTest_NonEmptyFile_LessThanBufferSizeWrite\", false, true, 2 * TEST_BUFFER_SIZE, Math.abs(HALF_TEST_BUFFER_SIZE), 1, false }, { \"OptmON_FlushCloseTest_EmptyFile_MultiSmallWritesStillLessThanBufferSize\", true, false, 0, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, true }, { \"OptmON_FlushCloseTest_NonEmptyFile_MultiSmallWritesStillLessThanBufferSize\", true, false, 2 * TEST_BUFFER_SIZE, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, true }, { \"OptmON_CloseTest_EmptyFile_MultiSmallWritesStillLessThanBufferSize\", true, true, 0, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, true }, { \"OptmON_CloseTest_NonEmptyFile_MultiSmallWritesStillLessThanBufferSize\", true, true, 2 * TEST_BUFFER_SIZE, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, true }, { \"OptmOFF_FlushCloseTest_EmptyFile_MultiSmallWritesStillLessThanBufferSize\", false, false, 0, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmOFF_FlushCloseTest_NonEmptyFile_MultiSmallWritesStillLessThanBufferSize\", false, false, 2 * TEST_BUFFER_SIZE, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmOFF_CloseTest_EmptyFile_MultiSmallWritesStillLessThanBufferSize\", false, true, 0, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmOFF_CloseTest_NonEmptyFile_MultiSmallWritesStillLessThanBufferSize\", false, true, 2 * TEST_BUFFER_SIZE, Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmON_FlushCloseTest_EmptyFile_MultiBufferSizeWrite\", true, false, 0, TEST_BUFFER_SIZE, 3, false }, { \"OptmON_FlushCloseTest_NonEmptyFile_MultiBufferSizeWrite\", true, false, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 3, false }, { \"OptmON_CloseTest_EmptyFile_MultiBufferSizeWrite\", true, true, 0, TEST_BUFFER_SIZE, 3, false }, { \"OptmON_CloseTest_NonEmptyFile_MultiBufferSizeWrite\", true, true, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 3, false }, { \"OptmOFF_FlushCloseTest_EmptyFile_MultiBufferSizeWrite\", false, false, 0, TEST_BUFFER_SIZE, 3, false }, { \"OptmOFF_FlushCloseTest_NonEmptyFile_MultiBufferSizeWrite\", false, false, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 3, false }, { \"OptmOFF_CloseTest_EmptyFile_MultiBufferSizeWrite\", false, true, 0, TEST_BUFFER_SIZE, 3, false }, { \"OptmOFF_CloseTest_NonEmptyFile_MultiBufferSizeWrite\", false, true, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE, 3, false }, { \"OptmON_FlushCloseTest_EmptyFile_BufferAndExtraWrite\", true, false, 0, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmON_FlushCloseTest_NonEmptyFile_BufferAndExtraWrite\", true, false, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmON_CloseTest_EmptyFile__BufferAndExtraWrite\", true, true, 0, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmON_CloseTest_NonEmptyFile_BufferAndExtraWrite\", true, true, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmOFF_FlushCloseTest_EmptyFile_BufferAndExtraWrite\", false, false, 0, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmOFF_FlushCloseTest_NonEmptyFile_BufferAndExtraWrite\", false, false, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmOFF_CloseTest_EmptyFile_BufferAndExtraWrite\", false, true, 0, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmOFF_CloseTest_NonEmptyFile_BufferAndExtraWrite\", false, true, 2 * TEST_BUFFER_SIZE, TEST_BUFFER_SIZE + Math.abs(QUARTER_TEST_BUFFER_SIZE), 3, false }, { \"OptmON_FlushCloseTest_EmptyFile_0ByteWrite\", true, false, 0, 0, 1, false }, { \"OptmON_FlushCloseTest_NonEmptyFile_0ByteWrite\", true, false, 2 * TEST_BUFFER_SIZE, 0, 1, false }, { \"OptmON_CloseTest_EmptyFile_0ByteWrite\", true, true, 0, 0, 1, false }, { \"OptmON_CloseTest_NonEmptyFile_0ByteWrite\", true, true, 2 * TEST_BUFFER_SIZE, 0, 1, false }, { \"OptmOFF_FlushCloseTest_EmptyFile_0ByteWrite\", false, false, 0, 0, 1, false }, { \"OptmOFF_FlushCloseTest_NonEmptyFile_0ByteWrite\", false, false, 2 * TEST_BUFFER_SIZE, 0, 1, false }, { \"OptmOFF_CloseTest_EmptyFile_0ByteWrite\", false, true, 0, 0, 1, false }, { \"OptmOFF_CloseTest_NonEmptyFile_0ByteWrite\", false, true, 2 * TEST_BUFFER_SIZE, 0, 1, false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSmallWriteOptimization",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSmallWriteOptimization() throws IOException\n{\r\n    boolean serviceDefaultOptmSettings = DEFAULT_AZURE_ENABLE_SMALL_WRITE_OPTIMIZATION;\r\n    if (enableSmallWriteOptimization) {\r\n        Assume.assumeTrue(serviceDefaultOptmSettings);\r\n    }\r\n    final AzureBlobFileSystem currentfs = this.getFileSystem();\r\n    Configuration config = currentfs.getConf();\r\n    boolean isAppendBlobTestSettingEnabled = (config.get(FS_AZURE_TEST_APPENDBLOB_ENABLED) == \"true\");\r\n    Assume.assumeFalse(isAppendBlobTestSettingEnabled);\r\n    config.set(ConfigurationKeys.AZURE_WRITE_BUFFER_SIZE, Integer.toString(TEST_BUFFER_SIZE));\r\n    config.set(ConfigurationKeys.AZURE_ENABLE_SMALL_WRITE_OPTIMIZATION, Boolean.toString(enableSmallWriteOptimization));\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.get(currentfs.getUri(), config);\r\n    formulateSmallWriteTestAppendPattern(fs, startingFileSize, recurringClientWriteSize, numOfClientWrites, directCloseTest, flushExpectedToBeMergedWithAppend);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "formulateSmallWriteTestAppendPattern",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void formulateSmallWriteTestAppendPattern(final AzureBlobFileSystem fs, int startingFileSize, int recurringWriteSize, int numOfWrites, boolean isDirectCloseTest, boolean flushExpectedToBeMergedWithAppend) throws IOException\n{\r\n    int totalDataToBeAppended = 0;\r\n    int testIteration = 0;\r\n    int dataWrittenPerIteration = (numOfWrites * recurringWriteSize);\r\n    if (isDirectCloseTest) {\r\n        totalDataToBeAppended = dataWrittenPerIteration;\r\n        testIteration = 1;\r\n    } else {\r\n        testIteration = TEST_FLUSH_ITERATION;\r\n        totalDataToBeAppended = testIteration * dataWrittenPerIteration;\r\n    }\r\n    int totalFileSize = totalDataToBeAppended + startingFileSize;\r\n    final byte[] writeBuffer = new byte[totalFileSize];\r\n    new Random().nextBytes(writeBuffer);\r\n    int writeBufferCursor = 0;\r\n    Path testPath = new Path(getMethodName() + UUID.randomUUID().toString());\r\n    FSDataOutputStream opStream;\r\n    if (startingFileSize > 0) {\r\n        writeBufferCursor += createFileWithStartingTestSize(fs, writeBuffer, writeBufferCursor, testPath, startingFileSize);\r\n        opStream = fs.append(testPath);\r\n    } else {\r\n        opStream = fs.create(testPath);\r\n    }\r\n    final int writeBufferSize = fs.getAbfsStore().getAbfsConfiguration().getWriteBufferSize();\r\n    long expectedTotalRequestsMade = fs.getInstrumentationMap().get(CONNECTIONS_MADE.getStatName());\r\n    long expectedRequestsMadeWithData = fs.getInstrumentationMap().get(SEND_REQUESTS.getStatName());\r\n    long expectedBytesSent = fs.getInstrumentationMap().get(BYTES_SENT.getStatName());\r\n    while (testIteration > 0) {\r\n        writeBufferCursor += executeWritePattern(opStream, writeBuffer, writeBufferCursor, numOfWrites, recurringWriteSize);\r\n        int numOfBuffersWrittenToStore = (int) Math.floor(dataWrittenPerIteration / writeBufferSize);\r\n        int dataSizeWrittenToStore = numOfBuffersWrittenToStore * writeBufferSize;\r\n        int pendingDataToStore = dataWrittenPerIteration - dataSizeWrittenToStore;\r\n        expectedTotalRequestsMade += numOfBuffersWrittenToStore;\r\n        expectedRequestsMadeWithData += numOfBuffersWrittenToStore;\r\n        expectedBytesSent += dataSizeWrittenToStore;\r\n        if (isDirectCloseTest) {\r\n            opStream.close();\r\n        } else {\r\n            opStream.hflush();\r\n        }\r\n        boolean wasDataPendingToBeWrittenToServer = (pendingDataToStore > 0);\r\n        final boolean smallWriteOptimizationEnabled = fs.getAbfsStore().getAbfsConfiguration().isSmallWriteOptimizationEnabled();\r\n        boolean flushWillBeMergedWithAppend = smallWriteOptimizationEnabled && (numOfBuffersWrittenToStore == 0) && (wasDataPendingToBeWrittenToServer);\r\n        Assertions.assertThat(flushWillBeMergedWithAppend).describedAs(flushExpectedToBeMergedWithAppend ? \"Flush was to be merged with Append\" : \"Flush should not have been merged with Append\").isEqualTo(flushExpectedToBeMergedWithAppend);\r\n        int totalAppendFlushCalls = (flushWillBeMergedWithAppend ? 1 : (wasDataPendingToBeWrittenToServer) ? 2 : 1);\r\n        expectedTotalRequestsMade += totalAppendFlushCalls;\r\n        expectedRequestsMadeWithData += totalAppendFlushCalls;\r\n        expectedBytesSent += wasDataPendingToBeWrittenToServer ? pendingDataToStore : 0;\r\n        assertOpStats(fs.getInstrumentationMap(), expectedTotalRequestsMade, expectedRequestsMadeWithData, expectedBytesSent);\r\n        if (isDirectCloseTest) {\r\n            validateStoreAppends(fs, testPath, totalFileSize, writeBuffer);\r\n            return;\r\n        }\r\n        testIteration--;\r\n    }\r\n    opStream.close();\r\n    expectedTotalRequestsMade += 1;\r\n    expectedRequestsMadeWithData += 1;\r\n    assertOpStats(fs.getInstrumentationMap(), expectedTotalRequestsMade, expectedRequestsMadeWithData, expectedBytesSent);\r\n    validateStoreAppends(fs, testPath, totalFileSize, writeBuffer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFileWithStartingTestSize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int createFileWithStartingTestSize(AzureBlobFileSystem fs, byte[] writeBuffer, int writeBufferCursor, Path testPath, int startingFileSize) throws IOException\n{\r\n    FSDataOutputStream opStream = fs.create(testPath);\r\n    writeBufferCursor += executeWritePattern(opStream, writeBuffer, writeBufferCursor, 1, startingFileSize);\r\n    opStream.close();\r\n    Assertions.assertThat(fs.getFileStatus(testPath).getLen()).describedAs(\"File should be of size %d at the start of test.\", startingFileSize).isEqualTo(startingFileSize);\r\n    return writeBufferCursor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateStoreAppends",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateStoreAppends(AzureBlobFileSystem fs, Path testPath, int totalFileSize, byte[] bufferWritten) throws IOException\n{\r\n    Assertions.assertThat(fs.getFileStatus(testPath).getLen()).describedAs(\"File should be of size %d at the end of test.\", totalFileSize).isEqualTo(totalFileSize);\r\n    byte[] fileReadFromStore = new byte[totalFileSize];\r\n    fs.open(testPath).read(fileReadFromStore, 0, totalFileSize);\r\n    assertArrayEquals(\"Test file content incorrect\", bufferWritten, fileReadFromStore);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertOpStats",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertOpStats(Map<String, Long> metricMap, long expectedTotalRequestsMade, long expectedRequestsMadeWithData, long expectedBytesSent)\n{\r\n    assertAbfsStatistics(CONNECTIONS_MADE, expectedTotalRequestsMade, metricMap);\r\n    assertAbfsStatistics(SEND_REQUESTS, expectedRequestsMadeWithData, metricMap);\r\n    assertAbfsStatistics(BYTES_SENT, expectedBytesSent, metricMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "executeWritePattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int executeWritePattern(FSDataOutputStream opStream, byte[] buffer, int startOffset, int writeLoopCount, int writeSize) throws IOException\n{\r\n    int dataSizeWritten = startOffset;\r\n    while (writeLoopCount > 0) {\r\n        opStream.write(buffer, startOffset, writeSize);\r\n        startOffset += writeSize;\r\n        writeLoopCount--;\r\n    }\r\n    dataSizeWritten = startOffset - dataSizeWritten;\r\n    return dataSizeWritten;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount = AzureTestUtils.cleanup(testAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMode()\n{\r\n    runningInSASMode = AzureBlobStorageTestAccount.createTestConfiguration().getBoolean(AzureNativeFileSystemStore.KEY_USE_SECURE_MODE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testContainerExistAfterDoesNotExist",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testContainerExistAfterDoesNotExist() throws Exception\n{\r\n    testAccount = blobStorageTestAccount();\r\n    assumeNotNull(testAccount);\r\n    CloudBlobContainer container = testAccount.getRealContainer();\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    assertFalse(container.exists());\r\n    try {\r\n        fs.listStatus(new Path(\"/\"));\r\n        assertTrue(\"Should've thrown.\", false);\r\n    } catch (FileNotFoundException ex) {\r\n        assertTrue(\"Unexpected exception: \" + ex, ex.getMessage().contains(\"is not found\"));\r\n    }\r\n    assertFalse(container.exists());\r\n    container.create();\r\n    CloudBlockBlob blob = testAccount.getBlobReference(\"foo\");\r\n    BlobOutputStream outputStream = blob.openOutputStream();\r\n    outputStream.write(new byte[10]);\r\n    outputStream.close();\r\n    assertTrue(fs.exists(new Path(\"/foo\")));\r\n    assertTrue(container.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "blobStorageTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount blobStorageTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create(\"\", EnumSet.noneOf(CreateOptions.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testContainerCreateAfterDoesNotExist",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testContainerCreateAfterDoesNotExist() throws Exception\n{\r\n    testAccount = blobStorageTestAccount();\r\n    assumeNotNull(testAccount);\r\n    CloudBlobContainer container = testAccount.getRealContainer();\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    assertFalse(container.exists());\r\n    try {\r\n        assertNull(fs.listStatus(new Path(\"/\")));\r\n        assertTrue(\"Should've thrown.\", false);\r\n    } catch (FileNotFoundException ex) {\r\n        assertTrue(\"Unexpected exception: \" + ex, ex.getMessage().contains(\"is not found\"));\r\n    }\r\n    assertFalse(container.exists());\r\n    assertTrue(fs.createNewFile(new Path(\"/foo\")));\r\n    assertTrue(container.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testContainerCreateOnWrite",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testContainerCreateOnWrite() throws Exception\n{\r\n    testAccount = blobStorageTestAccount();\r\n    assumeNotNull(testAccount);\r\n    CloudBlobContainer container = testAccount.getRealContainer();\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    assertFalse(container.exists());\r\n    try {\r\n        fs.listStatus(new Path(\"/\"));\r\n        assertTrue(\"Should've thrown.\", false);\r\n    } catch (FileNotFoundException ex) {\r\n        assertTrue(\"Unexpected exception: \" + ex, ex.getMessage().contains(\"is not found\"));\r\n    }\r\n    assertFalse(container.exists());\r\n    Path foo = new Path(\"/testContainerCreateOnWrite-foo\");\r\n    Path bar = new Path(\"/testContainerCreateOnWrite-bar\");\r\n    LambdaTestUtils.intercept(FileNotFoundException.class, new Callable<String>() {\r\n\r\n        @Override\r\n        public String call() throws Exception {\r\n            fs.open(foo).close();\r\n            return \"Stream to \" + foo;\r\n        }\r\n    });\r\n    assertFalse(container.exists());\r\n    assertFalse(fs.rename(foo, bar));\r\n    assertFalse(container.exists());\r\n    assertTrue(fs.createNewFile(foo));\r\n    assertTrue(container.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testContainerChecksWithSas",
  "errType" : [ "AzureException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testContainerChecksWithSas() throws Exception\n{\r\n    Assume.assumeFalse(runningInSASMode);\r\n    testAccount = AzureBlobStorageTestAccount.create(\"\", EnumSet.of(CreateOptions.UseSas));\r\n    assumeNotNull(testAccount);\r\n    CloudBlobContainer container = testAccount.getRealContainer();\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    assertFalse(container.exists());\r\n    try {\r\n        fs.createNewFile(new Path(\"/testContainerChecksWithSas-foo\"));\r\n        assertFalse(\"Should've thrown.\", true);\r\n    } catch (AzureException ex) {\r\n    }\r\n    assertFalse(container.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testWithNoOptimization",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testWithNoOptimization() throws Exception\n{\r\n    for (int i = 2; i <= 7; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(false, false, fileSize);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        testWithNoOptimization(fs, testFilePath, HUNDRED, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testWithNoOptimization",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testWithNoOptimization(final FileSystem fs, final Path testFilePath, final int seekPos, final byte[] fileContent) throws IOException\n{\r\n    FSDataInputStream iStream = fs.open(testFilePath);\r\n    try {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        iStream = new FSDataInputStream(abfsInputStream);\r\n        seek(iStream, seekPos);\r\n        long totalBytesRead = 0;\r\n        int length = HUNDRED * HUNDRED;\r\n        do {\r\n            byte[] buffer = new byte[length];\r\n            int bytesRead = iStream.read(buffer, 0, length);\r\n            totalBytesRead += bytesRead;\r\n            if ((totalBytesRead + seekPos) >= fileContent.length) {\r\n                length = (fileContent.length - seekPos) % length;\r\n            }\r\n            assertEquals(length, bytesRead);\r\n            assertContentReadCorrectly(fileContent, (int) (seekPos + totalBytesRead - length), length, buffer, testFilePath);\r\n            assertTrue(abfsInputStream.getFCursor() >= seekPos + totalBytesRead);\r\n            assertTrue(abfsInputStream.getFCursorAfterLastRead() >= seekPos + totalBytesRead);\r\n            assertTrue(abfsInputStream.getBCursor() >= totalBytesRead % abfsInputStream.getBufferSize());\r\n            assertTrue(abfsInputStream.getLimit() >= totalBytesRead % abfsInputStream.getBufferSize());\r\n        } while (totalBytesRead + seekPos < fileContent.length);\r\n    } finally {\r\n        iStream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testExceptionInOptimization",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testExceptionInOptimization() throws Exception\n{\r\n    for (int i = 2; i <= 7; i++) {\r\n        int fileSize = i * ONE_MB;\r\n        final AzureBlobFileSystem fs = getFileSystem(true, true, fileSize);\r\n        String fileName = methodName.getMethodName() + i;\r\n        byte[] fileContent = getRandomBytesArray(fileSize);\r\n        Path testFilePath = createFileWithContent(fs, fileName, fileContent);\r\n        testExceptionInOptimization(fs, testFilePath, fileSize - HUNDRED, fileSize / 4, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testExceptionInOptimization",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testExceptionInOptimization(final FileSystem fs, final Path testFilePath, final int seekPos, final int length, final byte[] fileContent) throws IOException\n{\r\n    FSDataInputStream iStream = fs.open(testFilePath);\r\n    try {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n        abfsInputStream = spy(abfsInputStream);\r\n        doThrow(new IOException()).doCallRealMethod().when(abfsInputStream).readRemote(anyLong(), any(), anyInt(), anyInt(), any(TracingContext.class));\r\n        iStream = new FSDataInputStream(abfsInputStream);\r\n        verifyBeforeSeek(abfsInputStream);\r\n        seek(iStream, seekPos);\r\n        byte[] buffer = new byte[length];\r\n        int bytesRead = iStream.read(buffer, 0, length);\r\n        long actualLength = length;\r\n        if (seekPos + length > fileContent.length) {\r\n            long delta = seekPos + length - fileContent.length;\r\n            actualLength = length - delta;\r\n        }\r\n        assertEquals(bytesRead, actualLength);\r\n        assertContentReadCorrectly(fileContent, seekPos, (int) actualLength, buffer, testFilePath);\r\n        assertEquals(fileContent.length, abfsInputStream.getFCursor());\r\n        assertEquals(fileContent.length, abfsInputStream.getFCursorAfterLastRead());\r\n        assertEquals(actualLength, abfsInputStream.getBCursor());\r\n        assertTrue(abfsInputStream.getLimit() >= actualLength);\r\n    } finally {\r\n        iStream.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureBlobFileSystem getFileSystem(boolean readSmallFilesCompletely) throws IOException\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    getAbfsStore(fs).getAbfsConfiguration().setReadSmallFilesCompletely(readSmallFilesCompletely);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AzureBlobFileSystem getFileSystem(boolean optimizeFooterRead, boolean readSmallFileCompletely, int fileSize) throws IOException\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    getAbfsStore(fs).getAbfsConfiguration().setOptimizeFooterRead(optimizeFooterRead);\r\n    if (fileSize <= getAbfsStore(fs).getAbfsConfiguration().getReadBufferSize()) {\r\n        getAbfsStore(fs).getAbfsConfiguration().setReadSmallFilesCompletely(readSmallFileCompletely);\r\n    }\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRandomBytesArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getRandomBytesArray(int length)\n{\r\n    final byte[] b = new byte[length];\r\n    new Random().nextBytes(b);\r\n    return b;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createFileWithContent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createFileWithContent(FileSystem fs, String fileName, byte[] fileContent) throws IOException\n{\r\n    Path testFilePath = path(fileName);\r\n    try (FSDataOutputStream oStream = fs.create(testFilePath)) {\r\n        oStream.write(fileContent);\r\n        oStream.flush();\r\n    }\r\n    return testFilePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsStore",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureBlobFileSystemStore getAbfsStore(FileSystem fs) throws NoSuchFieldException, IllegalAccessException\n{\r\n    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fs;\r\n    Field abfsStoreField = AzureBlobFileSystem.class.getDeclaredField(\"abfsStore\");\r\n    abfsStoreField.setAccessible(true);\r\n    return (AzureBlobFileSystemStore) abfsStoreField.get(abfs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getInstrumentationMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, Long> getInstrumentationMap(FileSystem fs) throws NoSuchFieldException, IllegalAccessException\n{\r\n    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fs;\r\n    Field abfsCountersField = AzureBlobFileSystem.class.getDeclaredField(\"abfsCounters\");\r\n    abfsCountersField.setAccessible(true);\r\n    AbfsCounters abfsCounters = (AbfsCounters) abfsCountersField.get(abfs);\r\n    return abfsCounters.toMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertContentReadCorrectly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertContentReadCorrectly(byte[] actualFileContent, int from, int len, byte[] contentRead, Path testFilePath)\n{\r\n    for (int i = 0; i < len; i++) {\r\n        assertEquals(\"The test file path is \" + testFilePath, contentRead[i], actualFileContent[i + from]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertBuffersAreNotEqual",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertBuffersAreNotEqual(byte[] actualContent, byte[] contentRead, AbfsConfiguration conf, Path testFilePath)\n{\r\n    assertBufferEquality(actualContent, contentRead, conf, false, testFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertBuffersAreEqual",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertBuffersAreEqual(byte[] actualContent, byte[] contentRead, AbfsConfiguration conf, Path testFilePath)\n{\r\n    assertBufferEquality(actualContent, contentRead, conf, true, testFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertBufferEquality",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertBufferEquality(byte[] actualContent, byte[] contentRead, AbfsConfiguration conf, boolean assertEqual, Path testFilePath)\n{\r\n    int bufferSize = conf.getReadBufferSize();\r\n    int actualContentSize = actualContent.length;\r\n    int n = (actualContentSize < bufferSize) ? actualContentSize : bufferSize;\r\n    int matches = 0;\r\n    for (int i = 0; i < n; i++) {\r\n        if (actualContent[i] == contentRead[i]) {\r\n            matches++;\r\n        }\r\n    }\r\n    if (assertEqual) {\r\n        assertEquals(\"The test file path is \" + testFilePath, n, matches);\r\n    } else {\r\n        assertNotEquals(\"The test file path is \" + testFilePath, n, matches);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void seek(FSDataInputStream iStream, long seekPos) throws IOException\n{\r\n    AbfsInputStream abfsInputStream = (AbfsInputStream) iStream.getWrappedStream();\r\n    verifyBeforeSeek(abfsInputStream);\r\n    iStream.seek(seekPos);\r\n    verifyAfterSeek(abfsInputStream, seekPos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyBeforeSeek",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyBeforeSeek(AbfsInputStream abfsInputStream)\n{\r\n    assertEquals(0, abfsInputStream.getFCursor());\r\n    assertEquals(-1, abfsInputStream.getFCursorAfterLastRead());\r\n    assertEquals(0, abfsInputStream.getLimit());\r\n    assertEquals(0, abfsInputStream.getBCursor());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyAfterSeek",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyAfterSeek(AbfsInputStream abfsInputStream, long seekPos) throws IOException\n{\r\n    assertEquals(seekPos, abfsInputStream.getPos());\r\n    assertEquals(-1, abfsInputStream.getFCursorAfterLastRead());\r\n    assertEquals(0, abfsInputStream.getLimit());\r\n    assertEquals(0, abfsInputStream.getBCursor());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = super.createConfiguration();\r\n    conf.setBoolean(NativeAzureFileSystem.APPEND_SUPPORT_ENABLE_PROPERTY_NAME, true);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    testPath = methodPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create(createConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getTestData(int size)\n{\r\n    byte[] testData = new byte[size];\r\n    System.arraycopy(RandomStringUtils.randomAlphabetic(size).getBytes(), 0, testData, 0, size);\r\n    return testData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBaseFileWithData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] createBaseFileWithData(int fileSize, Path testPath) throws Throwable\n{\r\n    try (FSDataOutputStream createStream = fs.create(testPath)) {\r\n        byte[] fileData = null;\r\n        if (fileSize != 0) {\r\n            fileData = getTestData(fileSize);\r\n            createStream.write(fileData);\r\n        }\r\n        return fileData;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyFileData",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean verifyFileData(int dataLength, byte[] testData, int testDataIndex, FSDataInputStream srcStream)\n{\r\n    try {\r\n        byte[] fileBuffer = new byte[dataLength];\r\n        byte[] testDataBuffer = new byte[dataLength];\r\n        int fileBytesRead = srcStream.read(fileBuffer);\r\n        if (fileBytesRead < dataLength) {\r\n            return false;\r\n        }\r\n        System.arraycopy(testData, testDataIndex, testDataBuffer, 0, dataLength);\r\n        if (!Arrays.equals(fileBuffer, testDataBuffer)) {\r\n            return false;\r\n        }\r\n        return true;\r\n    } catch (Exception ex) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyAppend",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean verifyAppend(byte[] testData, Path testFile)\n{\r\n    try (FSDataInputStream srcStream = fs.open(testFile)) {\r\n        int baseBufferSize = 2048;\r\n        int testDataSize = testData.length;\r\n        int testDataIndex = 0;\r\n        while (testDataSize > baseBufferSize) {\r\n            if (!verifyFileData(baseBufferSize, testData, testDataIndex, srcStream)) {\r\n                return false;\r\n            }\r\n            testDataIndex += baseBufferSize;\r\n            testDataSize -= baseBufferSize;\r\n        }\r\n        if (!verifyFileData(testDataSize, testData, testDataIndex, srcStream)) {\r\n            return false;\r\n        }\r\n        return true;\r\n    } catch (Exception ex) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleAppend",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSingleAppend() throws Throwable\n{\r\n    FSDataOutputStream appendStream = null;\r\n    try {\r\n        int baseDataSize = 50;\r\n        byte[] baseDataBuffer = createBaseFileWithData(baseDataSize, testPath);\r\n        int appendDataSize = 20;\r\n        byte[] appendDataBuffer = getTestData(appendDataSize);\r\n        appendStream = fs.append(testPath, 10);\r\n        appendStream.write(appendDataBuffer);\r\n        appendStream.close();\r\n        byte[] testData = new byte[baseDataSize + appendDataSize];\r\n        System.arraycopy(baseDataBuffer, 0, testData, 0, baseDataSize);\r\n        System.arraycopy(appendDataBuffer, 0, testData, baseDataSize, appendDataSize);\r\n        assertTrue(verifyAppend(testData, testPath));\r\n    } finally {\r\n        if (appendStream != null) {\r\n            appendStream.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleAppendOnEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSingleAppendOnEmptyFile() throws Throwable\n{\r\n    FSDataOutputStream appendStream = null;\r\n    try {\r\n        createBaseFileWithData(0, testPath);\r\n        int appendDataSize = 20;\r\n        byte[] appendDataBuffer = getTestData(appendDataSize);\r\n        appendStream = fs.append(testPath, 10);\r\n        appendStream.write(appendDataBuffer);\r\n        appendStream.close();\r\n        assertTrue(verifyAppend(appendDataBuffer, testPath));\r\n    } finally {\r\n        if (appendStream != null) {\r\n            appendStream.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testSingleAppenderScenario",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSingleAppenderScenario() throws Throwable\n{\r\n    FSDataOutputStream appendStream1 = null;\r\n    FSDataOutputStream appendStream2 = null;\r\n    IOException ioe = null;\r\n    try {\r\n        createBaseFileWithData(0, testPath);\r\n        appendStream1 = fs.append(testPath, 10);\r\n        boolean encounteredException = false;\r\n        try {\r\n            appendStream2 = fs.append(testPath, 10);\r\n        } catch (IOException ex) {\r\n            encounteredException = true;\r\n            ioe = ex;\r\n        }\r\n        appendStream1.close();\r\n        assertTrue(encounteredException);\r\n        GenericTestUtils.assertExceptionContains(\"Unable to set Append lease on the Blob\", ioe);\r\n    } finally {\r\n        if (appendStream1 != null) {\r\n            appendStream1.close();\r\n        }\r\n        if (appendStream2 != null) {\r\n            appendStream2.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultipleAppends",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMultipleAppends() throws Throwable\n{\r\n    int baseDataSize = 50;\r\n    byte[] baseDataBuffer = createBaseFileWithData(baseDataSize, testPath);\r\n    int appendDataSize = 100;\r\n    int targetAppendCount = 50;\r\n    byte[] testData = new byte[baseDataSize + (appendDataSize * targetAppendCount)];\r\n    int testDataIndex = 0;\r\n    System.arraycopy(baseDataBuffer, 0, testData, testDataIndex, baseDataSize);\r\n    testDataIndex += baseDataSize;\r\n    int appendCount = 0;\r\n    FSDataOutputStream appendStream = null;\r\n    try {\r\n        while (appendCount < targetAppendCount) {\r\n            byte[] appendDataBuffer = getTestData(appendDataSize);\r\n            appendStream = fs.append(testPath, 30);\r\n            appendStream.write(appendDataBuffer);\r\n            appendStream.close();\r\n            System.arraycopy(appendDataBuffer, 0, testData, testDataIndex, appendDataSize);\r\n            testDataIndex += appendDataSize;\r\n            appendCount++;\r\n        }\r\n        assertTrue(verifyAppend(testData, testPath));\r\n    } finally {\r\n        if (appendStream != null) {\r\n            appendStream.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultipleAppendsOnSameStream",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMultipleAppendsOnSameStream() throws Throwable\n{\r\n    int baseDataSize = 50;\r\n    byte[] baseDataBuffer = createBaseFileWithData(baseDataSize, testPath);\r\n    int appendDataSize = 100;\r\n    int targetAppendCount = 50;\r\n    byte[] testData = new byte[baseDataSize + (appendDataSize * targetAppendCount)];\r\n    int testDataIndex = 0;\r\n    System.arraycopy(baseDataBuffer, 0, testData, testDataIndex, baseDataSize);\r\n    testDataIndex += baseDataSize;\r\n    int appendCount = 0;\r\n    FSDataOutputStream appendStream = null;\r\n    try {\r\n        while (appendCount < targetAppendCount) {\r\n            appendStream = fs.append(testPath, 50);\r\n            int singleAppendChunkSize = 20;\r\n            int appendRunSize = 0;\r\n            while (appendRunSize < appendDataSize) {\r\n                byte[] appendDataBuffer = getTestData(singleAppendChunkSize);\r\n                appendStream.write(appendDataBuffer);\r\n                System.arraycopy(appendDataBuffer, 0, testData, testDataIndex + appendRunSize, singleAppendChunkSize);\r\n                appendRunSize += singleAppendChunkSize;\r\n            }\r\n            appendStream.close();\r\n            testDataIndex += appendDataSize;\r\n            appendCount++;\r\n        }\r\n        assertTrue(verifyAppend(testData, testPath));\r\n    } finally {\r\n        if (appendStream != null) {\r\n            appendStream.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testFalseConfigurationFlagBehavior",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFalseConfigurationFlagBehavior() throws Throwable\n{\r\n    fs = testAccount.getFileSystem();\r\n    Configuration conf = fs.getConf();\r\n    conf.setBoolean(NativeAzureFileSystem.APPEND_SUPPORT_ENABLE_PROPERTY_NAME, false);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    FSDataOutputStream appendStream = null;\r\n    try {\r\n        createBaseFileWithData(0, testPath);\r\n        appendStream = fs.append(testPath, 10);\r\n    } finally {\r\n        if (appendStream != null) {\r\n            appendStream.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.createMock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConsistencyAfterSmallFlushes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConsistencyAfterSmallFlushes() throws Exception\n{\r\n    testConsistencyAfterManyFlushes(FlushFrequencyVariation.BeforeSingleBufferFull);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConsistencyAfterMediumFlushes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConsistencyAfterMediumFlushes() throws Exception\n{\r\n    testConsistencyAfterManyFlushes(FlushFrequencyVariation.AfterSingleBufferFull);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConsistencyAfterLargeFlushes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConsistencyAfterLargeFlushes() throws Exception\n{\r\n    testConsistencyAfterManyFlushes(FlushFrequencyVariation.AfterAllRingBufferFull);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertDataInStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertDataInStream(InputStream inStream, int expectedSize) throws Exception\n{\r\n    int byteRead;\r\n    int countBytes = 0;\r\n    while ((byteRead = inStream.read()) != -1) {\r\n        assertEquals(countBytes % byteValuePeriod, byteRead);\r\n        countBytes++;\r\n    }\r\n    assertEquals(expectedSize, countBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertDataInFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertDataInFile(Path file, int expectedSize) throws Exception\n{\r\n    try (InputStream inStream = getFileSystem().open(file)) {\r\n        assertDataInStream(inStream, expectedSize);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "assertDataInTempBlob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void assertDataInTempBlob(int expectedSize) throws Exception\n{\r\n    InMemoryBlockBlobStore backingStore = getTestAccount().getMockStorage().getBackingStore();\r\n    String tempKey = null;\r\n    for (String key : backingStore.getKeys()) {\r\n        if (key.contains(NativeAzureFileSystem.AZURE_TEMP_FOLDER)) {\r\n            tempKey = key;\r\n            break;\r\n        }\r\n    }\r\n    assertNotNull(tempKey);\r\n    try (InputStream inStream = new ByteArrayInputStream(backingStore.getContent(tempKey))) {\r\n        assertDataInStream(inStream, expectedSize);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testConsistencyAfterManyFlushes",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testConsistencyAfterManyFlushes(FlushFrequencyVariation variation) throws Exception\n{\r\n    Path uploadedFile = methodPath();\r\n    try {\r\n        OutputStream outStream = getFileSystem().create(uploadedFile);\r\n        final int totalSize = 9123;\r\n        int flushPeriod;\r\n        switch(variation) {\r\n            case BeforeSingleBufferFull:\r\n                flushPeriod = 300;\r\n                break;\r\n            case AfterSingleBufferFull:\r\n                flushPeriod = 600;\r\n                break;\r\n            case AfterAllRingBufferFull:\r\n                flushPeriod = 1600;\r\n                break;\r\n            default:\r\n                throw new IllegalArgumentException(\"Unknown variation: \" + variation);\r\n        }\r\n        for (int i = 0; i < totalSize; i++) {\r\n            outStream.write(i % byteValuePeriod);\r\n            if ((i + 1) % flushPeriod == 0) {\r\n                outStream.flush();\r\n                assertDataInTempBlob(i + 1);\r\n            }\r\n        }\r\n        outStream.close();\r\n        assertDataInFile(uploadedFile, totalSize);\r\n    } finally {\r\n        getFileSystem().delete(uploadedFile, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    binding.setup();\r\n    fSys = binding.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem createFileSystem() throws Exception\n{\r\n    return fSys;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListStatusThrowsExceptionForUnreadableDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testListStatusThrowsExceptionForUnreadableDir()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGlobStatusThrowsExceptionForUnreadableDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testGlobStatusThrowsExceptionForUnreadableDir()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsHttpCallsDurations",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testAbfsHttpCallsDurations() throws IOException\n{\r\n    describe(\"test to verify if the DurationTrackers for abfs http calls \" + \"work as expected.\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFilePath = path(getMethodName());\r\n    AbfsOutputStream out = null;\r\n    AbfsInputStream in = null;\r\n    try {\r\n        out = createAbfsOutputStreamWithFlushEnabled(fs, testFilePath);\r\n        out.write('a');\r\n        out.hflush();\r\n        in = fs.getAbfsStore().openFileForRead(testFilePath, fs.getFsStatistics(), getTestTracingContext(fs, false));\r\n        int res = in.read();\r\n        LOG.info(\"Result of Read: {}\", res);\r\n        fs.delete(testFilePath, false);\r\n        IOStatistics ioStatistics = extractStatistics(fs);\r\n        LOG.info(ioStatisticsToPrettyString(ioStatistics));\r\n        assertDurationTracker(ioStatistics);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out, in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertDurationTracker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertDurationTracker(IOStatistics ioStatistics)\n{\r\n    for (AbfsStatistic abfsStatistic : HTTP_DURATION_TRACKER_LIST) {\r\n        Assertions.assertThat(lookupMeanStatistic(ioStatistics, abfsStatistic.getStatName() + StoreStatisticNames.SUFFIX_MEAN).mean()).describedAs(\"The DurationTracker Named \" + abfsStatistic.getStatName() + \" Doesn't match the expected value.\").isGreaterThan(0.0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBasicRead",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBasicRead() throws Exception\n{\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testBasicRead\");\r\n    assumeHugeFileExists(testPath);\r\n    try (FSDataInputStream inputStream = this.getFileSystem().open(testPath)) {\r\n        byte[] buffer = new byte[3 * MEGABYTE];\r\n        inputStream.seek(5 * MEGABYTE);\r\n        int numBytesRead = inputStream.read(buffer, 0, KILOBYTE);\r\n        assertEquals(\"Wrong number of bytes read\", KILOBYTE, numBytesRead);\r\n        int len = MEGABYTE;\r\n        int offset = buffer.length - len;\r\n        inputStream.seek(3 * MEGABYTE);\r\n        numBytesRead = inputStream.read(buffer, offset, len);\r\n        assertEquals(\"Wrong number of bytes read after seek\", len, numBytesRead);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRandomRead",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testRandomRead() throws Exception\n{\r\n    Assume.assumeFalse(\"This test does not support namespace enabled account\", getIsNamespaceEnabled(getFileSystem()));\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testRandomRead\");\r\n    assumeHugeFileExists(testPath);\r\n    try (FSDataInputStream inputStreamV1 = this.getFileSystem().open(testPath);\r\n        FSDataInputStream inputStreamV2 = this.getWasbFileSystem().open(testPath)) {\r\n        final int bufferSize = 4 * KILOBYTE;\r\n        byte[] bufferV1 = new byte[bufferSize];\r\n        byte[] bufferV2 = new byte[bufferV1.length];\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        inputStreamV1.seek(0);\r\n        inputStreamV2.seek(0);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        inputStreamV1.seek(SEEK_POSITION_ONE);\r\n        inputStreamV2.seek(SEEK_POSITION_ONE);\r\n        inputStreamV1.seek(0);\r\n        inputStreamV2.seek(0);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        inputStreamV1.seek(SEEK_POSITION_TWO);\r\n        inputStreamV2.seek(SEEK_POSITION_TWO);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        inputStreamV1.seek(SEEK_POSITION_THREE);\r\n        inputStreamV2.seek(SEEK_POSITION_THREE);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n        inputStreamV1.seek(SEEK_POSITION_FOUR);\r\n        inputStreamV2.seek(SEEK_POSITION_FOUR);\r\n        verifyConsistentReads(inputStreamV1, inputStreamV2, bufferV1, bufferV2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSeekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSeekToNewSource() throws Exception\n{\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testSeekToNewSource\");\r\n    assumeHugeFileExists(testPath);\r\n    try (FSDataInputStream inputStream = this.getFileSystem().open(testPath)) {\r\n        assertFalse(inputStream.seekToNewSource(0));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSkipBounds",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSkipBounds() throws Exception\n{\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testSkipBounds\");\r\n    long testFileLength = assumeHugeFileExists(testPath);\r\n    try (FSDataInputStream inputStream = this.getFileSystem().open(testPath)) {\r\n        ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n        long skipped = inputStream.skip(-1);\r\n        assertEquals(0, skipped);\r\n        skipped = inputStream.skip(0);\r\n        assertEquals(0, skipped);\r\n        assertTrue(testFileLength > 0);\r\n        skipped = inputStream.skip(testFileLength);\r\n        assertEquals(testFileLength, skipped);\r\n        intercept(EOFException.class, new Callable<Long>() {\r\n\r\n            @Override\r\n            public Long call() throws Exception {\r\n                return inputStream.skip(1);\r\n            }\r\n        });\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        assertTrue(String.format(\"There should not be any network I/O (elapsedTimeMs=%1$d).\", elapsedTimeMs), elapsedTimeMs < MAX_ELAPSEDTIMEMS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testValidateSeekBounds",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testValidateSeekBounds() throws Exception\n{\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testValidateSeekBounds\");\r\n    long testFileLength = assumeHugeFileExists(testPath);\r\n    try (FSDataInputStream inputStream = this.getFileSystem().open(testPath)) {\r\n        ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n        inputStream.seek(0);\r\n        assertEquals(0, inputStream.getPos());\r\n        intercept(EOFException.class, FSExceptionMessages.NEGATIVE_SEEK, new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.seek(-1);\r\n                return inputStream;\r\n            }\r\n        });\r\n        assertTrue(\"Test file length only \" + testFileLength, testFileLength > 0);\r\n        inputStream.seek(testFileLength);\r\n        assertEquals(testFileLength, inputStream.getPos());\r\n        intercept(EOFException.class, FSExceptionMessages.CANNOT_SEEK_PAST_EOF, new Callable<FSDataInputStream>() {\r\n\r\n            @Override\r\n            public FSDataInputStream call() throws Exception {\r\n                inputStream.seek(testFileLength + 1);\r\n                return inputStream;\r\n            }\r\n        });\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        assertTrue(String.format(\"There should not be any network I/O (elapsedTimeMs=%1$d).\", elapsedTimeMs), elapsedTimeMs < MAX_ELAPSEDTIMEMS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSeekAndAvailableAndPosition",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testSeekAndAvailableAndPosition() throws Exception\n{\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testSeekAndAvailableAndPosition\");\r\n    long testFileLength = assumeHugeFileExists(testPath);\r\n    try (FSDataInputStream inputStream = this.getFileSystem().open(testPath)) {\r\n        byte[] expected1 = { (byte) 'a', (byte) 'b', (byte) 'c' };\r\n        byte[] expected2 = { (byte) 'd', (byte) 'e', (byte) 'f' };\r\n        byte[] expected3 = { (byte) 'b', (byte) 'c', (byte) 'd' };\r\n        byte[] expected4 = { (byte) 'g', (byte) 'h', (byte) 'i' };\r\n        byte[] buffer = new byte[3];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected1, buffer);\r\n        assertEquals(buffer.length, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected2, buffer);\r\n        assertEquals(2 * buffer.length, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        int seekPos = 0;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected1, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        seekPos = 1;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected3, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        seekPos = 6;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected4, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSkipAndAvailableAndPosition",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testSkipAndAvailableAndPosition() throws Exception\n{\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testSkipAndAvailableAndPosition\");\r\n    long testFileLength = assumeHugeFileExists(testPath);\r\n    try (FSDataInputStream inputStream = this.getFileSystem().open(testPath)) {\r\n        byte[] expected1 = { (byte) 'a', (byte) 'b', (byte) 'c' };\r\n        byte[] expected2 = { (byte) 'd', (byte) 'e', (byte) 'f' };\r\n        byte[] expected3 = { (byte) 'b', (byte) 'c', (byte) 'd' };\r\n        byte[] expected4 = { (byte) 'g', (byte) 'h', (byte) 'i' };\r\n        assertEquals(testFileLength, inputStream.available());\r\n        assertEquals(0, inputStream.getPos());\r\n        int n = 3;\r\n        long skipped = inputStream.skip(n);\r\n        assertEquals(skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        assertEquals(skipped, n);\r\n        byte[] buffer = new byte[3];\r\n        int bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected2, buffer);\r\n        assertEquals(buffer.length + skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        int seekPos = 1;\r\n        inputStream.seek(seekPos);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected3, buffer);\r\n        assertEquals(buffer.length + seekPos, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        long currentPosition = inputStream.getPos();\r\n        n = 2;\r\n        skipped = inputStream.skip(n);\r\n        assertEquals(currentPosition + skipped, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n        assertEquals(skipped, n);\r\n        bytesRead = inputStream.read(buffer);\r\n        assertEquals(buffer.length, bytesRead);\r\n        assertArrayEquals(expected4, buffer);\r\n        assertEquals(buffer.length + skipped + currentPosition, inputStream.getPos());\r\n        assertEquals(testFileLength - inputStream.getPos(), inputStream.available());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSequentialReadAfterReverseSeekPerformance",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSequentialReadAfterReverseSeekPerformance() throws Exception\n{\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testSequentialReadAfterReverseSeekPerformance\");\r\n    assumeHugeFileExists(testPath);\r\n    final int maxAttempts = 10;\r\n    final double maxAcceptableRatio = 1.01;\r\n    double beforeSeekElapsedMs = 0, afterSeekElapsedMs = 0;\r\n    double ratio = Double.MAX_VALUE;\r\n    for (int i = 0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {\r\n        beforeSeekElapsedMs = sequentialRead(ABFS, testPath, this.getFileSystem(), false);\r\n        afterSeekElapsedMs = sequentialRead(ABFS, testPath, this.getFileSystem(), true);\r\n        ratio = afterSeekElapsedMs / beforeSeekElapsedMs;\r\n        LOG.info((String.format(\"beforeSeekElapsedMs=%1$d, afterSeekElapsedMs=%2$d, ratio=%3$.2f\", (long) beforeSeekElapsedMs, (long) afterSeekElapsedMs, ratio)));\r\n    }\r\n    assertTrue(String.format(\"Performance of ABFS stream after reverse seek is not acceptable:\" + \" beforeSeekElapsedMs=%1$d, afterSeekElapsedMs=%2$d,\" + \" ratio=%3$.2f\", (long) beforeSeekElapsedMs, (long) afterSeekElapsedMs, ratio), ratio < maxAcceptableRatio);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRandomReadPerformance",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRandomReadPerformance() throws Exception\n{\r\n    Assume.assumeFalse(\"This test does not support namespace enabled account\", getIsNamespaceEnabled(getFileSystem()));\r\n    Path testPath = path(TEST_FILE_PREFIX + \"_testRandomReadPerformance\");\r\n    assumeHugeFileExists(testPath);\r\n    final AzureBlobFileSystem abFs = this.getFileSystem();\r\n    final NativeAzureFileSystem wasbFs = this.getWasbFileSystem();\r\n    final int maxAttempts = 10;\r\n    final double maxAcceptableRatio = 1.025;\r\n    double v1ElapsedMs = 0, v2ElapsedMs = 0;\r\n    double ratio = Double.MAX_VALUE;\r\n    for (int i = 0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {\r\n        v1ElapsedMs = randomRead(1, testPath, wasbFs);\r\n        v2ElapsedMs = randomRead(2, testPath, abFs);\r\n        ratio = v2ElapsedMs / v1ElapsedMs;\r\n        LOG.info(String.format(\"v1ElapsedMs=%1$d, v2ElapsedMs=%2$d, ratio=%3$.2f\", (long) v1ElapsedMs, (long) v2ElapsedMs, ratio));\r\n    }\r\n    assertTrue(String.format(\"Performance of version 2 is not acceptable: v1ElapsedMs=%1$d,\" + \" v2ElapsedMs=%2$d, ratio=%3$.2f\", (long) v1ElapsedMs, (long) v2ElapsedMs, ratio), ratio < maxAcceptableRatio);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAlwaysReadBufferSizeConfig",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAlwaysReadBufferSizeConfig() throws Throwable\n{\r\n    testAlwaysReadBufferSizeConfig(false);\r\n    testAlwaysReadBufferSizeConfig(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAlwaysReadBufferSizeConfig",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testAlwaysReadBufferSizeConfig(boolean alwaysReadBufferSizeConfigValue) throws Throwable\n{\r\n    final AzureBlobFileSystem currentFs = getFileSystem();\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(\"fs.azure.readaheadqueue.depth\", \"0\");\r\n    config.set(\"fs.azure.read.alwaysReadBufferSize\", Boolean.toString(alwaysReadBufferSizeConfigValue));\r\n    final Path testFile = new Path(\"/FileName_\" + UUID.randomUUID().toString());\r\n    final AzureBlobFileSystem fs = createTestFile(testFile, 16 * MEGABYTE, 1 * MEGABYTE, config);\r\n    String eTag = fs.getAbfsClient().getPathStatus(testFile.toUri().getPath(), false, getTestTracingContext(fs, false)).getResult().getResponseHeader(ETAG);\r\n    TestAbfsInputStream testInputStream = new TestAbfsInputStream();\r\n    AbfsInputStream inputStream = testInputStream.getAbfsInputStream(fs.getAbfsClient(), testFile.getName(), ALWAYS_READ_BUFFER_SIZE_TEST_FILE_SIZE, eTag, DISABLED_READAHEAD_DEPTH, FOUR_MB, alwaysReadBufferSizeConfigValue, FOUR_MB);\r\n    long connectionsAtStart = fs.getInstrumentationMap().get(GET_RESPONSES.getStatName());\r\n    long dateSizeReadStatAtStart = fs.getInstrumentationMap().get(BYTES_RECEIVED.getStatName());\r\n    long newReqCount = 0;\r\n    long newDataSizeRead = 0;\r\n    byte[] buffer20b = new byte[TWENTY_BYTES];\r\n    byte[] buffer30b = new byte[THIRTY_BYTES];\r\n    byte[] byteBuffer5 = new byte[FIVE_BYTES];\r\n    inputStream.read(byteBuffer5, 0, FIVE_BYTES);\r\n    newReqCount++;\r\n    newDataSizeRead += FOUR_MB;\r\n    assertAbfsStatistics(GET_RESPONSES, connectionsAtStart + newReqCount, fs.getInstrumentationMap());\r\n    assertAbfsStatistics(BYTES_RECEIVED, dateSizeReadStatAtStart + newDataSizeRead, fs.getInstrumentationMap());\r\n    inputStream.seek(NINE_MB);\r\n    inputStream.read(buffer20b, 0, BYTE);\r\n    newReqCount++;\r\n    if (alwaysReadBufferSizeConfigValue) {\r\n        newDataSizeRead += FOUR_MB;\r\n    } else {\r\n        newDataSizeRead += TWENTY_BYTES;\r\n    }\r\n    assertAbfsStatistics(GET_RESPONSES, connectionsAtStart + newReqCount, fs.getInstrumentationMap());\r\n    assertAbfsStatistics(BYTES_RECEIVED, dateSizeReadStatAtStart + newDataSizeRead, fs.getInstrumentationMap());\r\n    inputStream.seek(NINE_MB + TWENTY_BYTES + THREE_BYTES);\r\n    inputStream.read(buffer30b, 0, THREE_BYTES);\r\n    if (!alwaysReadBufferSizeConfigValue) {\r\n        newReqCount++;\r\n        newDataSizeRead += THIRTY_BYTES;\r\n    }\r\n    assertAbfsStatistics(GET_RESPONSES, connectionsAtStart + newReqCount, fs.getInstrumentationMap());\r\n    assertAbfsStatistics(BYTES_RECEIVED, dateSizeReadStatAtStart + newDataSizeRead, fs.getInstrumentationMap());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "sequentialRead",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "long sequentialRead(String version, Path testPath, FileSystem fs, boolean afterReverseSeek) throws IOException\n{\r\n    byte[] buffer = new byte[SEQUENTIAL_READ_BUFFER_SIZE];\r\n    long totalBytesRead = 0;\r\n    long bytesRead = 0;\r\n    long testFileLength = fs.getFileStatus(testPath).getLen();\r\n    try (FSDataInputStream inputStream = fs.open(testPath)) {\r\n        if (afterReverseSeek) {\r\n            while (bytesRead > 0 && totalBytesRead < 4 * MEGABYTE) {\r\n                bytesRead = inputStream.read(buffer);\r\n                totalBytesRead += bytesRead;\r\n            }\r\n            totalBytesRead = 0;\r\n            inputStream.seek(0);\r\n        }\r\n        ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n        while ((bytesRead = inputStream.read(buffer)) > 0) {\r\n            totalBytesRead += bytesRead;\r\n        }\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        LOG.info(String.format(\"v%1$s: bytesRead=%2$d, elapsedMs=%3$d, Mbps=%4$.2f,\" + \" afterReverseSeek=%5$s\", version, totalBytesRead, elapsedTimeMs, toMbps(totalBytesRead, elapsedTimeMs), afterReverseSeek));\r\n        assertEquals(testFileLength, totalBytesRead);\r\n        inputStream.close();\r\n        return elapsedTimeMs;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "randomRead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "long randomRead(int version, Path testPath, FileSystem fs) throws Exception\n{\r\n    assumeHugeFileExists(testPath);\r\n    final long minBytesToRead = 2 * MEGABYTE;\r\n    Random random = new Random();\r\n    byte[] buffer = new byte[8 * KILOBYTE];\r\n    long totalBytesRead = 0;\r\n    long bytesRead = 0;\r\n    try (FSDataInputStream inputStream = fs.open(testPath)) {\r\n        ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n        do {\r\n            bytesRead = inputStream.read(buffer);\r\n            totalBytesRead += bytesRead;\r\n            inputStream.seek(random.nextInt((int) (TEST_FILE_SIZE - buffer.length)));\r\n        } while (bytesRead > 0 && totalBytesRead < minBytesToRead);\r\n        long elapsedTimeMs = timer.elapsedTimeMs();\r\n        inputStream.close();\r\n        LOG.info(String.format(\"v%1$d: totalBytesRead=%2$d, elapsedTimeMs=%3$d, Mbps=%4$.2f\", version, totalBytesRead, elapsedTimeMs, toMbps(totalBytesRead, elapsedTimeMs)));\r\n        assertTrue(minBytesToRead <= totalBytesRead);\r\n        return elapsedTimeMs;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "toMbps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double toMbps(long bytes, long milliseconds)\n{\r\n    return bytes / 1000.0 * 8 / milliseconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createTestFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long createTestFile(Path testPath) throws Exception\n{\r\n    createTestFile(testPath, TEST_FILE_SIZE, MEGABYTE, null);\r\n    return TEST_FILE_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createTestFile",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "AzureBlobFileSystem createTestFile(Path testFilePath, long testFileSize, int createBufferSize, Configuration config) throws Exception\n{\r\n    AzureBlobFileSystem fs;\r\n    if (config == null) {\r\n        config = this.getRawConfiguration();\r\n    }\r\n    final AzureBlobFileSystem currentFs = getFileSystem();\r\n    fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config);\r\n    if (fs.exists(testFilePath)) {\r\n        FileStatus status = fs.getFileStatus(testFilePath);\r\n        if (status.getLen() == testFileSize) {\r\n            return fs;\r\n        }\r\n    }\r\n    byte[] buffer = new byte[createBufferSize];\r\n    char character = 'a';\r\n    for (int i = 0; i < buffer.length; i++) {\r\n        buffer[i] = (byte) character;\r\n        character = (character == 'z') ? 'a' : (char) ((int) character + 1);\r\n    }\r\n    LOG.info(String.format(\"Creating test file %s of size: %d \", testFilePath, testFileSize));\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    try (FSDataOutputStream outputStream = fs.create(testFilePath)) {\r\n        String bufferContents = new String(buffer);\r\n        int bytesWritten = 0;\r\n        while (bytesWritten < testFileSize) {\r\n            outputStream.write(buffer);\r\n            bytesWritten += buffer.length;\r\n        }\r\n        LOG.info(\"Closing stream {}\", outputStream);\r\n        ContractTestUtils.NanoTimer closeTimer = new ContractTestUtils.NanoTimer();\r\n        outputStream.close();\r\n        closeTimer.end(\"time to close() output stream\");\r\n    }\r\n    timer.end(\"time to write %d KB\", testFileSize / 1024);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assumeHugeFileExists",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long assumeHugeFileExists(Path testPath) throws Exception\n{\r\n    long fileSize = createTestFile(testPath);\r\n    FileSystem fs = this.getFileSystem();\r\n    ContractTestUtils.assertPathExists(this.getFileSystem(), \"huge file not created\", testPath);\r\n    FileStatus status = fs.getFileStatus(testPath);\r\n    ContractTestUtils.assertIsFile(testPath, status);\r\n    assertTrue(\"File \" + testPath + \" is not of expected size \" + fileSize + \":actual=\" + status.getLen(), status.getLen() == fileSize);\r\n    return fileSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "verifyConsistentReads",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyConsistentReads(FSDataInputStream inputStreamV1, FSDataInputStream inputStreamV2, byte[] bufferV1, byte[] bufferV2) throws IOException\n{\r\n    int size = bufferV1.length;\r\n    final int numBytesReadV1 = inputStreamV1.read(bufferV1, 0, size);\r\n    assertEquals(\"Bytes read from wasb stream\", size, numBytesReadV1);\r\n    final int numBytesReadV2 = inputStreamV2.read(bufferV2, 0, size);\r\n    assertEquals(\"Bytes read from abfs stream\", size, numBytesReadV2);\r\n    assertArrayEquals(\"Mismatch in read data\", bufferV1, bufferV2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testXNSAccount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testXNSAccount() throws IOException\n{\r\n    Assume.assumeTrue(\"Skip this test because the account being used for test is a non XNS account\", isUsingXNSAccount);\r\n    assertTrue(\"Expecting getIsNamespaceEnabled() return true\", getIsNamespaceEnabled(getFileSystem()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testNonXNSAccount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNonXNSAccount() throws IOException\n{\r\n    Assume.assumeFalse(\"Skip this test because the account being used for test is a XNS account\", isUsingXNSAccount);\r\n    assertFalse(\"Expecting getIsNamespaceEnabled() return false\", getIsNamespaceEnabled(getFileSystem()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetIsNamespaceEnabledWhenConfigIsTrue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetIsNamespaceEnabledWhenConfigIsTrue() throws Exception\n{\r\n    AzureBlobFileSystem fs = getNewFSWithHnsConf(TRUE_STR);\r\n    Assertions.assertThat(getIsNamespaceEnabled(fs)).describedAs(\"getIsNamespaceEnabled should return true when the \" + \"config is set as true\").isTrue();\r\n    fs.getAbfsStore().deleteFilesystem(getTestTracingContext(fs, false));\r\n    unsetAndAssert();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetIsNamespaceEnabledWhenConfigIsFalse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetIsNamespaceEnabledWhenConfigIsFalse() throws Exception\n{\r\n    AzureBlobFileSystem fs = getNewFSWithHnsConf(FALSE_STR);\r\n    Assertions.assertThat(getIsNamespaceEnabled(fs)).describedAs(\"getIsNamespaceEnabled should return false when the \" + \"config is set as false\").isFalse();\r\n    fs.getAbfsStore().deleteFilesystem(getTestTracingContext(fs, false));\r\n    unsetAndAssert();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "unsetAndAssert",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void unsetAndAssert() throws Exception\n{\r\n    AzureBlobFileSystem fs = getNewFSWithHnsConf(DEFAULT_FS_AZURE_ACCOUNT_IS_HNS_ENABLED);\r\n    boolean expectedValue = this.getConfiguration().getBoolean(FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\r\n    Assertions.assertThat(getIsNamespaceEnabled(fs)).describedAs(\"getIsNamespaceEnabled should return the value \" + \"configured for fs.azure.test.namespace.enabled\").isEqualTo(expectedValue);\r\n    fs.getAbfsStore().deleteFilesystem(getTestTracingContext(fs, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getNewFSWithHnsConf",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AzureBlobFileSystem getNewFSWithHnsConf(String isNamespaceEnabledAccount) throws Exception\n{\r\n    Configuration rawConfig = new Configuration();\r\n    rawConfig.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    rawConfig.set(FS_AZURE_ACCOUNT_IS_HNS_ENABLED, isNamespaceEnabledAccount);\r\n    rawConfig.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, true);\r\n    rawConfig.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, getNonExistingUrl());\r\n    return (AzureBlobFileSystem) FileSystem.get(rawConfig);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getNonExistingUrl",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getNonExistingUrl()\n{\r\n    String testUri = this.getTestUrl();\r\n    return getAbfsScheme() + \"://\" + UUID.randomUUID() + testUri.substring(testUri.indexOf(\"@\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFailedRequestWhenFSNotExist",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFailedRequestWhenFSNotExist() throws Exception\n{\r\n    AbfsConfiguration config = this.getConfiguration();\r\n    config.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, false);\r\n    String testUri = this.getTestUrl();\r\n    String nonExistingFsUrl = getAbfsScheme() + \"://\" + UUID.randomUUID() + testUri.substring(testUri.indexOf(\"@\"));\r\n    AzureBlobFileSystem fs = this.getFileSystem(nonExistingFsUrl);\r\n    intercept(FileNotFoundException.class, \"\\\"The specified filesystem does not exist.\\\", 404\", () -> {\r\n        fs.getFileStatus(new Path(\"/\"));\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureGetAclCallIsMadeOnceWhenConfigIsInvalid",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEnsureGetAclCallIsMadeOnceWhenConfigIsInvalid() throws Exception\n{\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n    ensureGetAclCallIsMadeOnceForInvalidConf(\" \");\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n    ensureGetAclCallIsMadeOnceForInvalidConf(\"Invalid conf\");\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureGetAclCallIsNeverMadeWhenConfigIsValid",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testEnsureGetAclCallIsNeverMadeWhenConfigIsValid() throws Exception\n{\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n    ensureGetAclCallIsNeverMadeForValidConf(FALSE_STR.toLowerCase());\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n    ensureGetAclCallIsNeverMadeForValidConf(FALSE_STR.toUpperCase());\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n    ensureGetAclCallIsNeverMadeForValidConf(TRUE_STR.toLowerCase());\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n    ensureGetAclCallIsNeverMadeForValidConf(TRUE_STR.toUpperCase());\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureGetAclCallIsMadeOnceWhenConfigIsNotPresent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testEnsureGetAclCallIsMadeOnceWhenConfigIsNotPresent() throws IOException\n{\r\n    unsetConfAndEnsureGetAclCallIsMadeOnce();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureGetAclCallIsMadeOnceForInvalidConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void ensureGetAclCallIsMadeOnceForInvalidConf(String invalidConf) throws Exception\n{\r\n    this.getFileSystem().getAbfsStore().setNamespaceEnabled(Trilean.getTrilean(invalidConf));\r\n    AbfsClient mockClient = callAbfsGetIsNamespaceEnabledAndReturnMockAbfsClient();\r\n    verify(mockClient, times(1)).getAclStatus(anyString(), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureGetAclCallIsNeverMadeForValidConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void ensureGetAclCallIsNeverMadeForValidConf(String validConf) throws Exception\n{\r\n    this.getFileSystem().getAbfsStore().setNamespaceEnabled(Trilean.getTrilean(validConf));\r\n    AbfsClient mockClient = callAbfsGetIsNamespaceEnabledAndReturnMockAbfsClient();\r\n    verify(mockClient, never()).getAclStatus(anyString(), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "unsetConfAndEnsureGetAclCallIsMadeOnce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void unsetConfAndEnsureGetAclCallIsMadeOnce() throws IOException\n{\r\n    this.getFileSystem().getAbfsStore().setNamespaceEnabled(Trilean.UNKNOWN);\r\n    AbfsClient mockClient = callAbfsGetIsNamespaceEnabledAndReturnMockAbfsClient();\r\n    verify(mockClient, times(1)).getAclStatus(anyString(), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "callAbfsGetIsNamespaceEnabledAndReturnMockAbfsClient",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AbfsClient callAbfsGetIsNamespaceEnabledAndReturnMockAbfsClient() throws IOException\n{\r\n    final AzureBlobFileSystem abfs = this.getFileSystem();\r\n    final AzureBlobFileSystemStore abfsStore = abfs.getAbfsStore();\r\n    final AbfsClient mockClient = mock(AbfsClient.class);\r\n    doReturn(mock(AbfsRestOperation.class)).when(mockClient).getAclStatus(anyString(), any(TracingContext.class));\r\n    abfsStore.setClient(mockClient);\r\n    getIsNamespaceEnabled(abfs);\r\n    return mockClient;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getUserAgentString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getUserAgentString(AbfsConfiguration config, boolean includeSSLProvider) throws IOException\n{\r\n    AbfsClientContext abfsClientContext = new AbfsClientContextBuilder().build();\r\n    AbfsClient client = new AbfsClient(new URL(\"https://azure.com\"), null, config, (AccessTokenProvider) null, abfsClientContext);\r\n    String sslProviderName = null;\r\n    if (includeSSLProvider) {\r\n        sslProviderName = DelegatingSSLSocketFactory.getDefaultFactory().getProviderName();\r\n    }\r\n    return client.initializeUserAgent(config, sslProviderName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifybBasicInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifybBasicInfo() throws Exception\n{\r\n    final Configuration configuration = new Configuration();\r\n    configuration.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    AbfsConfiguration abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    verifybBasicInfo(getUserAgentString(abfsConfiguration, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifybBasicInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifybBasicInfo(String userAgentStr)\n{\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string [\" + userAgentStr + \"] should be of the pattern: \" + this.userAgentStringPattern.pattern()).matches(this.userAgentStringPattern).describedAs(\"User-Agent string should contain java vendor\").contains(System.getProperty(JAVA_VENDOR).replaceAll(SINGLE_WHITE_SPACE, EMPTY_STRING)).describedAs(\"User-Agent string should contain java version\").contains(System.getProperty(JAVA_VERSION)).describedAs(\"User-Agent string should contain  OS name\").contains(System.getProperty(OS_NAME).replaceAll(SINGLE_WHITE_SPACE, EMPTY_STRING)).describedAs(\"User-Agent string should contain OS version\").contains(System.getProperty(OS_VERSION)).describedAs(\"User-Agent string should contain OS arch\").contains(System.getProperty(OS_ARCH));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyUserAgentPrefix",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyUserAgentPrefix() throws IOException, IllegalAccessException\n{\r\n    final Configuration configuration = new Configuration();\r\n    configuration.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    configuration.set(ConfigurationKeys.FS_AZURE_USER_AGENT_PREFIX_KEY, FS_AZURE_USER_AGENT_PREFIX);\r\n    AbfsConfiguration abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    String userAgentStr = getUserAgentString(abfsConfiguration, false);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should contain \" + FS_AZURE_USER_AGENT_PREFIX).contains(FS_AZURE_USER_AGENT_PREFIX);\r\n    configuration.unset(ConfigurationKeys.FS_AZURE_USER_AGENT_PREFIX_KEY);\r\n    abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    userAgentStr = getUserAgentString(abfsConfiguration, false);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should not contain \" + FS_AZURE_USER_AGENT_PREFIX).doesNotContain(FS_AZURE_USER_AGENT_PREFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyUserAgentWithoutSSLProvider",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void verifyUserAgentWithoutSSLProvider() throws Exception\n{\r\n    final Configuration configuration = new Configuration();\r\n    configuration.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    configuration.set(ConfigurationKeys.FS_AZURE_SSL_CHANNEL_MODE_KEY, DelegatingSSLSocketFactory.SSLChannelMode.Default_JSSE.name());\r\n    AbfsConfiguration abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    String userAgentStr = getUserAgentString(abfsConfiguration, true);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should contain sslProvider\").contains(DelegatingSSLSocketFactory.getDefaultFactory().getProviderName());\r\n    userAgentStr = getUserAgentString(abfsConfiguration, false);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should not contain sslProvider\").doesNotContain(DelegatingSSLSocketFactory.getDefaultFactory().getProviderName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyUserAgentClusterName",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyUserAgentClusterName() throws Exception\n{\r\n    final String clusterName = \"testClusterName\";\r\n    final Configuration configuration = new Configuration();\r\n    configuration.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    configuration.set(FS_AZURE_CLUSTER_NAME, clusterName);\r\n    AbfsConfiguration abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    String userAgentStr = getUserAgentString(abfsConfiguration, false);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should contain cluster name\").contains(clusterName);\r\n    configuration.unset(FS_AZURE_CLUSTER_NAME);\r\n    abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    userAgentStr = getUserAgentString(abfsConfiguration, false);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should not contain cluster name\").doesNotContain(clusterName).describedAs(\"User-Agent string should contain UNKNOWN as cluster name config is absent\").contains(DEFAULT_VALUE_UNKNOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyUserAgentClusterType",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyUserAgentClusterType() throws Exception\n{\r\n    final String clusterType = \"testClusterType\";\r\n    final Configuration configuration = new Configuration();\r\n    configuration.addResource(TEST_CONFIGURATION_FILE_NAME);\r\n    configuration.set(FS_AZURE_CLUSTER_TYPE, clusterType);\r\n    AbfsConfiguration abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    String userAgentStr = getUserAgentString(abfsConfiguration, false);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should contain cluster type\").contains(clusterType);\r\n    configuration.unset(FS_AZURE_CLUSTER_TYPE);\r\n    abfsConfiguration = new AbfsConfiguration(configuration, ACCOUNT_NAME);\r\n    userAgentStr = getUserAgentString(abfsConfiguration, false);\r\n    verifybBasicInfo(userAgentStr);\r\n    assertThat(userAgentStr).describedAs(\"User-Agent string should not contain cluster type\").doesNotContain(clusterType).describedAs(\"User-Agent string should contain UNKNOWN as cluster type config is absent\").contains(DEFAULT_VALUE_UNKNOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createTestClientFromCurrentContext",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AbfsClient createTestClientFromCurrentContext(AbfsClient baseAbfsClientInstance, AbfsConfiguration abfsConfig) throws IOException\n{\r\n    AuthType currentAuthType = abfsConfig.getAuthType(abfsConfig.getAccountName());\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", abfsConfig.getAccountName(), abfsConfig);\r\n    AbfsClientContext abfsClientContext = new AbfsClientContextBuilder().withAbfsPerfTracker(tracker).withExponentialRetryPolicy(new ExponentialRetryPolicy(abfsConfig.getMaxIoRetries())).build();\r\n    AbfsClient testClient = new AbfsClient(baseAbfsClientInstance.getBaseUrl(), (currentAuthType == AuthType.SharedKey ? new SharedKeyCredentials(abfsConfig.getAccountName().substring(0, abfsConfig.getAccountName().indexOf(DOT)), abfsConfig.getStorageAccountKey()) : null), abfsConfig, (currentAuthType == AuthType.OAuth ? abfsConfig.getTokenProvider() : null), abfsClientContext);\r\n    return testClient;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMockAbfsClient",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "AbfsClient getMockAbfsClient(AbfsClient baseAbfsClientInstance, AbfsConfiguration abfsConfig) throws Exception\n{\r\n    AuthType currentAuthType = abfsConfig.getAuthType(abfsConfig.getAccountName());\r\n    org.junit.Assume.assumeTrue((currentAuthType == AuthType.SharedKey) || (currentAuthType == AuthType.OAuth));\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", abfsConfig.getAccountName(), abfsConfig);\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    when(client.getAuthType()).thenReturn(currentAuthType);\r\n    when(client.getRetryPolicy()).thenReturn(new ExponentialRetryPolicy(1));\r\n    when(client.createDefaultUriQueryBuilder()).thenCallRealMethod();\r\n    when(client.createRequestUrl(any(), any())).thenCallRealMethod();\r\n    when(client.getAccessToken()).thenCallRealMethod();\r\n    when(client.getSharedKeyCredentials()).thenCallRealMethod();\r\n    when(client.createDefaultHeaders()).thenCallRealMethod();\r\n    client = TestAbfsClient.setAbfsClientField(client, \"abfsConfiguration\", abfsConfig);\r\n    client = TestAbfsClient.setAbfsClientField(client, \"baseUrl\", baseAbfsClientInstance.getBaseUrl());\r\n    if (currentAuthType == AuthType.SharedKey) {\r\n        client = TestAbfsClient.setAbfsClientField(client, \"sharedKeyCredentials\", new SharedKeyCredentials(abfsConfig.getAccountName().substring(0, abfsConfig.getAccountName().indexOf(DOT)), abfsConfig.getStorageAccountKey()));\r\n    } else {\r\n        client = TestAbfsClient.setAbfsClientField(client, \"tokenProvider\", abfsConfig.getTokenProvider());\r\n    }\r\n    String userAgent = \"APN/1.0 Azure Blob FS/3.4.0-SNAPSHOT (PrivateBuild \" + \"JavaJRE 1.8.0_252; Linux 5.3.0-59-generic/amd64; openssl-1.0; \" + \"UNKNOWN/UNKNOWN) MSFT\";\r\n    client = TestAbfsClient.setAbfsClientField(client, \"userAgent\", userAgent);\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setAbfsClientField",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AbfsClient setAbfsClientField(final AbfsClient client, final String fieldName, Object fieldObject) throws Exception\n{\r\n    Field field = AbfsClient.class.getDeclaredField(fieldName);\r\n    field.setAccessible(true);\r\n    Field modifiersField = Field.class.getDeclaredField(\"modifiers\");\r\n    modifiersField.setAccessible(true);\r\n    modifiersField.setInt(field, field.getModifiers() & ~java.lang.reflect.Modifier.FINAL);\r\n    field.set(client, fieldObject);\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTestUrl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "URL getTestUrl(AbfsClient client, String path) throws AzureBlobFileSystemException\n{\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = client.createDefaultUriQueryBuilder();\r\n    return client.createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTestRequestHeaders",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<AbfsHttpHeader> getTestRequestHeaders(AbfsClient client)\n{\r\n    return client.createDefaultHeaders();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRestOp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsRestOperation getRestOp(AbfsRestOperationType type, AbfsClient client, String method, URL url, List<AbfsHttpHeader> requestHeaders)\n{\r\n    return new AbfsRestOperation(type, client, method, url, requestHeaders);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return AzureTestConstants.SCALE_TEST_TIMEOUT_MILLIS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NativeAzureFileSystemContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    assumeScaleTestsEnabled(getContract().getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void init() throws IOException\n{\r\n    userMappingFile = tempDir.newFile(\"user-mapping.conf\");\r\n    groupMappingFile = tempDir.newFile(\"group-mapping.conf\");\r\n    FileUtils.writeStringToFile(userMappingFile, testUserDataLine1, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(userMappingFile, testUserDataLine2, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(userMappingFile, testUserDataLine3, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(userMappingFile, testUserDataLine4, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(userMappingFile, testUserDataLine5, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(userMappingFile, testUserDataLine6, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(userMappingFile, testUserDataLine7, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(userMappingFile, NEW_LINE, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(groupMappingFile, testGroupDataLine1, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(groupMappingFile, testGroupDataLine2, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(groupMappingFile, testGroupDataLine3, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(groupMappingFile, testGroupDataLine4, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(groupMappingFile, testGroupDataLine5, Charset.forName(\"UTF-8\"), true);\r\n    FileUtils.writeStringToFile(groupMappingFile, NEW_LINE, Charset.forName(\"UTF-8\"), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertUserLookup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertUserLookup(TextFileBasedIdentityHandler handler, String userInTest, String expectedUser) throws IOException\n{\r\n    String actualUser = handler.lookupForLocalUserIdentity(userInTest);\r\n    Assert.assertEquals(\"Wrong user identity for \", expectedUser, actualUser);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testLookupForUser",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLookupForUser() throws IOException\n{\r\n    TextFileBasedIdentityHandler handler = new TextFileBasedIdentityHandler(userMappingFile.getPath(), groupMappingFile.getPath());\r\n    assertUserLookup(handler, testUserDataLine3.split(\":\")[0], testUserDataLine3.split(\":\")[1]);\r\n    assertUserLookup(handler, \"bogusIdentity\", \"\");\r\n    assertUserLookup(handler, \"\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testLookupForUserFileNotFound",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLookupForUserFileNotFound() throws Exception\n{\r\n    TextFileBasedIdentityHandler handler = new TextFileBasedIdentityHandler(userMappingFile.getPath() + \".test\", groupMappingFile.getPath());\r\n    intercept(FileNotFoundException.class, \"FileNotFoundException\", () -> handler.lookupForLocalUserIdentity(testUserDataLine3.split(\":\")[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "assertGroupLookup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertGroupLookup(TextFileBasedIdentityHandler handler, String groupInTest, String expectedGroup) throws IOException\n{\r\n    String actualGroup = handler.lookupForLocalGroupIdentity(groupInTest);\r\n    Assert.assertEquals(\"Wrong group identity for \", expectedGroup, actualGroup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testLookupForGroup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLookupForGroup() throws IOException\n{\r\n    TextFileBasedIdentityHandler handler = new TextFileBasedIdentityHandler(userMappingFile.getPath(), groupMappingFile.getPath());\r\n    assertGroupLookup(handler, testGroupDataLine2.split(\":\")[0], testGroupDataLine2.split(\":\")[1]);\r\n    assertGroupLookup(handler, \"bogusIdentity\", \"\");\r\n    assertGroupLookup(handler, \"\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testLookupForGroupFileNotFound",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLookupForGroupFileNotFound() throws Exception\n{\r\n    TextFileBasedIdentityHandler handler = new TextFileBasedIdentityHandler(userMappingFile.getPath(), groupMappingFile.getPath() + \".test\");\r\n    intercept(FileNotFoundException.class, \"FileNotFoundException\", () -> handler.lookupForLocalGroupIdentity(testGroupDataLine2.split(\":\")[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n    binding.getFileSystem().delete(binding.getTestPath(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testModifyAclEntries() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.mkdirs(path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, USER, FOO, READ_EXECUTE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, READ_EXECUTE), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesOnlyAccess",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testModifyAclEntriesOnlyAccess() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO, READ_EXECUTE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, READ_EXECUTE), aclEntry(ACCESS, GROUP, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesOnlyDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testModifyAclEntriesOnlyDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, READ_EXECUTE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesMinimal",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testModifyAclEntriesMinimal() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO, READ_WRITE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, READ_WRITE), aclEntry(ACCESS, GROUP, READ) }, returned);\r\n    assertPermission(fs, (short) RW_RW);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesMinimalDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testModifyAclEntriesMinimalDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesCustomMask",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testModifyAclEntriesCustomMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, MASK, NONE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ) }, returned);\r\n    assertPermission(fs, (short) RW);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesStickyBit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testModifyAclEntriesStickyBit() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) 01750));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, USER, FOO, READ_EXECUTE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, READ_EXECUTE), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) 01750);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesPathNotFound",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testModifyAclEntriesPathNotFound() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesDefaultOnFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testModifyAclEntriesDefaultOnFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesWithDefaultMask",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testModifyAclEntriesWithDefaultMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, MASK, EXECUTE));\r\n    fs.setAcl(path, aclSpec);\r\n    List<AclEntry> modifyAclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, READ_WRITE));\r\n    fs.modifyAclEntries(path, modifyAclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, READ_WRITE), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesWithAccessMask",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testModifyAclEntriesWithAccessMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, MASK, EXECUTE));\r\n    fs.setAcl(path, aclSpec);\r\n    List<AclEntry> modifyAclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE));\r\n    fs.modifyAclEntries(path, modifyAclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, GROUP, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, (short) RW_X);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesWithDuplicateEntries",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testModifyAclEntriesWithDuplicateEntries() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, MASK, EXECUTE));\r\n    fs.setAcl(path, aclSpec);\r\n    List<AclEntry> modifyAclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, READ_WRITE));\r\n    fs.modifyAclEntries(path, modifyAclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntries",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRemoveAclEntries() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO), aclEntry(DEFAULT, USER, FOO));\r\n    fs.removeAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesOnlyAccess",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRemoveAclEntriesOnlyAccess() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, USER, BAR, READ_WRITE), aclEntry(ACCESS, GROUP, READ_WRITE), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO));\r\n    fs.removeAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, BAR, READ_WRITE), aclEntry(ACCESS, GROUP, READ_WRITE) }, returned);\r\n    assertPermission(fs, (short) RWX_RW);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesOnlyDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRemoveAclEntriesOnlyDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, USER, BAR, READ_EXECUTE));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO));\r\n    fs.removeAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, BAR, READ_EXECUTE), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesMinimal",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRemoveAclEntriesMinimal() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RWX_RW));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_WRITE), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO), aclEntry(ACCESS, MASK));\r\n    fs.removeAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) RWX_RW);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesMinimalDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRemoveAclEntriesMinimalDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO), aclEntry(ACCESS, MASK), aclEntry(DEFAULT, USER, FOO), aclEntry(DEFAULT, MASK));\r\n    fs.removeAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesStickyBit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRemoveAclEntriesStickyBit() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) 01750));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO), aclEntry(DEFAULT, USER, FOO));\r\n    fs.removeAclEntries(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) 01750);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesPathNotFound",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRemoveAclEntriesPathNotFound() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO));\r\n    fs.removeAclEntries(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesAccessMask",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRemoveAclEntriesAccessMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, MASK, EXECUTE), aclEntry(ACCESS, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeAclEntries(path, Lists.newArrayList(aclEntry(ACCESS, MASK, NONE)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesDefaultMask",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRemoveAclEntriesDefaultMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, MASK, EXECUTE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeAclEntries(path, Lists.newArrayList(aclEntry(DEFAULT, MASK, NONE)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesWithDuplicateEntries",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRemoveAclEntriesWithDuplicateEntries() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, MASK, EXECUTE));\r\n    fs.setAcl(path, aclSpec);\r\n    List<AclEntry> removeAclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, READ_WRITE), aclEntry(DEFAULT, USER, READ_WRITE));\r\n    fs.removeAclEntries(path, removeAclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoveDefaultAcl() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeDefaultAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, (short) RWX_RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAclOnlyAccess",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRemoveDefaultAclOnlyAccess() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeDefaultAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, (short) RWX_RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAclOnlyDefault",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoveDefaultAclOnlyDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeDefaultAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAclMinimal",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRemoveDefaultAclMinimal() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    fs.removeDefaultAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAclStickyBit",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoveDefaultAclStickyBit() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) 01750));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeDefaultAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, (short) STICKY_RWX_RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAclPathNotFound",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRemoveDefaultAclPathNotFound() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.removeDefaultAcl(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAcl",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoveAcl() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclMinimalAcl",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRemoveAclMinimalAcl() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    fs.removeAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) RW_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclStickyBit",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoveAclStickyBit() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) 01750));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) 01750);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclOnlyDefault",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoveAclOnlyDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.removeAcl(path);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclPathNotFound",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRemoveAclPathNotFound() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.removeAcl(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAcl",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetAcl() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclOnlyAccess",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetAclOnlyAccess() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ) }, returned);\r\n    assertPermission(fs, (short) RW_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclOnlyDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetAclOnlyDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclMinimal",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSetAclMinimal() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, (short) RW_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclMinimalDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetAclMinimalDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclCustomMask",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetAclCustomMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, MASK, ALL), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ) }, returned);\r\n    assertPermission(fs, (short) RW_RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclStickyBit",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetAclStickyBit() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) 01750));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) STICKY_RWX_RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclPathNotFound",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSetAclPathNotFound() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclDefaultOnFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSetAclDefaultOnFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclDoesNotChangeDefaultMask",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSetAclDoesNotChangeDefaultMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, MASK, EXECUTE));\r\n    fs.setAcl(path, aclSpec);\r\n    List<AclEntry> aclSpec2 = Lists.newArrayList(aclEntry(ACCESS, OTHER, READ_EXECUTE));\r\n    fs.setAcl(path, aclSpec2);\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX_RX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclWithDuplicateEntries",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetAclWithDuplicateEntries() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, MASK, EXECUTE), aclEntry(ACCESS, MASK, EXECUTE));\r\n    fs.setAcl(path, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermission",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetPermission() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RWX));\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionOnlyAccess",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSetPermissionOnlyAccess() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.create(path).close();\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW_R));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RW));\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, READ), aclEntry(ACCESS, GROUP, READ) }, returned);\r\n    assertPermission(fs, (short) RW);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionOnlyDefault",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetPermissionOnlyDefault() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, NONE), aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    fs.setPermission(path, FsPermission.createImmutable((short) RWX));\r\n    AclStatus s = fs.getAclStatus(path);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, (short) RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultAclNewFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDefaultAclNewFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    Path filePath = new Path(path, \"file1\");\r\n    fs.create(filePath).close();\r\n    AclStatus s = fs.getAclStatus(filePath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, filePath, (short) RW_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOnlyAccessAclNewFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testOnlyAccessAclNewFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO, ALL));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    Path filePath = new Path(path, \"file1\");\r\n    fs.create(filePath).close();\r\n    AclStatus s = fs.getAclStatus(filePath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, filePath, (short) RW_R_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultMinimalAclNewFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDefaultMinimalAclNewFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    Path filePath = new Path(path, \"file1\");\r\n    fs.create(filePath).close();\r\n    AclStatus s = fs.getAclStatus(filePath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, filePath, (short) RW_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultAclNewDir",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDefaultAclNewDir() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    Path dirPath = new Path(path, \"dir1\");\r\n    fs.mkdirs(dirPath);\r\n    AclStatus s = fs.getAclStatus(dirPath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, dirPath, (short) RWX_RWX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOnlyAccessAclNewDir",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testOnlyAccessAclNewDir() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(ACCESS, USER, FOO, ALL));\r\n    fs.modifyAclEntries(path, aclSpec);\r\n    Path dirPath = new Path(path, \"dir1\");\r\n    fs.mkdirs(dirPath);\r\n    AclStatus s = fs.getAclStatus(dirPath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] {}, returned);\r\n    assertPermission(fs, dirPath, (short) RWX_RX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultMinimalAclNewDir",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDefaultMinimalAclNewDir() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE));\r\n    fs.setAcl(path, aclSpec);\r\n    Path dirPath = new Path(path, \"dir1\");\r\n    fs.mkdirs(dirPath);\r\n    AclStatus s = fs.getAclStatus(dirPath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, OTHER, NONE) }, returned);\r\n    assertPermission(fs, dirPath, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultAclNewFileWithMode",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDefaultAclNewFileWithMode() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    Path filePath = new Path(path, \"file1\");\r\n    int bufferSize = 4 * 1024 * 1024;\r\n    fs.create(filePath, new FsPermission((short) RWX_R), false, bufferSize, fs.getDefaultReplication(filePath), fs.getDefaultBlockSize(path), null).close();\r\n    AclStatus s = fs.getAclStatus(filePath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, filePath, (short) RWX_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultAclNewDirWithMode",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDefaultAclNewDirWithMode() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short) RWX_RX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(path, aclSpec);\r\n    Path dirPath = new Path(path, \"dir1\");\r\n    fs.mkdirs(dirPath, new FsPermission((short) RWX_R));\r\n    AclStatus s = fs.getAclStatus(dirPath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(new AclEntry[] { aclEntry(ACCESS, USER, FOO, ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, READ_EXECUTE) }, returned);\r\n    assertPermission(fs, dirPath, (short) RWX_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultAclRenamedFile",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testDefaultAclRenamedFile() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    Path dirPath = new Path(path, \"dir\");\r\n    FileSystem.mkdirs(fs, dirPath, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(dirPath, aclSpec);\r\n    Path filePath = new Path(path, \"file1\");\r\n    fs.create(filePath).close();\r\n    fs.setPermission(filePath, FsPermission.createImmutable((short) RW_R));\r\n    Path renamedFilePath = new Path(dirPath, \"file1\");\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.RENAME, true, 0));\r\n    fs.rename(filePath, renamedFilePath);\r\n    fs.registerListener(null);\r\n    AclEntry[] expected = new AclEntry[] {};\r\n    AclStatus s = fs.getAclStatus(renamedFilePath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(expected, returned);\r\n    assertPermission(fs, renamedFilePath, (short) RW_R);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDefaultAclRenamedDir",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDefaultAclRenamedDir() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    Path dirPath = new Path(path, \"dir\");\r\n    FileSystem.mkdirs(fs, dirPath, FsPermission.createImmutable((short) RWX_RX));\r\n    List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL));\r\n    fs.setAcl(dirPath, aclSpec);\r\n    Path subdirPath = new Path(path, \"subdir\");\r\n    FileSystem.mkdirs(fs, subdirPath, FsPermission.createImmutable((short) RWX_RX));\r\n    Path renamedSubdirPath = new Path(dirPath, \"subdir\");\r\n    fs.rename(subdirPath, renamedSubdirPath);\r\n    AclEntry[] expected = new AclEntry[] {};\r\n    AclStatus s = fs.getAclStatus(renamedSubdirPath);\r\n    AclEntry[] returned = s.getEntries().toArray(new AclEntry[0]);\r\n    assertArrayEquals(expected, returned);\r\n    assertPermission(fs, renamedSubdirPath, (short) RWX_RX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureAclOperationWorksForRoot",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testEnsureAclOperationWorksForRoot() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    assumeTrue(getIsNamespaceEnabled(fs));\r\n    Path rootPath = new Path(\"/\");\r\n    List<AclEntry> aclSpec1 = Lists.newArrayList(aclEntry(DEFAULT, GROUP, FOO, ALL), aclEntry(ACCESS, GROUP, BAR, ALL));\r\n    fs.registerListener(new TracingHeaderValidator(fs.getAbfsStore().getAbfsConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.SET_ACL, true, 0));\r\n    fs.setAcl(rootPath, aclSpec1);\r\n    fs.setListenerOperation(FSOperationType.GET_ACL_STATUS);\r\n    fs.getAclStatus(rootPath);\r\n    fs.setListenerOperation(FSOperationType.SET_OWNER);\r\n    fs.setOwner(rootPath, TEST_OWNER, TEST_GROUP);\r\n    fs.setListenerOperation(FSOperationType.SET_PERMISSION);\r\n    fs.setPermission(rootPath, new FsPermission(\"777\"));\r\n    List<AclEntry> aclSpec2 = Lists.newArrayList(aclEntry(DEFAULT, USER, FOO, ALL), aclEntry(ACCESS, USER, BAR, ALL));\r\n    fs.setListenerOperation(FSOperationType.MODIFY_ACL);\r\n    fs.modifyAclEntries(rootPath, aclSpec2);\r\n    fs.setListenerOperation(FSOperationType.REMOVE_ACL_ENTRIES);\r\n    fs.removeAclEntries(rootPath, aclSpec2);\r\n    fs.setListenerOperation(FSOperationType.REMOVE_DEFAULT_ACL);\r\n    fs.removeDefaultAcl(rootPath);\r\n    fs.setListenerOperation(FSOperationType.REMOVE_ACL);\r\n    fs.removeAcl(rootPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetOwnerForNonNamespaceEnabledAccount",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSetOwnerForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    assertPathExists(fs, \"This path should exist\", filePath);\r\n    TracingHeaderValidator tracingHeaderValidator = new TracingHeaderValidator(conf.getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.GET_FILESTATUS, false, 0);\r\n    fs.registerListener(tracingHeaderValidator);\r\n    FileStatus oldFileStatus = fs.getFileStatus(filePath);\r\n    tracingHeaderValidator.setOperation(FSOperationType.SET_OWNER);\r\n    fs.setOwner(filePath, TEST_OWNER, TEST_GROUP);\r\n    fs.registerListener(null);\r\n    FileStatus newFileStatus = fs.getFileStatus(filePath);\r\n    assertEquals(oldFileStatus.getOwner(), newFileStatus.getOwner());\r\n    assertEquals(oldFileStatus.getGroup(), newFileStatus.getGroup());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionForNonNamespaceEnabledAccount",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetPermissionForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    assertPathExists(fs, \"This path should exist\", filePath);\r\n    FsPermission oldPermission = fs.getFileStatus(filePath).getPermission();\r\n    FsPermission newPermission = new FsPermission(\"557\");\r\n    assertNotEquals(oldPermission, newPermission);\r\n    fs.setPermission(filePath, newPermission);\r\n    FsPermission updatedPermission = fs.getFileStatus(filePath).getPermission();\r\n    assertEquals(oldPermission, updatedPermission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testModifyAclEntriesForNonNamespaceEnabledAccount",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testModifyAclEntriesForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    try {\r\n        List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, GROUP, FOO, ALL), aclEntry(ACCESS, GROUP, BAR, ALL));\r\n        fs.modifyAclEntries(filePath, aclSpec);\r\n        assertFalse(\"UnsupportedOperationException is expected\", false);\r\n    } catch (UnsupportedOperationException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclEntriesEntriesForNonNamespaceEnabledAccount",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRemoveAclEntriesEntriesForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    try {\r\n        List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, GROUP, FOO, ALL), aclEntry(ACCESS, GROUP, BAR, ALL));\r\n        fs.removeAclEntries(filePath, aclSpec);\r\n        assertFalse(\"UnsupportedOperationException is expected\", false);\r\n    } catch (UnsupportedOperationException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveDefaultAclForNonNamespaceEnabledAccount",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRemoveDefaultAclForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    try {\r\n        fs.removeDefaultAcl(filePath);\r\n        assertFalse(\"UnsupportedOperationException is expected\", false);\r\n    } catch (UnsupportedOperationException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRemoveAclForNonNamespaceEnabledAccount",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRemoveAclForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    try {\r\n        fs.removeAcl(filePath);\r\n        assertFalse(\"UnsupportedOperationException is expected\", false);\r\n    } catch (UnsupportedOperationException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetAclForNonNamespaceEnabledAccount",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSetAclForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    try {\r\n        List<AclEntry> aclSpec = Lists.newArrayList(aclEntry(DEFAULT, GROUP, FOO, ALL), aclEntry(ACCESS, GROUP, BAR, ALL));\r\n        fs.setAcl(filePath, aclSpec);\r\n        assertFalse(\"UnsupportedOperationException is expected\", false);\r\n    } catch (UnsupportedOperationException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAclStatusForNonNamespaceEnabledAccount",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetAclStatusForNonNamespaceEnabledAccount() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\r\n    final Path filePath = new Path(methodName.getMethodName());\r\n    fs.create(filePath);\r\n    try {\r\n        AclStatus aclSpec = fs.getAclStatus(filePath);\r\n        assertFalse(\"UnsupportedOperationException is expected\", false);\r\n    } catch (UnsupportedOperationException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPermission(FileSystem fs, short perm) throws Exception\n{\r\n    assertPermission(fs, path, perm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPermission(FileSystem fs, Path pathToCheck, short perm) throws Exception\n{\r\n    AclTestHelpers.assertPermission(fs, pathToCheck, perm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMultipleRenameFileOperationsToSameDestination",
  "errType" : [ "InterruptedException", "IOException", "InterruptedException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testMultipleRenameFileOperationsToSameDestination() throws IOException, InterruptedException\n{\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    final AtomicInteger successfulRenameCount = new AtomicInteger(0);\r\n    final AtomicReference<IOException> unexpectedError = new AtomicReference<IOException>();\r\n    final Path dest = path(\"dest\");\r\n    List<Thread> threads = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        final int threadNumber = i;\r\n        Path src = path(\"test\" + threadNumber);\r\n        threads.add(new Thread(() -> {\r\n            try {\r\n                latch.await(Long.MAX_VALUE, TimeUnit.SECONDS);\r\n            } catch (InterruptedException e) {\r\n            }\r\n            try {\r\n                try (OutputStream output = fs.create(src)) {\r\n                    output.write((\"Source file number \" + threadNumber).getBytes());\r\n                }\r\n                if (fs.rename(src, dest)) {\r\n                    LOG.info(\"rename succeeded for thread \" + threadNumber);\r\n                    successfulRenameCount.incrementAndGet();\r\n                }\r\n            } catch (IOException e) {\r\n                unexpectedError.compareAndSet(null, e);\r\n                ContractTestUtils.fail(\"Exception unexpected\", e);\r\n            }\r\n        }));\r\n    }\r\n    threads.forEach(t -> t.start());\r\n    Thread.sleep(2000);\r\n    latch.countDown();\r\n    threads.forEach(t -> {\r\n        try {\r\n            t.join();\r\n        } catch (InterruptedException e) {\r\n        }\r\n    });\r\n    if (unexpectedError.get() != null) {\r\n        throw unexpectedError.get();\r\n    }\r\n    assertEquals(1, successfulRenameCount.get());\r\n    LOG.info(\"Success, only one rename operation succeeded!\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testLazyRenamePendingCanOverwriteExistingFile",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testLazyRenamePendingCanOverwriteExistingFile() throws Exception\n{\r\n    final String srcFile = \"srcFile\";\r\n    final String dstFile = \"dstFile\";\r\n    Path srcPath = path(srcFile);\r\n    FSDataOutputStream srcStream = fs.create(srcPath);\r\n    assertTrue(fs.exists(srcPath));\r\n    Path dstPath = path(dstFile);\r\n    FSDataOutputStream dstStream = fs.create(dstPath);\r\n    assertTrue(fs.exists(dstPath));\r\n    NativeAzureFileSystem nfs = fs;\r\n    final String fullSrcKey = nfs.pathToKey(nfs.makeAbsolute(srcPath));\r\n    final String fullDstKey = nfs.pathToKey(nfs.makeAbsolute(dstPath));\r\n    nfs.getStoreInterface().rename(fullSrcKey, fullDstKey, true, null);\r\n    assertTrue(fs.exists(dstPath));\r\n    assertFalse(fs.exists(srcPath));\r\n    IOUtils.cleanupWithLogger(null, srcStream);\r\n    IOUtils.cleanupWithLogger(null, dstStream);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testDeleteThrowsExceptionWithLeaseExistsErrorMessage",
  "errType" : [ "AzureException", "InterruptedException", "InterruptedException", "StorageException", "InterruptedException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testDeleteThrowsExceptionWithLeaseExistsErrorMessage() throws Exception\n{\r\n    LOG.info(\"Starting test\");\r\n    Path path = methodPath();\r\n    fs.create(path);\r\n    assertPathExists(\"test file\", path);\r\n    NativeAzureFileSystem nfs = fs;\r\n    final String fullKey = nfs.pathToKey(nfs.makeAbsolute(path));\r\n    final AzureNativeFileSystemStore store = nfs.getStore();\r\n    final CountDownLatch leaseAttemptComplete = new CountDownLatch(1);\r\n    final CountDownLatch beginningDeleteAttempt = new CountDownLatch(1);\r\n    Thread t = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            SelfRenewingLease lease = null;\r\n            try {\r\n                lease = store.acquireLease(fullKey);\r\n                LOG.info(\"Lease acquired: \" + lease.getLeaseID());\r\n            } catch (AzureException e) {\r\n                LOG.warn(\"Lease acqusition thread unable to acquire lease\", e);\r\n            } finally {\r\n                leaseAttemptComplete.countDown();\r\n            }\r\n            try {\r\n                beginningDeleteAttempt.await();\r\n            } catch (InterruptedException e) {\r\n                Thread.currentThread().interrupt();\r\n            }\r\n            try {\r\n                Thread.sleep(SelfRenewingLease.LEASE_ACQUIRE_RETRY_INTERVAL * 3);\r\n            } catch (InterruptedException ex) {\r\n                Thread.currentThread().interrupt();\r\n            }\r\n            try {\r\n                if (lease != null) {\r\n                    LOG.info(\"Freeing lease\");\r\n                    lease.free();\r\n                }\r\n            } catch (StorageException se) {\r\n                LOG.warn(\"Unable to free lease.\", se);\r\n            }\r\n        }\r\n    };\r\n    t.start();\r\n    try {\r\n        leaseAttemptComplete.await();\r\n    } catch (InterruptedException ex) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n    beginningDeleteAttempt.countDown();\r\n    store.delete(fullKey);\r\n    assertPathDoesNotExist(\"Leased path\", path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testIsPageBlobKey",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testIsPageBlobKey()\n{\r\n    AzureNativeFileSystemStore store = fs.getStore();\r\n    assertEquals(AzureBlobStorageTestAccount.DEFAULT_PAGE_BLOB_DIRECTORY, \"pageBlobs\");\r\n    String uriPrefix = \"file:///\";\r\n    String[] negativeKeys = { \"\", \"/\", \"bar\", \"bar/\", \"bar/pageBlobs\", \"bar/pageBlobs/foo\", \"bar/pageBlobs/foo/\", \"/pageBlobs/\", \"/pageBlobs\", \"pageBlobsxyz/\" };\r\n    for (String s : negativeKeys) {\r\n        assertFalse(store.isPageBlobKey(s));\r\n        assertFalse(store.isPageBlobKey(uriPrefix + s));\r\n    }\r\n    String[] positiveKeys = { \"pageBlobs/\", \"pageBlobs/foo/\", \"pageBlobs/foo/bar/\" };\r\n    for (String s : positiveKeys) {\r\n        assertTrue(store.isPageBlobKey(s));\r\n        assertTrue(store.isPageBlobKey(uriPrefix + s));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testIsAtomicRenameKey",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testIsAtomicRenameKey()\n{\r\n    AzureNativeFileSystemStore store = fs.getStore();\r\n    assertEquals(AzureBlobStorageTestAccount.DEFAULT_ATOMIC_RENAME_DIRECTORIES, \"/atomicRenameDir1,/atomicRenameDir2\");\r\n    String uriPrefix = \"file:///\";\r\n    String[] negativeKeys = { \"\", \"/\", \"bar\", \"bar/\", \"bar/hbase\", \"bar/hbase/foo\", \"bar/hbase/foo/\", \"/hbase/\", \"/hbase\", \"hbasexyz/\", \"foo/atomicRenameDir1/\" };\r\n    for (String s : negativeKeys) {\r\n        assertFalse(store.isAtomicRenameKey(s));\r\n        assertFalse(store.isAtomicRenameKey(uriPrefix + s));\r\n    }\r\n    String[] positiveKeys = { \"hbase/\", \"hbase/foo/\", \"hbase/foo/bar/\", \"atomicRenameDir1/foo/\", \"atomicRenameDir2/bar/\" };\r\n    for (String s : positiveKeys) {\r\n        assertTrue(store.isAtomicRenameKey(s));\r\n        assertTrue(store.isAtomicRenameKey(uriPrefix + s));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMkdirOnExistingFolderWithLease",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMkdirOnExistingFolderWithLease() throws Exception\n{\r\n    SelfRenewingLease lease;\r\n    Path path = methodPath();\r\n    fs.mkdirs(path);\r\n    NativeAzureFileSystem nfs = fs;\r\n    String fullKey = nfs.pathToKey(nfs.makeAbsolute(path));\r\n    AzureNativeFileSystemStore store = nfs.getStore();\r\n    lease = store.acquireLease(fullKey);\r\n    assertNotNull(\"lease ID\", lease.getLeaseID() != null);\r\n    store.storeEmptyFolder(fullKey, nfs.createPermissionStatus(FsPermission.getDirDefault()));\r\n    lease.free();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMockRestOp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AbfsRestOperation getMockRestOp()\n{\r\n    AbfsRestOperation op = mock(AbfsRestOperation.class);\r\n    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\r\n    when(httpOp.getBytesReceived()).thenReturn(1024L);\r\n    when(op.getResult()).thenReturn(httpOp);\r\n    when(op.getSasToken()).thenReturn(TestCachedSASToken.getTestCachedSASTokenInstance().get());\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMockAbfsClient",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AbfsClient getMockAbfsClient()\n{\r\n    AbfsClient client = mock(AbfsClient.class);\r\n    AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", this.getAccountName(), this.getConfiguration());\r\n    when(client.getAbfsPerfTracker()).thenReturn(tracker);\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsInputStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) throws IOException\n{\r\n    AbfsInputStreamContext inputStreamContext = new AbfsInputStreamContext(-1);\r\n    AbfsInputStream inputStream = new AbfsInputStream(mockAbfsClient, null, FORWARD_SLASH + fileName, THREE_KB, inputStreamContext.withReadBufferSize(ONE_KB).withReadAheadQueueDepth(10).withReadAheadBlockSize(ONE_KB), \"eTag\", getTestTracingContext(null, false));\r\n    inputStream.setCachedSasToken(TestCachedSASToken.getTestCachedSASTokenInstance());\r\n    return inputStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsInputStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AbfsInputStream getAbfsInputStream(AbfsClient abfsClient, String fileName, int fileSize, String eTag, int readAheadQueueDepth, int readBufferSize, boolean alwaysReadBufferSize, int readAheadBlockSize) throws IOException\n{\r\n    AbfsInputStreamContext inputStreamContext = new AbfsInputStreamContext(-1);\r\n    AbfsInputStream inputStream = new AbfsInputStream(abfsClient, null, FORWARD_SLASH + fileName, fileSize, inputStreamContext.withReadBufferSize(readBufferSize).withReadAheadQueueDepth(readAheadQueueDepth).withShouldReadBufferSizeAlways(alwaysReadBufferSize).withReadAheadBlockSize(readAheadBlockSize), eTag, getTestTracingContext(getFileSystem(), false));\r\n    inputStream.setCachedSasToken(TestCachedSASToken.getTestCachedSASTokenInstance());\r\n    return inputStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "queueReadAheads",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void queueReadAheads(AbfsInputStream inputStream)\n{\r\n    ReadBufferManager.getBufferManager().queueReadAhead(inputStream, 0, ONE_KB, inputStream.getTracingContext());\r\n    ReadBufferManager.getBufferManager().queueReadAhead(inputStream, ONE_KB, ONE_KB, inputStream.getTracingContext());\r\n    ReadBufferManager.getBufferManager().queueReadAhead(inputStream, TWO_KB, TWO_KB, inputStream.getTracingContext());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyReadCallCount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyReadCallCount(AbfsClient client, int count) throws AzureBlobFileSystemException, InterruptedException\n{\r\n    Thread.sleep(1000);\r\n    verify(client, times(count)).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "checkEvictedStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException) throws Exception\n{\r\n    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\r\n    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\r\n    while (numOfCompletedReadListItems > 0) {\r\n        ReadBufferManager.getBufferManager().callTryEvict();\r\n        numOfCompletedReadListItems--;\r\n    }\r\n    if (expectedToThrowException) {\r\n        intercept(IOException.class, () -> inputStream.read(position, new byte[ONE_KB], 0, ONE_KB));\r\n    } else {\r\n        inputStream.read(position, new byte[ONE_KB], 0, ONE_KB);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "writeBufferToNewFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeBufferToNewFile(Path testFile, byte[] buffer) throws IOException\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    fs.create(testFile);\r\n    FSDataOutputStream out = fs.append(testFile);\r\n    out.write(buffer);\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "verifyOpenWithProvidedStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyOpenWithProvidedStatus(Path path, FileStatus fileStatus, byte[] buf, AbfsRestOperationType source) throws IOException, ExecutionException, InterruptedException\n{\r\n    byte[] readBuf = new byte[buf.length];\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    FutureDataInputStreamBuilder builder = fs.openFile(path);\r\n    builder.withFileStatus(fileStatus);\r\n    FSDataInputStream in = builder.build().get();\r\n    assertEquals(String.format(\"Open with fileStatus [from %s result]: Incorrect number of bytes read\", source), buf.length, in.read(readBuf));\r\n    assertArrayEquals(String.format(\"Open with fileStatus [from %s result]: Incorrect read data\", source), readBuf, buf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "checkGetPathStatusCalls",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkGetPathStatusCalls(Path testFile, FileStatus fileStatus, AzureBlobFileSystemStore abfsStore, AbfsClient mockClient, AbfsRestOperationType source, TracingContext tracingContext) throws IOException\n{\r\n    abfsStore.openFileForRead(testFile, Optional.ofNullable(new OpenFileParameters().withStatus(fileStatus)), null, tracingContext);\r\n    verify(mockClient, times(0).description((String.format(\"FileStatus [from %s result] provided, GetFileStatus should not be invoked\", source)))).getPathStatus(anyString(), anyBoolean(), any(TracingContext.class));\r\n    abfsStore.openFileForRead(testFile, Optional.empty(), null, tracingContext);\r\n    verify(mockClient, times(1).description(\"GetPathStatus should be invoked when FileStatus not provided\")).getPathStatus(anyString(), anyBoolean(), any(TracingContext.class));\r\n    Mockito.reset(mockClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOpenFileWithOptions",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testOpenFileWithOptions() throws Exception\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    String testFolder = \"/testFolder\";\r\n    Path smallTestFile = new Path(testFolder + \"/testFile0\");\r\n    Path largeTestFile = new Path(testFolder + \"/testFile1\");\r\n    fs.mkdirs(new Path(testFolder));\r\n    int readBufferSize = getConfiguration().getReadBufferSize();\r\n    byte[] smallBuffer = new byte[5];\r\n    byte[] largeBuffer = new byte[readBufferSize + 5];\r\n    new Random().nextBytes(smallBuffer);\r\n    new Random().nextBytes(largeBuffer);\r\n    writeBufferToNewFile(smallTestFile, smallBuffer);\r\n    writeBufferToNewFile(largeTestFile, largeBuffer);\r\n    FileStatus[] getFileStatusResults = { fs.getFileStatus(smallTestFile), fs.getFileStatus(largeTestFile) };\r\n    FileStatus[] listStatusResults = fs.listStatus(new Path(testFolder));\r\n    verifyOpenWithProvidedStatus(smallTestFile, getFileStatusResults[0], smallBuffer, AbfsRestOperationType.GetPathStatus);\r\n    verifyOpenWithProvidedStatus(largeTestFile, getFileStatusResults[1], largeBuffer, AbfsRestOperationType.GetPathStatus);\r\n    verifyOpenWithProvidedStatus(smallTestFile, listStatusResults[0], smallBuffer, AbfsRestOperationType.ListPaths);\r\n    verifyOpenWithProvidedStatus(largeTestFile, listStatusResults[1], largeBuffer, AbfsRestOperationType.ListPaths);\r\n    AzureBlobFileSystemStore abfsStore = getAbfsStore(fs);\r\n    AbfsClient mockClient = spy(getAbfsClient(abfsStore));\r\n    setAbfsClient(abfsStore, mockClient);\r\n    TracingContext tracingContext = getTestTracingContext(fs, false);\r\n    checkGetPathStatusCalls(smallTestFile, getFileStatusResults[0], abfsStore, mockClient, AbfsRestOperationType.GetPathStatus, tracingContext);\r\n    checkGetPathStatusCalls(largeTestFile, getFileStatusResults[1], abfsStore, mockClient, AbfsRestOperationType.GetPathStatus, tracingContext);\r\n    checkGetPathStatusCalls(smallTestFile, listStatusResults[0], abfsStore, mockClient, AbfsRestOperationType.ListPaths, tracingContext);\r\n    checkGetPathStatusCalls(largeTestFile, listStatusResults[1], abfsStore, mockClient, AbfsRestOperationType.ListPaths, tracingContext);\r\n    getFileStatusResults[0].setPath(new Path(\"wrongPath\"));\r\n    intercept(ExecutionException.class, () -> verifyOpenWithProvidedStatus(smallTestFile, getFileStatusResults[0], smallBuffer, AbfsRestOperationType.GetPathStatus));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testFailedReadAhead",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testFailedReadAhead() throws Exception\n{\r\n    AbfsClient client = getMockAbfsClient();\r\n    AbfsRestOperation successOp = getMockRestOp();\r\n    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\")).doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\")).doReturn(successOp).when(client).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\r\n    intercept(IOException.class, () -> inputStream.read(new byte[ONE_KB]));\r\n    verifyReadCallCount(client, 3);\r\n    checkEvictedStatus(inputStream, 0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testFailedReadAheadEviction",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFailedReadAheadEviction() throws Exception\n{\r\n    AbfsClient client = getMockAbfsClient();\r\n    AbfsRestOperation successOp = getMockRestOp();\r\n    ReadBufferManager.setThresholdAgeMilliseconds(INCREASED_READ_BUFFER_AGE_THRESHOLD);\r\n    doThrow(new TimeoutException(\"Internal Server error\")).when(client).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAheadEviction.txt\");\r\n    ReadBuffer buff = new ReadBuffer();\r\n    buff.setStatus(ReadBufferStatus.READ_FAILED);\r\n    ReadBufferManager.getBufferManager().testMimicFullUseAndAddFailedBuffer(buff);\r\n    ReadBufferManager.getBufferManager().queueReadAhead(inputStream, 0, ONE_KB, getTestTracingContext(getFileSystem(), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOlderReadAheadFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testOlderReadAheadFailure() throws Exception\n{\r\n    AbfsClient client = getMockAbfsClient();\r\n    AbfsRestOperation successOp = getMockRestOp();\r\n    doThrow(new TimeoutException(\"Internal Server error for RAH-X\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Y\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Z\")).doReturn(successOp).doReturn(successOp).when(client).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\r\n    intercept(IOException.class, () -> inputStream.read(new byte[ONE_KB]));\r\n    verifyReadCallCount(client, 3);\r\n    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\r\n    inputStream.read(ONE_KB, new byte[ONE_KB], 0, ONE_KB);\r\n    verifyReadCallCount(client, 4);\r\n    checkEvictedStatus(inputStream, 0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testSuccessfulReadAhead",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSuccessfulReadAhead() throws Exception\n{\r\n    AbfsClient client = getMockAbfsClient();\r\n    AbfsRestOperation op = getMockRestOp();\r\n    doReturn(op).doReturn(op).doReturn(op).doThrow(new TimeoutException(\"Internal Server error for RAH-X\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Y\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Z\")).when(client).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\r\n    int beforeReadCompletedListSize = ReadBufferManager.getBufferManager().getCompletedReadListSize();\r\n    inputStream.read(new byte[ONE_KB]);\r\n    verifyReadCallCount(client, 3);\r\n    int newAdditionsToCompletedRead = ReadBufferManager.getBufferManager().getCompletedReadListSize() - beforeReadCompletedListSize;\r\n    Assertions.assertThat(newAdditionsToCompletedRead).describedAs(\"New additions to completed reads should be same or less than as number of readaheads\").isLessThanOrEqualTo(3);\r\n    inputStream.read(ONE_KB, new byte[ONE_KB], 0, ONE_KB);\r\n    verifyReadCallCount(client, 3);\r\n    checkEvictedStatus(inputStream, 0, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testReadAheadManagerForFailedReadAhead",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testReadAheadManagerForFailedReadAhead() throws Exception\n{\r\n    AbfsClient client = getMockAbfsClient();\r\n    AbfsRestOperation successOp = getMockRestOp();\r\n    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\")).doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\")).doReturn(successOp).when(client).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\r\n    queueReadAheads(inputStream);\r\n    Thread.sleep(1000);\r\n    intercept(IOException.class, () -> ReadBufferManager.getBufferManager().getBlock(inputStream, 0, ONE_KB, new byte[ONE_KB]));\r\n    verifyReadCallCount(client, 3);\r\n    checkEvictedStatus(inputStream, 0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testReadAheadManagerForOlderReadAheadFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testReadAheadManagerForOlderReadAheadFailure() throws Exception\n{\r\n    AbfsClient client = getMockAbfsClient();\r\n    AbfsRestOperation successOp = getMockRestOp();\r\n    doThrow(new TimeoutException(\"Internal Server error for RAH-X\")).doThrow(new TimeoutException(\"Internal Server error for RAH-X\")).doThrow(new TimeoutException(\"Internal Server error for RAH-X\")).doReturn(successOp).doReturn(successOp).when(client).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\r\n    queueReadAheads(inputStream);\r\n    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\r\n    verifyReadCallCount(client, 3);\r\n    int bytesRead = ReadBufferManager.getBufferManager().getBlock(inputStream, ONE_KB, ONE_KB, new byte[ONE_KB]);\r\n    Assert.assertEquals(\"bytesRead should be zero when previously read \" + \"ahead buffer had failed\", 0, bytesRead);\r\n    checkEvictedStatus(inputStream, 0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testReadAheadManagerForSuccessfulReadAhead",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testReadAheadManagerForSuccessfulReadAhead() throws Exception\n{\r\n    AbfsClient client = getMockAbfsClient();\r\n    AbfsRestOperation op = getMockRestOp();\r\n    doReturn(op).doReturn(op).doReturn(op).doThrow(new TimeoutException(\"Internal Server error for RAH-X\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Y\")).doThrow(new TimeoutException(\"Internal Server error for RAH-Z\")).when(client).read(any(String.class), any(Long.class), any(byte[].class), any(Integer.class), any(Integer.class), any(String.class), any(String.class), any(TracingContext.class));\r\n    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\r\n    queueReadAheads(inputStream);\r\n    Thread.sleep(1000);\r\n    verifyReadCallCount(client, 3);\r\n    int bytesRead = ReadBufferManager.getBufferManager().getBlock(inputStream, ONE_KB, ONE_KB, new byte[ONE_KB]);\r\n    Assert.assertTrue(\"bytesRead should be non-zero from the \" + \"buffer that was read-ahead\", bytesRead > 0);\r\n    verifyReadCallCount(client, 3);\r\n    checkEvictedStatus(inputStream, 0, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testDiffReadRequestSizeAndRAHBlockSize",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDiffReadRequestSizeAndRAHBlockSize() throws Exception\n{\r\n    resetReadBufferManager(FOUR_MB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\r\n    testReadAheadConfigs(FOUR_MB, TEST_READAHEAD_DEPTH_4, false, EIGHT_MB);\r\n    resetReadBufferManager(SIXTEEN_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\r\n    AbfsInputStream inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\r\n    testReadAheads(inputStream, SIXTEEN_KB, SIXTEEN_KB);\r\n    resetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\r\n    inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true, FORTY_EIGHT_KB);\r\n    testReadAheads(inputStream, SIXTEEN_KB, FORTY_EIGHT_KB);\r\n    resetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\r\n    inputStream = testReadAheadConfigs(FORTY_EIGHT_KB, TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\r\n    testReadAheads(inputStream, FORTY_EIGHT_KB, SIXTEEN_KB);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testDefaultReadaheadQueueDepth",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDefaultReadaheadQueueDepth() throws Exception\n{\r\n    Configuration config = getRawConfiguration();\r\n    config.unset(FS_AZURE_READ_AHEAD_QUEUE_DEPTH);\r\n    AzureBlobFileSystem fs = getFileSystem(config);\r\n    Path testFile = path(\"/testFile\");\r\n    fs.create(testFile).close();\r\n    FSDataInputStream in = fs.open(testFile);\r\n    Assertions.assertThat(((AbfsInputStream) in.getWrappedStream()).getReadAheadQueueDepth()).describedAs(\"readahead queue depth should be set to default value 2\").isEqualTo(2);\r\n    in.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testReadAheads",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testReadAheads(AbfsInputStream inputStream, int readRequestSize, int readAheadRequestSize) throws Exception\n{\r\n    if (readRequestSize > readAheadRequestSize) {\r\n        readAheadRequestSize = readRequestSize;\r\n    }\r\n    byte[] firstReadBuffer = new byte[readRequestSize];\r\n    byte[] secondReadBuffer = new byte[readAheadRequestSize];\r\n    byte[] expectedFirstReadAheadBufferContents = new byte[readRequestSize];\r\n    byte[] expectedSecondReadAheadBufferContents = new byte[readAheadRequestSize];\r\n    getExpectedBufferData(0, readRequestSize, expectedFirstReadAheadBufferContents);\r\n    getExpectedBufferData(readRequestSize, readAheadRequestSize, expectedSecondReadAheadBufferContents);\r\n    Assertions.assertThat(inputStream.read(firstReadBuffer, 0, readRequestSize)).describedAs(\"Read should be of exact requested size\").isEqualTo(readRequestSize);\r\n    assertTrue(\"Data mismatch found in RAH1\", Arrays.equals(firstReadBuffer, expectedFirstReadAheadBufferContents));\r\n    Assertions.assertThat(inputStream.read(secondReadBuffer, 0, readAheadRequestSize)).describedAs(\"Read should be of exact requested size\").isEqualTo(readAheadRequestSize);\r\n    assertTrue(\"Data mismatch found in RAH2\", Arrays.equals(secondReadBuffer, expectedSecondReadAheadBufferContents));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testReadAheadConfigs",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "AbfsInputStream testReadAheadConfigs(int readRequestSize, int readAheadQueueDepth, boolean alwaysReadBufferSizeEnabled, int readAheadBlockSize) throws Exception\n{\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(\"fs.azure.read.request.size\", Integer.toString(readRequestSize));\r\n    config.set(\"fs.azure.readaheadqueue.depth\", Integer.toString(readAheadQueueDepth));\r\n    config.set(\"fs.azure.read.alwaysReadBufferSize\", Boolean.toString(alwaysReadBufferSizeEnabled));\r\n    config.set(\"fs.azure.read.readahead.blocksize\", Integer.toString(readAheadBlockSize));\r\n    if (readRequestSize > readAheadBlockSize) {\r\n        readAheadBlockSize = readRequestSize;\r\n    }\r\n    Path testPath = path(\"/testReadAheadConfigs\");\r\n    final AzureBlobFileSystem fs = createTestFile(testPath, ALWAYS_READ_BUFFER_SIZE_TEST_FILE_SIZE, config);\r\n    byte[] byteBuffer = new byte[ONE_MB];\r\n    AbfsInputStream inputStream = this.getAbfsStore(fs).openFileForRead(testPath, null, getTestTracingContext(fs, false));\r\n    Assertions.assertThat(inputStream.getBufferSize()).describedAs(\"Unexpected AbfsInputStream buffer size\").isEqualTo(readRequestSize);\r\n    Assertions.assertThat(inputStream.getReadAheadQueueDepth()).describedAs(\"Unexpected ReadAhead queue depth\").isEqualTo(readAheadQueueDepth);\r\n    Assertions.assertThat(inputStream.shouldAlwaysReadBufferSize()).describedAs(\"Unexpected AlwaysReadBufferSize settings\").isEqualTo(alwaysReadBufferSizeEnabled);\r\n    Assertions.assertThat(ReadBufferManager.getBufferManager().getReadAheadBlockSize()).describedAs(\"Unexpected readAhead block size\").isEqualTo(readAheadBlockSize);\r\n    return inputStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getExpectedBufferData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void getExpectedBufferData(int offset, int length, byte[] b)\n{\r\n    boolean startFillingIn = false;\r\n    int indexIntoBuffer = 0;\r\n    char character = 'a';\r\n    for (int i = 0; i < (offset + length); i++) {\r\n        if (i == offset) {\r\n            startFillingIn = true;\r\n        }\r\n        if ((startFillingIn) && (indexIntoBuffer < length)) {\r\n            b[indexIntoBuffer] = (byte) character;\r\n            indexIntoBuffer++;\r\n        }\r\n        character = (character == 'z') ? 'a' : (char) ((int) character + 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createTestFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "AzureBlobFileSystem createTestFile(Path testFilePath, long testFileSize, Configuration config) throws Exception\n{\r\n    AzureBlobFileSystem fs;\r\n    if (config == null) {\r\n        fs = this.getFileSystem();\r\n    } else {\r\n        final AzureBlobFileSystem currentFs = getFileSystem();\r\n        fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config);\r\n    }\r\n    if (fs.exists(testFilePath)) {\r\n        FileStatus status = fs.getFileStatus(testFilePath);\r\n        if (status.getLen() >= testFileSize) {\r\n            return fs;\r\n        }\r\n    }\r\n    byte[] buffer = new byte[EIGHT_MB];\r\n    char character = 'a';\r\n    for (int i = 0; i < buffer.length; i++) {\r\n        buffer[i] = (byte) character;\r\n        character = (character == 'z') ? 'a' : (char) ((int) character + 1);\r\n    }\r\n    try (FSDataOutputStream outputStream = fs.create(testFilePath)) {\r\n        int bytesWritten = 0;\r\n        while (bytesWritten < testFileSize) {\r\n            outputStream.write(buffer);\r\n            bytesWritten += buffer.length;\r\n        }\r\n    }\r\n    Assertions.assertThat(fs.getFileStatus(testFilePath).getLen()).describedAs(\"File not created of expected size\").isEqualTo(testFileSize);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "resetReadBufferManager",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void resetReadBufferManager(int bufferSize, int threshold)\n{\r\n    ReadBufferManager.getBufferManager().testResetReadBufferManager(bufferSize, threshold);\r\n    System.gc();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initialize(Configuration configuration, String accountName) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "ResetStatusToFirstTokenFetch",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void ResetStatusToFirstTokenFetch()\n{\r\n    isThisFirstTokenFetch = true;\r\n    reTryCount = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getAccessToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAccessToken() throws IOException\n{\r\n    if (isThisFirstTokenFetch) {\r\n        isThisFirstTokenFetch = false;\r\n    } else {\r\n        reTryCount++;\r\n    }\r\n    LOG.debug(\"RetryTestTokenProvider: Throw an exception in fetching tokens\");\r\n    throw new IOException(\"test exception\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getExpiryTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Date getExpiryTime()\n{\r\n    return new Date();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return \"wasb\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "getTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestPath()\n{\r\n    return AzureTestUtils.createTestPath(super.getTestPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AbfsCommitTestHelper.prepareTestConfiguration(binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "testUpdateAndGet",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testUpdateAndGet() throws IOException\n{\r\n    CachedSASToken cachedSasToken = new CachedSASToken();\r\n    String se1 = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS * 2, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String token1 = \"se=\" + se1;\r\n    cachedSasToken.update(token1);\r\n    String cachedToken = cachedSasToken.get();\r\n    Assert.assertTrue(token1 == cachedToken);\r\n    cachedSasToken.update(token1);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertTrue(token1 == cachedToken);\r\n    String se2 = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS * 2, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String token2 = \"se=\" + se2;\r\n    cachedSasToken.update(token2);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertTrue(token2 == cachedToken);\r\n    String se3 = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS * 4, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String ske3 = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS * 2, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String token3 = \"se=\" + se3 + \"&ske=\" + ske3;\r\n    cachedSasToken.update(token3);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertTrue(token3 == cachedToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "testGetExpiration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetExpiration() throws IOException\n{\r\n    CachedSASToken cachedSasToken = new CachedSASToken();\r\n    String se = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS - 1, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    OffsetDateTime seDate = OffsetDateTime.parse(se, DateTimeFormatter.ISO_DATE_TIME);\r\n    String token = \"se=\" + se;\r\n    cachedSasToken.setForTesting(token, seDate);\r\n    String cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "testUpdateAndGetWithExpiredToken",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testUpdateAndGetWithExpiredToken() throws IOException\n{\r\n    CachedSASToken cachedSasToken = new CachedSASToken();\r\n    String se1 = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS - 1, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String token1 = \"se=\" + se1;\r\n    cachedSasToken.update(token1);\r\n    String cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n    String se2 = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS * 2, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String ske2 = OffsetDateTime.now(ZoneOffset.UTC).plus(DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS - 1, SECONDS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String token2 = \"se=\" + se2 + \"&ske=\" + ske2;\r\n    cachedSasToken.update(token2);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "testUpdateAndGetWithInvalidToken",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testUpdateAndGetWithInvalidToken() throws IOException\n{\r\n    CachedSASToken cachedSasToken = new CachedSASToken();\r\n    String token1 = \"se=\";\r\n    cachedSasToken.update(token1);\r\n    String cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n    String token2 = \"se=xyz\";\r\n    cachedSasToken.update(token2);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n    String token3 = \"se=2100-01-01T00:00:00Z&ske=\";\r\n    cachedSasToken.update(token3);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n    String token4 = \"se=2100-01-01T00:00:00Z&ske=xyz&\";\r\n    cachedSasToken.update(token4);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n    String token5 = \"se=abc&ske=xyz&\";\r\n    cachedSasToken.update(token5);\r\n    cachedToken = cachedSasToken.get();\r\n    Assert.assertNull(cachedToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "getTestCachedSASTokenInstance",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "CachedSASToken getTestCachedSASTokenInstance()\n{\r\n    String expiryPostADay = OffsetDateTime.now(ZoneOffset.UTC).plus(1, DAYS).format(DateTimeFormatter.ISO_DATE_TIME);\r\n    String version = \"2020-20-20\";\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"skoid=\");\r\n    sb.append(UUID.randomUUID().toString());\r\n    sb.append(\"&sktid=\");\r\n    sb.append(UUID.randomUUID().toString());\r\n    sb.append(\"&skt=\");\r\n    sb.append(OffsetDateTime.now(ZoneOffset.UTC).minus(1, DAYS).format(DateTimeFormatter.ISO_DATE_TIME));\r\n    sb.append(\"&ske=\");\r\n    sb.append(expiryPostADay);\r\n    sb.append(\"&sks=b\");\r\n    sb.append(\"&skv=\");\r\n    sb.append(version);\r\n    sb.append(\"&sp=rw\");\r\n    sb.append(\"&sr=b\");\r\n    sb.append(\"&se=\");\r\n    sb.append(expiryPostADay);\r\n    sb.append(\"&sv=2\");\r\n    sb.append(version);\r\n    CachedSASToken cachedSASToken = new CachedSASToken();\r\n    cachedSASToken.update(sb.toString());\r\n    return cachedSASToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_001_NativeAzureFileSystemMocked",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_001_NativeAzureFileSystemMocked() throws Exception\n{\r\n    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.createMock();\r\n    assumeNotNull(testAccount);\r\n    testStatisticsWithAccount(testAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_002_NativeAzureFileSystemPageBlobLive",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_002_NativeAzureFileSystemPageBlobLive() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, \"/\");\r\n    conf.set(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES, \"/\");\r\n    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.create(conf);\r\n    assumeNotNull(testAccount);\r\n    testStatisticsWithAccount(testAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "test_003_NativeAzureFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_003_NativeAzureFileSystem() throws Exception\n{\r\n    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount.create();\r\n    assumeNotNull(testAccount);\r\n    testStatisticsWithAccount(testAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testStatisticsWithAccount",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testStatisticsWithAccount(AzureBlobStorageTestAccount testAccount) throws Exception\n{\r\n    assumeNotNull(testAccount);\r\n    NativeAzureFileSystem fs = testAccount.getFileSystem();\r\n    testStatistics(fs);\r\n    cleanupTestAccount(testAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testStatistics",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testStatistics(NativeAzureFileSystem fs) throws Exception\n{\r\n    FileSystem.clearStatistics();\r\n    FileSystem.Statistics stats = FileSystem.getStatistics(\"wasb\", NativeAzureFileSystem.class);\r\n    assertEquals(0, stats.getBytesRead());\r\n    assertEquals(0, stats.getBytesWritten());\r\n    Path newFile = new Path(\"testStats\");\r\n    writeStringToFile(fs, newFile, \"12345678\");\r\n    assertEquals(8, stats.getBytesWritten());\r\n    assertEquals(0, stats.getBytesRead());\r\n    String readBack = readStringFromFile(fs, newFile);\r\n    assertEquals(\"12345678\", readBack);\r\n    assertEquals(8, stats.getBytesRead());\r\n    assertEquals(8, stats.getBytesWritten());\r\n    assertTrue(fs.delete(newFile, true));\r\n    assertEquals(8, stats.getBytesRead());\r\n    assertEquals(8, stats.getBytesWritten());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPageBlobTestStorageAccount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureBlobStorageTestAccount getPageBlobTestStorageAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, \"/\");\r\n    conf.set(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES, \"/\");\r\n    return AzureBlobStorageTestAccount.create(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createEmptyFile(AzureBlobStorageTestAccount testAccount, Path testPath) throws Exception\n{\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    FSDataOutputStream inputStream = fs.create(testPath);\r\n    inputStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestFolder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createTestFolder(AzureBlobStorageTestAccount testAccount, Path testFolderPath) throws Exception\n{\r\n    FileSystem fs = testAccount.getFileSystem();\r\n    fs.mkdirs(testFolderPath);\r\n    String testFolderFilePathBase = \"test\";\r\n    for (int i = 0; i < 10; i++) {\r\n        Path p = new Path(testFolderPath.toString() + \"/\" + testFolderFilePathBase + i + \".dat\");\r\n        fs.create(p).close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "nameThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameThread()\n{\r\n    Thread.currentThread().setName(\"JUnit-\" + methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, \"/\");\r\n    conf.set(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES, \"/\");\r\n    return AzureBlobStorageTestAccount.create(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    testAccount = createTestAccount();\r\n    assumeNotNull(testAccount);\r\n    fs = testAccount.getFileSystem();\r\n    basePath = AzureTestUtils.pathForTests(fs, \"filesystemcontractpageblob\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount = AzureTestUtils.cleanup(testAccount);\r\n    fs = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getGlobalTimeout",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getGlobalTimeout()\n{\r\n    return AzureTestConstants.AZURE_TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestBaseDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTestBaseDir()\n{\r\n    return basePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMoveFileUnderParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMoveFileUnderParent() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameFileToSelf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameFileToSelf() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameChildDirForbidden",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameChildDirForbidden() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testMoveDirUnderParent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMoveDirUnderParent() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testRenameDirToSelf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testRenameDirToSelf() throws Throwable\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBlobDataContributor",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testBlobDataContributor() throws Exception\n{\r\n    String clientId = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_CONTRIBUTOR_CLIENT_ID);\r\n    Assume.assumeTrue(\"Contributor client id not provided\", clientId != null);\r\n    String secret = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_CONTRIBUTOR_CLIENT_SECRET);\r\n    Assume.assumeTrue(\"Contributor client secret not provided\", secret != null);\r\n    Path existedFilePath = path(EXISTED_FILE_PATH);\r\n    Path existedFolderPath = path(EXISTED_FOLDER_PATH);\r\n    prepareFiles(existedFilePath, existedFolderPath);\r\n    final AzureBlobFileSystem fs = getBlobConributor();\r\n    try (FSDataOutputStream stream = fs.create(FILE_PATH)) {\r\n        stream.write(0);\r\n    }\r\n    assertPathExists(fs, \"This path should exist\", FILE_PATH);\r\n    FileStatus fileStatus = fs.getFileStatus(FILE_PATH);\r\n    assertEquals(1, fileStatus.getLen());\r\n    assertTrue(fs.delete(FILE_PATH, true));\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", FILE_PATH);\r\n    assertPathExists(fs, \"This path should exist\", existedFolderPath);\r\n    fs.delete(existedFolderPath, true);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", existedFolderPath);\r\n    try (FSDataInputStream stream = fs.open(existedFilePath)) {\r\n        assertTrue(stream.read() != 0);\r\n    }\r\n    assertEquals(0, fs.getFileStatus(existedFilePath).getLen());\r\n    try (FSDataOutputStream stream = fs.append(existedFilePath)) {\r\n        stream.write(0);\r\n    }\r\n    assertEquals(1, fs.getFileStatus(existedFilePath).getLen());\r\n    fs.delete(existedFilePath, true);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", existedFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBlobDataReader",
  "errType" : [ "AbfsRestOperationException", "AbfsRestOperationException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testBlobDataReader() throws Exception\n{\r\n    String clientId = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_READER_CLIENT_ID);\r\n    Assume.assumeTrue(\"Reader client id not provided\", clientId != null);\r\n    String secret = this.getConfiguration().get(TestConfigurationKeys.FS_AZURE_BLOB_DATA_READER_CLIENT_SECRET);\r\n    Assume.assumeTrue(\"Reader client secret not provided\", secret != null);\r\n    Path existedFilePath = path(EXISTED_FILE_PATH);\r\n    Path existedFolderPath = path(EXISTED_FOLDER_PATH);\r\n    prepareFiles(existedFilePath, existedFolderPath);\r\n    final AzureBlobFileSystem fs = getBlobReader();\r\n    AzureBlobFileSystemStore abfsStore = fs.getAbfsStore();\r\n    TracingContext tracingContext = getTestTracingContext(fs, true);\r\n    Map<String, String> properties = abfsStore.getFilesystemProperties(tracingContext);\r\n    assertPathExists(fs, \"This path should exist\", existedFolderPath);\r\n    try {\r\n        abfsStore.delete(existedFolderPath, true, tracingContext);\r\n    } catch (AbfsRestOperationException e) {\r\n        assertEquals(AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH, e.getErrorCode());\r\n    }\r\n    try (InputStream inputStream = abfsStore.openFileForRead(existedFilePath, null, tracingContext)) {\r\n        assertTrue(inputStream.read() != 0);\r\n    }\r\n    try {\r\n        abfsStore.openFileForWrite(existedFilePath, fs.getFsStatistics(), true, tracingContext);\r\n    } catch (AbfsRestOperationException e) {\r\n        assertEquals(AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH, e.getErrorCode());\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, abfsStore);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "prepareFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void prepareFiles(Path existedFilePath, Path existedFolderPath) throws IOException\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    fs.create(existedFilePath).close();\r\n    assertPathExists(fs, \"This path should exist\", existedFilePath);\r\n    fs.mkdirs(existedFolderPath);\r\n    assertPathExists(fs, \"This path should exist\", existedFolderPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getBlobConributor",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AzureBlobFileSystem getBlobConributor() throws Exception\n{\r\n    AbfsConfiguration abfsConfig = this.getConfiguration();\r\n    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID + \".\" + this.getAccountName(), abfsConfig.get(FS_AZURE_BLOB_DATA_CONTRIBUTOR_CLIENT_ID));\r\n    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET + \".\" + this.getAccountName(), abfsConfig.get(FS_AZURE_BLOB_DATA_CONTRIBUTOR_CLIENT_SECRET));\r\n    Configuration rawConfig = abfsConfig.getRawConfiguration();\r\n    return getFileSystem(rawConfig);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getBlobReader",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AzureBlobFileSystem getBlobReader() throws Exception\n{\r\n    AbfsConfiguration abfsConfig = this.getConfiguration();\r\n    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID + \".\" + this.getAccountName(), abfsConfig.get(FS_AZURE_BLOB_DATA_READER_CLIENT_ID));\r\n    abfsConfig.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET + \".\" + this.getAccountName(), abfsConfig.get(FS_AZURE_BLOB_DATA_READER_CLIENT_SECRET));\r\n    Configuration rawConfig = abfsConfig.getRawConfiguration();\r\n    return getFileSystem(rawConfig);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testBasicFunctionality",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBasicFunctionality() throws Exception\n{\r\n    RollingWindowAverage average = new RollingWindowAverage(100);\r\n    assertEquals(0, average.getCurrentAverage());\r\n    average.addPoint(5);\r\n    assertEquals(5, average.getCurrentAverage());\r\n    Thread.sleep(50);\r\n    average.addPoint(15);\r\n    assertEquals(10, average.getCurrentAverage());\r\n    Thread.sleep(60);\r\n    assertEquals(15, average.getCurrentAverage());\r\n    Thread.sleep(50);\r\n    assertEquals(0, average.getCurrentAverage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyStorageClientLogs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean verifyStorageClientLogs(String capturedLogs, String entity) throws Exception\n{\r\n    URI uri = testAccount.getRealAccount().getBlobEndpoint();\r\n    String container = testAccount.getRealContainer().getName();\r\n    String validateString = uri + Path.SEPARATOR + container + Path.SEPARATOR + entity;\r\n    boolean entityFound = false;\r\n    StringTokenizer tokenizer = new StringTokenizer(capturedLogs, \"\\n\");\r\n    while (tokenizer.hasMoreTokens()) {\r\n        String token = tokenizer.nextToken();\r\n        if (token.contains(validateString)) {\r\n            entityFound = true;\r\n            break;\r\n        }\r\n    }\r\n    return entityFound;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateFileSystemConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateFileSystemConfiguration(Boolean loggingFlag) throws Exception\n{\r\n    Configuration conf = fs.getConf();\r\n    conf.set(KEY_LOGGING_CONF_STRING, loggingFlag.toString());\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "performWASBOperations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void performWASBOperations() throws Exception\n{\r\n    Path tempDir = new Path(Path.SEPARATOR + TEMP_DIR);\r\n    fs.mkdirs(tempDir);\r\n    fs.delete(tempDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testLoggingEnabled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLoggingEnabled() throws Exception\n{\r\n    LogCapturer logs = LogCapturer.captureLogs(new Log4JLogger(Logger.getRootLogger()));\r\n    updateFileSystemConfiguration(true);\r\n    performWASBOperations();\r\n    String output = getLogOutput(logs);\r\n    assertTrue(\"Log entry \" + TEMP_DIR + \" not found  in \" + output, verifyStorageClientLogs(output, TEMP_DIR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getLogOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getLogOutput(LogCapturer logs)\n{\r\n    String output = logs.getOutput();\r\n    assertTrue(\"No log created/captured\", !output.isEmpty());\r\n    return output;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testLoggingDisabled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLoggingDisabled() throws Exception\n{\r\n    LogCapturer logs = LogCapturer.captureLogs(new Log4JLogger(Logger.getRootLogger()));\r\n    updateFileSystemConfiguration(false);\r\n    performWASBOperations();\r\n    String output = getLogOutput(logs);\r\n    assertFalse(\"Log entry \" + TEMP_DIR + \" found  in \" + output, verifyStorageClientLogs(output, TEMP_DIR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testStringPrecedence",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testStringPrecedence() throws IllegalAccessException, IOException, InvalidConfigurationValueException\n{\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    final String accountName1 = \"account1\";\r\n    final String accountName2 = \"account2\";\r\n    final String accountName3 = \"account3\";\r\n    final String globalKey = \"fs.azure.configuration\";\r\n    final String accountKey1 = globalKey + \".\" + accountName1;\r\n    final String accountKey2 = globalKey + \".\" + accountName2;\r\n    final String accountKey3 = globalKey + \".\" + accountName3;\r\n    final String globalValue = \"global\";\r\n    final String accountValue1 = \"one\";\r\n    final String accountValue2 = \"two\";\r\n    conf.set(accountKey1, accountValue1);\r\n    conf.set(accountKey2, accountValue2);\r\n    conf.set(globalKey, globalValue);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    assertEquals(\"Wrong value returned when account-specific value was requested\", abfsConf.get(accountKey1), accountValue1);\r\n    assertEquals(\"Account-specific value was not returned when one existed\", abfsConf.get(globalKey), accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName2);\r\n    assertEquals(\"Wrong value returned when a different account-specific value was requested\", abfsConf.get(accountKey1), accountValue1);\r\n    assertEquals(\"Wrong value returned when account-specific value was requested\", abfsConf.get(accountKey2), accountValue2);\r\n    assertEquals(\"Account-agnostic value return even though account-specific value was set\", abfsConf.get(globalKey), accountValue2);\r\n    abfsConf = new AbfsConfiguration(conf, accountName3);\r\n    assertNull(\"Account-specific value returned when none was set\", abfsConf.get(accountKey3));\r\n    assertEquals(\"Account-agnostic value not returned when no account-specific value was set\", abfsConf.get(globalKey), globalValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testPasswordPrecedence",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testPasswordPrecedence() throws IllegalAccessException, IOException, InvalidConfigurationValueException\n{\r\n    AbfsConfiguration abfsConf;\r\n    final Configuration conf = new Configuration();\r\n    final String accountName1 = \"account1\";\r\n    final String accountName2 = \"account2\";\r\n    final String accountName3 = \"account3\";\r\n    final String globalKey = \"fs.azure.password\";\r\n    final String accountKey1 = globalKey + \".\" + accountName1;\r\n    final String accountKey2 = globalKey + \".\" + accountName2;\r\n    final String accountKey3 = globalKey + \".\" + accountName3;\r\n    final String globalValue = \"global\";\r\n    final String accountValue1 = \"one\";\r\n    final String accountValue2 = \"two\";\r\n    conf.set(accountKey1, accountValue1);\r\n    conf.set(accountKey2, accountValue2);\r\n    conf.set(globalKey, globalValue);\r\n    abfsConf = new AbfsConfiguration(conf, accountName1);\r\n    assertEquals(\"Wrong value returned when account-specific value was requested\", abfsConf.getPasswordString(accountKey1), accountValue1);\r\n    assertEquals(\"Account-specific value was not returned when one existed\", abfsConf.getPasswordString(globalKey), accountValue1);\r\n    abfsConf = new AbfsConfiguration(conf, accountName2);\r\n    assertEquals(\"Wrong value returned when a different account-specific value was requested\", abfsConf.getPasswordString(accountKey1), accountValue1);\r\n    assertEquals(\"Wrong value returned when account-specific value was requested\", abfsConf.getPasswordString(accountKey2), accountValue2);\r\n    assertEquals(\"Account-agnostic value return even though account-specific value was set\", abfsConf.getPasswordString(globalKey), accountValue2);\r\n    abfsConf = new AbfsConfiguration(conf, accountName3);\r\n    assertNull(\"Account-specific value returned when none was set\", abfsConf.getPasswordString(accountKey3));\r\n    assertEquals(\"Account-agnostic value not returned when no account-specific value was set\", abfsConf.getPasswordString(globalKey), globalValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBooleanPrecedence",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testBooleanPrecedence() throws IllegalAccessException, IOException, InvalidConfigurationValueException\n{\r\n    final String accountName = \"account\";\r\n    final String globalKey = \"fs.azure.bool\";\r\n    final String accountKey = globalKey + \".\" + accountName;\r\n    final Configuration conf = new Configuration();\r\n    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\r\n    conf.setBoolean(globalKey, false);\r\n    assertEquals(\"Default value returned even though account-agnostic config was set\", abfsConf.getBoolean(globalKey, true), false);\r\n    conf.unset(globalKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getBoolean(globalKey, true), true);\r\n    conf.setBoolean(accountKey, false);\r\n    assertEquals(\"Default value returned even though account-specific config was set\", abfsConf.getBoolean(globalKey, true), false);\r\n    conf.unset(accountKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getBoolean(globalKey, true), true);\r\n    conf.setBoolean(accountKey, true);\r\n    conf.setBoolean(globalKey, false);\r\n    assertEquals(\"Account-agnostic or default value returned even though account-specific config was set\", abfsConf.getBoolean(globalKey, false), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testLongPrecedence",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testLongPrecedence() throws IllegalAccessException, IOException, InvalidConfigurationValueException\n{\r\n    final String accountName = \"account\";\r\n    final String globalKey = \"fs.azure.long\";\r\n    final String accountKey = globalKey + \".\" + accountName;\r\n    final Configuration conf = new Configuration();\r\n    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\r\n    conf.setLong(globalKey, 0);\r\n    assertEquals(\"Default value returned even though account-agnostic config was set\", abfsConf.getLong(globalKey, 1), 0);\r\n    conf.unset(globalKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getLong(globalKey, 1), 1);\r\n    conf.setLong(accountKey, 0);\r\n    assertEquals(\"Default value returned even though account-specific config was set\", abfsConf.getLong(globalKey, 1), 0);\r\n    conf.unset(accountKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getLong(globalKey, 1), 1);\r\n    conf.setLong(accountKey, 1);\r\n    conf.setLong(globalKey, 0);\r\n    assertEquals(\"Account-agnostic or default value returned even though account-specific config was set\", abfsConf.getLong(globalKey, 0), 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnumPrecedence",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testEnumPrecedence() throws IllegalAccessException, IOException, InvalidConfigurationValueException\n{\r\n    final String accountName = \"account\";\r\n    final String globalKey = \"fs.azure.enum\";\r\n    final String accountKey = globalKey + \".\" + accountName;\r\n    final Configuration conf = new Configuration();\r\n    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\r\n    conf.setEnum(globalKey, GetEnumType.FALSE);\r\n    assertEquals(\"Default value returned even though account-agnostic config was set\", abfsConf.getEnum(globalKey, GetEnumType.TRUE), GetEnumType.FALSE);\r\n    conf.unset(globalKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getEnum(globalKey, GetEnumType.TRUE), GetEnumType.TRUE);\r\n    conf.setEnum(accountKey, GetEnumType.FALSE);\r\n    assertEquals(\"Default value returned even though account-specific config was set\", abfsConf.getEnum(globalKey, GetEnumType.TRUE), GetEnumType.FALSE);\r\n    conf.unset(accountKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getEnum(globalKey, GetEnumType.TRUE), GetEnumType.TRUE);\r\n    conf.setEnum(accountKey, GetEnumType.TRUE);\r\n    conf.setEnum(globalKey, GetEnumType.FALSE);\r\n    assertEquals(\"Account-agnostic or default value returned even though account-specific config was set\", abfsConf.getEnum(globalKey, GetEnumType.FALSE), GetEnumType.TRUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testClass",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testClass() throws IllegalAccessException, IOException, InvalidConfigurationValueException\n{\r\n    final String accountName = \"account\";\r\n    final String globalKey = \"fs.azure.class\";\r\n    final String accountKey = globalKey + \".\" + accountName;\r\n    final Configuration conf = new Configuration();\r\n    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\r\n    final Class class0 = GetClassImpl0.class;\r\n    final Class class1 = GetClassImpl1.class;\r\n    final Class xface = GetClassInterface.class;\r\n    conf.setClass(globalKey, class0, xface);\r\n    assertEquals(\"Default value returned even though account-agnostic config was set\", abfsConf.getAccountAgnosticClass(globalKey, class1, xface), class0);\r\n    conf.unset(globalKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getAccountAgnosticClass(globalKey, class1, xface), class1);\r\n    conf.setClass(accountKey, class0, xface);\r\n    assertEquals(\"Default value returned even though account-specific config was set\", abfsConf.getAccountSpecificClass(globalKey, class1, xface), class0);\r\n    conf.unset(accountKey);\r\n    assertEquals(\"Default value not returned even though config was unset\", abfsConf.getAccountSpecificClass(globalKey, class1, xface), class1);\r\n    conf.setClass(accountKey, class1, xface);\r\n    conf.setClass(globalKey, class0, xface);\r\n    assertEquals(\"Account-agnostic or default value returned even though account-specific config was set\", abfsConf.getAccountSpecificClass(globalKey, class0, xface), class1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSASProviderPrecedence",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSASProviderPrecedence() throws IOException, IllegalAccessException\n{\r\n    final String accountName = \"account\";\r\n    final Configuration conf = new Configuration();\r\n    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\r\n    abfsConf.set(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME + \".\" + accountName, \"SAS\");\r\n    abfsConf.set(FS_AZURE_SAS_TOKEN_PROVIDER_TYPE + \".\" + accountName, TEST_SAS_PROVIDER_CLASS_CONFIG_1);\r\n    abfsConf.set(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SAS.toString());\r\n    abfsConf.set(FS_AZURE_SAS_TOKEN_PROVIDER_TYPE, TEST_SAS_PROVIDER_CLASS_CONFIG_2);\r\n    Assertions.assertThat(abfsConf.getSASTokenProvider().getClass().getName()).describedAs(\"Account-specific SAS token provider should be in effect.\").isEqualTo(TEST_SAS_PROVIDER_CLASS_CONFIG_1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAccessTokenProviderPrecedence",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAccessTokenProviderPrecedence() throws IllegalAccessException, IOException\n{\r\n    final String accountName = \"account\";\r\n    final Configuration conf = new Configuration();\r\n    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\r\n    testGlobalAndAccountOAuthPrecedence(abfsConf, AuthType.Custom, AuthType.OAuth);\r\n    testGlobalAndAccountOAuthPrecedence(abfsConf, AuthType.OAuth, AuthType.Custom);\r\n    testGlobalAndAccountOAuthPrecedence(abfsConf, AuthType.SAS, AuthType.Custom);\r\n    testGlobalAndAccountOAuthPrecedence(abfsConf, AuthType.Custom, null);\r\n    testGlobalAndAccountOAuthPrecedence(abfsConf, AuthType.OAuth, null);\r\n    testGlobalAndAccountOAuthPrecedence(abfsConf, null, AuthType.Custom);\r\n    testGlobalAndAccountOAuthPrecedence(abfsConf, null, AuthType.OAuth);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testConfigPropNotFound",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testConfigPropNotFound() throws Throwable\n{\r\n    final String accountName = \"account\";\r\n    final Configuration conf = new Configuration();\r\n    final AbfsConfiguration abfsConf = new AbfsConfiguration(conf, accountName);\r\n    for (String key : CONFIG_KEYS) {\r\n        setAuthConfig(abfsConf, true, AuthType.OAuth);\r\n        abfsConf.unset(key + \".\" + accountName);\r\n        testMissingConfigKey(abfsConf, key);\r\n    }\r\n    unsetAuthConfig(abfsConf, false);\r\n    unsetAuthConfig(abfsConf, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testMissingConfigKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMissingConfigKey(final AbfsConfiguration abfsConf, final String confKey) throws Throwable\n{\r\n    GenericTestUtils.assertExceptionContains(\"Configuration property \" + confKey + \" not found.\", LambdaTestUtils.verifyCause(ConfigurationPropertyNotFoundException.class, LambdaTestUtils.intercept(TokenAccessProviderException.class, () -> abfsConf.getTokenProvider().getClass().getTypeName())));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGlobalAndAccountOAuthPrecedence",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGlobalAndAccountOAuthPrecedence(AbfsConfiguration abfsConf, AuthType globalAuthType, AuthType accountSpecificAuthType) throws IOException\n{\r\n    if (globalAuthType == null) {\r\n        unsetAuthConfig(abfsConf, false);\r\n    } else {\r\n        setAuthConfig(abfsConf, false, globalAuthType);\r\n    }\r\n    if (accountSpecificAuthType == null) {\r\n        unsetAuthConfig(abfsConf, true);\r\n    } else {\r\n        setAuthConfig(abfsConf, true, accountSpecificAuthType);\r\n    }\r\n    AuthType expectedEffectiveAuthType;\r\n    if (accountSpecificAuthType != null) {\r\n        expectedEffectiveAuthType = accountSpecificAuthType;\r\n    } else {\r\n        expectedEffectiveAuthType = globalAuthType;\r\n    }\r\n    Class<?> expectedEffectiveTokenProviderClassType = (expectedEffectiveAuthType == AuthType.OAuth) ? ClientCredsTokenProvider.class : CustomTokenProviderAdapter.class;\r\n    Assertions.assertThat(abfsConf.getTokenProvider().getClass().getTypeName()).describedAs(\"Account-specific settings takes precendence to global\" + \" settings. In absence of Account settings, global settings \" + \"should take effect.\").isEqualTo(expectedEffectiveTokenProviderClassType.getTypeName());\r\n    unsetAuthConfig(abfsConf, false);\r\n    unsetAuthConfig(abfsConf, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setAuthConfig",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setAuthConfig(AbfsConfiguration abfsConf, boolean isAccountSetting, AuthType authType)\n{\r\n    final String accountNameSuffix = \".\" + abfsConf.getAccountName();\r\n    String authKey = FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME + (isAccountSetting ? accountNameSuffix : \"\");\r\n    String providerClassKey = \"\";\r\n    String providerClassValue = \"\";\r\n    switch(authType) {\r\n        case OAuth:\r\n            providerClassKey = FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME + (isAccountSetting ? accountNameSuffix : \"\");\r\n            providerClassValue = TEST_OAUTH_PROVIDER_CLASS_CONFIG;\r\n            abfsConf.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT + ((isAccountSetting) ? accountNameSuffix : \"\"), TEST_OAUTH_ENDPOINT);\r\n            abfsConf.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID + ((isAccountSetting) ? accountNameSuffix : \"\"), TEST_CLIENT_ID);\r\n            abfsConf.set(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET + ((isAccountSetting) ? accountNameSuffix : \"\"), TEST_CLIENT_SECRET);\r\n            break;\r\n        case Custom:\r\n            providerClassKey = FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME + (isAccountSetting ? accountNameSuffix : \"\");\r\n            providerClassValue = TEST_CUSTOM_PROVIDER_CLASS_CONFIG;\r\n            break;\r\n        case SAS:\r\n            providerClassKey = FS_AZURE_SAS_TOKEN_PROVIDER_TYPE + (isAccountSetting ? accountNameSuffix : \"\");\r\n            providerClassValue = TEST_SAS_PROVIDER_CLASS_CONFIG_1;\r\n            break;\r\n        default:\r\n    }\r\n    abfsConf.set(authKey, authType.toString());\r\n    abfsConf.set(providerClassKey, providerClassValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "unsetAuthConfig",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void unsetAuthConfig(AbfsConfiguration abfsConf, boolean isAccountSettings)\n{\r\n    String accountNameSuffix = isAccountSettings ? (\".\" + abfsConf.getAccountName()) : \"\";\r\n    abfsConf.unset(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME + accountNameSuffix);\r\n    abfsConf.unset(FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME + accountNameSuffix);\r\n    abfsConf.unset(FS_AZURE_SAS_TOKEN_PROVIDER_TYPE + accountNameSuffix);\r\n    abfsConf.unset(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT + accountNameSuffix);\r\n    abfsConf.unset(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID + accountNameSuffix);\r\n    abfsConf.unset(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET + accountNameSuffix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    boolean isHNSEnabled = this.getConfiguration().getBoolean(TestConfigurationKeys.FS_AZURE_TEST_NAMESPACE_ENABLED_ACCOUNT, false);\r\n    Assume.assumeTrue(isHNSEnabled);\r\n    createFilesystemForSASTests();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCheckAccess",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testCheckAccess() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path rootPath = new Path(\"/\");\r\n    fs.setOwner(rootPath, MockDelegationSASTokenProvider.TEST_OWNER, null);\r\n    fs.setPermission(rootPath, new FsPermission(FsAction.ALL, FsAction.READ_EXECUTE, FsAction.EXECUTE));\r\n    FileStatus rootStatus = fs.getFileStatus(rootPath);\r\n    assertEquals(\"The directory permissions are not expected.\", \"rwxr-x--x\", rootStatus.getPermission().toString());\r\n    assertEquals(\"The directory owner is not expected.\", MockDelegationSASTokenProvider.TEST_OWNER, rootStatus.getOwner());\r\n    Path dirPath = new Path(UUID.randomUUID().toString());\r\n    fs.mkdirs(dirPath);\r\n    Path filePath = new Path(dirPath, \"file1\");\r\n    fs.create(filePath).close();\r\n    fs.setPermission(filePath, new FsPermission(FsAction.READ, FsAction.READ, FsAction.NONE));\r\n    FileStatus dirStatus = fs.getFileStatus(dirPath);\r\n    FileStatus fileStatus = fs.getFileStatus(filePath);\r\n    assertEquals(\"The owner is not expected.\", MockDelegationSASTokenProvider.TEST_OWNER, dirStatus.getOwner());\r\n    assertEquals(\"The owner is not expected.\", MockDelegationSASTokenProvider.TEST_OWNER, fileStatus.getOwner());\r\n    assertEquals(\"The directory permissions are not expected.\", \"rwxr-xr-x\", dirStatus.getPermission().toString());\r\n    assertEquals(\"The file permissions are not expected.\", \"r--r-----\", fileStatus.getPermission().toString());\r\n    assertTrue(isAccessible(fs, dirPath, FsAction.READ_WRITE));\r\n    assertFalse(isAccessible(fs, filePath, FsAction.READ_WRITE));\r\n    fs.setPermission(filePath, new FsPermission(FsAction.READ_WRITE, FsAction.READ, FsAction.NONE));\r\n    fileStatus = fs.getFileStatus(filePath);\r\n    assertEquals(\"The file permissions are not expected.\", \"rw-r-----\", fileStatus.getPermission().toString());\r\n    assertTrue(isAccessible(fs, filePath, FsAction.READ_WRITE));\r\n    fs.setPermission(dirPath, new FsPermission(FsAction.EXECUTE, FsAction.NONE, FsAction.NONE));\r\n    dirStatus = fs.getFileStatus(dirPath);\r\n    assertEquals(\"The file permissions are not expected.\", \"--x------\", dirStatus.getPermission().toString());\r\n    assertFalse(isAccessible(fs, dirPath, FsAction.READ_WRITE));\r\n    assertTrue(isAccessible(fs, dirPath, FsAction.EXECUTE));\r\n    fs.setPermission(dirPath, new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));\r\n    dirStatus = fs.getFileStatus(dirPath);\r\n    assertEquals(\"The file permissions are not expected.\", \"---------\", dirStatus.getPermission().toString());\r\n    assertFalse(isAccessible(fs, filePath, FsAction.READ_WRITE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isAccessible",
  "errType" : [ "AccessControlException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isAccessible(FileSystem fs, Path path, FsAction fsAction) throws IOException\n{\r\n    try {\r\n        fs.access(path, fsAction);\r\n    } catch (AccessControlException ace) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadAndWrite",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testReadAndWrite() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path reqPath = new Path(UUID.randomUUID().toString());\r\n    final String msg1 = \"purple\";\r\n    final String msg2 = \"yellow\";\r\n    int expectedFileLength = msg1.length() * 2;\r\n    byte[] readBuffer = new byte[1024];\r\n    try (FSDataOutputStream stream = fs.create(reqPath)) {\r\n        stream.writeBytes(msg1);\r\n        stream.hflush();\r\n        stream.writeBytes(msg1);\r\n    }\r\n    try (FSDataInputStream stream = fs.open(reqPath)) {\r\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\r\n        assertEquals(expectedFileLength, bytesRead);\r\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\r\n        assertEquals(msg1 + msg1, fileContent);\r\n    }\r\n    try (FSDataOutputStream stream = fs.create(reqPath)) {\r\n        stream.writeBytes(msg2);\r\n        stream.hflush();\r\n        stream.writeBytes(msg2);\r\n    }\r\n    try (FSDataInputStream stream = fs.open(reqPath)) {\r\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\r\n        assertEquals(expectedFileLength, bytesRead);\r\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\r\n        assertEquals(msg2 + msg2, fileContent);\r\n    }\r\n    try (FSDataOutputStream stream = fs.append(reqPath)) {\r\n        stream.writeBytes(msg1);\r\n        stream.hflush();\r\n        stream.writeBytes(msg1);\r\n    }\r\n    try (FSDataInputStream stream = fs.open(reqPath)) {\r\n        int bytesRead = stream.read(readBuffer, 0, readBuffer.length);\r\n        assertEquals(2 * expectedFileLength, bytesRead);\r\n        String fileContent = new String(readBuffer, 0, bytesRead, StandardCharsets.UTF_8);\r\n        assertEquals(msg2 + msg2 + msg1 + msg1, fileContent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 6,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRename",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testRename() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path sourceDir = new Path(UUID.randomUUID().toString());\r\n    Path sourcePath = new Path(sourceDir, UUID.randomUUID().toString());\r\n    Path destinationPath = new Path(sourceDir, UUID.randomUUID().toString());\r\n    Path destinationDir = new Path(UUID.randomUUID().toString());\r\n    try (FSDataOutputStream stream = fs.create(sourcePath)) {\r\n        stream.writeBytes(\"hello\");\r\n    }\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", destinationPath);\r\n    fs.rename(sourcePath, destinationPath);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", sourcePath);\r\n    assertPathExists(fs, \"This path should exist\", destinationPath);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", destinationDir);\r\n    fs.rename(sourceDir, destinationDir);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", sourceDir);\r\n    assertPathExists(fs, \"This path should exist\", destinationDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDelete() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path dirPath = new Path(UUID.randomUUID().toString());\r\n    Path filePath = new Path(dirPath, UUID.randomUUID().toString());\r\n    try (FSDataOutputStream stream = fs.create(filePath)) {\r\n        stream.writeBytes(\"hello\");\r\n    }\r\n    assertPathExists(fs, \"This path should exist\", filePath);\r\n    fs.delete(filePath, false);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", filePath);\r\n    assertPathExists(fs, \"This path should exist\", dirPath);\r\n    fs.delete(dirPath, false);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", dirPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDeleteRecursive",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDeleteRecursive() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path dirPath = new Path(UUID.randomUUID().toString());\r\n    Path filePath = new Path(dirPath, UUID.randomUUID().toString());\r\n    try (FSDataOutputStream stream = fs.create(filePath)) {\r\n        stream.writeBytes(\"hello\");\r\n    }\r\n    assertPathExists(fs, \"This path should exist\", dirPath);\r\n    assertPathExists(fs, \"This path should exist\", filePath);\r\n    fs.delete(dirPath, true);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", filePath);\r\n    assertPathDoesNotExist(fs, \"This path should not exist\", dirPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testList",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testList() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path dirPath = new Path(UUID.randomUUID().toString());\r\n    Path filePath = new Path(dirPath, UUID.randomUUID().toString());\r\n    fs.mkdirs(dirPath);\r\n    try (FSDataOutputStream stream = fs.create(filePath)) {\r\n        stream.writeBytes(\"hello\");\r\n    }\r\n    fs.listStatus(filePath);\r\n    fs.listStatus(dirPath);\r\n    fs.listStatus(new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAcl",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAcl() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path reqPath = new Path(UUID.randomUUID().toString());\r\n    fs.create(reqPath).close();\r\n    fs.setAcl(reqPath, Arrays.asList(aclEntry(ACCESS, GROUP, TEST_GROUP, FsAction.ALL)));\r\n    AclStatus acl = fs.getAclStatus(reqPath);\r\n    assertEquals(MockDelegationSASTokenProvider.TEST_OWNER, acl.getOwner());\r\n    assertEquals(\"[group::r--, group:\" + TEST_GROUP + \":rwx]\", acl.getEntries().toString());\r\n    fs.removeAcl(reqPath);\r\n    acl = fs.getAclStatus(reqPath);\r\n    assertEquals(\"[]\", acl.getEntries().toString());\r\n    fs.setPermission(reqPath, new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE));\r\n    FileStatus status = fs.getFileStatus(reqPath);\r\n    assertEquals(\"rwx------\", status.getPermission().toString());\r\n    acl = fs.getAclStatus(reqPath);\r\n    assertEquals(\"rwx------\", acl.getPermission().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRootPath",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testRootPath() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path rootPath = new Path(AbfsHttpConstants.ROOT_PATH);\r\n    fs.setOwner(rootPath, MockDelegationSASTokenProvider.TEST_OWNER, null);\r\n    FileStatus status = fs.getFileStatus(rootPath);\r\n    assertEquals(\"rwxr-x---\", status.getPermission().toString());\r\n    assertEquals(MockDelegationSASTokenProvider.TEST_OWNER, status.getOwner());\r\n    assertTrue(status.isDirectory());\r\n    AclStatus acl = fs.getAclStatus(rootPath);\r\n    assertEquals(\"rwxr-x---\", acl.getPermission().toString());\r\n    List<AclEntry> aclSpec = new ArrayList<>();\r\n    int count = 0;\r\n    for (AclEntry entry : acl.getEntries()) {\r\n        aclSpec.add(entry);\r\n        if (entry.getScope() == AclEntryScope.DEFAULT) {\r\n            count++;\r\n        }\r\n    }\r\n    assertEquals(0, count);\r\n    aclSpec.add(aclEntry(DEFAULT, USER, \"cd548981-afec-4ab9-9d39-f6f2add2fd9b\", FsAction.EXECUTE));\r\n    fs.modifyAclEntries(rootPath, aclSpec);\r\n    acl = fs.getAclStatus(rootPath);\r\n    count = 0;\r\n    for (AclEntry entry : acl.getEntries()) {\r\n        aclSpec.add(entry);\r\n        if (entry.getScope() == AclEntryScope.DEFAULT) {\r\n            count++;\r\n        }\r\n    }\r\n    assertEquals(5, count);\r\n    fs.removeDefaultAcl(rootPath);\r\n    acl = fs.getAclStatus(rootPath);\r\n    count = 0;\r\n    for (AclEntry entry : acl.getEntries()) {\r\n        aclSpec.add(entry);\r\n        if (entry.getScope() == AclEntryScope.DEFAULT) {\r\n            count++;\r\n        }\r\n    }\r\n    assertEquals(0, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testProperties",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testProperties() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path reqPath = new Path(UUID.randomUUID().toString());\r\n    fs.create(reqPath).close();\r\n    final String propertyName = \"user.mime_type\";\r\n    final byte[] propertyValue = \"text/plain\".getBytes(\"utf-8\");\r\n    fs.setXAttr(reqPath, propertyName, propertyValue);\r\n    assertArrayEquals(propertyValue, fs.getXAttr(reqPath, propertyName));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSignatureMask",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSignatureMask() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    String src = String.format(\"/testABC/test%s.xt\", UUID.randomUUID());\r\n    fs.create(new Path(src)).close();\r\n    AbfsRestOperation abfsHttpRestOperation = fs.getAbfsClient().renamePath(src, \"/testABC\" + \"/abc.txt\", null, getTestTracingContext(fs, false), null).getLeft();\r\n    AbfsHttpOperation result = abfsHttpRestOperation.getResult();\r\n    String url = result.getMaskedUrl();\r\n    String encodedUrl = result.getMaskedEncodedUrl();\r\n    Assertions.assertThat(url.substring(url.indexOf(\"sig=\"))).describedAs(\"Signature query param should be masked\").startsWith(\"sig=XXXXX\");\r\n    Assertions.assertThat(encodedUrl.substring(encodedUrl.indexOf(\"sig%3D\"))).describedAs(\"Signature query param should be masked\").startsWith(\"sig%3DXXXXX\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSignatureMaskOnExceptionMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSignatureMaskOnExceptionMessage() throws Exception\n{\r\n    intercept(IOException.class, \"sig=XXXX\", () -> getFileSystem().getAbfsClient().renamePath(\"testABC/test.xt\", \"testABC/abc.txt\", null, getTestTracingContext(getFileSystem(), false), null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionForNonOwner",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetPermissionForNonOwner() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path rootPath = new Path(\"/\");\r\n    FileStatus rootStatus = fs.getFileStatus(rootPath);\r\n    assertEquals(\"The permissions are not expected.\", \"rwxr-x---\", rootStatus.getPermission().toString());\r\n    assertNotEquals(\"The owner is not expected.\", MockDelegationSASTokenProvider.TEST_OWNER, rootStatus.getOwner());\r\n    intercept(AccessDeniedException.class, AUTHORIZATION_PERMISSION_MISS_MATCH.getErrorCode(), () -> {\r\n        fs.setPermission(rootPath, new FsPermission(FsAction.ALL, FsAction.READ_EXECUTE, FsAction.EXECUTE));\r\n        return \"Set permission should fail because saoid is not the owner.\";\r\n    });\r\n    fs.setOwner(rootPath, MockDelegationSASTokenProvider.TEST_OWNER, null);\r\n    fs.setPermission(rootPath, new FsPermission(FsAction.ALL, FsAction.READ_EXECUTE, FsAction.EXECUTE));\r\n    rootStatus = fs.getFileStatus(rootPath);\r\n    assertEquals(\"The permissions are not expected.\", \"rwxr-x--x\", rootStatus.getPermission().toString());\r\n    assertEquals(\"The directory owner is not expected.\", MockDelegationSASTokenProvider.TEST_OWNER, rootStatus.getOwner());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetPermissionWithoutAgentForNonOwner",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetPermissionWithoutAgentForNonOwner() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path path = new Path(MockDelegationSASTokenProvider.NO_AGENT_PATH);\r\n    fs.create(path).close();\r\n    FileStatus status = fs.getFileStatus(path);\r\n    assertEquals(\"The permissions are not expected.\", \"rw-r--r--\", status.getPermission().toString());\r\n    assertNotEquals(\"The owner is not expected.\", TestConfigurationKeys.FS_AZURE_TEST_APP_SERVICE_PRINCIPAL_OBJECT_ID, status.getOwner());\r\n    fs.setPermission(path, new FsPermission(FsAction.READ, FsAction.READ, FsAction.NONE));\r\n    FileStatus fileStatus = fs.getFileStatus(path);\r\n    assertEquals(\"The permissions are not expected.\", \"r--r-----\", fileStatus.getPermission().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    conf = StubDelegationTokenManager.useStubDTManager(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "assertTokenKind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTokenKind(final Text kind, final Token<DelegationTokenIdentifier> dt)\n{\r\n    assertEquals(\"Token Kind\", kind, dt.getKind());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testClassicLifecycle",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testClassicLifecycle() throws Throwable\n{\r\n    AbfsDelegationTokenManager manager = new AbfsDelegationTokenManager(conf);\r\n    StubDelegationTokenManager stub = getTokenManager(manager);\r\n    assertTrue(\"Not initialized: \" + stub, stub.isInitialized());\r\n    Token<DelegationTokenIdentifier> dt = stub.getDelegationToken(RENEWER);\r\n    assertTokenKind(StubAbfsTokenIdentifier.TOKEN_KIND, dt);\r\n    assertNull(\"canonicalServiceName in \" + stub, manager.getCanonicalServiceName());\r\n    assertEquals(\"Issued count number in \" + stub, 1, stub.getIssued());\r\n    StubAbfsTokenIdentifier id = decodeIdentifier(dt);\r\n    assertEquals(\"Sequence number in \" + id, 1, id.getSequenceNumber());\r\n    stub.renewDelegationToken(dt);\r\n    assertEquals(\"Renewal count in \" + stub, 1, stub.getRenewals());\r\n    stub.cancelDelegationToken(dt);\r\n    assertEquals(\"Cancel count in \" + stub, 1, stub.getCancellations());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getTokenManager",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StubDelegationTokenManager getTokenManager(final AbfsDelegationTokenManager manager)\n{\r\n    return (StubDelegationTokenManager) manager.getTokenManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testBindingLifecycle",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBindingLifecycle() throws Throwable\n{\r\n    AbfsDelegationTokenManager manager = new AbfsDelegationTokenManager(conf);\r\n    StubDelegationTokenManager stub = getTokenManager(manager);\r\n    assertTrue(\"Not initialized: \" + stub, stub.isInitialized());\r\n    stub.bind(FSURI, conf);\r\n    assertEquals(\"URI in \" + stub, FSURI, stub.getFsURI());\r\n    decodeIdentifier(stub.getDelegationToken(RENEWER));\r\n    stub.close();\r\n    assertTrue(\"Not closed: \" + stub, stub.isClosed());\r\n    stub.close();\r\n    assertTrue(\"Not closed: \" + stub, stub.isClosed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testBindingThroughManager",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testBindingThroughManager() throws Throwable\n{\r\n    AbfsDelegationTokenManager manager = new AbfsDelegationTokenManager(conf);\r\n    manager.bind(FSURI, conf);\r\n    StubDelegationTokenManager stub = getTokenManager(manager);\r\n    assertEquals(\"Service in \" + manager, ABFS, stub.createServiceText().toString());\r\n    assertEquals(\"Binding URI of \" + stub, FSURI, stub.getFsURI());\r\n    Token<DelegationTokenIdentifier> token = manager.getDelegationToken(RENEWER);\r\n    assertEquals(\"Service in \" + token, ABFS, token.getService().toString());\r\n    decodeIdentifier(token);\r\n    assertTokenKind(StubAbfsTokenIdentifier.TOKEN_KIND, token);\r\n    stub.setKind(KIND2);\r\n    Token<DelegationTokenIdentifier> dt2 = manager.getDelegationToken(\"\");\r\n    assertTokenKind(KIND2, dt2);\r\n    assertNull(\"Token is of unknown kind, must not decode\", dt2.decodeIdentifier());\r\n    manager.close();\r\n    assertTrue(\"Not closed: \" + stub, stub.isClosed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "testRenewalThroughManager",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRenewalThroughManager() throws Throwable\n{\r\n    Token<DelegationTokenIdentifier> dt = createToken(0, FSURI, OWNER, new Text(RENEWER));\r\n    AbfsDelegationTokenManager manager = new AbfsDelegationTokenManager(conf);\r\n    StubDelegationTokenManager stub = getTokenManager(manager);\r\n    assertNull(\"Stub should not bebound \" + stub, stub.getFsURI());\r\n    StubAbfsTokenIdentifier dtId = (StubAbfsTokenIdentifier) dt.decodeIdentifier();\r\n    String idStr = dtId.toString();\r\n    assertEquals(\"URI in \" + idStr, FSURI, dtId.getUri());\r\n    assertEquals(\"renewer in \" + idStr, RENEWER, dtId.getRenewer().toString());\r\n    manager.renewDelegationToken(dt);\r\n    assertEquals(\"Renewal count in \" + stub, 1, stub.getRenewals());\r\n    manager.cancelDelegationToken(dt);\r\n    assertEquals(\"Cancel count in \" + stub, 1, stub.getCancellations());\r\n    manager.close();\r\n    assertTrue(\"Not closed: \" + stub, stub.isClosed());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamBytesFailed",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAbfsOutputStreamBytesFailed()\n{\r\n    describe(\"Testing number of bytes failed during upload in AbfsOutputSteam\");\r\n    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\r\n    assertEquals(\"Mismatch in number of bytes failed to upload\", 0, abfsOutputStreamStatistics.getBytesUploadFailed());\r\n    int randomBytesFailed = new Random().nextInt(LOW_RANGE_FOR_RANDOM_VALUE);\r\n    abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\r\n    assertEquals(\"Mismatch in number of bytes failed to upload\", randomBytesFailed, abfsOutputStreamStatistics.getBytesUploadFailed());\r\n    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\r\n    int expectedBytesFailed = 0;\r\n    for (int i = 0; i < OPERATIONS; i++) {\r\n        randomBytesFailed = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\r\n        abfsOutputStreamStatistics.uploadFailed(randomBytesFailed);\r\n        expectedBytesFailed += randomBytesFailed;\r\n    }\r\n    assertEquals(\"Mismatch in number of bytes failed to upload\", expectedBytesFailed, abfsOutputStreamStatistics.getBytesUploadFailed());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamTimeSpentOnWaitTask",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAbfsOutputStreamTimeSpentOnWaitTask()\n{\r\n    describe(\"Testing time Spent on waiting for task to be completed in \" + \"AbfsOutputStream\");\r\n    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\r\n    assertEquals(\"Mismatch in time spent on waiting for tasks to complete\", 0, abfsOutputStreamStatistics.getTimeSpentOnTaskWait());\r\n    abfsOutputStreamStatistics.timeSpentTaskWait();\r\n    assertEquals(\"Mismatch in time spent on waiting for tasks to complete\", 1, abfsOutputStreamStatistics.getTimeSpentOnTaskWait());\r\n    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\r\n    for (int i = 0; i < OPERATIONS; i++) {\r\n        abfsOutputStreamStatistics.timeSpentTaskWait();\r\n    }\r\n    assertEquals(\"Mismatch in time spent on waiting for tasks to complete\", OPERATIONS, abfsOutputStreamStatistics.getTimeSpentOnTaskWait());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsOutputStreamQueueShrink",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAbfsOutputStreamQueueShrink()\n{\r\n    describe(\"Testing queue shrink operations by AbfsOutputStream\");\r\n    AbfsOutputStreamStatisticsImpl abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\r\n    assertEquals(\"Mismatch in queue shrunk operations\", 0, abfsOutputStreamStatistics.getQueueShrunkOps());\r\n    abfsOutputStreamStatistics.queueShrunk();\r\n    assertEquals(\"Mismatch in queue shrunk operations\", 1, abfsOutputStreamStatistics.getQueueShrunkOps());\r\n    abfsOutputStreamStatistics = new AbfsOutputStreamStatisticsImpl();\r\n    int randomQueueValues = new Random().nextInt(HIGH_RANGE_FOR_RANDOM_VALUE);\r\n    for (int i = 0; i < randomQueueValues * OPERATIONS; i++) {\r\n        abfsOutputStreamStatistics.queueShrunk();\r\n    }\r\n    assertEquals(\"Mismatch in queue shrunk operations\", randomQueueValues * OPERATIONS, abfsOutputStreamStatistics.getQueueShrunkOps());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCloseWhenFlushThrowingIOException",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCloseWhenFlushThrowingIOException() throws Exception\n{\r\n    MockOutputStream out = new MockOutputStream();\r\n    SyncableDataOutputStream sdos = new SyncableDataOutputStream(out);\r\n    out.flushThrowIOE = true;\r\n    LambdaTestUtils.intercept(IOException.class, \"An IOE from flush\", () -> sdos.close());\r\n    MockOutputStream out2 = new MockOutputStream();\r\n    out2.flushThrowIOE = true;\r\n    LambdaTestUtils.intercept(IOException.class, \"An IOE from flush\", () -> {\r\n        try (SyncableDataOutputStream sdos2 = new SyncableDataOutputStream(out2)) {\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsHttpSendStatistics",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testAbfsHttpSendStatistics() throws IOException\n{\r\n    describe(\"Test to check correct values of statistics after Abfs http send \" + \"request is done.\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Map<String, Long> metricMap;\r\n    Path sendRequestPath = path(getMethodName());\r\n    String testNetworkStatsString = \"http_send\";\r\n    metricMap = fs.getInstrumentationMap();\r\n    long expectedConnectionsMade = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n    long expectedRequestsSent = metricMap.get(SEND_REQUESTS.getStatName());\r\n    long expectedBytesSent = 0;\r\n    try (AbfsOutputStream out = createAbfsOutputStreamWithFlushEnabled(fs, sendRequestPath)) {\r\n        expectedConnectionsMade++;\r\n        expectedRequestsSent++;\r\n        out.write(testNetworkStatsString.getBytes());\r\n        out.hflush();\r\n        if (fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(sendRequestPath).toString()) || (this.getConfiguration().isSmallWriteOptimizationEnabled())) {\r\n            expectedConnectionsMade++;\r\n            expectedRequestsSent++;\r\n        } else {\r\n            expectedConnectionsMade += 2;\r\n            expectedRequestsSent += 2;\r\n        }\r\n        expectedBytesSent += testNetworkStatsString.getBytes().length;\r\n        metricMap = fs.getInstrumentationMap();\r\n        assertAbfsStatistics(CONNECTIONS_MADE, expectedConnectionsMade, metricMap);\r\n        assertAbfsStatistics(SEND_REQUESTS, expectedRequestsSent, metricMap);\r\n        assertAbfsStatistics(AbfsStatistic.BYTES_SENT, expectedBytesSent, metricMap);\r\n    }\r\n    expectedConnectionsMade++;\r\n    expectedRequestsSent++;\r\n    try (AbfsOutputStream out = createAbfsOutputStreamWithFlushEnabled(fs, sendRequestPath)) {\r\n        if (this.getConfiguration().isConditionalCreateOverwriteEnabled()) {\r\n            expectedConnectionsMade += 3;\r\n            expectedRequestsSent += 2;\r\n        } else {\r\n            expectedConnectionsMade += 1;\r\n            expectedRequestsSent += 1;\r\n        }\r\n        for (int i = 0; i < WRITE_OPERATION_LOOP_COUNT; i++) {\r\n            out.write(testNetworkStatsString.getBytes());\r\n            out.hflush();\r\n            if (fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(sendRequestPath).toString()) || (this.getConfiguration().isSmallWriteOptimizationEnabled())) {\r\n                expectedConnectionsMade++;\r\n                expectedRequestsSent++;\r\n            } else {\r\n                expectedConnectionsMade += 2;\r\n                expectedRequestsSent += 2;\r\n            }\r\n            expectedBytesSent += testNetworkStatsString.getBytes().length;\r\n        }\r\n        metricMap = fs.getInstrumentationMap();\r\n        assertAbfsStatistics(CONNECTIONS_MADE, expectedConnectionsMade, metricMap);\r\n        assertAbfsStatistics(SEND_REQUESTS, expectedRequestsSent, metricMap);\r\n        assertAbfsStatistics(AbfsStatistic.BYTES_SENT, expectedBytesSent, metricMap);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsHttpResponseStatistics",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testAbfsHttpResponseStatistics() throws IOException\n{\r\n    describe(\"Test to check correct values of statistics after Http \" + \"Response is processed.\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path getResponsePath = path(getMethodName());\r\n    Map<String, Long> metricMap;\r\n    String testResponseString = \"some response\";\r\n    FSDataOutputStream out = null;\r\n    FSDataInputStream in = null;\r\n    long expectedConnectionsMade;\r\n    long expectedGetResponses;\r\n    long expectedBytesReceived;\r\n    try {\r\n        out = fs.create(getResponsePath);\r\n        out.write(testResponseString.getBytes());\r\n        out.hflush();\r\n        metricMap = fs.getInstrumentationMap();\r\n        long bytesWrittenToFile = testResponseString.getBytes().length;\r\n        expectedConnectionsMade = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n        expectedGetResponses = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n        expectedBytesReceived = metricMap.get(BYTES_RECEIVED.getStatName());\r\n        in = fs.open(getResponsePath);\r\n        expectedConnectionsMade++;\r\n        expectedGetResponses++;\r\n        int result = in.read();\r\n        expectedConnectionsMade++;\r\n        expectedGetResponses++;\r\n        expectedBytesReceived += bytesWrittenToFile;\r\n        metricMap = fs.getInstrumentationMap();\r\n        assertAbfsStatistics(CONNECTIONS_MADE, expectedConnectionsMade, metricMap);\r\n        assertAbfsStatistics(GET_RESPONSES, expectedGetResponses, metricMap);\r\n        assertAbfsStatistics(AbfsStatistic.BYTES_RECEIVED, expectedBytesReceived, metricMap);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out, in);\r\n    }\r\n    try {\r\n        StringBuilder largeBuffer = new StringBuilder();\r\n        out = fs.create(getResponsePath);\r\n        for (int i = 0; i < WRITE_OPERATION_LOOP_COUNT; i++) {\r\n            out.write(testResponseString.getBytes());\r\n            out.hflush();\r\n            largeBuffer.append(testResponseString);\r\n        }\r\n        metricMap = fs.getInstrumentationMap();\r\n        expectedConnectionsMade = metricMap.get(CONNECTIONS_MADE.getStatName());\r\n        expectedGetResponses = metricMap.get(GET_RESPONSES.getStatName());\r\n        in = fs.open(getResponsePath);\r\n        expectedConnectionsMade++;\r\n        expectedGetResponses++;\r\n        in.read(0, largeBuffer.toString().getBytes(), 0, largeBuffer.toString().getBytes().length);\r\n        expectedConnectionsMade++;\r\n        expectedGetResponses++;\r\n        expectedBytesReceived += (WRITE_OPERATION_LOOP_COUNT * testResponseString.getBytes().length);\r\n        metricMap = fs.getInstrumentationMap();\r\n        assertAbfsStatistics(CONNECTIONS_MADE, expectedConnectionsMade, metricMap);\r\n        assertAbfsStatistics(GET_RESPONSES, expectedGetResponses, metricMap);\r\n        assertAbfsStatistics(AbfsStatistic.BYTES_RECEIVED, expectedBytesReceived, metricMap);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out, in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsHttpResponseFailure",
  "errType" : [ "FileAlreadyExistsException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAbfsHttpResponseFailure() throws IOException\n{\r\n    describe(\"Test to check the values of bytes received counter when a \" + \"response is failed\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path responseFailurePath = path(getMethodName());\r\n    Map<String, Long> metricMap;\r\n    FSDataOutputStream out = null;\r\n    try {\r\n        out = fs.create(responseFailurePath);\r\n        out = fs.create(responseFailurePath, false);\r\n    } catch (FileAlreadyExistsException faee) {\r\n        metricMap = fs.getInstrumentationMap();\r\n        assertAbfsStatistics(AbfsStatistic.BYTES_RECEIVED, 0, metricMap);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, \"/\");\r\n    conf.set(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES, \"/\");\r\n    return AzureBlobStorageTestAccount.create(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initialize(final Configuration configuration) throws IOException\n{\r\n    initialized = true;\r\n    owner = UserGroupInformation.getCurrentUser();\r\n    LOG.info(\"Creating Stub DT manager for {}\", owner.getUserName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n    closed = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(final String renewer) throws IOException\n{\r\n    issued++;\r\n    URI uri = fsURI != null ? fsURI : UNSET_URI;\r\n    Text renewerT = new Text(renewer != null ? renewer : \"\");\r\n    Token t = createToken(issued, uri, new Text(owner.getUserName()), renewerT);\r\n    if (kind != null) {\r\n        t.setKind(kind);\r\n    }\r\n    t.setService(createServiceText());\r\n    LOG.info(\"Created token {}\", t);\r\n    return t;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "createServiceText",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Text createServiceText()\n{\r\n    return new Text(fsURI != null ? fsURI.toString() : UNSET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "createToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> createToken(final int sequenceNumber, final URI uri, final Text owner, final Text renewer)\n{\r\n    StubAbfsTokenIdentifier id = new StubAbfsTokenIdentifier(uri, owner, renewer);\r\n    id.setSequenceNumber(sequenceNumber);\r\n    Token<DelegationTokenIdentifier> token = new Token(id, new TokenSecretManager());\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(final Token<?> token) throws IOException\n{\r\n    renewals++;\r\n    decodeIdentifier(token);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(final Token<?> token) throws IOException\n{\r\n    cancellations++;\r\n    decodeIdentifier(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "innerBind",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void innerBind(final URI uri, final Configuration conf) throws IOException\n{\r\n    Preconditions.checkState(initialized, \"Not initialized\");\r\n    Preconditions.checkState(fsURI == null, \"already bound\");\r\n    fsURI = uri;\r\n    canonicalServiceName = uri.toString();\r\n    LOG.info(\"Bound to {}\", fsURI);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    return canonicalServiceName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "setCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCanonicalServiceName(final String canonicalServiceName)\n{\r\n    this.canonicalServiceName = canonicalServiceName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getFsURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getFsURI()\n{\r\n    return fsURI;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "isInitialized",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isInitialized()\n{\r\n    return initialized;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "isBound",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isBound()\n{\r\n    return fsURI != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return closed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getRenewals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRenewals()\n{\r\n    return renewals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getCancellations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCancellations()\n{\r\n    return cancellations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getIssued",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getIssued()\n{\r\n    return issued;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return kind;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "setKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setKind(final Text kind)\n{\r\n    this.kind = kind;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"StubDelegationTokenManager{\");\r\n    sb.append(\"fsURI=\").append(fsURI);\r\n    sb.append(\", initialized=\").append(initialized);\r\n    sb.append(\", closed=\").append(closed);\r\n    sb.append(\", renewals=\").append(renewals);\r\n    sb.append(\", cancellations=\").append(cancellations);\r\n    sb.append(\", issued=\").append(issued);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "useClassicDTManager",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration useClassicDTManager(Configuration conf)\n{\r\n    conf.setBoolean(FS_AZURE_ENABLE_DELEGATION_TOKEN, true);\r\n    conf.set(FS_AZURE_DELEGATION_TOKEN_PROVIDER_TYPE, ClassicDelegationTokenManager.NAME);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getSecretManagerPasssword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getSecretManagerPasssword()\n{\r\n    return \"non-password\".getBytes(Charset.forName(\"UTF-8\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    dest = path(\"ITestAbfsUnbuffer\");\r\n    byte[] data = ContractTestUtils.dataset(16, 'a', 26);\r\n    ContractTestUtils.writeDataset(getFileSystem(), dest, data, data.length, 16, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testUnbuffer",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUnbuffer() throws IOException\n{\r\n    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\r\n        assertTrue(\"unexpected stream type \" + inputStream.getWrappedStream().getClass().getSimpleName(), inputStream.getWrappedStream() instanceof AbfsInputStream);\r\n        readAndAssertBytesRead(inputStream, 8);\r\n        assertFalse(\"AbfsInputStream buffer should not be null\", isBufferNull(inputStream));\r\n        inputStream.unbuffer();\r\n        assertTrue(\"AbfsInputStream buffer should be null\", isBufferNull(inputStream));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isBufferNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isBufferNull(FSDataInputStream inputStream)\n{\r\n    return ((AbfsInputStream) inputStream.getWrappedStream()).getBuffer() == null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readAndAssertBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readAndAssertBytesRead(FSDataInputStream inputStream, int bytesToRead) throws IOException\n{\r\n    assertEquals(\"AbfsInputStream#read did not read the correct number of \" + \"bytes\", bytesToRead, inputStream.read(new byte[bytesToRead]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testSingleThreaded",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSingleThreaded() throws Exception\n{\r\n    AzureFileSystemInstrumentation instrumentation = new AzureFileSystemInstrumentation(new Configuration());\r\n    BandwidthGaugeUpdater updater = new BandwidthGaugeUpdater(instrumentation, 1000, true);\r\n    updater.triggerUpdate(true);\r\n    assertEquals(0, AzureMetricsTestUtil.getCurrentBytesWritten(instrumentation));\r\n    updater.blockUploaded(new Date(), new Date(), 150);\r\n    updater.triggerUpdate(true);\r\n    assertEquals(150, AzureMetricsTestUtil.getCurrentBytesWritten(instrumentation));\r\n    updater.blockUploaded(new Date(new Date().getTime() - 10000), new Date(), 200);\r\n    updater.triggerUpdate(true);\r\n    long currentBytes = AzureMetricsTestUtil.getCurrentBytesWritten(instrumentation);\r\n    assertTrue(\"We expect around (200/10 = 20) bytes written as the gauge value.\" + \"Got \" + currentBytes, currentBytes > 18 && currentBytes < 22);\r\n    updater.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "testMultiThreaded",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMultiThreaded() throws Exception\n{\r\n    final AzureFileSystemInstrumentation instrumentation = new AzureFileSystemInstrumentation(new Configuration());\r\n    final BandwidthGaugeUpdater updater = new BandwidthGaugeUpdater(instrumentation, 1000, true);\r\n    Thread[] threads = new Thread[10];\r\n    for (int i = 0; i < threads.length; i++) {\r\n        threads[i] = new Thread(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                updater.blockDownloaded(new Date(), new Date(), 10);\r\n                updater.blockDownloaded(new Date(0), new Date(0), 10);\r\n            }\r\n        });\r\n    }\r\n    for (Thread t : threads) {\r\n        t.start();\r\n    }\r\n    for (Thread t : threads) {\r\n        t.join();\r\n    }\r\n    updater.triggerUpdate(false);\r\n    assertEquals(10 * threads.length, AzureMetricsTestUtil.getCurrentBytesRead(instrumentation));\r\n    updater.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "abfsCreateNonRecursiveTestData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection abfsCreateNonRecursiveTestData() throws Exception\n{\r\n    final Collection<Object[]> datas = new ArrayList<>();\r\n    for (FsAction g : FsAction.values()) {\r\n        for (FsAction o : FsAction.values()) {\r\n            datas.add(new Object[] { new FsPermission(FsAction.ALL, g, o) });\r\n        }\r\n    }\r\n    return datas;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFilePermission",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFilePermission() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(getIsNamespaceEnabled(fs));\r\n    fs.getConf().set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, DEFAULT_UMASK_VALUE);\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.mkdirs(path.getParent(), new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE));\r\n    fs.removeDefaultAcl(path.getParent());\r\n    fs.create(path, permission, true, KILOBYTE, (short) 1, KILOBYTE - 1, null).close();\r\n    FileStatus status = fs.getFileStatus(path);\r\n    Assert.assertEquals(permission.applyUMask(DEFAULT_UMASK_PERMISSION), status.getPermission());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFolderPermission",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFolderPermission() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Assume.assumeTrue(getIsNamespaceEnabled(fs));\r\n    fs.getConf().set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, \"027\");\r\n    path = new Path(testRoot, UUID.randomUUID().toString());\r\n    fs.mkdirs(path.getParent(), new FsPermission(FsAction.ALL, FsAction.WRITE, FsAction.NONE));\r\n    fs.removeDefaultAcl(path.getParent());\r\n    fs.mkdirs(path, permission);\r\n    FileStatus status = fs.getFileStatus(path);\r\n    Assert.assertEquals(permission.applyUMask(DEFAULT_UMASK_PERMISSION), status.getPermission());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bind(final URI uri, final Configuration conf) throws IOException\n{\r\n    super.innerBind(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "createToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> createToken(final int sequenceNumber, final URI uri, final Text owner, final Text renewer)\n{\r\n    return ClassicDelegationTokenManager.createToken(sequenceNumber, uri, owner, renewer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "useStubDTManager",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration useStubDTManager(Configuration conf)\n{\r\n    conf.setBoolean(FS_AZURE_ENABLE_DELEGATION_TOKEN, true);\r\n    conf.set(FS_AZURE_DELEGATION_TOKEN_PROVIDER_TYPE, StubDelegationTokenManager.NAME);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testEnsureStatusWorksForRoot",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testEnsureStatusWorksForRoot() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    Path root = new Path(\"/\");\r\n    FileStatus[] rootls = fs.listStatus(root);\r\n    assertEquals(\"root listing\", 0, rootls.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFileStatusPermissionsAndOwnerAndGroup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFileStatusPermissionsAndOwnerAndGroup() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    fs.getConf().set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, DEFAULT_UMASK_VALUE);\r\n    Path testFile = path(TEST_FILE);\r\n    touch(testFile);\r\n    validateStatus(fs, testFile, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateStatus",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FileStatus validateStatus(final AzureBlobFileSystem fs, final Path name, final boolean isDir) throws IOException\n{\r\n    FileStatus fileStatus = fs.getFileStatus(name);\r\n    String errorInStatus = \"error in \" + fileStatus + \" from \" + fs;\r\n    if (!getIsNamespaceEnabled(fs)) {\r\n        assertEquals(errorInStatus + \": owner\", fs.getOwnerUser(), fileStatus.getOwner());\r\n        assertEquals(errorInStatus + \": group\", fs.getOwnerUserPrimaryGroup(), fileStatus.getGroup());\r\n        assertEquals(new FsPermission(FULL_PERMISSION), fileStatus.getPermission());\r\n    } else {\r\n        if (isDir) {\r\n            assertEquals(errorInStatus + \": permission\", new FsPermission(DEFAULT_DIR_PERMISSION_VALUE), fileStatus.getPermission());\r\n        } else {\r\n            assertEquals(errorInStatus + \": permission\", new FsPermission(DEFAULT_FILE_PERMISSION_VALUE), fileStatus.getPermission());\r\n        }\r\n    }\r\n    return fileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testFolderStatusPermissionsAndOwnerAndGroup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFolderStatusPermissionsAndOwnerAndGroup() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.getFileSystem();\r\n    fs.getConf().set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, DEFAULT_UMASK_VALUE);\r\n    Path testFolder = path(TEST_FOLDER);\r\n    fs.mkdirs(testFolder);\r\n    validateStatus(fs, testFolder, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsPathWithHost",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAbfsPathWithHost() throws IOException\n{\r\n    AzureBlobFileSystem fs = this.getFileSystem();\r\n    Path pathWithHost1 = new Path(\"abfs://mycluster/abfs/file1.txt\");\r\n    Path pathwithouthost1 = new Path(\"/abfs/file1.txt\");\r\n    Path pathWithHost2 = new Path(\"abfs://mycluster/abfs/file2.txt\");\r\n    Path pathwithouthost2 = new Path(\"/abfs/file2.txt\");\r\n    fs.create(pathWithHost1).close();\r\n    assertPathExists(fs, \"This path should exist\", pathwithouthost1);\r\n    fs.create(pathwithouthost2).close();\r\n    assertPathExists(fs, \"This path should exist\", pathWithHost2);\r\n    FileStatus fileStatus1 = fs.getFileStatus(pathWithHost1);\r\n    assertEquals(pathwithouthost1.getName(), fileStatus1.getPath().getName());\r\n    FileStatus fileStatus2 = fs.getFileStatus(pathwithouthost2);\r\n    assertEquals(pathWithHost2.getName(), fileStatus2.getPath().getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testLastModifiedTime",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLastModifiedTime() throws IOException\n{\r\n    AzureBlobFileSystem fs = this.getFileSystem();\r\n    Path testFilePath = path(\"childfile1.txt\");\r\n    long createStartTime = System.currentTimeMillis();\r\n    long minCreateStartTime = (createStartTime / 1000) * 1000 - 1;\r\n    fs.create(testFilePath).close();\r\n    long createEndTime = System.currentTimeMillis();\r\n    FileStatus fStat = fs.getFileStatus(testFilePath);\r\n    long lastModifiedTime = fStat.getModificationTime();\r\n    assertTrue(\"lastModifiedTime should be after minCreateStartTime\", minCreateStartTime < lastModifiedTime);\r\n    assertTrue(\"lastModifiedTime should be before createEndTime\", createEndTime > lastModifiedTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_uncleMkdirs",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void outOfBandFolder_uncleMkdirs() throws Exception\n{\r\n    String workingDir = \"user/\" + UserGroupInformation.getCurrentUser().getShortUserName() + \"/\";\r\n    CloudBlockBlob blob = testAccount.getBlobReference(workingDir + \"testFolder1/a/input/file\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    assertTrue(fs.exists(new Path(\"testFolder1/a/input/file\")));\r\n    Path targetFolder = new Path(\"testFolder1/a/output\");\r\n    assertTrue(fs.mkdirs(targetFolder));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_parentDelete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void outOfBandFolder_parentDelete() throws Exception\n{\r\n    String workingDir = \"user/\" + UserGroupInformation.getCurrentUser().getShortUserName() + \"/\";\r\n    CloudBlockBlob blob = testAccount.getBlobReference(workingDir + \"testFolder2/a/input/file\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    assertTrue(fs.exists(new Path(\"testFolder2/a/input/file\")));\r\n    Path targetFolder = new Path(\"testFolder2/a/input\");\r\n    assertTrue(fs.delete(targetFolder, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_rootFileDelete",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void outOfBandFolder_rootFileDelete() throws Exception\n{\r\n    CloudBlockBlob blob = testAccount.getBlobReference(\"fileY\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    assertTrue(fs.exists(new Path(\"/fileY\")));\r\n    assertTrue(fs.delete(new Path(\"/fileY\"), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_firstLevelFolderDelete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void outOfBandFolder_firstLevelFolderDelete() throws Exception\n{\r\n    CloudBlockBlob blob = testAccount.getBlobReference(\"folderW/file\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    assertTrue(fs.exists(new Path(\"/folderW\")));\r\n    assertTrue(fs.exists(new Path(\"/folderW/file\")));\r\n    assertTrue(fs.delete(new Path(\"/folderW\"), true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_siblingCreate",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void outOfBandFolder_siblingCreate() throws Exception\n{\r\n    String workingDir = \"user/\" + UserGroupInformation.getCurrentUser().getShortUserName() + \"/\";\r\n    CloudBlockBlob blob = testAccount.getBlobReference(workingDir + \"testFolder3/a/input/file\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    assertTrue(fs.exists(new Path(\"testFolder3/a/input/file\")));\r\n    Path targetFile = new Path(\"testFolder3/a/input/file2\");\r\n    FSDataOutputStream s2 = fs.create(targetFile);\r\n    s2.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_create_rootDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void outOfBandFolder_create_rootDir() throws Exception\n{\r\n    Path targetFile = new Path(\"/newInRoot\");\r\n    FSDataOutputStream s2 = fs.create(targetFile);\r\n    s2.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_rename",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void outOfBandFolder_rename() throws Exception\n{\r\n    String workingDir = \"user/\" + UserGroupInformation.getCurrentUser().getShortUserName() + \"/\";\r\n    CloudBlockBlob blob = testAccount.getBlobReference(workingDir + \"testFolder4/a/input/file\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    Path srcFilePath = new Path(\"testFolder4/a/input/file\");\r\n    assertTrue(fs.exists(srcFilePath));\r\n    Path destFilePath = new Path(\"testFolder4/a/input/file2\");\r\n    fs.rename(srcFilePath, destFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandSingleFile_rename",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void outOfBandSingleFile_rename() throws Exception\n{\r\n    String workingDir = \"user/\" + UserGroupInformation.getCurrentUser().getShortUserName() + \"/\";\r\n    CloudBlockBlob blob = testAccount.getBlobReference(workingDir + \"testFolder5/a/input/file\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    Path srcFilePath = new Path(\"testFolder5/a/input/file\");\r\n    assertTrue(fs.exists(srcFilePath));\r\n    Path destFilePath = new Path(\"testFolder5/file2\");\r\n    fs.rename(srcFilePath, destFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "outOfBandFolder_rename_rootLevelFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void outOfBandFolder_rename_rootLevelFiles() throws Exception\n{\r\n    CloudBlockBlob blob = testAccount.getBlobReference(\"fileX\");\r\n    BlobOutputStream s = blob.openOutputStream();\r\n    s.close();\r\n    Path srcFilePath = new Path(\"/fileX\");\r\n    assertTrue(fs.exists(srcFilePath));\r\n    Path destFilePath = new Path(\"/fileXrename\");\r\n    fs.rename(srcFilePath, destFilePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListFileStatus",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testListFileStatus() throws Exception\n{\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Assume.assumeFalse(\"Namespace enabled account does not support this test,\", getIsNamespaceEnabled(fs));\r\n    NativeAzureFileSystem wasb = getWasbFileSystem();\r\n    Path testFiles = path(\"/testfiles\");\r\n    Path path1 = new Path(testFiles + \"/~12/!008/3/abFsTestfile\");\r\n    try (FSDataOutputStream abfsStream = fs.create(path1, true)) {\r\n        abfsStream.write(ABFS_TEST_CONTEXT.getBytes());\r\n        abfsStream.flush();\r\n        abfsStream.hsync();\r\n    }\r\n    Path path2 = new Path(testFiles + \"/~12/!008/3/nativeFsTestfile\");\r\n    LOG.info(\"{}\", wasb.getUri());\r\n    try (FSDataOutputStream nativeFsStream = wasb.create(path2, true)) {\r\n        nativeFsStream.write(WASB_TEST_CONTEXT.getBytes());\r\n        nativeFsStream.flush();\r\n        nativeFsStream.hsync();\r\n    }\r\n    FileStatus[] abfsFileStatus = fs.listStatus(new Path(testFiles + \"/~12/!008/3/\"));\r\n    FileStatus[] nativeFsFileStatus = wasb.listStatus(new Path(testFiles + \"/~12/!008/3/\"));\r\n    assertEquals(2, abfsFileStatus.length);\r\n    assertEquals(2, nativeFsFileStatus.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadFile",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReadFile() throws Exception\n{\r\n    boolean[] createFileWithAbfs = new boolean[] { false, true, false, true };\r\n    boolean[] readFileWithAbfs = new boolean[] { false, true, true, false };\r\n    AzureBlobFileSystem abfs = getFileSystem();\r\n    Assume.assumeFalse(\"Namespace enabled account does not support this test\", getIsNamespaceEnabled(abfs));\r\n    NativeAzureFileSystem wasb = getWasbFileSystem();\r\n    Path testFile = path(\"/testReadFile\");\r\n    for (int i = 0; i < 4; i++) {\r\n        Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\r\n        final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\r\n        try (FSDataOutputStream nativeFsStream = createFs.create(path, true)) {\r\n            nativeFsStream.write(TEST_CONTEXT.getBytes());\r\n            nativeFsStream.flush();\r\n            nativeFsStream.hsync();\r\n        }\r\n        ContractTestUtils.assertIsFile(createFs, path);\r\n        final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\r\n        try (BufferedReader br = new BufferedReader(new InputStreamReader(readFs.open(path)))) {\r\n            String line = br.readLine();\r\n            assertEquals(\"Wrong text from \" + readFs, TEST_CONTEXT, line);\r\n        }\r\n        assertDeleted(readFs, path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testDir",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDir() throws Exception\n{\r\n    boolean[] createDirWithAbfs = new boolean[] { false, true, false, true };\r\n    boolean[] readDirWithAbfs = new boolean[] { false, true, true, false };\r\n    AzureBlobFileSystem abfs = getFileSystem();\r\n    Assume.assumeFalse(\"Namespace enabled account does not support this test\", getIsNamespaceEnabled(abfs));\r\n    NativeAzureFileSystem wasb = getWasbFileSystem();\r\n    Path testDir = path(\"/testDir\");\r\n    for (int i = 0; i < 4; i++) {\r\n        Path path = new Path(testDir + \"/t\" + i);\r\n        final FileSystem createFs = createDirWithAbfs[i] ? abfs : wasb;\r\n        assertTrue(createFs.mkdirs(path));\r\n        assertPathExists(createFs, \"Created dir not found with \" + createFs, path);\r\n        final FileSystem readFs = readDirWithAbfs[i] ? abfs : wasb;\r\n        assertPathExists(readFs, \"Created dir not found with \" + readFs, path);\r\n        assertIsDirectory(readFs, path);\r\n        assertDeleted(readFs, path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testUrlConversion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testUrlConversion()\n{\r\n    String abfsUrl = \"abfs://abcde-1111-1111-1111-1111@xxxx.dfs.xxx.xxx.xxxx.xxxx\";\r\n    String wabsUrl = \"wasb://abcde-1111-1111-1111-1111@xxxx.blob.xxx.xxx.xxxx.xxxx\";\r\n    assertEquals(abfsUrl, wasbUrlToAbfsUrl(wabsUrl));\r\n    assertEquals(wabsUrl, abfsUrlToWasbUrl(abfsUrl, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSetWorkingDirectory() throws Exception\n{\r\n    AzureBlobFileSystem abfs = getFileSystem();\r\n    Assume.assumeFalse(\"Namespace enabled account does not support this test\", getIsNamespaceEnabled(abfs));\r\n    NativeAzureFileSystem wasb = getWasbFileSystem();\r\n    Path d1 = path(\"/d1\");\r\n    Path d1d4 = new Path(d1 + \"/d2/d3/d4\");\r\n    assertMkdirs(abfs, d1d4);\r\n    Path path1 = new Path(d1 + \"/d2\");\r\n    wasb.setWorkingDirectory(path1);\r\n    abfs.setWorkingDirectory(path1);\r\n    assertEquals(path1, wasb.getWorkingDirectory());\r\n    assertEquals(path1, abfs.getWorkingDirectory());\r\n    Path path2 = new Path(\"d3/d4\");\r\n    wasb.setWorkingDirectory(path2);\r\n    abfs.setWorkingDirectory(path2);\r\n    Path path3 = d1d4;\r\n    assertEquals(path3, wasb.getWorkingDirectory());\r\n    assertEquals(path3, abfs.getWorkingDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWriteBytesToFileAndEnsureThreadPoolCleanup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testReadWriteBytesToFileAndEnsureThreadPoolCleanup() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testPath = path(TEST_PATH);\r\n    try (FSDataOutputStream stream = fs.create(testPath)) {\r\n        stream.write(TEST_DATA);\r\n    }\r\n    FileStatus fileStatus = fs.getFileStatus(testPath);\r\n    assertEquals(1, fileStatus.getLen());\r\n    try (FSDataInputStream inputStream = fs.open(testPath, 4 * 1024 * 1024)) {\r\n        int i = inputStream.read();\r\n        assertEquals(TEST_DATA, i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWriteOneByteToFileAndEnsureThreadPoolCleanup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWriteOneByteToFileAndEnsureThreadPoolCleanup() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testPath = path(TEST_PATH);\r\n    try (FSDataOutputStream stream = fs.create(testPath)) {\r\n        stream.write(TEST_DATA);\r\n    }\r\n    FileStatus fileStatus = fs.getFileStatus(testPath);\r\n    assertEquals(1, fileStatus.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBase64FileSystemProperties",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testBase64FileSystemProperties() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Hashtable<String, String> properties = new Hashtable<>();\r\n    properties.put(\"key\", \"{ value: value }\");\r\n    TracingContext tracingContext = getTestTracingContext(fs, true);\r\n    fs.getAbfsStore().setFilesystemProperties(properties, tracingContext);\r\n    Hashtable<String, String> fetchedProperties = fs.getAbfsStore().getFilesystemProperties(tracingContext);\r\n    assertEquals(properties, fetchedProperties);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBase64PathProperties",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testBase64PathProperties() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Hashtable<String, String> properties = new Hashtable<>();\r\n    properties.put(\"key\", \"{ value: valueTest }\");\r\n    Path testPath = path(TEST_PATH);\r\n    touch(testPath);\r\n    TracingContext tracingContext = getTestTracingContext(fs, true);\r\n    fs.getAbfsStore().setPathProperties(testPath, properties, tracingContext);\r\n    Hashtable<String, String> fetchedProperties = fs.getAbfsStore().getPathStatus(testPath, tracingContext);\r\n    assertEquals(properties, fetchedProperties);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBase64InvalidFileSystemProperties",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testBase64InvalidFileSystemProperties() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Hashtable<String, String> properties = new Hashtable<>();\r\n    properties.put(\"key\", \"{ value: value歲 }\");\r\n    TracingContext tracingContext = getTestTracingContext(fs, true);\r\n    fs.getAbfsStore().setFilesystemProperties(properties, tracingContext);\r\n    Hashtable<String, String> fetchedProperties = fs.getAbfsStore().getFilesystemProperties(tracingContext);\r\n    assertEquals(properties, fetchedProperties);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testBase64InvalidPathProperties",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testBase64InvalidPathProperties() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Hashtable<String, String> properties = new Hashtable<>();\r\n    properties.put(\"key\", \"{ value: valueTest兩 }\");\r\n    Path testPath = path(TEST_PATH);\r\n    touch(testPath);\r\n    TracingContext tracingContext = getTestTracingContext(fs, true);\r\n    fs.getAbfsStore().setPathProperties(testPath, properties, tracingContext);\r\n    Hashtable<String, String> fetchedProperties = fs.getAbfsStore().getPathStatus(testPath, tracingContext);\r\n    assertEquals(properties, fetchedProperties);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSetFileSystemProperties",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetFileSystemProperties() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final Hashtable<String, String> properties = new Hashtable<>();\r\n    properties.put(\"containerForDevTest\", \"true\");\r\n    TracingContext tracingContext = getTestTracingContext(fs, true);\r\n    fs.getAbfsStore().setFilesystemProperties(properties, tracingContext);\r\n    Hashtable<String, String> fetchedProperties = fs.getAbfsStore().getFilesystemProperties(tracingContext);\r\n    assertEquals(properties, fetchedProperties);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AbfsCommitTestHelper.prepareTestConfiguration(binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "expectPageBlobKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectPageBlobKey(boolean expectedOutcome, AzureNativeFileSystemStore store, String path)\n{\r\n    assertEquals(\"Unexpected result for isPageBlobKey(\" + path + \")\", expectedOutcome, store.isPageBlobKey(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testKeySetWithoutAsterisk",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testKeySetWithoutAsterisk() throws Exception\n{\r\n    NativeAzureFileSystem azureFs = fs;\r\n    AzureNativeFileSystemStore store = azureFs.getStore();\r\n    Configuration conf = fs.getConf();\r\n    String dirList = \"/service/WALs,/data/mypageblobfiles\";\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, dirList);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    expectPageBlobKey(false, store, \"/\");\r\n    expectPageBlobKey(false, store, \"service\");\r\n    expectPageBlobKey(false, store, \"service/dir/recovered.edits\");\r\n    expectPageBlobKey(true, store, \"service/WALs/recovered.edits\");\r\n    expectPageBlobKey(false, store, \"data/dir/recovered.txt\");\r\n    expectPageBlobKey(true, store, \"data/mypageblobfiles/recovered.txt\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testKeySetWithAsterisk",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testKeySetWithAsterisk() throws Exception\n{\r\n    NativeAzureFileSystem azureFs = fs;\r\n    AzureNativeFileSystemStore store = azureFs.getStore();\r\n    Configuration conf = fs.getConf();\r\n    String dirList = \"/service/*/*/*/recovered.edits,/*/recovered.edits,/*/*/*/WALs, /*/*/oldWALs/*/*\";\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, dirList);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    expectPageBlobKey(false, store, \"/\");\r\n    expectPageBlobKey(false, store, \"service\");\r\n    expectPageBlobKey(false, store, \"service/dir/recovered.edits\");\r\n    expectPageBlobKey(true, store, \"service/dir1/dir2/dir3/recovered.edits\");\r\n    expectPageBlobKey(false, store, \"data/dir/recovered.edits\");\r\n    expectPageBlobKey(true, store, \"data/recovered.edits\");\r\n    expectPageBlobKey(false, store, \"dir1/dir2/WALs/data\");\r\n    expectPageBlobKey(true, store, \"dir1/dir2/dir3/WALs/data1\");\r\n    expectPageBlobKey(true, store, \"dir1/dir2/dir3/WALs/data2\");\r\n    expectPageBlobKey(false, store, \"dir1/oldWALs/data\");\r\n    expectPageBlobKey(false, store, \"dir1/dir2/oldWALs/data\");\r\n    expectPageBlobKey(true, store, \"dir1/dir2/oldWALs/dir3/dir4/data\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testKeySetUsingFullName",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testKeySetUsingFullName() throws Exception\n{\r\n    NativeAzureFileSystem azureFs = fs;\r\n    AzureNativeFileSystemStore store = azureFs.getStore();\r\n    Configuration conf = fs.getConf();\r\n    String dirList = \"/service/WALs,/data/mypageblobfiles,/*/*/WALs,/*/*/recover.edits\";\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, dirList);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    final String defaultFS = FileSystem.getDefaultUri(conf).toString();\r\n    expectPageBlobKey(false, store, defaultFS + \"service/recover.edits\");\r\n    expectPageBlobKey(true, store, defaultFS + \"service/WALs/recover.edits\");\r\n    expectPageBlobKey(false, store, defaultFS + \"data/mismatch/mypageblobfiles/data\");\r\n    expectPageBlobKey(true, store, defaultFS + \"data/mypageblobfiles/data\");\r\n    expectPageBlobKey(false, store, defaultFS + \"dir1/dir2/dir3/WALs/data\");\r\n    expectPageBlobKey(true, store, defaultFS + \"dir1/dir2/WALs/data\");\r\n    expectPageBlobKey(false, store, defaultFS + \"dir1/dir2/dir3/recover.edits\");\r\n    expectPageBlobKey(true, store, defaultFS + \"dir1/dir2/recover.edits\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testKeyContainsAsterisk",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testKeyContainsAsterisk() throws IOException\n{\r\n    NativeAzureFileSystem azureFs = fs;\r\n    AzureNativeFileSystemStore store = azureFs.getStore();\r\n    Configuration conf = fs.getConf();\r\n    String dirList = \"/service/*/*/*/d*ir,/*/fi**le.data,/*/*/*/WALs*, /*/*/oldWALs\";\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, dirList);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    expectPageBlobKey(false, store, \"/\");\r\n    expectPageBlobKey(false, store, \"service\");\r\n    expectPageBlobKey(false, store, \"service/d*ir/data\");\r\n    expectPageBlobKey(true, store, \"service/dir1/dir2/dir3/d*ir/data\");\r\n    expectPageBlobKey(false, store, \"dir/fi*le.data\");\r\n    expectPageBlobKey(true, store, \"dir/fi**le.data\");\r\n    expectPageBlobKey(false, store, \"dir1/dir2/WALs/data\");\r\n    expectPageBlobKey(false, store, \"dir1/dir2/dir3/WALs/data\");\r\n    expectPageBlobKey(true, store, \"dir1/dir2/dir3/WALs*/data1\");\r\n    expectPageBlobKey(true, store, \"dir1/dir2/dir3/WALs*/data2\");\r\n    expectPageBlobKey(false, store, \"dir1/oldWALs/data\");\r\n    expectPageBlobKey(true, store, \"dir1/dir2/oldWALs/data1\");\r\n    expectPageBlobKey(true, store, \"dir1/dir2/oldWALs/data2\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testKeyWithCommonPrefix",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testKeyWithCommonPrefix() throws IOException\n{\r\n    NativeAzureFileSystem azureFs = fs;\r\n    AzureNativeFileSystemStore store = azureFs.getStore();\r\n    Configuration conf = fs.getConf();\r\n    String dirList = \"/service/WALs,/*/*/WALs\";\r\n    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, dirList);\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n    expectPageBlobKey(false, store, \"/\");\r\n    expectPageBlobKey(false, store, \"service\");\r\n    expectPageBlobKey(false, store, \"service/WALsssssss/dir\");\r\n    expectPageBlobKey(true, store, \"service/WALs/dir\");\r\n    expectPageBlobKey(false, store, \"service/dir/WALsss/data\");\r\n    expectPageBlobKey(true, store, \"service/dir/WALs/data\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    testAccount = createTestAccount();\r\n    fs = testAccount.getFileSystem();\r\n    Configuration conf = fs.getConf();\r\n    conf.setBoolean(NativeAzureFileSystem.APPEND_SUPPORT_ENABLE_PROPERTY_NAME, true);\r\n    conf.set(AzureNativeFileSystemStore.KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES, \"/user/active\");\r\n    URI uri = fs.getUri();\r\n    fs.initialize(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getTestData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getTestData(int size)\n{\r\n    byte[] testData = new byte[size];\r\n    System.arraycopy(RandomStringUtils.randomAlphabetic(size).getBytes(), 0, testData, 0, size);\r\n    return testData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBlockBlobAppendStream",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "BlockBlobAppendStream getBlockBlobAppendStream(FSDataOutputStream appendStream)\n{\r\n    SyncableDataOutputStream dataOutputStream = null;\r\n    if (appendStream.getWrappedStream() instanceof NativeAzureFileSystem.NativeAzureFsOutputStream) {\r\n        NativeAzureFileSystem.NativeAzureFsOutputStream fsOutputStream = (NativeAzureFileSystem.NativeAzureFsOutputStream) appendStream.getWrappedStream();\r\n        dataOutputStream = (SyncableDataOutputStream) fsOutputStream.getOutStream();\r\n    }\r\n    if (appendStream.getWrappedStream() instanceof SyncableDataOutputStream) {\r\n        dataOutputStream = (SyncableDataOutputStream) appendStream.getWrappedStream();\r\n    }\r\n    Assert.assertNotNull(\"Did not recognize \" + dataOutputStream, dataOutputStream);\r\n    return (BlockBlobAppendStream) dataOutputStream.getOutStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyBlockList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyBlockList(BlockBlobAppendStream blockBlobStream, int[] testData) throws Throwable\n{\r\n    List<BlockEntry> blockList = blockBlobStream.getBlockList();\r\n    Assert.assertEquals(\"Block list length\", testData.length, blockList.size());\r\n    int i = 0;\r\n    for (BlockEntry block : blockList) {\r\n        Assert.assertTrue(block.getSize() == testData[i++]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "appendBlockList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void appendBlockList(FSDataOutputStream fsStream, ByteArrayOutputStream memStream, int[] testData) throws Throwable\n{\r\n    for (int d : testData) {\r\n        byte[] data = getTestData(d);\r\n        memStream.write(data);\r\n        fsStream.write(data);\r\n    }\r\n    fsStream.hflush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCompactionDisabled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCompactionDisabled() throws Throwable\n{\r\n    try (FSDataOutputStream appendStream = fs.create(TEST_PATH_NORMAL)) {\r\n        SyncableDataOutputStream dataOutputStream = null;\r\n        OutputStream wrappedStream = appendStream.getWrappedStream();\r\n        if (wrappedStream instanceof NativeAzureFileSystem.NativeAzureFsOutputStream) {\r\n            NativeAzureFileSystem.NativeAzureFsOutputStream fsOutputStream = (NativeAzureFileSystem.NativeAzureFsOutputStream) wrappedStream;\r\n            dataOutputStream = (SyncableDataOutputStream) fsOutputStream.getOutStream();\r\n        } else if (wrappedStream instanceof SyncableDataOutputStream) {\r\n            dataOutputStream = (SyncableDataOutputStream) wrappedStream;\r\n        } else {\r\n            Assert.fail(\"Unable to determine type of \" + wrappedStream + \" class of \" + wrappedStream.getClass());\r\n        }\r\n        Assert.assertFalse(\"Data output stream is a BlockBlobAppendStream: \" + dataOutputStream, dataOutputStream.getOutStream() instanceof BlockBlobAppendStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "testCompaction",
  "errType" : null,
  "containingMethodsNum" : 53,
  "sourceCodeText" : "void testCompaction() throws Throwable\n{\r\n    final int n2 = 2;\r\n    final int n4 = 4;\r\n    final int n10 = 10;\r\n    final int n12 = 12;\r\n    final int n14 = 14;\r\n    final int n16 = 16;\r\n    final int maxBlockSize = 16;\r\n    final int compactionBlockCount = 4;\r\n    ByteArrayOutputStream memStream = new ByteArrayOutputStream();\r\n    try (FSDataOutputStream appendStream = fs.create(TEST_PATH)) {\r\n        BlockBlobAppendStream blockBlobStream = getBlockBlobAppendStream(appendStream);\r\n        blockBlobStream.setMaxBlockSize(maxBlockSize);\r\n        blockBlobStream.setCompactionBlockCount(compactionBlockCount);\r\n        appendBlockList(appendStream, memStream, new int[] { n2 });\r\n        verifyBlockList(blockBlobStream, new int[] { n2 });\r\n        appendStream.hflush();\r\n        verifyBlockList(blockBlobStream, new int[] { n2 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n2, n4 });\r\n        appendStream.hsync();\r\n        verifyBlockList(blockBlobStream, new int[] { n2, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n2, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n2, n4, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n4, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n2, n4, n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n10 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n10, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4, n4, n4, n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4, n4, n4, n4, n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n4, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n4, n4, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        appendStream.close();\r\n        ContractTestUtils.verifyFileContents(fs, TEST_PATH, memStream.toByteArray());\r\n    }\r\n    try (FSDataOutputStream appendStream = fs.append(TEST_PATH)) {\r\n        BlockBlobAppendStream blockBlobStream = getBlockBlobAppendStream(appendStream);\r\n        blockBlobStream.setMaxBlockSize(maxBlockSize);\r\n        blockBlobStream.setCompactionBlockCount(compactionBlockCount);\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n16, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n16, n4, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n16, n4, n4, n4, n4 });\r\n        appendBlockList(appendStream, memStream, new int[] { n4 });\r\n        verifyBlockList(blockBlobStream, new int[] { n14, n12, n14, n16, n16, n16, n16, n4 });\r\n        appendStream.close();\r\n        ContractTestUtils.verifyFileContents(fs, TEST_PATH, memStream.toByteArray());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testAbfsStreamOps",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testAbfsStreamOps() throws Exception\n{\r\n    describe(\"Test to see correct population of read and write operations in \" + \"Abfs\");\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path smallOperationsFile = path(\"testOneReadWriteOps\");\r\n    Path largeOperationsFile = path(\"testLargeReadWriteOps\");\r\n    FileSystem.Statistics statistics = fs.getFsStatistics();\r\n    String testReadWriteOps = \"test this\";\r\n    statistics.reset();\r\n    assertReadWriteOps(\"write\", 0, statistics.getWriteOps());\r\n    assertReadWriteOps(\"read\", 0, statistics.getReadOps());\r\n    FSDataOutputStream outForOneOperation = null;\r\n    FSDataInputStream inForOneOperation = null;\r\n    try {\r\n        outForOneOperation = fs.create(smallOperationsFile);\r\n        statistics.reset();\r\n        outForOneOperation.write(testReadWriteOps.getBytes());\r\n        assertReadWriteOps(\"write\", 1, statistics.getWriteOps());\r\n        outForOneOperation.hflush();\r\n        inForOneOperation = fs.open(smallOperationsFile);\r\n        statistics.reset();\r\n        int result = inForOneOperation.read(testReadWriteOps.getBytes(), 0, testReadWriteOps.getBytes().length);\r\n        LOG.info(\"Result of Read operation : {}\", result);\r\n        assertTrue(String.format(\"The actual value of %d was not equal to the \" + \"expected value of 2 or 3\", statistics.getReadOps()), statistics.getReadOps() == 2 || statistics.getReadOps() == 3);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, inForOneOperation, outForOneOperation);\r\n    }\r\n    assertTrue(\"Mismatch in content validation\", validateContent(fs, smallOperationsFile, testReadWriteOps.getBytes()));\r\n    FSDataOutputStream outForLargeOperations = null;\r\n    FSDataInputStream inForLargeOperations = null;\r\n    StringBuilder largeOperationsValidationString = new StringBuilder();\r\n    try {\r\n        outForLargeOperations = fs.create(largeOperationsFile);\r\n        statistics.reset();\r\n        int largeValue = LARGE_NUMBER_OF_OPS;\r\n        for (int i = 0; i < largeValue; i++) {\r\n            outForLargeOperations.write(testReadWriteOps.getBytes());\r\n            largeOperationsValidationString.append(testReadWriteOps);\r\n        }\r\n        LOG.info(\"Number of bytes of Large data written: {}\", largeOperationsValidationString.toString().getBytes().length);\r\n        assertReadWriteOps(\"write\", largeValue, statistics.getWriteOps());\r\n        inForLargeOperations = fs.open(largeOperationsFile);\r\n        for (int i = 0; i < largeValue; i++) {\r\n            inForLargeOperations.read(testReadWriteOps.getBytes(), 0, testReadWriteOps.getBytes().length);\r\n        }\r\n        if (fs.getAbfsStore().isAppendBlobKey(fs.makeQualified(largeOperationsFile).toString())) {\r\n            assertTrue(String.format(\"The actual value of %d was not equal to the \" + \"expected value\", statistics.getReadOps()), statistics.getReadOps() == (largeValue + 3) || statistics.getReadOps() == (largeValue + 4));\r\n        } else {\r\n            assertReadWriteOps(\"read\", largeValue, statistics.getReadOps());\r\n        }\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, inForLargeOperations, outForLargeOperations);\r\n    }\r\n    assertTrue(\"Mismatch in content validation\", validateContent(fs, largeOperationsFile, largeOperationsValidationString.toString().getBytes()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertReadWriteOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertReadWriteOps(String operation, long expectedValue, long actualValue)\n{\r\n    assertEquals(\"Mismatch in \" + operation + \" operations\", expectedValue, actualValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "nameTestThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameTestThread()\n{\r\n    Thread.currentThread().setName(\"JUnit\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "nameThread",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nameThread()\n{\r\n    Thread.currentThread().setName(\"JUnit-\" + methodName.getMethodName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return TEST_TIMEOUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "describe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void describe(String text, Object... args)\n{\r\n    LOG.info(\"\\n\\n{}: {}\\n\", methodName.getMethodName(), String.format(text, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateContent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean validateContent(AzureBlobFileSystem fs, Path path, byte[] originalByteArray) throws IOException\n{\r\n    int pos = 0;\r\n    int lenOfOriginalByteArray = originalByteArray.length;\r\n    try (FSDataInputStream in = fs.open(path)) {\r\n        byte valueOfContentAtPos = (byte) in.read();\r\n        while (valueOfContentAtPos != -1 && pos < lenOfOriginalByteArray) {\r\n            if (originalByteArray[pos] != valueOfContentAtPos) {\r\n                assertEquals(\"Mismatch in content validation at position {}\", pos, originalByteArray[pos], valueOfContentAtPos);\r\n                return false;\r\n            }\r\n            valueOfContentAtPos = (byte) in.read();\r\n            pos++;\r\n        }\r\n        if (valueOfContentAtPos != -1) {\r\n            assertEquals(\"Expected end of file\", -1, valueOfContentAtPos);\r\n            return false;\r\n        }\r\n        return true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return binding.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    conf.setInt(AZURE_READ_AHEAD_RANGE, MIN_BUFFER_SIZE);\r\n    conf.setInt(AZURE_READ_BUFFER_SIZE, MIN_BUFFER_SIZE);\r\n    return new AbfsFileSystemContract(conf, isSecure);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testSeekAndReadWithReadAhead",
  "errType" : null,
  "containingMethodsNum" : 60,
  "sourceCodeText" : "void testSeekAndReadWithReadAhead() throws IOException\n{\r\n    describe(\" Testing seek and read with read ahead \" + \"enabled for random reads\");\r\n    Path testSeekFile = path(getMethodName() + \"bigseekfile.txt\");\r\n    createDataSet(testSeekFile);\r\n    try (FSDataInputStream in = getFileSystem().open(testSeekFile)) {\r\n        AbfsInputStream inStream = ((AbfsInputStream) in.getWrappedStream());\r\n        AbfsInputStreamStatisticsImpl streamStatistics = (AbfsInputStreamStatisticsImpl) inStream.getStreamStatistics();\r\n        assertEquals(String.format(\"Value of %s is not set correctly\", AZURE_READ_AHEAD_RANGE), MIN_BUFFER_SIZE, inStream.getReadAheadRange());\r\n        long remoteReadOperationsOldVal = streamStatistics.getRemoteReadOperations();\r\n        Assertions.assertThat(remoteReadOperationsOldVal).describedAs(\"Number of remote read ops should be 0 \" + \"before any read call is made\").isEqualTo(0);\r\n        Assertions.assertThat(inStream.getPos()).describedAs(\"First call to getPos() should return 0\").isEqualTo(0);\r\n        assertDataAtPos(0, (byte) in.read());\r\n        assertSeekBufferStats(0, streamStatistics.getSeekInBuffer());\r\n        long remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n        remoteReadOperationsOldVal = remoteReadOperationsNewVal;\r\n        int newSeek = inStream.getReadAheadRange() - 1;\r\n        in.seek(newSeek);\r\n        assertGetPosition(newSeek, in.getPos());\r\n        assertDataAtPos(newSeek, (byte) in.read());\r\n        assertSeekBufferStats(1, streamStatistics.getSeekInBuffer());\r\n        remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertNoIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n        remoteReadOperationsOldVal = remoteReadOperationsNewVal;\r\n        newSeek = inStream.getReadAheadRange();\r\n        inStream.seek(newSeek);\r\n        assertGetPosition(newSeek, in.getPos());\r\n        assertDataAtPos(newSeek, (byte) in.read());\r\n        assertSeekBufferStats(1, streamStatistics.getSeekInBuffer());\r\n        remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertNoIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n        remoteReadOperationsOldVal = remoteReadOperationsNewVal;\r\n        newSeek = inStream.getReadAheadRange() + 1;\r\n        in.seek(newSeek);\r\n        assertGetPosition(newSeek, in.getPos());\r\n        assertDataAtPos(newSeek, (byte) in.read());\r\n        assertSeekBufferStats(2, streamStatistics.getSeekInBuffer());\r\n        remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertNoIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n        remoteReadOperationsOldVal = remoteReadOperationsNewVal;\r\n        newSeek += 10;\r\n        in.seek(newSeek);\r\n        assertGetPosition(newSeek, in.getPos());\r\n        assertDataAtPos(newSeek, (byte) in.read());\r\n        assertSeekBufferStats(3, streamStatistics.getSeekInBuffer());\r\n        remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertNoIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n        remoteReadOperationsOldVal = remoteReadOperationsNewVal;\r\n        newSeek -= 106;\r\n        in.seek(newSeek);\r\n        assertGetPosition(newSeek, in.getPos());\r\n        assertDataAtPos(newSeek, (byte) in.read());\r\n        assertSeekBufferStats(3, streamStatistics.getSeekInBuffer());\r\n        remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n        remoteReadOperationsOldVal = remoteReadOperationsNewVal;\r\n        newSeek += 10;\r\n        in.seek(newSeek);\r\n        assertGetPosition(newSeek, in.getPos());\r\n        assertDataAtPos(newSeek, (byte) in.read());\r\n        assertSeekBufferStats(4, streamStatistics.getSeekInBuffer());\r\n        remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertNoIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n        remoteReadOperationsOldVal = remoteReadOperationsNewVal;\r\n        long oldSeek = newSeek;\r\n        newSeek = 2 * inStream.getReadAheadRange() - 1;\r\n        byte[] bytes = new byte[5];\r\n        in.readFully(newSeek, bytes);\r\n        assertGetPosition(oldSeek + 1, in.getPos());\r\n        assertSeekBufferStats(4, streamStatistics.getSeekInBuffer());\r\n        assertDatasetEquals(newSeek, \"Read across read ahead \", bytes, bytes.length);\r\n        remoteReadOperationsNewVal = streamStatistics.getRemoteReadOperations();\r\n        assertIncrementInRemoteReadOps(remoteReadOperationsOldVal, remoteReadOperationsNewVal);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testSeekAfterUnbuffer",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSeekAfterUnbuffer() throws IOException\n{\r\n    describe(\"Test to make sure that seeking in AbfsInputStream after \" + \"unbuffer() call is not doing anyIO.\");\r\n    Path testFile = path(getMethodName() + \".txt\");\r\n    createDataSet(testFile);\r\n    final CompletableFuture<FSDataInputStream> future = getFileSystem().openFile(testFile).build();\r\n    try (FSDataInputStream inputStream = awaitFuture(future)) {\r\n        AbfsInputStream abfsInputStream = (AbfsInputStream) inputStream.getWrappedStream();\r\n        AbfsInputStreamStatisticsImpl streamStatistics = (AbfsInputStreamStatisticsImpl) abfsInputStream.getStreamStatistics();\r\n        int readAheadRange = abfsInputStream.getReadAheadRange();\r\n        long seekPos = readAheadRange;\r\n        inputStream.seek(seekPos);\r\n        assertDataAtPos(readAheadRange, (byte) inputStream.read());\r\n        long currentRemoteReadOps = streamStatistics.getRemoteReadOperations();\r\n        assertIncrementInRemoteReadOps(0, currentRemoteReadOps);\r\n        inputStream.unbuffer();\r\n        seekPos -= 10;\r\n        inputStream.seek(seekPos);\r\n        assertNoIncrementInRemoteReadOps(currentRemoteReadOps, streamStatistics.getRemoteReadOperations());\r\n        assertGetPosition(seekPos, inputStream.getPos());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "createDataSet",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createDataSet(Path path) throws IOException\n{\r\n    createFile(getFileSystem(), path, true, BLOCK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "assertGetPosition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertGetPosition(long expected, long actual)\n{\r\n    final String seekPosErrorMsg = \"getPos() should return %s\";\r\n    Assertions.assertThat(actual).describedAs(seekPosErrorMsg, expected).isEqualTo(actual);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "assertDataAtPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertDataAtPos(int pos, byte actualData)\n{\r\n    final String dataErrorMsg = \"Mismatch in data@%s\";\r\n    Assertions.assertThat(actualData).describedAs(dataErrorMsg, pos).isEqualTo(BLOCK[pos]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "assertSeekBufferStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertSeekBufferStats(long expected, long actual)\n{\r\n    final String statsErrorMsg = \"Mismatch in seekInBuffer counts\";\r\n    Assertions.assertThat(actual).describedAs(statsErrorMsg).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "assertNoIncrementInRemoteReadOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertNoIncrementInRemoteReadOps(long oldVal, long newVal)\n{\r\n    final String incrementErrorMsg = \"Number of remote read ops shouldn't increase\";\r\n    Assertions.assertThat(newVal).describedAs(incrementErrorMsg).isEqualTo(oldVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "assertIncrementInRemoteReadOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertIncrementInRemoteReadOps(long oldVal, long newVal)\n{\r\n    final String incrementErrorMsg = \"Number of remote read ops should increase\";\r\n    Assertions.assertThat(newVal).describedAs(incrementErrorMsg).isGreaterThan(oldVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "assertDatasetEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertDatasetEquals(final int readOffset, final String operation, final byte[] data, int length)\n{\r\n    for (int i = 0; i < length; i++) {\r\n        int o = readOffset + i;\r\n        Assertions.assertThat(data[i]).describedAs(operation + \"with read offset \" + readOffset + \": data[\" + i + \"] != actualData[\" + o + \"]\").isEqualTo(BLOCK[o]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "generateSAS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateSAS(byte[] accountKey, String accountName, String fileSystemName)\n{\r\n    return generator.getContainerSASWithFullControl(accountName, fileSystemName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "initialize",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(Configuration configuration, String accountName) throws IOException\n{\r\n    try {\r\n        AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration, accountName);\r\n        accountKey = Base64.decode(abfsConfig.getStorageAccountKey());\r\n    } catch (Exception ex) {\r\n        throw new IOException(ex);\r\n    }\r\n    generator = new ServiceSASGenerator(accountKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getSASToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getSASToken(String accountName, String fileSystem, String path, String operation) throws IOException, AccessControlException\n{\r\n    if (!isSkipAuthorizationForTestSetup() && path.contains(\"unauthorized\")) {\r\n        throw new AccessControlException(\"The user is not authorized to perform this operation.\");\r\n    }\r\n    return generateSAS(accountKey, accountName, fileSystem);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "isSkipAuthorizationForTestSetup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSkipAuthorizationForTestSetup()\n{\r\n    return skipAuthorizationForTestSetup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "setSkipAuthorizationForTestSetup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSkipAuthorizationForTestSetup(boolean skipAuthorizationForTestSetup)\n{\r\n    this.skipAuthorizationForTestSetup = skipAuthorizationForTestSetup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testValidateFunctionsInConfigServiceImpl",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testValidateFunctionsInConfigServiceImpl() throws Exception\n{\r\n    Field[] fields = this.getClass().getDeclaredFields();\r\n    for (Field field : fields) {\r\n        field.setAccessible(true);\r\n        if (field.isAnnotationPresent(IntegerConfigurationValidatorAnnotation.class)) {\r\n            assertEquals(TEST_INT, abfsConfiguration.validateInt(field));\r\n        } else if (field.isAnnotationPresent(LongConfigurationValidatorAnnotation.class)) {\r\n            assertEquals(DEFAULT_LONG, abfsConfiguration.validateLong(field));\r\n        } else if (field.isAnnotationPresent(StringConfigurationValidatorAnnotation.class)) {\r\n            assertEquals(\"stringValue\", abfsConfiguration.validateString(field));\r\n        } else if (field.isAnnotationPresent(Base64StringConfigurationValidatorAnnotation.class)) {\r\n            assertEquals(this.encodedString, abfsConfiguration.validateBase64String(field));\r\n        } else if (field.isAnnotationPresent(BooleanConfigurationValidatorAnnotation.class)) {\r\n            assertEquals(true, abfsConfiguration.validateBoolean(field));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testConfigServiceImplAnnotatedFieldsInitialized",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testConfigServiceImplAnnotatedFieldsInitialized() throws Exception\n{\r\n    assertEquals(DEFAULT_WRITE_BUFFER_SIZE, abfsConfiguration.getWriteBufferSize());\r\n    assertEquals(DEFAULT_READ_BUFFER_SIZE, abfsConfiguration.getReadBufferSize());\r\n    assertEquals(DEFAULT_MIN_BACKOFF_INTERVAL, abfsConfiguration.getMinBackoffIntervalMilliseconds());\r\n    assertEquals(DEFAULT_MAX_BACKOFF_INTERVAL, abfsConfiguration.getMaxBackoffIntervalMilliseconds());\r\n    assertEquals(DEFAULT_BACKOFF_INTERVAL, abfsConfiguration.getBackoffIntervalMilliseconds());\r\n    assertEquals(DEFAULT_MAX_RETRY_ATTEMPTS, abfsConfiguration.getMaxIoRetries());\r\n    assertEquals(MAX_AZURE_BLOCK_SIZE, abfsConfiguration.getAzureBlockSize());\r\n    assertEquals(AZURE_BLOCK_LOCATION_HOST_DEFAULT, abfsConfiguration.getAzureBlockLocationHost());\r\n    assertEquals(DEFAULT_READ_AHEAD_RANGE, abfsConfiguration.getReadAheadRange());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testConfigBlockSizeInitialized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConfigBlockSizeInitialized() throws Exception\n{\r\n    assertEquals(MAX_AZURE_BLOCK_SIZE, abfsConfiguration.getAzureBlockSize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAccountKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetAccountKey() throws Exception\n{\r\n    String accountKey = abfsConfiguration.getStorageAccountKey();\r\n    assertEquals(this.encodedAccountKey, accountKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testGetAccountKeyWithNonExistingAccountName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetAccountKeyWithNonExistingAccountName() throws Exception\n{\r\n    Configuration configuration = new Configuration();\r\n    configuration.addResource(TestConfigurationKeys.TEST_CONFIGURATION_FILE_NAME);\r\n    configuration.unset(ConfigurationKeys.FS_AZURE_ACCOUNT_KEY_PROPERTY_NAME);\r\n    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration, \"bogusAccountName\");\r\n    abfsConfig.getStorageAccountKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSSLSocketFactoryConfiguration",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSSLSocketFactoryConfiguration() throws InvalidConfigurationValueException, IllegalAccessException, IOException\n{\r\n    assertEquals(DelegatingSSLSocketFactory.SSLChannelMode.Default, abfsConfiguration.getPreferredSSLFactoryOption());\r\n    assertNotEquals(DelegatingSSLSocketFactory.SSLChannelMode.Default_JSSE, abfsConfiguration.getPreferredSSLFactoryOption());\r\n    assertNotEquals(DelegatingSSLSocketFactory.SSLChannelMode.OpenSSL, abfsConfiguration.getPreferredSSLFactoryOption());\r\n    Configuration configuration = new Configuration();\r\n    configuration.setEnum(FS_AZURE_SSL_CHANNEL_MODE_KEY, DelegatingSSLSocketFactory.SSLChannelMode.Default_JSSE);\r\n    AbfsConfiguration localAbfsConfiguration = new AbfsConfiguration(configuration, accountName);\r\n    assertEquals(DelegatingSSLSocketFactory.SSLChannelMode.Default_JSSE, localAbfsConfiguration.getPreferredSSLFactoryOption());\r\n    configuration = new Configuration();\r\n    configuration.setEnum(FS_AZURE_SSL_CHANNEL_MODE_KEY, DelegatingSSLSocketFactory.SSLChannelMode.OpenSSL);\r\n    localAbfsConfiguration = new AbfsConfiguration(configuration, accountName);\r\n    assertEquals(DelegatingSSLSocketFactory.SSLChannelMode.OpenSSL, localAbfsConfiguration.getPreferredSSLFactoryOption());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "updateRetryConfigs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbfsConfiguration updateRetryConfigs(AbfsConfiguration abfsConfig, int retryCount, int backoffTime)\n{\r\n    abfsConfig.setMaxIoRetries(retryCount);\r\n    abfsConfig.setMaxBackoffIntervalMilliseconds(backoffTime);\r\n    return abfsConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testMatchingJSON",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMatchingJSON() throws IOException\n{\r\n    String matchingJson = \"{ \\\"paths\\\": [ { \\\"contentLength\\\": \\\"0\\\", \\\"etag\\\": \" + \"\\\"0x8D8186452785ADA\\\", \\\"group\\\": \\\"$superuser\\\", \" + \"\\\"lastModified\\\": \\\"Wed, 24 Jun 2020 17:30:43 GMT\\\", \\\"name\\\": \" + \"\\\"dest/filename\\\", \\\"owner\\\": \\\"$superuser\\\", \\\"permissions\\\": \" + \"\\\"rw-r--r--\\\" } ] } \";\r\n    final ObjectMapper objectMapper = new ObjectMapper();\r\n    final ListResultSchema listResultSchema = objectMapper.readValue(matchingJson, ListResultSchema.class);\r\n    assertThat(listResultSchema.paths().size()).describedAs(\"Only one path is expected as present in the input JSON\").isEqualTo(1);\r\n    ListResultEntrySchema path = listResultSchema.paths().get(0);\r\n    assertThat(path.contentLength()).describedAs(\"contentLength should match the value in the input JSON\").isEqualTo(0L);\r\n    assertThat(path.eTag()).describedAs(\"eTag should match the value in the input JSON\").isEqualTo(\"0x8D8186452785ADA\");\r\n    assertThat(path.group()).describedAs(\"group should match the value in the input JSON\").isEqualTo(\"$superuser\");\r\n    assertThat(path.lastModified()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"Wed, 24 Jun 2020 17:30:43 GMT\");\r\n    assertThat(path.name()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"dest/filename\");\r\n    assertThat(path.owner()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"$superuser\");\r\n    assertThat(path.permissions()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"rw-r--r--\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "testJSONWithUnknownFields",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJSONWithUnknownFields() throws IOException\n{\r\n    String matchingJson = \"{ \\\"paths\\\": [ { \\\"contentLength\\\": \\\"0\\\", \" + \"\\\"unknownProperty\\\": \\\"132374934429527192\\\", \\\"etag\\\": \" + \"\\\"0x8D8186452785ADA\\\", \\\"group\\\": \\\"$superuser\\\", \" + \"\\\"lastModified\\\": \\\"Wed, 24 Jun 2020 17:30:43 GMT\\\", \\\"name\\\": \" + \"\\\"dest/filename\\\", \\\"owner\\\": \\\"$superuser\\\", \\\"permissions\\\": \" + \"\\\"rw-r--r--\\\" } ] } \";\r\n    final ObjectMapper objectMapper = new ObjectMapper();\r\n    final ListResultSchema listResultSchema = objectMapper.readValue(matchingJson, ListResultSchema.class);\r\n    assertThat(listResultSchema.paths().size()).describedAs(\"Only one path is expected as present in the input JSON\").isEqualTo(1);\r\n    ListResultEntrySchema path = listResultSchema.paths().get(0);\r\n    assertThat(path.contentLength()).describedAs(\"contentLength should match the value in the input JSON\").isEqualTo(0L);\r\n    assertThat(path.eTag()).describedAs(\"eTag should match the value in the input JSON\").isEqualTo(\"0x8D8186452785ADA\");\r\n    assertThat(path.group()).describedAs(\"group should match the value in the input JSON\").isEqualTo(\"$superuser\");\r\n    assertThat(path.lastModified()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"Wed, 24 Jun 2020 17:30:43 GMT\");\r\n    assertThat(path.name()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"dest/filename\");\r\n    assertThat(path.owner()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"$superuser\");\r\n    assertThat(path.permissions()).describedAs(\"lastModified should match the value in the input JSON\").isEqualTo(\"rw-r--r--\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "checkContainers",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkContainers() throws Throwable\n{\r\n    Assume.assumeTrue(this.getAuthType() == AuthType.SharedKey);\r\n    int count = 0;\r\n    CloudStorageAccount storageAccount = AzureBlobStorageTestAccount.createTestAccount();\r\n    CloudBlobClient blobClient = storageAccount.createCloudBlobClient();\r\n    Iterable<CloudBlobContainer> containers = blobClient.listContainers(TEST_CONTAINER_PREFIX);\r\n    for (CloudBlobContainer container : containers) {\r\n        count++;\r\n        LOG.info(\"Container {}, URI {}\", container.getName(), container.getUri());\r\n    }\r\n    LOG.info(\"Found {} test containers\", count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "deleteContainers",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void deleteContainers() throws Throwable\n{\r\n    Assume.assumeTrue(this.getAuthType() == AuthType.SharedKey);\r\n    int count = 0;\r\n    CloudStorageAccount storageAccount = AzureBlobStorageTestAccount.createTestAccount();\r\n    CloudBlobClient blobClient = storageAccount.createCloudBlobClient();\r\n    Iterable<CloudBlobContainer> containers = blobClient.listContainers(TEST_CONTAINER_PREFIX);\r\n    for (CloudBlobContainer container : containers) {\r\n        LOG.info(\"Container {} URI {}\", container.getName(), container.getUri());\r\n        if (container.deleteIfExists()) {\r\n            count++;\r\n        }\r\n    }\r\n    LOG.info(\"Deleted {} test containers\", count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "aclEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntry aclEntry(AclEntryScope scope, AclEntryType type, FsAction permission)\n{\r\n    return new AclEntry.Builder().setScope(scope).setType(type).setPermission(permission).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "aclEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntry aclEntry(AclEntryScope scope, AclEntryType type, String name, FsAction permission)\n{\r\n    return new AclEntry.Builder().setScope(scope).setType(type).setName(name).setPermission(permission).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "aclEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntry aclEntry(AclEntryScope scope, AclEntryType type, String name)\n{\r\n    return new AclEntry.Builder().setScope(scope).setType(type).setName(name).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "aclEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntry aclEntry(AclEntryScope scope, AclEntryType type)\n{\r\n    return new AclEntry.Builder().setScope(scope).setType(type).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "assertPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPermission(FileSystem fs, Path pathToCheck, short perm) throws IOException\n{\r\n    assertEquals(perm, fs.getFileStatus(pathToCheck).getPermission().toShort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { false, false }, { false, true }, { true, true }, { true, false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testClientUrlScheme",
  "errType" : [ "AbfsRestOperationException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testClientUrlScheme() throws Exception\n{\r\n    String[] urlWithoutScheme = this.getTestUrl().split(\":\");\r\n    String fsUrl;\r\n    if (useSecureScheme) {\r\n        fsUrl = FileSystemUriSchemes.ABFS_SECURE_SCHEME + \":\" + urlWithoutScheme[1];\r\n    } else {\r\n        fsUrl = FileSystemUriSchemes.ABFS_SCHEME + \":\" + urlWithoutScheme[1];\r\n    }\r\n    Configuration config = getRawConfiguration();\r\n    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, fsUrl.toString());\r\n    config.setBoolean(FS_AZURE_ALWAYS_USE_HTTPS, alwaysUseHttps);\r\n    boolean expectHttpConnection = !useSecureScheme && !alwaysUseHttps;\r\n    AbfsClient client = null;\r\n    try {\r\n        client = this.getFileSystem(config).getAbfsClient();\r\n    } catch (AbfsRestOperationException e) {\r\n        if (AzureServiceErrorCode.ACCOUNT_REQUIRES_HTTPS.equals(e.getErrorCode()) && expectHttpConnection) {\r\n            return;\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n    Field baseUrlField = AbfsClient.class.getDeclaredField(\"baseUrl\");\r\n    baseUrlField.setAccessible(true);\r\n    String url = ((URL) baseUrlField.get(client)).toString();\r\n    if (expectHttpConnection) {\r\n        Assert.assertTrue(url.startsWith(FileSystemUriSchemes.HTTP_SCHEME));\r\n    } else {\r\n        Assert.assertTrue(url.startsWith(FileSystemUriSchemes.HTTPS_SCHEME));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contract",
  "methodName" : "isSecureMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSecureMode()\n{\r\n    return this.getAuthType() == AuthType.SharedKey ? false : true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new NativeAzureFileSystemContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void teardown()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureFilesystemWillNotBeCreatedIfCreationConfigIsNotSet",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void ensureFilesystemWillNotBeCreatedIfCreationConfigIsNotSet() throws Exception\n{\r\n    final AzureBlobFileSystem fs = this.createFileSystem();\r\n    FileStatus[] fileStatuses = fs.listStatus(new Path(\"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    binding.setup();\r\n    super.setup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    return AbfsCommitTestHelper.prepareTestConfiguration(binding);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbstractFSContract createContract(final Configuration conf)\n{\r\n    return new AbfsFileSystemContract(conf, binding.isSecureMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testMkdirRootNonExistentContainer",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMkdirRootNonExistentContainer() throws Exception\n{\r\n    final Configuration rawConf = getRawConfiguration();\r\n    final String account = rawConf.get(FS_AZURE_ABFS_ACCOUNT_NAME, null);\r\n    rawConf.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, false);\r\n    String nonExistentContainer = \"nonexistent-\" + UUID.randomUUID();\r\n    FsShell fsShell = new FsShell(rawConf);\r\n    int result = fsShell.run(new String[] { \"-mkdir\", ABFS_SCHEME + \"://\" + nonExistentContainer + \"@\" + account + \"/\" });\r\n    assertEquals(1, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "testIfUriContainsAbfs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testIfUriContainsAbfs() throws Exception\n{\r\n    Assert.assertTrue(UriUtils.containsAbfsUrl(\"abfs.dfs.core.windows.net\"));\r\n    Assert.assertTrue(UriUtils.containsAbfsUrl(\"abfs.dfs.preprod.core.windows.net\"));\r\n    Assert.assertFalse(UriUtils.containsAbfsUrl(\"abfs.dfs.cores.windows.net\"));\r\n    Assert.assertFalse(UriUtils.containsAbfsUrl(\"\"));\r\n    Assert.assertFalse(UriUtils.containsAbfsUrl(null));\r\n    Assert.assertFalse(UriUtils.containsAbfsUrl(\"abfs.dfs.cores.windows.net\"));\r\n    Assert.assertFalse(UriUtils.containsAbfsUrl(\"xhdfs.blob.core.windows.net\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "testExtractRawAccountName",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testExtractRawAccountName() throws Exception\n{\r\n    Assert.assertEquals(\"abfs\", UriUtils.extractAccountNameFromHostName(\"abfs.dfs.core.windows.net\"));\r\n    Assert.assertEquals(\"abfs\", UriUtils.extractAccountNameFromHostName(\"abfs.dfs.preprod.core.windows.net\"));\r\n    Assert.assertEquals(null, UriUtils.extractAccountNameFromHostName(\"abfs.dfs.cores.windows.net\"));\r\n    Assert.assertEquals(null, UriUtils.extractAccountNameFromHostName(\"\"));\r\n    Assert.assertEquals(null, UriUtils.extractAccountNameFromHostName(null));\r\n    Assert.assertEquals(null, UriUtils.extractAccountNameFromHostName(\"abfs.dfs.cores.windows.net\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "testMaskUrlQueryParameters",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testMaskUrlQueryParameters() throws Exception\n{\r\n    Set<String> fullMask = new HashSet<>(Arrays.asList(\"abc\", \"bcd\"));\r\n    Set<String> partialMask = new HashSet<>(Arrays.asList(\"pqr\", \"xyz\"));\r\n    List<NameValuePair> keyValueList = URLEncodedUtils.parse(\"abc=123&pqr=45678&def=789&bcd=012&xyz=678\", StandardCharsets.UTF_8);\r\n    Assert.assertEquals(\"Incorrect masking\", \"abc=XXXXX&pqr=456XX&def=789&bcd=XXXXX&xyz=67X\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n    keyValueList = URLEncodedUtils.parse(\"abc=123&pqr=256877f2-c094-48c8-83df-ddb5825694fd&def=789\", StandardCharsets.UTF_8);\r\n    Assert.assertEquals(\"Incorrect partial masking for guid\", \"abc=XXXXX&pqr=256877f2-c094-48c8XXXXXXXXXXXXXXXXXX&def=789\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n    partialMask.add(\"abc\");\r\n    Assert.assertEquals(\"Full mask should apply\", \"abc=XXXXX&pqr=256877f2-c094-48c8XXXXXXXXXXXXXXXXXX&def=789\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n    keyValueList = URLEncodedUtils.parse(\"abc=123&pqr=4561234&abc=789\", StandardCharsets.UTF_8);\r\n    Assert.assertEquals(\"Duplicate key: Both values should get masked\", \"abc=XXXXX&pqr=4561XXX&abc=XXXXX\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n    keyValueList = URLEncodedUtils.parse(\"abc=123&def=456&pqrs=789&def=000\", StandardCharsets.UTF_8);\r\n    Assert.assertEquals(\"Duplicate key: Values should not get masked\", \"abc=XXXXX&def=456&pqrs=789&def=000\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n    keyValueList = URLEncodedUtils.parse(\"abc=123&def=&pqr=789&s=1\", StandardCharsets.UTF_8);\r\n    Assert.assertEquals(\"Incorrect url with empty query value\", \"abc=XXXXX&def=&pqr=78X&s=1\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n    keyValueList = URLEncodedUtils.parse(\"def=2&pqr=789&s=1\", StandardCharsets.UTF_8);\r\n    keyValueList.add(new BasicNameValuePair(\"\", \"m1\"));\r\n    List<NameValuePair> finalKeyValueList = keyValueList;\r\n    intercept(IllegalArgumentException.class, () -> UriUtils.maskUrlQueryParameters(finalKeyValueList, fullMask, partialMask));\r\n    keyValueList = URLEncodedUtils.parse(\"abc=123&s=1\", StandardCharsets.UTF_8);\r\n    keyValueList.add(new BasicNameValuePair(\"null1\", null));\r\n    Assert.assertEquals(\"Null value, incorrect query construction\", \"abc=XXXXX&s=1&null1=\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n    keyValueList.add(new BasicNameValuePair(\"null2\", null));\r\n    fullMask.add(\"null2\");\r\n    Assert.assertEquals(\"No mask should be added for null value\", \"abc=XXXXX&s=1&null1=&null2=\", UriUtils.maskUrlQueryParameters(keyValueList, fullMask, partialMask));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return AzureTestConstants.SCALE_TEST_TIMEOUT_MILLIS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    LOG.debug(\"Scale test operation count = {}\", getOperationCount());\r\n    assumeScaleTestsEnabled(getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create(createConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "getOperationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getOperationCount()\n{\r\n    return getConfiguration().getLong(KEY_OPERATION_COUNT, DEFAULT_OPERATION_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testInitValues",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testInitValues() throws IOException\n{\r\n    describe(\"Testing the initial values of AbfsInputStream Statistics\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\r\n    Path initValuesPath = path(getMethodName());\r\n    AbfsOutputStream outputStream = null;\r\n    AbfsInputStream inputStream = null;\r\n    try {\r\n        outputStream = createAbfsOutputStreamWithFlushEnabled(fs, initValuesPath);\r\n        inputStream = abfss.openFileForRead(initValuesPath, fs.getFsStatistics(), getTestTracingContext(fs, false));\r\n        AbfsInputStreamStatisticsImpl stats = (AbfsInputStreamStatisticsImpl) inputStream.getStreamStatistics();\r\n        checkInitValue(stats.getSeekOperations(), \"seekOps\");\r\n        checkInitValue(stats.getForwardSeekOperations(), \"forwardSeekOps\");\r\n        checkInitValue(stats.getBackwardSeekOperations(), \"backwardSeekOps\");\r\n        checkInitValue(stats.getBytesRead(), \"bytesRead\");\r\n        checkInitValue(stats.getBytesSkippedOnSeek(), \"bytesSkippedOnSeek\");\r\n        checkInitValue(stats.getBytesBackwardsOnSeek(), \"bytesBackwardsOnSeek\");\r\n        checkInitValue(stats.getSeekInBuffer(), \"seekInBuffer\");\r\n        checkInitValue(stats.getReadOperations(), \"readOps\");\r\n        checkInitValue(stats.getBytesReadFromBuffer(), \"bytesReadFromBuffer\");\r\n        checkInitValue(stats.getRemoteReadOperations(), \"remoteReadOps\");\r\n        checkInitValue(stats.getReadAheadBytesRead(), \"readAheadBytesRead\");\r\n        checkInitValue(stats.getRemoteBytesRead(), \"readAheadRemoteBytesRead\");\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, outputStream, inputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testSeekStatistics",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testSeekStatistics() throws IOException\n{\r\n    describe(\"Testing the values of statistics from seek operations in \" + \"AbfsInputStream\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\r\n    Path seekStatPath = path(getMethodName());\r\n    AbfsOutputStream out = null;\r\n    AbfsInputStream in = null;\r\n    try {\r\n        out = createAbfsOutputStreamWithFlushEnabled(fs, seekStatPath);\r\n        out.write(defBuffer);\r\n        out.hflush();\r\n        in = abfss.openFileForRead(seekStatPath, fs.getFsStatistics(), getTestTracingContext(fs, false));\r\n        int result = in.read(defBuffer, 0, ONE_MB);\r\n        LOG.info(\"Result of read : {}\", result);\r\n        for (int i = 0; i < OPERATIONS; i++) {\r\n            in.seek(0);\r\n            in.read();\r\n            in.seek(ONE_MB);\r\n        }\r\n        AbfsInputStreamStatisticsImpl stats = (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\r\n        LOG.info(\"STATISTICS: {}\", stats.toString());\r\n        assertEquals(\"Mismatch in seekOps value\", 2 * OPERATIONS, stats.getSeekOperations());\r\n        assertEquals(\"Mismatch in backwardSeekOps value\", OPERATIONS, stats.getBackwardSeekOperations());\r\n        assertEquals(\"Mismatch in forwardSeekOps value\", OPERATIONS, stats.getForwardSeekOperations());\r\n        assertEquals(\"Mismatch in bytesBackwardsOnSeek value\", OPERATIONS * ONE_MB, stats.getBytesBackwardsOnSeek());\r\n        assertEquals(\"Mismatch in bytesSkippedOnSeek value\", 0, stats.getBytesSkippedOnSeek());\r\n        assertEquals(\"Mismatch in seekInBuffer value\", OPERATIONS, stats.getSeekInBuffer());\r\n        in.close();\r\n        LOG.info(\"STATISTICS after closing: {}\", stats.toString());\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out, in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadStatistics",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testReadStatistics() throws IOException\n{\r\n    describe(\"Testing the values of statistics from read operation in \" + \"AbfsInputStream\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\r\n    Path readStatPath = path(getMethodName());\r\n    AbfsOutputStream out = null;\r\n    AbfsInputStream in = null;\r\n    try {\r\n        out = createAbfsOutputStreamWithFlushEnabled(fs, readStatPath);\r\n        out.write(defBuffer);\r\n        out.hflush();\r\n        in = abfss.openFileForRead(readStatPath, fs.getFsStatistics(), getTestTracingContext(fs, false));\r\n        for (int i = 0; i < OPERATIONS; i++) {\r\n            in.read();\r\n        }\r\n        AbfsInputStreamStatisticsImpl stats = (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\r\n        LOG.info(\"STATISTICS: {}\", stats.toString());\r\n        assertEquals(\"Mismatch in bytesRead value\", OPERATIONS, stats.getBytesRead());\r\n        assertEquals(\"Mismatch in readOps value\", OPERATIONS, stats.getReadOperations());\r\n        assertEquals(\"Mismatch in remoteReadOps value\", 1, stats.getRemoteReadOperations());\r\n        in.close();\r\n        LOG.info(\"STATISTICS after closing: {}\", stats.toString());\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out, in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWithNullStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testWithNullStreamStatistics() throws IOException\n{\r\n    describe(\"Testing AbfsInputStream operations with statistics as null\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    Path nullStatFilePath = path(getMethodName());\r\n    byte[] oneKbBuff = new byte[ONE_KB];\r\n    AbfsInputStreamContext abfsInputStreamContext = new AbfsInputStreamContext(getConfiguration().getSasTokenRenewPeriodForStreamsInSeconds()).withReadBufferSize(getConfiguration().getReadBufferSize()).withReadAheadQueueDepth(getConfiguration().getReadAheadQueueDepth()).withStreamStatistics(null).withReadAheadRange(getConfiguration().getReadAheadRange()).build();\r\n    AbfsOutputStream out = null;\r\n    AbfsInputStream in = null;\r\n    try {\r\n        out = createAbfsOutputStreamWithFlushEnabled(fs, nullStatFilePath);\r\n        out.write(oneKbBuff);\r\n        out.hflush();\r\n        AbfsRestOperation abfsRestOperation = fs.getAbfsClient().getPathStatus(nullStatFilePath.toUri().getPath(), false, getTestTracingContext(fs, false));\r\n        in = new AbfsInputStream(fs.getAbfsClient(), null, nullStatFilePath.toUri().getPath(), ONE_KB, abfsInputStreamContext, abfsRestOperation.getResult().getResponseHeader(\"ETag\"), getTestTracingContext(fs, false));\r\n        assertNotEquals(\"AbfsInputStream read() with null statistics should \" + \"work\", -1, in.read());\r\n        in.seek(ONE_KB);\r\n        LOG.info(\"AbfsInputStream: {}\", in.toString());\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out, in);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadAheadCounters",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testReadAheadCounters() throws IOException\n{\r\n    describe(\"Test to check correct values for readAhead counters in \" + \"AbfsInputStream\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\r\n    Path readAheadCountersPath = path(getMethodName());\r\n    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\r\n    AbfsOutputStream out = null;\r\n    AbfsInputStream in = null;\r\n    try {\r\n        out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\r\n        out.write(defBuffer);\r\n        out.close();\r\n        in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics(), getTestTracingContext(fs, false));\r\n        for (int i = 0; i < 5; i++) {\r\n            in.seek(ONE_KB * i);\r\n            in.read(defBuffer, ONE_KB * i, ONE_KB);\r\n        }\r\n        AbfsInputStreamStatisticsImpl stats = (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\r\n        Assertions.assertThat(stats.getReadAheadBytesRead()).describedAs(\"Mismatch in readAheadBytesRead counter value\").isGreaterThanOrEqualTo(in.getBytesFromReadAhead());\r\n        Assertions.assertThat(stats.getRemoteBytesRead()).describedAs(\"Mismatch in remoteBytesRead counter value\").isGreaterThanOrEqualTo(in.getBytesFromRemoteRead());\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out, in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testActionHttpGetRequest",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testActionHttpGetRequest() throws IOException\n{\r\n    describe(\"Test to check the correct value of Time taken by http get \" + \"request in AbfsInputStream\");\r\n    AzureBlobFileSystem fs = getFileSystem();\r\n    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\r\n    Path actionHttpGetRequestPath = path(getMethodName());\r\n    AbfsInputStream abfsInputStream = null;\r\n    AbfsOutputStream abfsOutputStream = null;\r\n    try {\r\n        abfsOutputStream = createAbfsOutputStreamWithFlushEnabled(fs, actionHttpGetRequestPath);\r\n        abfsOutputStream.write('a');\r\n        abfsOutputStream.hflush();\r\n        abfsInputStream = abfss.openFileForRead(actionHttpGetRequestPath, fs.getFsStatistics(), getTestTracingContext(fs, false));\r\n        abfsInputStream.read();\r\n        IOStatistics ioStatistics = extractStatistics(fs);\r\n        LOG.info(\"AbfsInputStreamStats info: {}\", ioStatisticsToPrettyString(ioStatistics));\r\n        Assertions.assertThat(lookupMeanStatistic(ioStatistics, AbfsStatistic.HTTP_GET_REQUEST.getStatName() + StoreStatisticNames.SUFFIX_MEAN).mean()).describedAs(\"Mismatch in time taken by a GET request\").isGreaterThan(0.0);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, abfsInputStream, abfsOutputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkInitValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkInitValue(long actualValue, String statistic)\n{\r\n    assertEquals(\"Mismatch in \" + statistic + \" value\", 0, actualValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getIsNamespaceEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getIsNamespaceEnabled(AzureBlobFileSystem fs) throws IOException\n{\r\n    return fs.getIsNamespaceEnabled(getTestTracingContext(fs, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTestTracingContext",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TracingContext getTestTracingContext(AzureBlobFileSystem fs, boolean needsPrimaryReqId)\n{\r\n    String correlationId, fsId;\r\n    TracingHeaderFormat format;\r\n    if (fs == null) {\r\n        correlationId = \"test-corr-id\";\r\n        fsId = \"test-filesystem-id\";\r\n        format = TracingHeaderFormat.ALL_ID_FORMAT;\r\n    } else {\r\n        AbfsConfiguration abfsConf = fs.getAbfsStore().getAbfsConfiguration();\r\n        correlationId = abfsConf.getClientCorrelationId();\r\n        fsId = fs.getFileSystemId();\r\n        format = abfsConf.getTracingHeaderFormat();\r\n    }\r\n    return new TracingContext(correlationId, fsId, FSOperationType.TEST_OP, needsPrimaryReqId, format, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    createFileSystem();\r\n    if (!isIPAddress && (abfsConfig.getAuthType(accountName) != AuthType.SAS) && !abfs.getIsNamespaceEnabled(getTestTracingContext(getFileSystem(), false))) {\r\n        final URI wasbUri = new URI(abfsUrlToWasbUrl(getTestUrl(), abfsConfig.isHttpsAlwaysUsed()));\r\n        final AzureNativeFileSystemStore azureNativeFileSystemStore = new AzureNativeFileSystemStore();\r\n        String accountNameWithoutDomain = accountName.split(\"\\\\.\")[0];\r\n        String wasbAccountName = accountNameWithoutDomain + WASB_ACCOUNT_NAME_DOMAIN_SUFFIX;\r\n        String keyProperty = FS_AZURE_ACCOUNT_KEY + \".\" + wasbAccountName;\r\n        if (rawConfig.get(keyProperty) == null) {\r\n            rawConfig.set(keyProperty, getAccountKey());\r\n        }\r\n        azureNativeFileSystemStore.initialize(wasbUri, rawConfig, new AzureFileSystemInstrumentation(rawConfig));\r\n        wasb = new NativeAzureFileSystem(azureNativeFileSystemStore);\r\n        wasb.initialize(wasbUri, rawConfig);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "teardown",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    try {\r\n        IOUtils.closeStream(wasb);\r\n        wasb = null;\r\n        if (abfs == null) {\r\n            return;\r\n        }\r\n        TracingContext tracingContext = getTestTracingContext(getFileSystem(), false);\r\n        if (usingFilesystemForSASTests) {\r\n            abfsConfig.set(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SharedKey.name());\r\n            AzureBlobFileSystem tempFs = (AzureBlobFileSystem) FileSystem.newInstance(rawConfig);\r\n            tempFs.getAbfsStore().deleteFilesystem(tracingContext);\r\n        } else if (!useConfiguredFileSystem) {\r\n            final AzureBlobFileSystemStore abfsStore = abfs.getAbfsStore();\r\n            abfsStore.deleteFilesystem(tracingContext);\r\n            AbfsRestOperationException ex = intercept(AbfsRestOperationException.class, new Callable<Hashtable<String, String>>() {\r\n\r\n                @Override\r\n                public Hashtable<String, String> call() throws Exception {\r\n                    return abfsStore.getFilesystemProperties(tracingContext);\r\n                }\r\n            });\r\n            if (FILE_SYSTEM_NOT_FOUND.getStatusCode() != ex.getStatusCode()) {\r\n                LOG.warn(\"Deleted test filesystem may still exist: {}\", abfs, ex);\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.warn(\"During cleanup: {}\", e, e);\r\n    } finally {\r\n        IOUtils.closeStream(abfs);\r\n        abfs = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "loadConfiguredFileSystem",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void loadConfiguredFileSystem() throws Exception\n{\r\n    abfsConfig.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, false);\r\n    String[] authorityParts = (new URI(rawConfig.get(FS_AZURE_CONTRACT_TEST_URI))).getRawAuthority().split(AbfsHttpConstants.AZURE_DISTRIBUTED_FILE_SYSTEM_AUTHORITY_DELIMITER, 2);\r\n    this.fileSystemName = authorityParts[0];\r\n    final String abfsUrl = this.getFileSystemName() + \"@\" + this.getAccountName();\r\n    URI defaultUri = null;\r\n    defaultUri = new URI(abfsScheme, abfsUrl, null, null, null);\r\n    this.testUrl = defaultUri.toString();\r\n    abfsConfig.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, defaultUri.toString());\r\n    useConfiguredFileSystem = true;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFilesystemForSASTests",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createFilesystemForSASTests() throws Exception\n{\r\n    try (AzureBlobFileSystem tempFs = (AzureBlobFileSystem) FileSystem.newInstance(rawConfig)) {\r\n        ContractTestUtils.assertPathExists(tempFs, \"This path should exist\", new Path(\"/\"));\r\n        abfsConfig.set(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SAS.name());\r\n        usingFilesystemForSASTests = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureBlobFileSystem getFileSystem() throws IOException\n{\r\n    return abfs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobFileSystem getFileSystem(Configuration configuration) throws Exception\n{\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.get(configuration);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureBlobFileSystem getFileSystem(String abfsUri) throws Exception\n{\r\n    abfsConfig.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, abfsUri);\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.get(rawConfig);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobFileSystem createFileSystem() throws IOException\n{\r\n    if (abfs == null) {\r\n        abfs = (AzureBlobFileSystem) FileSystem.newInstance(rawConfig);\r\n    }\r\n    return abfs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getWasbFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NativeAzureFileSystem getWasbFileSystem()\n{\r\n    return wasb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getHostName()\n{\r\n    String endPoint = abfsConfig.get(AZURE_ABFS_ENDPOINT);\r\n    return endPoint.split(\":\")[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setTestUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTestUrl(String testUrl)\n{\r\n    this.testUrl = testUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTestUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTestUrl()\n{\r\n    return testUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setFileSystemName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFileSystemName(String fileSystemName)\n{\r\n    this.fileSystemName = fileSystemName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMethodName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMethodName()\n{\r\n    return methodName.getMethodName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileSystemName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFileSystemName()\n{\r\n    return fileSystemName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAccountName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAccountName()\n{\r\n    return this.accountName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAccountKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAccountKey()\n{\r\n    return abfsConfig.get(FS_AZURE_ACCOUNT_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsConfiguration getConfiguration()\n{\r\n    return abfsConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRawConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getRawConfiguration()\n{\r\n    return abfsConfig.getRawConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAuthType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuthType getAuthType()\n{\r\n    return this.authType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfsScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAbfsScheme()\n{\r\n    return this.abfsScheme;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isIPAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isIPAddress()\n{\r\n    return isIPAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(Path path, byte[] buffer) throws IOException\n{\r\n    ContractTestUtils.writeDataset(getFileSystem(), path, buffer, buffer.length, CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "touch",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void touch(Path path) throws IOException\n{\r\n    ContractTestUtils.touch(getFileSystem(), path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "wasbUrlToAbfsUrl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String wasbUrlToAbfsUrl(final String wasbUrl)\n{\r\n    return convertTestUrls(wasbUrl, FileSystemUriSchemes.WASB_SCHEME, FileSystemUriSchemes.WASB_SECURE_SCHEME, FileSystemUriSchemes.WASB_DNS_PREFIX, FileSystemUriSchemes.ABFS_SCHEME, FileSystemUriSchemes.ABFS_SECURE_SCHEME, FileSystemUriSchemes.ABFS_DNS_PREFIX, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "abfsUrlToWasbUrl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String abfsUrlToWasbUrl(final String abfsUrl, final boolean isAlwaysHttpsUsed)\n{\r\n    return convertTestUrls(abfsUrl, FileSystemUriSchemes.ABFS_SCHEME, FileSystemUriSchemes.ABFS_SECURE_SCHEME, FileSystemUriSchemes.ABFS_DNS_PREFIX, FileSystemUriSchemes.WASB_SCHEME, FileSystemUriSchemes.WASB_SECURE_SCHEME, FileSystemUriSchemes.WASB_DNS_PREFIX, isAlwaysHttpsUsed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "convertTestUrls",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String convertTestUrls(final String url, final String fromNonSecureScheme, final String fromSecureScheme, final String fromDnsPrefix, final String toNonSecureScheme, final String toSecureScheme, final String toDnsPrefix, final boolean isAlwaysHttpsUsed)\n{\r\n    String data = null;\r\n    if (url.startsWith(fromNonSecureScheme + \"://\") && isAlwaysHttpsUsed) {\r\n        data = url.replace(fromNonSecureScheme + \"://\", toSecureScheme + \"://\");\r\n    } else if (url.startsWith(fromNonSecureScheme + \"://\")) {\r\n        data = url.replace(fromNonSecureScheme + \"://\", toNonSecureScheme + \"://\");\r\n    } else if (url.startsWith(fromSecureScheme + \"://\")) {\r\n        data = url.replace(fromSecureScheme + \"://\", toSecureScheme + \"://\");\r\n    }\r\n    if (data != null) {\r\n        data = data.replace(\".\" + fromDnsPrefix + \".\", \".\" + toDnsPrefix + \".\");\r\n    }\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestPath()\n{\r\n    Path path = new Path(UriUtils.generateUniqueTestPath());\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfsStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobFileSystemStore getAbfsStore(final AzureBlobFileSystem fs)\n{\r\n    return fs.getAbfsStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfsClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsClient getAbfsClient(final AzureBlobFileSystemStore abfsStore)\n{\r\n    return abfsStore.getClient();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setAbfsClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAbfsClient(AzureBlobFileSystemStore abfsStore, AbfsClient client)\n{\r\n    abfsStore.setClient(client);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "makeQualified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path makeQualified(Path path) throws java.io.IOException\n{\r\n    return getFileSystem().makeQualified(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "path",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path path(String filepath) throws IOException\n{\r\n    return getFileSystem().makeQualified(new Path(getTestPath(), getUniquePath(filepath)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getUniquePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getUniquePath(String filepath)\n{\r\n    if (filepath.equals(\"/\")) {\r\n        return new Path(filepath);\r\n    }\r\n    return new Path(filepath + StringUtils.right(UUID.randomUUID().toString(), SHORTENED_GUID_LEN));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getDelegationTokenManager",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsDelegationTokenManager getDelegationTokenManager() throws IOException\n{\r\n    return getFileSystem().getDelegationTokenManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createAbfsOutputStreamWithFlushEnabled",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AbfsOutputStream createAbfsOutputStreamWithFlushEnabled(AzureBlobFileSystem fs, Path path) throws IOException\n{\r\n    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\r\n    abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\r\n    return (AbfsOutputStream) abfss.createFile(path, fs.getFsStatistics(), true, FsPermission.getDefault(), FsPermission.getUMask(fs.getConf()), getTestTracingContext(fs, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertAbfsStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long assertAbfsStatistics(AbfsStatistic statistic, long expectedValue, Map<String, Long> metricMap)\n{\r\n    assertEquals(\"Mismatch in \" + statistic.getStatName(), expectedValue, (long) metricMap.get(statistic.getStatName()));\r\n    return expectedValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testScriptPathNotSpecified",
  "errType" : [ "KeyProviderException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testScriptPathNotSpecified() throws Exception\n{\r\n    if (!Shell.WINDOWS) {\r\n        return;\r\n    }\r\n    ShellDecryptionKeyProvider provider = new ShellDecryptionKeyProvider();\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    String key = \"key\";\r\n    conf.set(ConfigurationKeys.FS_AZURE_ACCOUNT_KEY_PROPERTY_NAME + account, key);\r\n    try {\r\n        provider.getStorageAccountKey(account, conf);\r\n        Assert.fail(\"fs.azure.shellkeyprovider.script is not specified, we should throw\");\r\n    } catch (KeyProviderException e) {\r\n        LOG.info(\"Received an expected exception: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testValidScript",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testValidScript() throws Exception\n{\r\n    if (!Shell.WINDOWS) {\r\n        return;\r\n    }\r\n    String expectedResult = \"decretedKey\";\r\n    File scriptFile = new File(TEST_ROOT_DIR, \"testScript.cmd\");\r\n    FileUtils.writeStringToFile(scriptFile, \"@echo %1 \" + expectedResult, Charset.forName(\"UTF-8\"));\r\n    ShellDecryptionKeyProvider provider = new ShellDecryptionKeyProvider();\r\n    Configuration conf = new Configuration();\r\n    String account = \"testacct\";\r\n    String key = \"key1\";\r\n    conf.set(ConfigurationKeys.FS_AZURE_ACCOUNT_KEY_PROPERTY_NAME + account, key);\r\n    conf.set(ConfigurationKeys.AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT, \"cmd /c \" + scriptFile.getAbsolutePath());\r\n    String result = provider.getStorageAccountKey(account, conf);\r\n    assertEquals(key + \" \" + expectedResult, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListPath",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testListPath() throws Exception\n{\r\n    Configuration config = new Configuration(this.getRawConfiguration());\r\n    config.set(AZURE_LIST_MAX_RESULTS, \"5000\");\r\n    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), config);\r\n    final List<Future<Void>> tasks = new ArrayList<>();\r\n    ExecutorService es = Executors.newFixedThreadPool(10);\r\n    for (int i = 0; i < TEST_FILES_NUMBER; i++) {\r\n        final Path fileName = new Path(\"/test\" + i);\r\n        Callable<Void> callable = new Callable<Void>() {\r\n\r\n            @Override\r\n            public Void call() throws Exception {\r\n                touch(fileName);\r\n                return null;\r\n            }\r\n        };\r\n        tasks.add(es.submit(callable));\r\n    }\r\n    for (Future<Void> task : tasks) {\r\n        task.get();\r\n    }\r\n    es.shutdownNow();\r\n    fs.registerListener(new TracingHeaderValidator(getConfiguration().getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.LISTSTATUS, true, 0));\r\n    FileStatus[] files = fs.listStatus(new Path(\"/\"));\r\n    assertEquals(TEST_FILES_NUMBER, files.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListFileVsListDir",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testListFileVsListDir() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path path = path(\"/testFile\");\r\n    try (FSDataOutputStream ignored = fs.create(path)) {\r\n        FileStatus[] testFiles = fs.listStatus(path);\r\n        assertEquals(\"length of test files\", 1, testFiles.length);\r\n        FileStatus status = testFiles[0];\r\n        assertIsFileReference(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListFileVsListDir2",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testListFileVsListDir2() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testFolder = path(\"/testFolder\");\r\n    fs.mkdirs(testFolder);\r\n    fs.mkdirs(new Path(testFolder + \"/testFolder2\"));\r\n    fs.mkdirs(new Path(testFolder + \"/testFolder2/testFolder3\"));\r\n    Path testFile0Path = new Path(testFolder + \"/testFolder2/testFolder3/testFile\");\r\n    ContractTestUtils.touch(fs, testFile0Path);\r\n    FileStatus[] testFiles = fs.listStatus(testFile0Path);\r\n    assertEquals(\"Wrong listing size of file \" + testFile0Path, 1, testFiles.length);\r\n    FileStatus file0 = testFiles[0];\r\n    assertEquals(\"Wrong path for \" + file0, new Path(getTestUrl(), testFolder + \"/testFolder2/testFolder3/testFile\"), file0.getPath());\r\n    assertIsFileReference(file0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListNonExistentDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testListNonExistentDir() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    fs.listStatus(new Path(\"/testFile/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testListFiles",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testListFiles() throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path testDir = path(\"/test\");\r\n    fs.mkdirs(testDir);\r\n    FileStatus[] fileStatuses = fs.listStatus(new Path(\"/\"));\r\n    assertEquals(1, fileStatuses.length);\r\n    fs.mkdirs(new Path(testDir + \"/sub\"));\r\n    fileStatuses = fs.listStatus(testDir);\r\n    assertEquals(1, fileStatuses.length);\r\n    assertEquals(\"sub\", fileStatuses[0].getPath().getName());\r\n    assertIsDirectoryReference(fileStatuses[0]);\r\n    Path childF = fs.makeQualified(new Path(testDir + \"/f\"));\r\n    touch(childF);\r\n    fileStatuses = fs.listStatus(testDir);\r\n    assertEquals(2, fileStatuses.length);\r\n    final FileStatus childStatus = fileStatuses[0];\r\n    assertEquals(childF, childStatus.getPath());\r\n    assertEquals(\"f\", childStatus.getPath().getName());\r\n    assertIsFileReference(childStatus);\r\n    assertEquals(0, childStatus.getLen());\r\n    final FileStatus status1 = fileStatuses[1];\r\n    assertEquals(\"sub\", status1.getPath().getName());\r\n    assertIsDirectoryReference(status1);\r\n    LocatedFileStatus locatedChildStatus = fs.listFiles(childF, false).next();\r\n    assertIsFileReference(locatedChildStatus);\r\n    fs.delete(testDir, true);\r\n    intercept(FileNotFoundException.class, () -> fs.listFiles(childF, false).next());\r\n    assertEquals(\"Path mismatch of \" + locatedChildStatus, childF, locatedChildStatus.getPath());\r\n    assertEquals(\"locatedstatus.equals(status)\", locatedChildStatus, childStatus);\r\n    assertEquals(\"status.equals(locatedstatus)\", childStatus, locatedChildStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertIsDirectoryReference",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertIsDirectoryReference(FileStatus status)\n{\r\n    assertTrue(\"Not a directory: \" + status, status.isDirectory());\r\n    assertFalse(\"Not a directory: \" + status, status.isFile());\r\n    assertEquals(0, status.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "assertIsFileReference",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertIsFileReference(FileStatus status)\n{\r\n    assertFalse(\"Not a file: \" + status, status.isDirectory());\r\n    assertTrue(\"Not a file: \" + status, status.isFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testMkdirTrailingPeriodDirName",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMkdirTrailingPeriodDirName() throws IOException\n{\r\n    boolean exceptionThrown = false;\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path nontrailingPeriodDir = path(\"testTrailingDir/dir\");\r\n    Path trailingPeriodDir = new Path(\"testMkdirTrailingDir/dir.\");\r\n    assertMkdirs(fs, nontrailingPeriodDir);\r\n    try {\r\n        fs.mkdirs(trailingPeriodDir);\r\n    } catch (IllegalArgumentException e) {\r\n        exceptionThrown = true;\r\n    }\r\n    assertTrue(\"Attempt to create file that ended with a dot should\" + \" throw IllegalArgumentException\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testCreateTrailingPeriodFileName",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCreateTrailingPeriodFileName() throws IOException\n{\r\n    boolean exceptionThrown = false;\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path trailingPeriodFile = new Path(\"testTrailingDir/file.\");\r\n    Path nontrailingPeriodFile = path(\"testCreateTrailingDir/file\");\r\n    createFile(fs, nontrailingPeriodFile, false, new byte[0]);\r\n    assertPathExists(fs, \"Trailing period file does not exist\", nontrailingPeriodFile);\r\n    try {\r\n        createFile(fs, trailingPeriodFile, false, new byte[0]);\r\n    } catch (IllegalArgumentException e) {\r\n        exceptionThrown = true;\r\n    }\r\n    assertTrue(\"Attempt to create file that ended with a dot should\" + \" throw IllegalArgumentException\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testRenameTrailingPeriodFile",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRenameTrailingPeriodFile() throws IOException\n{\r\n    boolean exceptionThrown = false;\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    Path nonTrailingPeriodFile = path(\"testTrailingDir/file\");\r\n    Path trailingPeriodFile = new Path(\"testRenameTrailingDir/file.\");\r\n    createFile(fs, nonTrailingPeriodFile, false, new byte[0]);\r\n    try {\r\n        rename(fs, nonTrailingPeriodFile, trailingPeriodFile);\r\n    } catch (IllegalArgumentException e) {\r\n        exceptionThrown = true;\r\n    }\r\n    assertTrue(\"Attempt to create file that ended with a dot should\" + \" throw IllegalArgumentException\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testOauthOverSchemeAbfs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testOauthOverSchemeAbfs() throws Exception\n{\r\n    String[] urlWithoutScheme = this.getTestUrl().split(\":\");\r\n    String fsUrl;\r\n    fsUrl = FileSystemUriSchemes.ABFS_SCHEME + \":\" + urlWithoutScheme[1];\r\n    Configuration config = getRawConfiguration();\r\n    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, fsUrl.toString());\r\n    AbfsClient client = this.getFileSystem(config).getAbfsClient();\r\n    Field baseUrlField = AbfsClient.class.getDeclaredField(\"baseUrl\");\r\n    baseUrlField.setAccessible(true);\r\n    String url = ((URL) baseUrlField.get(client)).toString();\r\n    Assume.assumeTrue(\"OAuth authentication over scheme abfs must use HTTPS\", url.startsWith(FileSystemUriSchemes.HTTPS_SCHEME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testOneParam",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOneParam()\n{\r\n    String key = PARAM_ARRAY[0][0];\r\n    String value = PARAM_ARRAY[0][1];\r\n    Map<String, String> paramMap = new HashMap<>();\r\n    paramMap.put(key, value);\r\n    QueryParams qp = new QueryParams();\r\n    qp.add(key, value);\r\n    Assert.assertEquals(key + \"=\" + value, qp.serialize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMultipleParams",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultipleParams()\n{\r\n    QueryParams qp = new QueryParams();\r\n    for (String[] entry : PARAM_ARRAY) {\r\n        qp.add(entry[0], entry[1]);\r\n    }\r\n    Map<String, String> paramMap = constructMap(qp.serialize());\r\n    Assert.assertEquals(PARAM_ARRAY.length, paramMap.size());\r\n    for (String[] entry : PARAM_ARRAY) {\r\n        Assert.assertTrue(paramMap.containsKey(entry[0]));\r\n        Assert.assertEquals(entry[1], paramMap.get(entry[0]));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "constructMap",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, String> constructMap(String input)\n{\r\n    String[] entries = input.split(SEPARATOR);\r\n    Map<String, String> paramMap = new HashMap<>();\r\n    for (String entry : entries) {\r\n        String[] keyValue = entry.split(\"=\");\r\n        paramMap.put(keyValue[0], keyValue[1]);\r\n    }\r\n    return paramMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "sizes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<Object[]> sizes()\n{\r\n    return Arrays.asList(new Object[][] { { MIN_BUFFER_SIZE }, { DEFAULT_READ_BUFFER_SIZE }, { APPENDBLOB_MAX_WRITE_BUFFER_SIZE }, { MAX_BUFFER_SIZE } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadAndWriteWithDifferentBufferSizesAndSeek",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReadAndWriteWithDifferentBufferSizesAndSeek() throws Exception\n{\r\n    testReadWriteAndSeek(size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadWriteAndSeek",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testReadWriteAndSeek(int bufferSize) throws Exception\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\r\n    abfsConfiguration.setWriteBufferSize(bufferSize);\r\n    abfsConfiguration.setReadBufferSize(bufferSize);\r\n    final byte[] b = new byte[2 * bufferSize];\r\n    new Random().nextBytes(b);\r\n    Path testPath = path(TEST_PATH);\r\n    FSDataOutputStream stream = fs.create(testPath);\r\n    try {\r\n        stream.write(b);\r\n    } finally {\r\n        stream.close();\r\n    }\r\n    IOStatisticsLogging.logIOStatisticsAtLevel(LOG, IOSTATISTICS_LOGGING_LEVEL_INFO, stream);\r\n    final byte[] readBuffer = new byte[2 * bufferSize];\r\n    int result;\r\n    IOStatisticsSource statisticsSource = null;\r\n    try (FSDataInputStream inputStream = fs.open(testPath)) {\r\n        statisticsSource = inputStream;\r\n        ((AbfsInputStream) inputStream.getWrappedStream()).registerListener(new TracingHeaderValidator(abfsConfiguration.getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.READ, true, 0, ((AbfsInputStream) inputStream.getWrappedStream()).getStreamID()));\r\n        inputStream.seek(bufferSize);\r\n        result = inputStream.read(readBuffer, bufferSize, bufferSize);\r\n        assertNotEquals(-1, result);\r\n        inputStream.seek(0);\r\n        byte[] temp = new byte[5];\r\n        int t = inputStream.read(temp, 0, 1);\r\n        inputStream.seek(0);\r\n        result = inputStream.read(readBuffer, 0, bufferSize);\r\n    }\r\n    IOStatisticsLogging.logIOStatisticsAtLevel(LOG, IOSTATISTICS_LOGGING_LEVEL_INFO, statisticsSource);\r\n    assertNotEquals(\"data read in final read()\", -1, result);\r\n    assertArrayEquals(readBuffer, b);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testReadAheadRequestID",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReadAheadRequestID() throws java.io.IOException\n{\r\n    final AzureBlobFileSystem fs = getFileSystem();\r\n    final AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\r\n    int bufferSize = MIN_BUFFER_SIZE;\r\n    abfsConfiguration.setReadBufferSize(bufferSize);\r\n    final byte[] b = new byte[bufferSize * 10];\r\n    new Random().nextBytes(b);\r\n    Path testPath = path(TEST_PATH);\r\n    try (FSDataOutputStream stream = fs.create(testPath)) {\r\n        ((AbfsOutputStream) stream.getWrappedStream()).registerListener(new TracingHeaderValidator(abfsConfiguration.getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.WRITE, false, 0, ((AbfsOutputStream) stream.getWrappedStream()).getStreamID()));\r\n        stream.write(b);\r\n    }\r\n    final byte[] readBuffer = new byte[4 * bufferSize];\r\n    int result;\r\n    fs.registerListener(new TracingHeaderValidator(abfsConfiguration.getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.OPEN, false, 0));\r\n    try (FSDataInputStream inputStream = fs.open(testPath)) {\r\n        ((AbfsInputStream) inputStream.getWrappedStream()).registerListener(new TracingHeaderValidator(abfsConfiguration.getClientCorrelationId(), fs.getFileSystemId(), FSOperationType.READ, false, 0, ((AbfsInputStream) inputStream.getWrappedStream()).getStreamID()));\r\n        result = inputStream.read(readBuffer, 0, bufferSize * 4);\r\n    }\r\n    fs.registerListener(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "testWithWrongSharedKey",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWithWrongSharedKey() throws Exception\n{\r\n    Assume.assumeTrue(this.getAuthType() == AuthType.SharedKey);\r\n    Configuration config = this.getRawConfiguration();\r\n    config.setBoolean(AZURE_CREATE_REMOTE_FILESYSTEM_DURING_INITIALIZATION, false);\r\n    String accountName = this.getAccountName();\r\n    String configkKey = FS_AZURE_ACCOUNT_KEY_PROPERTY_NAME + \".\" + accountName;\r\n    String secret = \"XjUjsGherkDpljuyThd7RpljhR6uhsFjhlxRpmhgD12lnj7lhfRn8kgPt5\" + \"+MJHS7UJNDER+jn6KP6Jnm2ONQlm==\";\r\n    config.set(configkKey, secret);\r\n    AbfsClient abfsClient = this.getFileSystem(config).getAbfsClient();\r\n    intercept(AbfsRestOperationException.class, \"\\\"Server failed to authenticate the request. Make sure the value of \" + \"Authorization header is formed correctly including the \" + \"signature.\\\", 403\", () -> {\r\n        abfsClient.getAclStatus(\"/\", getTestTracingContext(getFileSystem(), false));\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    prepareToTerasort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "applyCustomConfigOptions",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void applyCustomConfigOptions(JobConf conf)\n{\r\n    conf.setInt(TeraSortConfigKeys.SAMPLE_SIZE.key(), getSampleSizeForEachPartition());\r\n    conf.setInt(TeraSortConfigKeys.NUM_PARTITIONS.key(), getExpectedPartitionCount());\r\n    conf.setBoolean(TeraSortConfigKeys.USE_SIMPLE_PARTITIONER.key(), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "getExpectedPartitionCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getExpectedPartitionCount()\n{\r\n    return EXPECTED_PARTITION_COUNT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "getSampleSizeForEachPartition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSampleSizeForEachPartition()\n{\r\n    return PARTITION_SAMPLE_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "getRowCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRowCount()\n{\r\n    return ROW_COUNT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "prepareToTerasort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void prepareToTerasort()\n{\r\n    terasortPath = getFileSystem().makeQualified(TERASORT_PATH);\r\n    sortInput = new Path(terasortPath, \"sortin\");\r\n    sortOutput = new Path(terasortPath, \"sortout\");\r\n    sortValidate = new Path(terasortPath, \"validate\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "completedStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void completedStage(final String stage, final DurationInfo d)\n{\r\n    COMPLETED_STAGES.put(stage, d);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "requireStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireStage(final String stage)\n{\r\n    Assume.assumeTrue(\"Required stage was not completed: \" + stage, COMPLETED_STAGES.get(stage) != null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void executeStage(final String stage, final JobConf jobConf, final Path dest, final Tool tool, final String[] args, final int minimumFileCount) throws Exception\n{\r\n    int result;\r\n    DurationInfo d = new DurationInfo(LOG, stage);\r\n    try {\r\n        result = ToolRunner.run(jobConf, tool, args);\r\n    } finally {\r\n        d.close();\r\n    }\r\n    dumpOutputTree(dest);\r\n    assertEquals(stage + \"(\" + StringUtils.join(\", \", args) + \")\" + \" failed\", 0, result);\r\n    final ManifestSuccessData successFile = validateSuccessFile(getFileSystem(), dest, minimumFileCount, \"\");\r\n    JOB_IOSTATS.aggregate(successFile.getIOStatistics());\r\n    completedStage(stage, d);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "test_100_terasort_setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void test_100_terasort_setup() throws Throwable\n{\r\n    describe(\"Setting up for a terasort\");\r\n    getFileSystem().delete(terasortPath, true);\r\n    terasortDuration = Optional.of(new DurationInfo(LOG, false, \"Terasort\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "test_110_teragen",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void test_110_teragen() throws Throwable\n{\r\n    describe(\"Teragen to %s\", sortInput);\r\n    getFileSystem().delete(sortInput, true);\r\n    JobConf jobConf = newJobConf();\r\n    patchConfigurationForCommitter(jobConf);\r\n    executeStage(\"teragen\", jobConf, sortInput, new TeraGen(), new String[] { Integer.toString(getRowCount()), sortInput.toString() }, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "test_120_terasort",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test_120_terasort() throws Throwable\n{\r\n    describe(\"Terasort from %s to %s\", sortInput, sortOutput);\r\n    requireStage(\"teragen\");\r\n    getFileSystem().delete(sortOutput, true);\r\n    loadSuccessFile(getFileSystem(), sortInput);\r\n    JobConf jobConf = newJobConf();\r\n    patchConfigurationForCommitter(jobConf);\r\n    executeStage(\"terasort\", jobConf, sortOutput, new TeraSort(), new String[] { sortInput.toString(), sortOutput.toString() }, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "test_130_teravalidate",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test_130_teravalidate() throws Throwable\n{\r\n    describe(\"TeraValidate from %s to %s\", sortOutput, sortValidate);\r\n    requireStage(\"terasort\");\r\n    getFileSystem().delete(sortValidate, true);\r\n    loadSuccessFile(getFileSystem(), sortOutput);\r\n    JobConf jobConf = newJobConf();\r\n    patchConfigurationForCommitter(jobConf);\r\n    executeStage(\"teravalidate\", jobConf, sortValidate, new TeraValidate(), new String[] { sortOutput.toString(), sortValidate.toString() }, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "test_140_teracomplete",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void test_140_teracomplete() throws Throwable\n{\r\n    terasortDuration.ifPresent(d -> {\r\n        d.close();\r\n        completedStage(\"overall\", d);\r\n    });\r\n    IOStatisticsLogging.logIOStatisticsAtLevel(LOG, IOSTATISTICS_LOGGING_LEVEL_INFO, JOB_IOSTATS);\r\n    final StringBuilder results = new StringBuilder();\r\n    results.append(\"\\\"Operation\\\"\\t\\\"Duration\\\"\\n\");\r\n    Consumer<String> stage = (s) -> {\r\n        DurationInfo duration = COMPLETED_STAGES.get(s);\r\n        results.append(String.format(\"\\\"%s\\\"\\t\\\"%s\\\"\\n\", s, duration == null ? \"\" : duration));\r\n    };\r\n    stage.accept(\"teragen\");\r\n    stage.accept(\"terasort\");\r\n    stage.accept(\"teravalidate\");\r\n    stage.accept(\"overall\");\r\n    String text = results.toString();\r\n    File resultsFile = File.createTempFile(\"results\", \".csv\");\r\n    FileUtils.write(resultsFile, text, StandardCharsets.UTF_8);\r\n    LOG.info(\"Results are in {}\\n{}\", resultsFile, text);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "test_150_teracleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_150_teracleanup() throws Throwable\n{\r\n    terasortDuration = Optional.empty();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "test_200_directory_deletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void test_200_directory_deletion() throws Throwable\n{\r\n    getFileSystem().delete(terasortPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "dumpOutputTree",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void dumpOutputTree(Path path) throws Exception\n{\r\n    LOG.info(\"Files under output directory {}\", path);\r\n    try {\r\n        RemoteIterators.foreach(getFileSystem().listFiles(path, true), (status) -> LOG.info(\"{}\", status));\r\n    } catch (FileNotFoundException e) {\r\n        LOG.info(\"Output directory {} not found\", path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    testPath = path(\"ITestAzureHugeFiles\");\r\n    scaleTestDir = new Path(testPath, \"scale\");\r\n    hugefile = new Path(scaleTestDir, \"hugefile\");\r\n    hugefileRenamed = new Path(scaleTestDir, \"hugefileRenamed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    testAccount = null;\r\n    super.tearDown();\r\n    if (testAccountForCleanup != null) {\r\n        cleanupTestAccount(testAccount);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "createTestAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobStorageTestAccount createTestAccount() throws Exception\n{\r\n    return AzureBlobStorageTestAccount.create(\"testazurehugefiles\", EnumSet.of(AzureBlobStorageTestAccount.CreateOptions.CreateContainer), createConfiguration(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "deleteTestDirInTeardown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void deleteTestDirInTeardown() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "deleteHugeFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteHugeFile() throws IOException\n{\r\n    describe(\"Deleting %s\", hugefile);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    getFileSystem().delete(hugefile, false);\r\n    timer.end(\"time to delete %s\", hugefile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "logTimePerIOP",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logTimePerIOP(String operation, ContractTestUtils.NanoTimer timer, long count)\n{\r\n    LOG.info(\"Time per {}: {} nS\", operation, toHuman(timer.duration() / count));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "assumeHugeFileExists",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileStatus assumeHugeFileExists() throws IOException\n{\r\n    assertPathExists(getFileSystem(), \"huge file not created\", hugefile);\r\n    try {\r\n        FileStatus status = getFileSystem().getFileStatus(hugefile);\r\n        Assume.assumeTrue(\"Not a file: \" + status, status.isFile());\r\n        Assume.assumeTrue(\"File \" + hugefile + \" is empty\", status.getLen() > 0);\r\n        return status;\r\n    } catch (FileNotFoundException e) {\r\n        skip(\"huge file not created: \" + hugefile);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "logFSState",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void logFSState()\n{\r\n    StorageStatistics statistics = getFileSystem().getStorageStatistics();\r\n    Iterator<StorageStatistics.LongStatistic> longStatistics = statistics.getLongStatistics();\r\n    while (longStatistics.hasNext()) {\r\n        StorageStatistics.LongStatistic next = longStatistics.next();\r\n        LOG.info(\"{} = {}\", next.getName(), next.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "test_010_CreateHugeFile",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void test_010_CreateHugeFile() throws IOException\n{\r\n    long filesize = getTestPropertyBytes(getConfiguration(), KEY_HUGE_FILESIZE, DEFAULT_HUGE_FILESIZE);\r\n    long filesizeMB = filesize / S_1M;\r\n    deleteHugeFile();\r\n    describe(\"Creating file %s of size %d MB\", hugefile, filesizeMB);\r\n    assertEquals(\"File size set in \" + KEY_HUGE_FILESIZE + \" = \" + filesize + \" is not a multiple of \" + UPLOAD_BLOCKSIZE, 0, filesize % UPLOAD_BLOCKSIZE);\r\n    byte[] data = SOURCE_DATA;\r\n    long blocks = filesize / UPLOAD_BLOCKSIZE;\r\n    long blocksPerMB = S_1M / UPLOAD_BLOCKSIZE;\r\n    NativeAzureFileSystem fs = getFileSystem();\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    long blocksPer10MB = blocksPerMB * 10;\r\n    fs.mkdirs(hugefile.getParent());\r\n    try (FSDataOutputStream out = fs.create(hugefile, true, UPLOAD_BLOCKSIZE, null)) {\r\n        for (long block = 1; block <= blocks; block++) {\r\n            out.write(data);\r\n            long written = block * UPLOAD_BLOCKSIZE;\r\n            if (block % blocksPer10MB == 0 || written == filesize) {\r\n                long percentage = written * 100 / filesize;\r\n                double elapsedTime = timer.elapsedTime() / NANOSEC;\r\n                double writtenMB = 1.0 * written / S_1M;\r\n                LOG.info(String.format(\"[%02d%%] Buffered %.2f MB out of %d MB;\" + \" elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s\", percentage, writtenMB, filesizeMB, elapsedTime, writtenMB / elapsedTime));\r\n            }\r\n        }\r\n        LOG.info(\"Closing stream {}\", out);\r\n        ContractTestUtils.NanoTimer closeTimer = new ContractTestUtils.NanoTimer();\r\n        out.close();\r\n        closeTimer.end(\"time to close() output stream\");\r\n    }\r\n    timer.end(\"time to write %d MB in blocks of %d\", filesizeMB, UPLOAD_BLOCKSIZE);\r\n    logFSState();\r\n    bandwidth(timer, filesize);\r\n    ContractTestUtils.assertPathExists(fs, \"Huge file\", hugefile);\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    ContractTestUtils.assertIsFile(hugefile, status);\r\n    assertEquals(\"File size in \" + status, filesize, status.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "test_040_PositionedReadHugeFile",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void test_040_PositionedReadHugeFile() throws Throwable\n{\r\n    assumeHugeFileExists();\r\n    describe(\"Positioned reads of file %s\", hugefile);\r\n    NativeAzureFileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    long filesize = status.getLen();\r\n    int ops = 0;\r\n    final int bufferSize = 8192;\r\n    byte[] buffer = new byte[bufferSize];\r\n    long eof = filesize - 1;\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    ContractTestUtils.NanoTimer readAtByte0, readAtByte0Again, readAtEOF;\r\n    try (FSDataInputStream in = openDataFile()) {\r\n        readAtByte0 = new ContractTestUtils.NanoTimer();\r\n        in.readFully(0, buffer);\r\n        readAtByte0.end(\"time to read data at start of file\");\r\n        ops++;\r\n        readAtEOF = new ContractTestUtils.NanoTimer();\r\n        in.readFully(eof - bufferSize, buffer);\r\n        readAtEOF.end(\"time to read data at end of file\");\r\n        ops++;\r\n        readAtByte0Again = new ContractTestUtils.NanoTimer();\r\n        in.readFully(0, buffer);\r\n        readAtByte0Again.end(\"time to read data at start of file again\");\r\n        ops++;\r\n        LOG.info(\"Final stream state: {}\", in);\r\n    }\r\n    long mb = Math.max(filesize / S_1M, 1);\r\n    logFSState();\r\n    timer.end(\"time to performed positioned reads of %d MB \", mb);\r\n    LOG.info(\"Time per positioned read = {} nS\", toHuman(timer.nanosPerOperation(ops)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "openDataFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream openDataFile() throws IOException\n{\r\n    NanoTimer openTimer = new NanoTimer();\r\n    FSDataInputStream inputStream = getFileSystem().open(hugefile, UPLOAD_BLOCKSIZE);\r\n    openTimer.end(\"open data file\");\r\n    return inputStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "bandwidthInBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double bandwidthInBytes(NanoTimer timer, long bytes)\n{\r\n    return bytes * NANOSEC / timer.duration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "test_050_readHugeFile",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void test_050_readHugeFile() throws Throwable\n{\r\n    assumeHugeFileExists();\r\n    describe(\"Reading %s\", hugefile);\r\n    NativeAzureFileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    long filesize = status.getLen();\r\n    long blocks = filesize / UPLOAD_BLOCKSIZE;\r\n    byte[] data = new byte[UPLOAD_BLOCKSIZE];\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    try (FSDataInputStream in = openDataFile()) {\r\n        for (long block = 0; block < blocks; block++) {\r\n            in.readFully(data);\r\n        }\r\n        LOG.info(\"Final stream state: {}\", in);\r\n    }\r\n    long mb = Math.max(filesize / S_1M, 1);\r\n    timer.end(\"time to read file of %d MB \", mb);\r\n    LOG.info(\"Time per MB to read = {} nS\", toHuman(timer.nanosPerOperation(mb)));\r\n    bandwidth(timer, filesize);\r\n    logFSState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "test_060_openAndReadWholeFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void test_060_openAndReadWholeFileBlocks() throws Throwable\n{\r\n    FileStatus status = assumeHugeFileExists();\r\n    int blockSize = S_1M;\r\n    describe(\"Open the test file and read it in blocks of size %d\", blockSize);\r\n    long len = status.getLen();\r\n    FSDataInputStream in = openDataFile();\r\n    NanoTimer timer2 = null;\r\n    long blockCount = 0;\r\n    long totalToRead = 0;\r\n    int resetCount = 0;\r\n    try {\r\n        byte[] block = new byte[blockSize];\r\n        timer2 = new NanoTimer();\r\n        long count = 0;\r\n        blockCount = len / blockSize;\r\n        totalToRead = blockCount * blockSize;\r\n        long minimumBandwidth = S_128K;\r\n        int maxResetCount = 4;\r\n        resetCount = 0;\r\n        for (long i = 0; i < blockCount; i++) {\r\n            int offset = 0;\r\n            int remaining = blockSize;\r\n            long blockId = i + 1;\r\n            NanoTimer blockTimer = new NanoTimer();\r\n            int reads = 0;\r\n            while (remaining > 0) {\r\n                NanoTimer readTimer = new NanoTimer();\r\n                int bytesRead = in.read(block, offset, remaining);\r\n                reads++;\r\n                if (bytesRead == 1) {\r\n                    break;\r\n                }\r\n                remaining -= bytesRead;\r\n                offset += bytesRead;\r\n                count += bytesRead;\r\n                readTimer.end();\r\n                if (bytesRead != 0) {\r\n                    LOG.debug(\"Bytes in read #{}: {} , block bytes: {},\" + \" remaining in block: {}\" + \" duration={} nS; ns/byte: {}, bandwidth={} MB/s\", reads, bytesRead, blockSize - remaining, remaining, readTimer.duration(), readTimer.nanosPerOperation(bytesRead), readTimer.bandwidthDescription(bytesRead));\r\n                } else {\r\n                    LOG.warn(\"0 bytes returned by read() operation #{}\", reads);\r\n                }\r\n            }\r\n            blockTimer.end(\"Reading block %d in %d reads\", blockId, reads);\r\n            String bw = blockTimer.bandwidthDescription(blockSize);\r\n            LOG.info(\"Bandwidth of block {}: {} MB/s: \", blockId, bw);\r\n            if (bandwidthInBytes(blockTimer, blockSize) < minimumBandwidth) {\r\n                LOG.warn(\"Bandwidth {} too low on block {}: resetting connection\", bw, blockId);\r\n                Assert.assertTrue(\"Bandwidth of \" + bw + \" too low after \" + resetCount + \" attempts\", resetCount <= maxResetCount);\r\n                resetCount++;\r\n            }\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(in);\r\n    }\r\n    timer2.end(\"Time to read %d bytes in %d blocks\", totalToRead, blockCount);\r\n    LOG.info(\"Overall Bandwidth {} MB/s; reset connections {}\", timer2.bandwidth(totalToRead), resetCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "test_100_renameHugeFile",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void test_100_renameHugeFile() throws Throwable\n{\r\n    assumeHugeFileExists();\r\n    describe(\"renaming %s to %s\", hugefile, hugefileRenamed);\r\n    NativeAzureFileSystem fs = getFileSystem();\r\n    FileStatus status = fs.getFileStatus(hugefile);\r\n    long filesize = status.getLen();\r\n    fs.delete(hugefileRenamed, false);\r\n    ContractTestUtils.NanoTimer timer = new ContractTestUtils.NanoTimer();\r\n    fs.rename(hugefile, hugefileRenamed);\r\n    long mb = Math.max(filesize / S_1M, 1);\r\n    timer.end(\"time to rename file of %d MB\", mb);\r\n    LOG.info(\"Time per MB to rename = {} nS\", toHuman(timer.nanosPerOperation(mb)));\r\n    bandwidth(timer, filesize);\r\n    logFSState();\r\n    FileStatus destFileStatus = fs.getFileStatus(hugefileRenamed);\r\n    assertEquals(filesize, destFileStatus.getLen());\r\n    ContractTestUtils.NanoTimer timer2 = new ContractTestUtils.NanoTimer();\r\n    fs.rename(hugefileRenamed, hugefile);\r\n    timer2.end(\"Renaming back\");\r\n    LOG.info(\"Time per MB to rename = {} nS\", toHuman(timer2.nanosPerOperation(mb)));\r\n    bandwidth(timer2, filesize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\test\\java\\org\\apache\\hadoop\\fs\\azure\\integration",
  "methodName" : "test_999_deleteHugeFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void test_999_deleteHugeFiles() throws IOException\n{\r\n    testAccountForCleanup = testAccount;\r\n    deleteHugeFile();\r\n    ContractTestUtils.NanoTimer timer2 = new ContractTestUtils.NanoTimer();\r\n    NativeAzureFileSystem fs = getFileSystem();\r\n    fs.delete(hugefileRenamed, false);\r\n    timer2.end(\"time to delete %s\", hugefileRenamed);\r\n    rm(fs, testPath, true, false);\r\n    assertPathDoesNotExist(fs, \"deleted huge file\", testPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
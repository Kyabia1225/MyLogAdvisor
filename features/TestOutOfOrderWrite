[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "XDR create()\n{\r\n    XDR request = new XDR();\r\n    RpcCall.getInstance(0x8000004c, Nfs3Constant.PROGRAM, Nfs3Constant.VERSION, Nfs3Constant.NFSPROC3.CREATE.getValue(), new CredentialsNone(), new VerifierNone()).write(request);\r\n    SetAttr3 objAttr = new SetAttr3();\r\n    CREATE3Request createReq = new CREATE3Request(new FileHandle(\"/\"), \"out-of-order-write\" + System.currentTimeMillis(), 0, objAttr, 0);\r\n    createReq.serialize(request);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "XDR write(FileHandle handle, int xid, long offset, int count, byte[] data)\n{\r\n    XDR request = new XDR();\r\n    RpcCall.getInstance(xid, Nfs3Constant.PROGRAM, Nfs3Constant.VERSION, Nfs3Constant.NFSPROC3.CREATE.getValue(), new CredentialsNone(), new VerifierNone()).write(request);\r\n    WRITE3Request write1 = new WRITE3Request(handle, offset, count, WriteStableHow.UNSTABLE, ByteBuffer.wrap(data));\r\n    write1.serialize(request);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "testRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRequest(XDR request)\n{\r\n    RegistrationClient registrationClient = new RegistrationClient(\"localhost\", Nfs3Constant.SUN_RPCBIND, request);\r\n    registrationClient.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void main(String[] args) throws InterruptedException\n{\r\n    Arrays.fill(data1, (byte) 7);\r\n    Arrays.fill(data2, (byte) 8);\r\n    Arrays.fill(data3, (byte) 9);\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    WriteClient client = new WriteClient(\"localhost\", conf.getInt(NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, NfsConfigKeys.DFS_NFS_SERVER_PORT_DEFAULT), create(), false);\r\n    client.run();\r\n    while (handle == null) {\r\n        Thread.sleep(1000);\r\n        System.out.println(\"handle is still null...\");\r\n    }\r\n    LOG.info(\"Send write1 request\");\r\n    XDR writeReq;\r\n    writeReq = write(handle, 0x8000005c, 2000, 1000, data3);\r\n    Nfs3Utils.writeChannel(channel, writeReq, 1);\r\n    writeReq = write(handle, 0x8000005d, 1000, 1000, data2);\r\n    Nfs3Utils.writeChannel(channel, writeReq, 2);\r\n    writeReq = write(handle, 0x8000005e, 0, 1000, data1);\r\n    Nfs3Utils.writeChannel(channel, writeReq, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testAlterWriteRequest",
  "errType" : null,
  "containingMethodsNum" : 52,
  "sourceCodeText" : "void testAlterWriteRequest() throws IOException\n{\r\n    int len = 20;\r\n    byte[] data = new byte[len];\r\n    ByteBuffer buffer = ByteBuffer.wrap(data);\r\n    for (int i = 0; i < len; i++) {\r\n        buffer.put((byte) i);\r\n    }\r\n    buffer.flip();\r\n    int originalCount = buffer.array().length;\r\n    WRITE3Request request = new WRITE3Request(new FileHandle(), 0, data.length, WriteStableHow.UNSTABLE, buffer);\r\n    WriteCtx writeCtx1 = new WriteCtx(request.getHandle(), request.getOffset(), request.getCount(), WriteCtx.INVALID_ORIGINAL_COUNT, request.getStableHow(), request.getData(), null, 1, false, WriteCtx.DataState.NO_DUMP);\r\n    Assert.assertTrue(writeCtx1.getData().array().length == originalCount);\r\n    OpenFileCtx.alterWriteRequest(request, 12);\r\n    WriteCtx writeCtx2 = new WriteCtx(request.getHandle(), request.getOffset(), request.getCount(), originalCount, request.getStableHow(), request.getData(), null, 2, false, WriteCtx.DataState.NO_DUMP);\r\n    ByteBuffer appendedData = writeCtx2.getData();\r\n    int position = appendedData.position();\r\n    int limit = appendedData.limit();\r\n    Assert.assertTrue(position == 12);\r\n    Assert.assertTrue(limit - position == 8);\r\n    Assert.assertTrue(appendedData.get(position) == (byte) 12);\r\n    Assert.assertTrue(appendedData.get(position + 1) == (byte) 13);\r\n    Assert.assertTrue(appendedData.get(position + 2) == (byte) 14);\r\n    Assert.assertTrue(appendedData.get(position + 7) == (byte) 19);\r\n    buffer.position(0);\r\n    request = new WRITE3Request(new FileHandle(), 0, data.length, WriteStableHow.UNSTABLE, buffer);\r\n    OpenFileCtx.alterWriteRequest(request, 1);\r\n    WriteCtx writeCtx3 = new WriteCtx(request.getHandle(), request.getOffset(), request.getCount(), originalCount, request.getStableHow(), request.getData(), null, 2, false, WriteCtx.DataState.NO_DUMP);\r\n    appendedData = writeCtx3.getData();\r\n    position = appendedData.position();\r\n    limit = appendedData.limit();\r\n    Assert.assertTrue(position == 1);\r\n    Assert.assertTrue(limit - position == 19);\r\n    Assert.assertTrue(appendedData.get(position) == (byte) 1);\r\n    Assert.assertTrue(appendedData.get(position + 18) == (byte) 19);\r\n    buffer.position(0);\r\n    request = new WRITE3Request(new FileHandle(), 0, data.length, WriteStableHow.UNSTABLE, buffer);\r\n    OpenFileCtx.alterWriteRequest(request, 19);\r\n    WriteCtx writeCtx4 = new WriteCtx(request.getHandle(), request.getOffset(), request.getCount(), originalCount, request.getStableHow(), request.getData(), null, 2, false, WriteCtx.DataState.NO_DUMP);\r\n    appendedData = writeCtx4.getData();\r\n    position = appendedData.position();\r\n    limit = appendedData.limit();\r\n    Assert.assertTrue(position == 19);\r\n    Assert.assertTrue(limit - position == 1);\r\n    Assert.assertTrue(appendedData.get(position) == (byte) 19);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCheckCommit",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void testCheckCommit() throws IOException\n{\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 0);\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);\r\n    OpenFileCtx ctx = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(conf), false, conf);\r\n    COMMIT_STATUS ret;\r\n    ctx.setActiveStatusForTest(false);\r\n    Channel ch = Mockito.mock(Channel.class);\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_CTX);\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(5, 10), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE);\r\n    ctx.setActiveStatusForTest(true);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 10);\r\n    ctx.setNextOffsetForTest(10);\r\n    COMMIT_STATUS status = ctx.checkCommitInternal(5, null, 1, attr, false);\r\n    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);\r\n    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);\r\n    status = ctx.checkCommitInternal(10, ch, 1, attr, false);\r\n    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);\r\n    ret = ctx.checkCommit(dfsClient, 10, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);\r\n    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx.getPendingCommitsForTest();\r\n    Assert.assertTrue(commits.size() == 0);\r\n    ret = ctx.checkCommit(dfsClient, 11, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_WAIT);\r\n    Assert.assertTrue(commits.size() == 1);\r\n    long key = commits.firstKey();\r\n    Assert.assertTrue(key == 11);\r\n    commits.remove(new Long(11));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_WAIT);\r\n    Assert.assertTrue(commits.size() == 1);\r\n    key = commits.firstKey();\r\n    Assert.assertTrue(key == 9);\r\n    ctx.getPendingWritesForTest().remove(new OffsetRange(5, 10));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCheckCommitLargeFileUpload",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testCheckCommitLargeFileUpload() throws IOException\n{\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 0);\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, true);\r\n    OpenFileCtx ctx = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(conf), false, conf);\r\n    COMMIT_STATUS ret;\r\n    ctx.setActiveStatusForTest(false);\r\n    Channel ch = Mockito.mock(Channel.class);\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_CTX);\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE);\r\n    ctx.setActiveStatusForTest(true);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 8);\r\n    ctx.setNextOffsetForTest(10);\r\n    COMMIT_STATUS status = ctx.checkCommitInternal(5, null, 1, attr, false);\r\n    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);\r\n    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);\r\n    status = ctx.checkCommitInternal(10, ch, 1, attr, false);\r\n    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);\r\n    ret = ctx.checkCommit(dfsClient, 10, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);\r\n    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx.getPendingCommitsForTest();\r\n    Assert.assertTrue(commits.size() == 1);\r\n    ret = ctx.checkCommit(dfsClient, 16, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS);\r\n    Assert.assertTrue(commits.size() == 1);\r\n    commits.remove(new Long(10));\r\n    ret = ctx.checkCommitInternal(0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);\r\n    ret = ctx.checkCommitInternal(9, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);\r\n    Assert.assertTrue(commits.size() == 2);\r\n    ctx.getPendingWritesForTest().remove(new OffsetRange(10, 15));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);\r\n    ctx.setNextOffsetForTest((long) 8);\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, false);\r\n    Assert.assertTrue(ret == COMMIT_STATUS.COMMIT_FINISHED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCheckCommitAixCompatMode",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCheckCommitAixCompatMode() throws IOException\n{\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);\r\n    OpenFileCtx ctx = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()), true, conf);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 2);\r\n    COMMIT_STATUS status = ctx.checkCommitInternal(5, null, 1, attr, false);\r\n    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_FINISHED);\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(0, 10), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    Mockito.when(fos.getPos()).thenReturn((long) 10);\r\n    ctx.setNextOffsetForTest((long) 10);\r\n    status = ctx.checkCommitInternal(5, null, 1, attr, false);\r\n    Assert.assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCheckCommitFromRead",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testCheckCommitFromRead() throws IOException\n{\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 0);\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    config.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);\r\n    OpenFileCtx ctx = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(config), false, config);\r\n    FileHandle h = new FileHandle(1);\r\n    COMMIT_STATUS ret;\r\n    WriteManager wm = new WriteManager(new ShellBasedIdMapping(config), config, false);\r\n    assertTrue(wm.addOpenFileStream(h, ctx));\r\n    ctx.setActiveStatusForTest(false);\r\n    Channel ch = Mockito.mock(Channel.class);\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_INACTIVE_CTX, ret);\r\n    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 0));\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE, ret);\r\n    assertEquals(Nfs3Status.NFS3ERR_IO, wm.commitBeforeRead(dfsClient, h, 0));\r\n    ctx.setActiveStatusForTest(true);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 10);\r\n    ctx.setNextOffsetForTest((long) 10);\r\n    COMMIT_STATUS status = ctx.checkCommitInternal(5, ch, 1, attr, false);\r\n    assertEquals(COMMIT_STATUS.COMMIT_DO_SYNC, status);\r\n    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);\r\n    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 5));\r\n    status = ctx.checkCommitInternal(10, ch, 1, attr, true);\r\n    assertTrue(status == COMMIT_STATUS.COMMIT_DO_SYNC);\r\n    ret = ctx.checkCommit(dfsClient, 10, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);\r\n    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 10));\r\n    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx.getPendingCommitsForTest();\r\n    assertTrue(commits.size() == 0);\r\n    ret = ctx.checkCommit(dfsClient, 11, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_WAIT, ret);\r\n    assertEquals(0, commits.size());\r\n    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 11));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_WAIT, ret);\r\n    assertEquals(0, commits.size());\r\n    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 0));\r\n    ctx.getPendingWritesForTest().remove(new OffsetRange(10, 15));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);\r\n    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCheckCommitFromReadLargeFileUpload",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testCheckCommitFromReadLargeFileUpload() throws IOException\n{\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 0);\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    config.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, true);\r\n    OpenFileCtx ctx = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(config), false, config);\r\n    FileHandle h = new FileHandle(1);\r\n    COMMIT_STATUS ret;\r\n    WriteManager wm = new WriteManager(new ShellBasedIdMapping(config), config, false);\r\n    assertTrue(wm.addOpenFileStream(h, ctx));\r\n    ctx.setActiveStatusForTest(false);\r\n    Channel ch = Mockito.mock(Channel.class);\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_INACTIVE_CTX, ret);\r\n    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 0));\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE, ret);\r\n    assertEquals(Nfs3Status.NFS3ERR_IO, wm.commitBeforeRead(dfsClient, h, 0));\r\n    ctx.setActiveStatusForTest(true);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 6);\r\n    ctx.setNextOffsetForTest((long) 10);\r\n    COMMIT_STATUS status = ctx.checkCommitInternal(5, ch, 1, attr, false);\r\n    assertEquals(COMMIT_STATUS.COMMIT_DO_SYNC, status);\r\n    ret = ctx.checkCommit(dfsClient, 5, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_FINISHED, ret);\r\n    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 5));\r\n    status = ctx.checkCommitInternal(9, ch, 1, attr, true);\r\n    assertTrue(status == COMMIT_STATUS.COMMIT_SPECIAL_WAIT);\r\n    ret = ctx.checkCommit(dfsClient, 9, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_WAIT, ret);\r\n    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 9));\r\n    ConcurrentNavigableMap<Long, CommitCtx> commits = ctx.getPendingCommitsForTest();\r\n    assertTrue(commits.size() == 0);\r\n    ret = ctx.checkCommit(dfsClient, 16, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS, ret);\r\n    assertEquals(0, commits.size());\r\n    assertEquals(Nfs3Status.NFS3_OK, wm.commitBeforeRead(dfsClient, h, 16));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_WAIT, ret);\r\n    assertEquals(0, commits.size());\r\n    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 0));\r\n    ctx.getPendingWritesForTest().remove(new OffsetRange(10, 15));\r\n    ret = ctx.checkCommit(dfsClient, 0, ch, 1, attr, true);\r\n    assertEquals(COMMIT_STATUS.COMMIT_SPECIAL_WAIT, ret);\r\n    assertEquals(Nfs3Status.NFS3ERR_JUKEBOX, wm.commitBeforeRead(dfsClient, h, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "waitWrite",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void waitWrite(RpcProgramNfs3 nfsd, FileHandle handle, int maxWaitTime) throws InterruptedException\n{\r\n    int waitedTime = 0;\r\n    OpenFileCtx ctx = nfsd.getWriteManager().getOpenFileCtxCache().get(handle);\r\n    assertTrue(ctx != null);\r\n    do {\r\n        Thread.sleep(3000);\r\n        waitedTime += 3000;\r\n        if (ctx.getPendingWritesForTest().size() == 0) {\r\n            return;\r\n        }\r\n    } while (waitedTime < maxWaitTime);\r\n    fail(\"Write can't finish.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testWriteStableHow",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testWriteStableHow() throws IOException, InterruptedException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    DFSClient client = null;\r\n    MiniDFSCluster cluster = null;\r\n    RpcProgramNfs3 nfsd;\r\n    SecurityHandler securityHandler = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandler.getUser()).thenReturn(System.getProperty(\"user.name\"));\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(currentUser), \"*\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(currentUser), \"*\");\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(config);\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n        cluster.waitActive();\r\n        client = new DFSClient(DFSUtilClient.getNNAddress(config), config);\r\n        int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n        config.setInt(\"nfs3.mountd.port\", 0);\r\n        config.setInt(\"nfs3.server.port\", 0);\r\n        Nfs3 nfs3 = new Nfs3(config);\r\n        nfs3.startServiceInternal(false);\r\n        nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();\r\n        HdfsFileStatus status = client.getFileInfo(\"/\");\r\n        FileHandle rootHandle = new FileHandle(status.getFileId(), namenodeId);\r\n        CREATE3Request createReq = new CREATE3Request(rootHandle, \"file1\", Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);\r\n        XDR createXdr = new XDR();\r\n        createReq.serialize(createXdr);\r\n        CREATE3Response createRsp = nfsd.create(createXdr.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        FileHandle handle = createRsp.getObjHandle();\r\n        byte[] buffer = new byte[10];\r\n        for (int i = 0; i < 10; i++) {\r\n            buffer[i] = (byte) i;\r\n        }\r\n        WRITE3Request writeReq = new WRITE3Request(handle, 0, 10, WriteStableHow.DATA_SYNC, ByteBuffer.wrap(buffer));\r\n        XDR writeXdr = new XDR();\r\n        writeReq.serialize(writeXdr);\r\n        nfsd.write(writeXdr.asReadOnlyWrap(), null, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        waitWrite(nfsd, handle, 60000);\r\n        READ3Request readReq = new READ3Request(handle, 0, 10);\r\n        XDR readXdr = new XDR();\r\n        readReq.serialize(readXdr);\r\n        READ3Response readRsp = nfsd.read(readXdr.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        assertTrue(Arrays.equals(buffer, readRsp.getData().array()));\r\n        CREATE3Request createReq2 = new CREATE3Request(rootHandle, \"file2\", Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);\r\n        XDR createXdr2 = new XDR();\r\n        createReq2.serialize(createXdr2);\r\n        CREATE3Response createRsp2 = nfsd.create(createXdr2.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        FileHandle handle2 = createRsp2.getObjHandle();\r\n        WRITE3Request writeReq2 = new WRITE3Request(handle2, 0, 10, WriteStableHow.FILE_SYNC, ByteBuffer.wrap(buffer));\r\n        XDR writeXdr2 = new XDR();\r\n        writeReq2.serialize(writeXdr2);\r\n        nfsd.write(writeXdr2.asReadOnlyWrap(), null, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        waitWrite(nfsd, handle2, 60000);\r\n        READ3Request readReq2 = new READ3Request(handle2, 0, 10);\r\n        XDR readXdr2 = new XDR();\r\n        readReq2.serialize(readXdr2);\r\n        READ3Response readRsp2 = nfsd.read(readXdr2.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        assertTrue(Arrays.equals(buffer, readRsp2.getData().array()));\r\n        status = client.getFileInfo(\"/file2\");\r\n        assertTrue(status.getLen() == 10);\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testOOOWrites",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testOOOWrites() throws IOException, InterruptedException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    RpcProgramNfs3 nfsd;\r\n    final int bufSize = 32;\r\n    final int numOOO = 3;\r\n    SecurityHandler securityHandler = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandler.getUser()).thenReturn(System.getProperty(\"user.name\"));\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(currentUser), \"*\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(currentUser), \"*\");\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(config);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n        cluster.waitActive();\r\n        Nfs3 nfs3 = new Nfs3(config);\r\n        nfs3.startServiceInternal(false);\r\n        nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();\r\n        DFSClient dfsClient = new DFSClient(DFSUtilClient.getNNAddress(config), config);\r\n        int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n        HdfsFileStatus status = dfsClient.getFileInfo(\"/\");\r\n        FileHandle rootHandle = new FileHandle(status.getFileId(), namenodeId);\r\n        CREATE3Request createReq = new CREATE3Request(rootHandle, \"out-of-order-write\" + System.currentTimeMillis(), Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);\r\n        XDR createXdr = new XDR();\r\n        createReq.serialize(createXdr);\r\n        CREATE3Response createRsp = nfsd.create(createXdr.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        FileHandle handle = createRsp.getObjHandle();\r\n        byte[][] oooBuf = new byte[numOOO][bufSize];\r\n        for (int i = 0; i < numOOO; i++) {\r\n            Arrays.fill(oooBuf[i], (byte) i);\r\n        }\r\n        for (int i = 0; i < numOOO; i++) {\r\n            final long offset = (numOOO - 1 - i) * bufSize;\r\n            WRITE3Request writeReq = new WRITE3Request(handle, offset, bufSize, WriteStableHow.UNSTABLE, ByteBuffer.wrap(oooBuf[i]));\r\n            XDR writeXdr = new XDR();\r\n            writeReq.serialize(writeXdr);\r\n            nfsd.write(writeXdr.asReadOnlyWrap(), null, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        }\r\n        waitWrite(nfsd, handle, 60000);\r\n        READ3Request readReq = new READ3Request(handle, bufSize, bufSize);\r\n        XDR readXdr = new XDR();\r\n        readReq.serialize(readXdr);\r\n        READ3Response readRsp = nfsd.read(readXdr.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", config.getInt(NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, NfsConfigKeys.DFS_NFS_SERVER_PORT_DEFAULT)));\r\n        assertTrue(Arrays.equals(oooBuf[1], readRsp.getData().array()));\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testOverlappingWrites",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testOverlappingWrites() throws IOException, InterruptedException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    RpcProgramNfs3 nfsd;\r\n    final int bufSize = 32;\r\n    SecurityHandler securityHandler = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandler.getUser()).thenReturn(System.getProperty(\"user.name\"));\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(currentUser), \"*\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(currentUser), \"*\");\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(config);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n        cluster.waitActive();\r\n        Nfs3 nfs3 = new Nfs3(config);\r\n        nfs3.startServiceInternal(false);\r\n        nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();\r\n        DFSClient dfsClient = new DFSClient(DFSUtilClient.getNNAddress(config), config);\r\n        int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n        HdfsFileStatus status = dfsClient.getFileInfo(\"/\");\r\n        FileHandle rootHandle = new FileHandle(status.getFileId(), namenodeId);\r\n        CREATE3Request createReq = new CREATE3Request(rootHandle, \"overlapping-writes\" + System.currentTimeMillis(), Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);\r\n        XDR createXdr = new XDR();\r\n        createReq.serialize(createXdr);\r\n        CREATE3Response createRsp = nfsd.create(createXdr.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        FileHandle handle = createRsp.getObjHandle();\r\n        byte[] buffer = new byte[bufSize];\r\n        for (int i = 0; i < bufSize; i++) {\r\n            buffer[i] = (byte) i;\r\n        }\r\n        int[][] ranges = new int[][] { { 0, 10 }, { 5, 7 }, { 5, 5 }, { 10, 6 }, { 18, 6 }, { 20, 6 }, { 28, 4 }, { 16, 2 }, { 25, 4 } };\r\n        for (int i = 0; i < ranges.length; i++) {\r\n            int[] x = ranges[i];\r\n            byte[] tbuffer = new byte[x[1]];\r\n            for (int j = 0; j < x[1]; j++) {\r\n                tbuffer[j] = buffer[x[0] + j];\r\n            }\r\n            WRITE3Request writeReq = new WRITE3Request(handle, (long) x[0], x[1], WriteStableHow.UNSTABLE, ByteBuffer.wrap(tbuffer));\r\n            XDR writeXdr = new XDR();\r\n            writeReq.serialize(writeXdr);\r\n            nfsd.write(writeXdr.asReadOnlyWrap(), null, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n        }\r\n        waitWrite(nfsd, handle, 60000);\r\n        READ3Request readReq = new READ3Request(handle, 0, bufSize);\r\n        XDR readXdr = new XDR();\r\n        readReq.serialize(readXdr);\r\n        READ3Response readRsp = nfsd.read(readXdr.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", config.getInt(NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, NfsConfigKeys.DFS_NFS_SERVER_PORT_DEFAULT)));\r\n        assertTrue(Arrays.equals(buffer, readRsp.getData().array()));\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCheckSequential",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCheckSequential() throws IOException\n{\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 0);\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    config.setBoolean(NfsConfigKeys.LARGE_FILE_UPLOAD, false);\r\n    OpenFileCtx ctx = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(config), false, config);\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(5, 10), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(10, 15), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    ctx.getPendingWritesForTest().put(new OffsetRange(20, 25), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    assertTrue(!ctx.checkSequential(5, 4));\r\n    assertTrue(ctx.checkSequential(9, 5));\r\n    assertTrue(ctx.checkSequential(10, 5));\r\n    assertTrue(ctx.checkSequential(14, 5));\r\n    assertTrue(!ctx.checkSequential(15, 5));\r\n    assertTrue(!ctx.checkSequential(20, 5));\r\n    assertTrue(!ctx.checkSequential(25, 5));\r\n    assertTrue(!ctx.checkSequential(999, 5));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    config.set(\"fs.permissions.umask-mode\", \"u=rwx,g=,o=\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(currentUser), \"*\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(currentUser), \"*\");\r\n    fsHelper = new FileSystemTestHelper();\r\n    String testRoot = fsHelper.getTestRootDir();\r\n    testRootDir = new File(testRoot).getAbsoluteFile();\r\n    final Path jksPath = new Path(testRootDir.toString(), \"test.jks\");\r\n    config.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH, JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + jksPath.toUri());\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(config);\r\n    cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n    cluster.waitActive();\r\n    hdfs = cluster.getFileSystem();\r\n    nn = cluster.getNameNode();\r\n    dfsAdmin = new HdfsAdmin(cluster.getURI(), config);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(\"dfs.nfs.exports.allowed.hosts\", \"* rw\");\r\n    nfs = new Nfs3(config);\r\n    nfs.startServiceInternal(false);\r\n    nfsd = (RpcProgramNfs3) nfs.getRpcProgram();\r\n    hdfs.getClient().setKeyProvider(nn.getNamesystem().getProvider());\r\n    DFSTestUtil.createKey(TEST_KEY, cluster, config);\r\n    securityHandler = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandler.getUser()).thenReturn(currentUser);\r\n    securityHandlerUnpriviledged = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandlerUnpriviledged.getUser()).thenReturn(\"harry\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createFiles() throws IllegalArgumentException, IOException\n{\r\n    hdfs.delete(new Path(testdir), true);\r\n    hdfs.mkdirs(new Path(testdir));\r\n    hdfs.mkdirs(new Path(testdir + \"/foo\"));\r\n    DFSTestUtil.createFile(hdfs, new Path(testdir + \"/bar\"), 0, (short) 1, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testGetattr",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetattr() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    GETATTR3Request req = new GETATTR3Request(handle);\r\n    req.serialize(xdr_req);\r\n    GETATTR3Response response1 = nfsd.getattr(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    GETATTR3Response response2 = nfsd.getattr(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testSetattr",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSetattr() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    SetAttr3 symAttr = new SetAttr3(0, 1, 0, 0, null, null, EnumSet.of(SetAttrField.UID));\r\n    SETATTR3Request req = new SETATTR3Request(handle, symAttr, false, null);\r\n    req.serialize(xdr_req);\r\n    SETATTR3Response response1 = nfsd.setattr(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    SETATTR3Response response2 = nfsd.setattr(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testLookup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testLookup() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    LOOKUP3Request lookupReq = new LOOKUP3Request(handle, \"bar\");\r\n    XDR xdr_req = new XDR();\r\n    lookupReq.serialize(xdr_req);\r\n    LOOKUP3Response response1 = nfsd.lookup(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    LOOKUP3Response response2 = nfsd.lookup(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testAccess",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAccess() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    ACCESS3Request req = new ACCESS3Request(handle);\r\n    req.serialize(xdr_req);\r\n    ACCESS3Response response1 = nfsd.access(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    ACCESS3Response response2 = nfsd.access(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testReadlink",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testReadlink() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    SYMLINK3Request req = new SYMLINK3Request(handle, \"fubar\", new SetAttr3(), \"bar\");\r\n    req.serialize(xdr_req);\r\n    SYMLINK3Response response = nfsd.symlink(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response.getStatus());\r\n    FileHandle handle2 = response.getObjFileHandle();\r\n    XDR xdr_req2 = new XDR();\r\n    READLINK3Request req2 = new READLINK3Request(handle2);\r\n    req2.serialize(xdr_req2);\r\n    READLINK3Response response1 = nfsd.readlink(xdr_req2.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    READLINK3Response response2 = nfsd.readlink(xdr_req2.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testRead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRead() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    READ3Request readReq = new READ3Request(handle, 0, 5);\r\n    XDR xdr_req = new XDR();\r\n    readReq.serialize(xdr_req);\r\n    READ3Response response1 = nfsd.read(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    READ3Response response2 = nfsd.read(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testEncryptedReadWrite",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testEncryptedReadWrite() throws Exception\n{\r\n    final int len = 8192;\r\n    final Path zone = new Path(\"/zone\");\r\n    hdfs.mkdirs(zone);\r\n    dfsAdmin.createEncryptionZone(zone, TEST_KEY, NO_TRASH);\r\n    final byte[] buffer = new byte[len];\r\n    for (int i = 0; i < len; i++) {\r\n        buffer[i] = (byte) i;\r\n    }\r\n    final String encFile1 = \"/zone/myfile\";\r\n    createFileUsingNfs(encFile1, buffer);\r\n    commit(encFile1, len);\r\n    assertArrayEquals(\"encFile1 not equal\", getFileContentsUsingNfs(encFile1, len), getFileContentsUsingDfs(encFile1, len));\r\n    final String encFile2 = \"/zone/myfile2\";\r\n    final Path encFile2Path = new Path(encFile2);\r\n    DFSTestUtil.createFile(hdfs, encFile2Path, len, (short) 1, 0xFEED);\r\n    assertArrayEquals(\"encFile2 not equal\", getFileContentsUsingNfs(encFile2, len), getFileContentsUsingDfs(encFile2, len));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "createFileUsingNfs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void createFileUsingNfs(String fileName, byte[] buffer) throws Exception\n{\r\n    DFSTestUtil.createFile(hdfs, new Path(fileName), 0, (short) 1, 0);\r\n    final HdfsFileStatus status = nn.getRpcServer().getFileInfo(fileName);\r\n    final long dirId = status.getFileId();\r\n    final int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    final FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    final WRITE3Request writeReq = new WRITE3Request(handle, 0, buffer.length, WriteStableHow.DATA_SYNC, ByteBuffer.wrap(buffer));\r\n    final XDR xdr_req = new XDR();\r\n    writeReq.serialize(xdr_req);\r\n    final WRITE3Response response = nfsd.write(xdr_req.asReadOnlyWrap(), null, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect response: \", null, response);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileContentsUsingNfs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "byte[] getFileContentsUsingNfs(String fileName, int len) throws Exception\n{\r\n    final HdfsFileStatus status = nn.getRpcServer().getFileInfo(fileName);\r\n    final long dirId = status.getFileId();\r\n    final int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    final FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    final READ3Request readReq = new READ3Request(handle, 0, len);\r\n    final XDR xdr_req = new XDR();\r\n    readReq.serialize(xdr_req);\r\n    final READ3Response response = nfsd.read(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code: \", Nfs3Status.NFS3_OK, response.getStatus());\r\n    assertTrue(\"expected full read\", response.isEof());\r\n    return response.getData().array();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileContentsUsingDfs",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "byte[] getFileContentsUsingDfs(String fileName, int len) throws Exception\n{\r\n    final FSDataInputStream in = hdfs.open(new Path(fileName));\r\n    final byte[] ret = new byte[len];\r\n    in.readFully(ret);\r\n    try {\r\n        in.readByte();\r\n        Assert.fail(\"expected end of file\");\r\n    } catch (EOFException e) {\r\n    }\r\n    in.close();\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void commit(String fileName, int len) throws Exception\n{\r\n    final HdfsFileStatus status = nn.getRpcServer().getFileInfo(fileName);\r\n    final long dirId = status.getFileId();\r\n    final int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    final FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    final XDR xdr_req = new XDR();\r\n    final COMMIT3Request req = new COMMIT3Request(handle, 0, len);\r\n    req.serialize(xdr_req);\r\n    Channel ch = Mockito.mock(Channel.class);\r\n    COMMIT3Response response2 = nfsd.commit(xdr_req.asReadOnlyWrap(), ch, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect COMMIT3Response:\", null, response2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testWrite",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testWrite() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    byte[] buffer = new byte[10];\r\n    for (int i = 0; i < 10; i++) {\r\n        buffer[i] = (byte) i;\r\n    }\r\n    WRITE3Request writeReq = new WRITE3Request(handle, 0, 10, WriteStableHow.DATA_SYNC, ByteBuffer.wrap(buffer));\r\n    XDR xdr_req = new XDR();\r\n    writeReq.serialize(xdr_req);\r\n    WRITE3Response response1 = nfsd.write(xdr_req.asReadOnlyWrap(), null, 1, securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    WRITE3Response response2 = nfsd.write(xdr_req.asReadOnlyWrap(), null, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect response:\", null, response2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCreate",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCreate() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    CREATE3Request req = new CREATE3Request(handle, \"fubar\", Nfs3Constant.CREATE_UNCHECKED, new SetAttr3(), 0);\r\n    req.serialize(xdr_req);\r\n    CREATE3Response response1 = nfsd.create(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    CREATE3Response response2 = nfsd.create(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testMkdir",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMkdir() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    MKDIR3Request req = new MKDIR3Request(handle, \"fubar1\", new SetAttr3());\r\n    req.serialize(xdr_req);\r\n    MKDIR3Response response1 = nfsd.mkdir(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    XDR xdr_req2 = new XDR();\r\n    MKDIR3Request req2 = new MKDIR3Request(handle, \"fubar2\", new SetAttr3());\r\n    req2.serialize(xdr_req2);\r\n    MKDIR3Response response2 = nfsd.mkdir(xdr_req2.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testSymlink",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSymlink() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    SYMLINK3Request req = new SYMLINK3Request(handle, \"fubar\", new SetAttr3(), \"bar\");\r\n    req.serialize(xdr_req);\r\n    SYMLINK3Response response1 = nfsd.symlink(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    SYMLINK3Response response2 = nfsd.symlink(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testRemove",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRemove() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    REMOVE3Request req = new REMOVE3Request(handle, \"bar\");\r\n    req.serialize(xdr_req);\r\n    REMOVE3Response response1 = nfsd.remove(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    REMOVE3Response response2 = nfsd.remove(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testRmdir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRmdir() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    RMDIR3Request req = new RMDIR3Request(handle, \"foo\");\r\n    req.serialize(xdr_req);\r\n    RMDIR3Response response1 = nfsd.rmdir(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    RMDIR3Response response2 = nfsd.rmdir(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testRename",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRename() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    RENAME3Request req = new RENAME3Request(handle, \"bar\", handle, \"fubar\");\r\n    req.serialize(xdr_req);\r\n    RENAME3Response response1 = nfsd.rename(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    RENAME3Response response2 = nfsd.rename(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testReaddir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReaddir() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    READDIR3Request req = new READDIR3Request(handle, 0, 0, 100);\r\n    req.serialize(xdr_req);\r\n    READDIR3Response response1 = nfsd.readdir(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    READDIR3Response response2 = nfsd.readdir(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testReaddirplus",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReaddirplus() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    READDIRPLUS3Request req = new READDIRPLUS3Request(handle, 0, 0, 3, 2);\r\n    req.serialize(xdr_req);\r\n    READDIRPLUS3Response response1 = nfsd.readdirplus(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    READDIRPLUS3Response response2 = nfsd.readdirplus(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testFsstat",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFsstat() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    FSSTAT3Request req = new FSSTAT3Request(handle);\r\n    req.serialize(xdr_req);\r\n    FSSTAT3Response response1 = nfsd.fsstat(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    FSSTAT3Response response2 = nfsd.fsstat(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testFsinfo",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFsinfo() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    FSINFO3Request req = new FSINFO3Request(handle);\r\n    req.serialize(xdr_req);\r\n    FSINFO3Response response1 = nfsd.fsinfo(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    FSINFO3Response response2 = nfsd.fsinfo(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testPathconf",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPathconf() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    PATHCONF3Request req = new PATHCONF3Request(handle);\r\n    req.serialize(xdr_req);\r\n    PATHCONF3Response response1 = nfsd.pathconf(xdr_req.asReadOnlyWrap(), securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    PATHCONF3Response response2 = nfsd.pathconf(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3_OK, response2.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCommit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCommit() throws Exception\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(\"/tmp/bar\");\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    XDR xdr_req = new XDR();\r\n    COMMIT3Request req = new COMMIT3Request(handle, 0, 5);\r\n    req.serialize(xdr_req);\r\n    Channel ch = Mockito.mock(Channel.class);\r\n    COMMIT3Response response1 = nfsd.commit(xdr_req.asReadOnlyWrap(), ch, 1, securityHandlerUnpriviledged, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code:\", Nfs3Status.NFS3ERR_ACCES, response1.getStatus());\r\n    COMMIT3Response response2 = nfsd.commit(xdr_req.asReadOnlyWrap(), ch, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect COMMIT3Response:\", null, response2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testIdempotent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testIdempotent()\n{\r\n    Object[][] procedures = { { Nfs3Constant.NFSPROC3.NULL, 1 }, { Nfs3Constant.NFSPROC3.GETATTR, 1 }, { Nfs3Constant.NFSPROC3.SETATTR, 1 }, { Nfs3Constant.NFSPROC3.LOOKUP, 1 }, { Nfs3Constant.NFSPROC3.ACCESS, 1 }, { Nfs3Constant.NFSPROC3.READLINK, 1 }, { Nfs3Constant.NFSPROC3.READ, 1 }, { Nfs3Constant.NFSPROC3.WRITE, 1 }, { Nfs3Constant.NFSPROC3.CREATE, 0 }, { Nfs3Constant.NFSPROC3.MKDIR, 0 }, { Nfs3Constant.NFSPROC3.SYMLINK, 0 }, { Nfs3Constant.NFSPROC3.MKNOD, 0 }, { Nfs3Constant.NFSPROC3.REMOVE, 0 }, { Nfs3Constant.NFSPROC3.RMDIR, 0 }, { Nfs3Constant.NFSPROC3.RENAME, 0 }, { Nfs3Constant.NFSPROC3.LINK, 0 }, { Nfs3Constant.NFSPROC3.READDIR, 1 }, { Nfs3Constant.NFSPROC3.READDIRPLUS, 1 }, { Nfs3Constant.NFSPROC3.FSSTAT, 1 }, { Nfs3Constant.NFSPROC3.FSINFO, 1 }, { Nfs3Constant.NFSPROC3.PATHCONF, 1 }, { Nfs3Constant.NFSPROC3.COMMIT, 1 } };\r\n    for (Object[] procedure : procedures) {\r\n        boolean idempotent = procedure[1].equals(Integer.valueOf(1));\r\n        Nfs3Constant.NFSPROC3 proc = (Nfs3Constant.NFSPROC3) procedure[0];\r\n        if (idempotent) {\r\n            Assert.assertTrue((\"Procedure \" + proc + \" should be idempotent\"), proc.isIdempotent());\r\n        } else {\r\n            Assert.assertFalse((\"Procedure \" + proc + \" should be non-idempotent\"), proc.isIdempotent());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testDeprecatedKeys",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testDeprecatedKeys()\n{\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.setInt(\"nfs3.server.port\", 998);\r\n    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, 0) == 998);\r\n    conf.setInt(\"nfs3.mountd.port\", 999);\r\n    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_MOUNTD_PORT_KEY, 0) == 999);\r\n    conf.set(\"dfs.nfs.exports.allowed.hosts\", \"host1\");\r\n    assertTrue(conf.get(CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY).equals(\"host1\"));\r\n    conf.setInt(\"dfs.nfs.exports.cache.expirytime.millis\", 1000);\r\n    assertTrue(conf.getInt(Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY, 0) == 1000);\r\n    conf.setInt(\"hadoop.nfs.userupdate.milly\", 10);\r\n    assertTrue(conf.getInt(IdMappingConstant.USERGROUPID_UPDATE_MILLIS_KEY, 0) == 10);\r\n    conf.set(\"dfs.nfs3.dump.dir\", \"/nfs/tmp\");\r\n    assertTrue(conf.get(NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_KEY).equals(\"/nfs/tmp\"));\r\n    conf.setBoolean(\"dfs.nfs3.enableDump\", false);\r\n    assertTrue(conf.getBoolean(NfsConfigKeys.DFS_NFS_FILE_DUMP_KEY, true) == false);\r\n    conf.setInt(\"dfs.nfs3.max.open.files\", 500);\r\n    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 0) == 500);\r\n    conf.setInt(\"dfs.nfs3.stream.timeout\", 6000);\r\n    assertTrue(conf.getInt(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_KEY, 0) == 6000);\r\n    conf.set(\"dfs.nfs3.export.point\", \"/dir1\");\r\n    assertTrue(conf.get(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY).equals(\"/dir1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testEviction",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testEviction() throws IOException, InterruptedException\n{\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.setInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 2);\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 0);\r\n    OpenFileCtx context1 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtx context2 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtx context3 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtx context4 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtx context5 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtxCache cache = new OpenFileCtxCache(conf, 10 * 60 * 100);\r\n    boolean ret = cache.put(new FileHandle(1), context1);\r\n    assertTrue(ret);\r\n    Thread.sleep(1000);\r\n    ret = cache.put(new FileHandle(2), context2);\r\n    assertTrue(ret);\r\n    ret = cache.put(new FileHandle(3), context3);\r\n    assertFalse(ret);\r\n    assertTrue(cache.size() == 2);\r\n    Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);\r\n    assertTrue(cache.size() == 2);\r\n    ret = cache.put(new FileHandle(3), context3);\r\n    assertTrue(ret);\r\n    assertTrue(cache.size() == 2);\r\n    assertTrue(cache.get(new FileHandle(1)) == null);\r\n    context3.setActiveStatusForTest(false);\r\n    ret = cache.put(new FileHandle(4), context4);\r\n    assertTrue(ret);\r\n    context2.getPendingWritesForTest().put(new OffsetRange(0, 100), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));\r\n    context4.getPendingCommitsForTest().put(new Long(100), new CommitCtx(0, null, 0, attr));\r\n    Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);\r\n    ret = cache.put(new FileHandle(5), context5);\r\n    assertFalse(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testScan",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testScan() throws IOException, InterruptedException\n{\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.setInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 2);\r\n    DFSClient dfsClient = Mockito.mock(DFSClient.class);\r\n    Nfs3FileAttributes attr = new Nfs3FileAttributes();\r\n    HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);\r\n    Mockito.when(fos.getPos()).thenReturn((long) 0);\r\n    OpenFileCtx context1 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtx context2 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtx context3 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtx context4 = new OpenFileCtx(fos, attr, \"/dumpFilePath\", dfsClient, new ShellBasedIdMapping(new NfsConfiguration()));\r\n    OpenFileCtxCache cache = new OpenFileCtxCache(conf, 10 * 60 * 100);\r\n    boolean ret = cache.put(new FileHandle(1), context1);\r\n    assertTrue(ret);\r\n    ret = cache.put(new FileHandle(2), context2);\r\n    assertTrue(ret);\r\n    Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT + 1);\r\n    cache.scan(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);\r\n    assertTrue(cache.size() == 0);\r\n    ret = cache.put(new FileHandle(3), context3);\r\n    assertTrue(ret);\r\n    ret = cache.put(new FileHandle(4), context4);\r\n    assertTrue(ret);\r\n    context3.setActiveStatusForTest(false);\r\n    cache.scan(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_DEFAULT);\r\n    assertTrue(cache.size() == 1);\r\n    assertTrue(cache.get(new FileHandle(3)) == null);\r\n    assertTrue(cache.get(new FileHandle(4)) != null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testHdfsExportPoint",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testHdfsExportPoint() throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(\"nfs.http.address\", \"0.0.0.0:0\");\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n        cluster.waitActive();\r\n        final Nfs3 nfsServer = new Nfs3(config);\r\n        nfsServer.startServiceInternal(false);\r\n        Mountd mountd = nfsServer.getMountd();\r\n        RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();\r\n        assertTrue(rpcMount.getExports().size() == 1);\r\n        String exportInMountd = rpcMount.getExports().get(0);\r\n        assertTrue(exportInMountd.equals(\"/\"));\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testViewFsMultipleExportPoint",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testViewFsMultipleExportPoint() throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    String clusterName = RandomStringUtils.randomAlphabetic(10);\r\n    String exportPoint = \"/hdfs1,/hdfs2\";\r\n    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);\r\n    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, FsConstants.VIEWFS_SCHEME + \"://\" + clusterName);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(\"nfs.http.address\", \"0.0.0.0:0\");\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)).numDataNodes(2).build();\r\n        cluster.waitActive();\r\n        DistributedFileSystem hdfs1 = cluster.getFileSystem(0);\r\n        DistributedFileSystem hdfs2 = cluster.getFileSystem(1);\r\n        cluster.waitActive();\r\n        Path base1 = new Path(\"/user1\");\r\n        Path base2 = new Path(\"/user2\");\r\n        hdfs1.delete(base1, true);\r\n        hdfs2.delete(base2, true);\r\n        hdfs1.mkdirs(base1);\r\n        hdfs2.mkdirs(base2);\r\n        ConfigUtil.addLink(config, clusterName, \"/hdfs1\", hdfs1.makeQualified(base1).toUri());\r\n        ConfigUtil.addLink(config, clusterName, \"/hdfs2\", hdfs2.makeQualified(base2).toUri());\r\n        final Nfs3 nfsServer = new Nfs3(config);\r\n        nfsServer.startServiceInternal(false);\r\n        Mountd mountd = nfsServer.getMountd();\r\n        RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();\r\n        assertTrue(rpcMount.getExports().size() == 2);\r\n        String exportInMountd1 = rpcMount.getExports().get(0);\r\n        assertTrue(exportInMountd1.equals(\"/hdfs1\"));\r\n        String exportInMountd2 = rpcMount.getExports().get(1);\r\n        assertTrue(exportInMountd2.equals(\"/hdfs2\"));\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testViewFsInternalExportPoint",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testViewFsInternalExportPoint() throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    String clusterName = RandomStringUtils.randomAlphabetic(10);\r\n    String exportPoint = \"/hdfs1/subpath\";\r\n    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);\r\n    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, FsConstants.VIEWFS_SCHEME + \"://\" + clusterName);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(\"nfs.http.address\", \"0.0.0.0:0\");\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)).numDataNodes(2).build();\r\n        cluster.waitActive();\r\n        DistributedFileSystem hdfs1 = cluster.getFileSystem(0);\r\n        DistributedFileSystem hdfs2 = cluster.getFileSystem(1);\r\n        cluster.waitActive();\r\n        Path base1 = new Path(\"/user1\");\r\n        Path base2 = new Path(\"/user2\");\r\n        hdfs1.delete(base1, true);\r\n        hdfs2.delete(base2, true);\r\n        hdfs1.mkdirs(base1);\r\n        hdfs2.mkdirs(base2);\r\n        ConfigUtil.addLink(config, clusterName, \"/hdfs1\", hdfs1.makeQualified(base1).toUri());\r\n        ConfigUtil.addLink(config, clusterName, \"/hdfs2\", hdfs2.makeQualified(base2).toUri());\r\n        Path subPath = new Path(base1, \"subpath\");\r\n        hdfs1.delete(subPath, true);\r\n        hdfs1.mkdirs(subPath);\r\n        final Nfs3 nfsServer = new Nfs3(config);\r\n        nfsServer.startServiceInternal(false);\r\n        Mountd mountd = nfsServer.getMountd();\r\n        RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();\r\n        assertTrue(rpcMount.getExports().size() == 1);\r\n        String exportInMountd = rpcMount.getExports().get(0);\r\n        assertTrue(exportInMountd.equals(exportPoint));\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testViewFsRootExportPoint",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testViewFsRootExportPoint() throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    String clusterName = RandomStringUtils.randomAlphabetic(10);\r\n    String exportPoint = \"/\";\r\n    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);\r\n    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, FsConstants.VIEWFS_SCHEME + \"://\" + clusterName);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(\"nfs.http.address\", \"0.0.0.0:0\");\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)).numDataNodes(2).build();\r\n        cluster.waitActive();\r\n        DistributedFileSystem hdfs1 = cluster.getFileSystem(0);\r\n        DistributedFileSystem hdfs2 = cluster.getFileSystem(1);\r\n        cluster.waitActive();\r\n        Path base1 = new Path(\"/user1\");\r\n        Path base2 = new Path(\"/user2\");\r\n        hdfs1.delete(base1, true);\r\n        hdfs2.delete(base2, true);\r\n        hdfs1.mkdirs(base1);\r\n        hdfs2.mkdirs(base2);\r\n        ConfigUtil.addLink(config, clusterName, \"/hdfs1\", hdfs1.makeQualified(base1).toUri());\r\n        ConfigUtil.addLink(config, clusterName, \"/hdfs2\", hdfs2.makeQualified(base2).toUri());\r\n        exception.expect(FileSystemException.class);\r\n        exception.expectMessage(\"Only HDFS is supported as underlyingFileSystem, \" + \"fs scheme:viewfs\");\r\n        final Nfs3 nfsServer = new Nfs3(config);\r\n        nfsServer.startServiceInternal(false);\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testHdfsInternalExportPoint",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testHdfsInternalExportPoint() throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    String exportPoint = \"/myexport1\";\r\n    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, exportPoint);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(\"nfs.http.address\", \"0.0.0.0:0\");\r\n    Path base = new Path(exportPoint);\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n        cluster.waitActive();\r\n        DistributedFileSystem hdfs = cluster.getFileSystem(0);\r\n        hdfs.delete(base, true);\r\n        hdfs.mkdirs(base);\r\n        final Nfs3 nfsServer = new Nfs3(config);\r\n        nfsServer.startServiceInternal(false);\r\n        Mountd mountd = nfsServer.getMountd();\r\n        RpcProgramMountd rpcMount = (RpcProgramMountd) mountd.getRpcProgram();\r\n        assertTrue(rpcMount.getExports().size() == 1);\r\n        String exportInMountd = rpcMount.getExports().get(0);\r\n        assertTrue(exportInMountd.equals(exportPoint));\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testInvalidFsExport",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testInvalidFsExport() throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = null;\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(\"nfs.http.address\", \"0.0.0.0:0\");\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n        cluster.waitActive();\r\n        config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, FsConstants.LOCAL_FS_URI.toString());\r\n        exception.expect(FileSystemException.class);\r\n        exception.expectMessage(\"Only HDFS is supported as underlyingFileSystem, \" + \"fs scheme:file\");\r\n        final Nfs3 nfsServer = new Nfs3(config);\r\n        nfsServer.startServiceInternal(false);\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "testRequest",
  "errType" : [ "UnknownHostException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRequest(XDR request, XDR request2)\n{\r\n    try {\r\n        DatagramSocket clientSocket = new DatagramSocket();\r\n        InetAddress IPAddress = InetAddress.getByName(\"localhost\");\r\n        byte[] sendData = request.getBytes();\r\n        byte[] receiveData = new byte[65535];\r\n        DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length, IPAddress, Nfs3Constant.SUN_RPCBIND);\r\n        clientSocket.send(sendPacket);\r\n        DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);\r\n        clientSocket.receive(receivePacket);\r\n        clientSocket.close();\r\n    } catch (UnknownHostException e) {\r\n        System.err.println(\"Don't know about host: localhost.\");\r\n        System.exit(1);\r\n    } catch (IOException e) {\r\n        System.err.println(\"Couldn't get I/O for \" + \"the connection to: localhost.\");\r\n        System.exit(1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws InterruptedException\n{\r\n    Thread t1 = new Runtest1();\r\n    t1.start();\r\n    t1.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "createPortmapXDRheader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createPortmapXDRheader(XDR xdr_out, int procedure)\n{\r\n    RpcCall.getInstance(0, 100000, 2, procedure, new CredentialsNone(), new VerifierNone()).write(xdr_out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "testGetportMount",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetportMount()\n{\r\n    XDR xdr_out = new XDR();\r\n    createPortmapXDRheader(xdr_out, 3);\r\n    xdr_out.writeInt(100005);\r\n    xdr_out.writeInt(1);\r\n    xdr_out.writeInt(6);\r\n    xdr_out.writeInt(0);\r\n    XDR request2 = new XDR();\r\n    createPortmapXDRheader(xdr_out, 3);\r\n    request2.writeInt(100005);\r\n    request2.writeInt(1);\r\n    request2.writeInt(6);\r\n    request2.writeInt(0);\r\n    testRequest(xdr_out, request2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "testGetport",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetport()\n{\r\n    XDR xdr_out = new XDR();\r\n    createPortmapXDRheader(xdr_out, 3);\r\n    xdr_out.writeInt(100003);\r\n    xdr_out.writeInt(3);\r\n    xdr_out.writeInt(6);\r\n    xdr_out.writeInt(0);\r\n    XDR request2 = new XDR();\r\n    createPortmapXDRheader(xdr_out, 3);\r\n    request2.writeInt(100003);\r\n    request2.writeInt(3);\r\n    request2.writeInt(6);\r\n    request2.writeInt(0);\r\n    testRequest(xdr_out, request2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "testDump",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDump()\n{\r\n    XDR xdr_out = new XDR();\r\n    createPortmapXDRheader(xdr_out, 4);\r\n    testRequest(xdr_out, xdr_out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs",
  "methodName" : "testStart",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testStart() throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    MiniDFSCluster cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n    cluster.waitActive();\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    int newTimeoutMillis = 1000;\r\n    config.setInt(NfsConfigKeys.NFS_UDP_CLIENT_PORTMAP_TIMEOUT_MILLIS_KEY, newTimeoutMillis);\r\n    assertTrue(config.getInt(NfsConfigKeys.NFS_UDP_CLIENT_PORTMAP_TIMEOUT_MILLIS_KEY, 0) == newTimeoutMillis);\r\n    Nfs3 nfs3 = new Nfs3(config);\r\n    nfs3.startServiceInternal(false);\r\n    RpcProgramMountd mountd = (RpcProgramMountd) nfs3.getMountd().getRpcProgram();\r\n    mountd.nullOp(new XDR(), 1234, InetAddress.getByName(\"localhost\"));\r\n    assertTrue(mountd.getPortmapUdpTimeoutMillis() == newTimeoutMillis);\r\n    RpcProgramNfs3 nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();\r\n    nfsd.nullProcedure();\r\n    assertTrue(nfsd.getPortmapUdpTimeoutMillis() == newTimeoutMillis);\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testConstructor1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testConstructor1() throws IOException\n{\r\n    new OffsetRange(0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testConstructor2",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testConstructor2() throws IOException\n{\r\n    new OffsetRange(-1, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testConstructor3",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testConstructor3() throws IOException\n{\r\n    new OffsetRange(-3, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testConstructor4",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testConstructor4() throws IOException\n{\r\n    new OffsetRange(-3, 100);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testCompare",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCompare() throws IOException\n{\r\n    OffsetRange r1 = new OffsetRange(0, 1);\r\n    OffsetRange r2 = new OffsetRange(1, 3);\r\n    OffsetRange r3 = new OffsetRange(1, 3);\r\n    OffsetRange r4 = new OffsetRange(3, 4);\r\n    assertEquals(0, OffsetRange.ReverseComparatorOnMin.compare(r2, r3));\r\n    assertEquals(0, OffsetRange.ReverseComparatorOnMin.compare(r2, r2));\r\n    assertTrue(OffsetRange.ReverseComparatorOnMin.compare(r2, r1) < 0);\r\n    assertTrue(OffsetRange.ReverseComparatorOnMin.compare(r2, r4) > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    conf.set(DFSConfigKeys.DFS_HTTP_POLICY_KEY, HttpConfig.Policy.HTTP_AND_HTTPS.name());\r\n    conf.set(NfsConfigKeys.NFS_HTTP_ADDRESS_KEY, \"localhost:0\");\r\n    conf.set(NfsConfigKeys.NFS_HTTPS_ADDRESS_KEY, \"localhost:0\");\r\n    conf.setInt(NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, 0);\r\n    conf.setInt(NfsConfigKeys.DFS_NFS_MOUNTD_PORT_KEY, 0);\r\n    File base = new File(BASEDIR);\r\n    FileUtil.fullyDelete(base);\r\n    base.mkdirs();\r\n    keystoresDir = new File(BASEDIR).getAbsolutePath();\r\n    sslConfDir = KeyStoreTestUtil.getClasspathDir(TestNfs3HttpServer.class);\r\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfDir, conf, false);\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\r\n    cluster.waitActive();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    FileUtil.fullyDelete(new File(BASEDIR));\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n    KeyStoreTestUtil.cleanupSSLConfig(keystoresDir, sslConfDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testHttpServer",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testHttpServer() throws Exception\n{\r\n    Nfs3 nfs = new Nfs3(conf);\r\n    nfs.startServiceInternal(false);\r\n    RpcProgramNfs3 nfsd = (RpcProgramNfs3) nfs.getRpcProgram();\r\n    Nfs3HttpServer infoServer = nfsd.getInfoServer();\r\n    String urlRoot = infoServer.getServerURI().toString();\r\n    String pageContents = DFSTestUtil.urlGet(new URL(urlRoot + \"/jmx\"));\r\n    assertTrue(\"Bad contents: \" + pageContents, pageContents.contains(\"java.lang:type=\"));\r\n    System.out.println(\"pc:\" + pageContents);\r\n    int port = infoServer.getSecurePort();\r\n    assertTrue(\"Can't get https port\", port > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    config.set(\"fs.permissions.umask-mode\", \"u=rwx,g=,o=\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(currentUser), \"*\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(currentUser), \"*\");\r\n    fsHelper = new FileSystemTestHelper();\r\n    String testRoot = fsHelper.getTestRootDir();\r\n    testRootDir = new File(testRoot).getAbsoluteFile();\r\n    final Path jksPath = new Path(testRootDir.toString(), \"test.jks\");\r\n    config.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH, JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + jksPath.toUri());\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(config);\r\n    cluster = new MiniDFSCluster.Builder(config).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)).numDataNodes(2).build();\r\n    cluster.waitActive();\r\n    hdfs1 = cluster.getFileSystem(0);\r\n    hdfs2 = cluster.getFileSystem(1);\r\n    nn1 = cluster.getNameNode(0);\r\n    nn2 = cluster.getNameNode(1);\r\n    nn2.getServiceRpcAddress();\r\n    dfsAdmin1 = new HdfsAdmin(cluster.getURI(0), config);\r\n    dfsAdmin2 = new HdfsAdmin(cluster.getURI(1), config);\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, FsConstants.VIEWFS_URI.toString());\r\n    config.set(\"dfs.nfs.exports.allowed.hosts\", \"* rw\");\r\n    Path base1 = new Path(\"/user1\");\r\n    Path base2 = new Path(\"/user2\");\r\n    hdfs1.delete(base1, true);\r\n    hdfs2.delete(base2, true);\r\n    hdfs1.mkdirs(base1);\r\n    hdfs2.mkdirs(base2);\r\n    ConfigUtil.addLink(config, \"/hdfs1\", hdfs1.makeQualified(base1).toUri());\r\n    ConfigUtil.addLink(config, \"/hdfs2\", hdfs2.makeQualified(base2).toUri());\r\n    viewFs = FileSystem.get(config);\r\n    config.setStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, \"/hdfs1\", \"/hdfs2\");\r\n    nfs = new Nfs3(config);\r\n    nfs.startServiceInternal(false);\r\n    nfsd = (RpcProgramNfs3) nfs.getRpcProgram();\r\n    mountd = (RpcProgramMountd) nfs.getMountd().getRpcProgram();\r\n    securityHandler = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandler.getUser()).thenReturn(currentUser);\r\n    viewFs.delete(new Path(\"/hdfs2/dir2\"), true);\r\n    viewFs.mkdirs(new Path(\"/hdfs2/dir2\"));\r\n    DFSTestUtil.createFile(viewFs, new Path(\"/hdfs1/file1\"), 0, (short) 1, 0);\r\n    DFSTestUtil.createFile(viewFs, new Path(\"/hdfs1/file2\"), 0, (short) 1, 0);\r\n    DFSTestUtil.createFile(viewFs, new Path(\"/hdfs1/write1\"), 0, (short) 1, 0);\r\n    DFSTestUtil.createFile(viewFs, new Path(\"/hdfs2/write2\"), 0, (short) 1, 0);\r\n    DFSTestUtil.createFile(viewFs, new Path(\"/hdfs1/renameMultiNN\"), 0, (short) 1, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNumExports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNumExports() throws Exception\n{\r\n    Assert.assertEquals(mountd.getExports().size(), viewFs.getChildFileSystems().length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testPaths",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPaths() throws Exception\n{\r\n    Assert.assertEquals(hdfs1.resolvePath(new Path(\"/user1/file1\")), viewFs.resolvePath(new Path(\"/hdfs1/file1\")));\r\n    Assert.assertEquals(hdfs1.resolvePath(new Path(\"/user1/file2\")), viewFs.resolvePath(new Path(\"/hdfs1/file2\")));\r\n    Assert.assertEquals(hdfs2.resolvePath(new Path(\"/user2/dir2\")), viewFs.resolvePath(new Path(\"/hdfs2/dir2\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testFileStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFileStatus() throws Exception\n{\r\n    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(\"/user1/file1\");\r\n    FileStatus st = viewFs.getFileStatus(new Path(\"/hdfs1/file1\"));\r\n    Assert.assertEquals(st.isDirectory(), status.isDirectory());\r\n    HdfsFileStatus status2 = nn2.getRpcServer().getFileInfo(\"/user2/dir2\");\r\n    FileStatus st2 = viewFs.getFileStatus(new Path(\"/hdfs2/dir2\"));\r\n    Assert.assertEquals(st2.isDirectory(), status2.isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsGetAttrResponse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNfsGetAttrResponse(long fileId, int namenodeId, int expectedStatus)\n{\r\n    FileHandle handle = new FileHandle(fileId, namenodeId);\r\n    XDR xdrReq = new XDR();\r\n    GETATTR3Request req = new GETATTR3Request(handle);\r\n    req.serialize(xdrReq);\r\n    GETATTR3Response response = nfsd.getattr(xdrReq.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    Assert.assertEquals(\"Incorrect return code\", expectedStatus, response.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsAccessNN1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNfsAccessNN1() throws Exception\n{\r\n    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(\"/user1/file1\");\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());\r\n    testNfsGetAttrResponse(status.getFileId(), namenodeId, Nfs3Status.NFS3_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsAccessNN2",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNfsAccessNN2() throws Exception\n{\r\n    HdfsFileStatus status = nn2.getRpcServer().getFileInfo(\"/user2/dir2\");\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());\r\n    testNfsGetAttrResponse(status.getFileId(), namenodeId, Nfs3Status.NFS3_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testWrongNfsAccess",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testWrongNfsAccess() throws Exception\n{\r\n    DFSTestUtil.createFile(viewFs, new Path(\"/hdfs1/file3\"), 0, (short) 1, 0);\r\n    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(\"/user1/file3\");\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());\r\n    testNfsGetAttrResponse(status.getFileId(), namenodeId, Nfs3Status.NFS3ERR_IO);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsWriteResponse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNfsWriteResponse(long dirId, int namenodeId) throws Exception\n{\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    byte[] buffer = new byte[10];\r\n    for (int i = 0; i < 10; i++) {\r\n        buffer[i] = (byte) i;\r\n    }\r\n    WRITE3Request writeReq = new WRITE3Request(handle, 0, 10, Nfs3Constant.WriteStableHow.DATA_SYNC, ByteBuffer.wrap(buffer));\r\n    XDR xdrReq = new XDR();\r\n    writeReq.serialize(xdrReq);\r\n    WRITE3Response response = nfsd.write(xdrReq.asReadOnlyWrap(), null, 1, securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    Assert.assertEquals(\"Incorrect response:\", null, response);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsWriteNN1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNfsWriteNN1() throws Exception\n{\r\n    HdfsFileStatus status = nn1.getRpcServer().getFileInfo(\"/user1/write1\");\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());\r\n    testNfsWriteResponse(status.getFileId(), namenodeId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsWriteNN2",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNfsWriteNN2() throws Exception\n{\r\n    HdfsFileStatus status = nn2.getRpcServer().getFileInfo(\"/user2/write2\");\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());\r\n    testNfsWriteResponse(status.getFileId(), namenodeId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsRename",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNfsRename(FileHandle fromDirHandle, String fromFileName, FileHandle toDirHandle, String toFileName, int expectedStatus) throws Exception\n{\r\n    XDR xdrReq = new XDR();\r\n    RENAME3Request req = new RENAME3Request(fromDirHandle, fromFileName, toDirHandle, toFileName);\r\n    req.serialize(xdrReq);\r\n    RENAME3Response response = nfsd.rename(xdrReq.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(expectedStatus, response.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsRenameMultiNN",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testNfsRenameMultiNN() throws Exception\n{\r\n    HdfsFileStatus fromFileStatus = nn1.getRpcServer().getFileInfo(\"/user1\");\r\n    int fromNNId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());\r\n    FileHandle fromHandle = new FileHandle(fromFileStatus.getFileId(), fromNNId);\r\n    HdfsFileStatus toFileStatus = nn2.getRpcServer().getFileInfo(\"/user2\");\r\n    int toNNId = Nfs3Utils.getNamenodeId(config, hdfs2.getUri());\r\n    FileHandle toHandle = new FileHandle(toFileStatus.getFileId(), toNNId);\r\n    HdfsFileStatus statusBeforeRename = nn1.getRpcServer().getFileInfo(\"/user1/renameMultiNN\");\r\n    Assert.assertEquals(statusBeforeRename.isDirectory(), false);\r\n    testNfsRename(fromHandle, \"renameMultiNN\", toHandle, \"renameMultiNNFail\", Nfs3Status.NFS3ERR_INVAL);\r\n    HdfsFileStatus statusAfterRename = nn2.getRpcServer().getFileInfo(\"/user2/renameMultiNNFail\");\r\n    Assert.assertEquals(statusAfterRename, null);\r\n    statusAfterRename = nn1.getRpcServer().getFileInfo(\"/user1/renameMultiNN\");\r\n    Assert.assertEquals(statusAfterRename.isDirectory(), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testNfsRenameSingleNN",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testNfsRenameSingleNN() throws Exception\n{\r\n    DFSTestUtil.createFile(viewFs, new Path(\"/hdfs1/renameSingleNN\"), 0, (short) 1, 0);\r\n    HdfsFileStatus fromFileStatus = nn1.getRpcServer().getFileInfo(\"/user1\");\r\n    int fromNNId = Nfs3Utils.getNamenodeId(config, hdfs1.getUri());\r\n    FileHandle fromHandle = new FileHandle(fromFileStatus.getFileId(), fromNNId);\r\n    HdfsFileStatus statusBeforeRename = nn1.getRpcServer().getFileInfo(\"/user1/renameSingleNN\");\r\n    Assert.assertEquals(statusBeforeRename.isDirectory(), false);\r\n    Path successFilePath = new Path(\"/user1/renameSingleNNSucess\");\r\n    hdfs1.delete(successFilePath, false);\r\n    testNfsRename(fromHandle, \"renameSingleNN\", fromHandle, \"renameSingleNNSucess\", Nfs3Status.NFS3_OK);\r\n    HdfsFileStatus statusAfterRename = nn1.getRpcServer().getFileInfo(\"/user1/renameSingleNNSucess\");\r\n    Assert.assertEquals(statusAfterRename.isDirectory(), false);\r\n    statusAfterRename = nn1.getRpcServer().getFileInfo(\"/user1/renameSingleNN\");\r\n    Assert.assertEquals(statusAfterRename, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testEviction",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testEviction() throws IOException\n{\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"hdfs://localhost\");\r\n    final int MAX_CACHE_SIZE = 1;\r\n    DFSClientCache cache = new DFSClientCache(conf, MAX_CACHE_SIZE);\r\n    int namenodeId = Nfs3Utils.getNamenodeId(conf);\r\n    DFSClient c1 = cache.getDfsClient(\"test1\", namenodeId);\r\n    assertTrue(cache.getDfsClient(\"test1\", namenodeId).toString().contains(\"ugi=test1\"));\r\n    assertEquals(c1, cache.getDfsClient(\"test1\", namenodeId));\r\n    assertFalse(isDfsClientClose(c1));\r\n    cache.getDfsClient(\"test2\", namenodeId);\r\n    assertTrue(isDfsClientClose(c1));\r\n    assertTrue(\"cache size should be the max size or less\", cache.getClientCache().size() <= MAX_CACHE_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testGetUserGroupInformationSecure",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetUserGroupInformationSecure() throws IOException\n{\r\n    String userName = \"user1\";\r\n    String currentUser = \"test-user\";\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"hdfs://localhost\");\r\n    UserGroupInformation currentUserUgi = UserGroupInformation.createRemoteUser(currentUser);\r\n    currentUserUgi.setAuthenticationMethod(KERBEROS);\r\n    UserGroupInformation.setLoginUser(currentUserUgi);\r\n    DFSClientCache cache = new DFSClientCache(conf);\r\n    UserGroupInformation ugiResult = cache.getUserGroupInformation(userName, currentUserUgi);\r\n    assertThat(ugiResult.getUserName(), is(userName));\r\n    assertThat(ugiResult.getRealUser(), is(currentUserUgi));\r\n    assertThat(ugiResult.getAuthenticationMethod(), is(UserGroupInformation.AuthenticationMethod.PROXY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testGetUserGroupInformation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetUserGroupInformation() throws IOException\n{\r\n    String userName = \"user1\";\r\n    String currentUser = \"currentUser\";\r\n    UserGroupInformation currentUserUgi = UserGroupInformation.createUserForTesting(currentUser, new String[0]);\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"hdfs://localhost\");\r\n    DFSClientCache cache = new DFSClientCache(conf);\r\n    UserGroupInformation ugiResult = cache.getUserGroupInformation(userName, currentUserUgi);\r\n    assertThat(ugiResult.getUserName(), is(userName));\r\n    assertThat(ugiResult.getRealUser(), is(currentUserUgi));\r\n    assertThat(ugiResult.getAuthenticationMethod(), is(UserGroupInformation.AuthenticationMethod.PROXY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "isDfsClientClose",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isDfsClientClose(DFSClient c)\n{\r\n    try {\r\n        c.exists(\"\");\r\n    } catch (IOException e) {\r\n        return e.getMessage().equals(\"Filesystem closed\");\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(currentUser), \"*\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(currentUser), \"*\");\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(config);\r\n    cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n    cluster.waitActive();\r\n    hdfs = cluster.getFileSystem();\r\n    nn = cluster.getNameNode();\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    Nfs3 nfs3 = new Nfs3(config);\r\n    nfs3.startServiceInternal(false);\r\n    nfsd = (RpcProgramNfs3) nfs3.getRpcProgram();\r\n    securityHandler = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandler.getUser()).thenReturn(System.getProperty(\"user.name\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createFiles() throws IllegalArgumentException, IOException\n{\r\n    hdfs.delete(new Path(testdir), true);\r\n    hdfs.mkdirs(new Path(testdir));\r\n    DFSTestUtil.createFile(hdfs, new Path(testdir + \"/f1\"), 0, (short) 1, 0);\r\n    DFSTestUtil.createFile(hdfs, new Path(testdir + \"/f2\"), 0, (short) 1, 0);\r\n    DFSTestUtil.createFile(hdfs, new Path(testdir + \"/f3\"), 0, (short) 1, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testReaddirBasic",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testReaddirBasic() throws IOException\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    handle.serialize(xdr_req);\r\n    xdr_req.writeLongAsHyper(0);\r\n    xdr_req.writeLongAsHyper(0);\r\n    xdr_req.writeInt(100);\r\n    READDIR3Response response = nfsd.readdir(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    List<Entry3> dirents = response.getDirList().getEntries();\r\n    assertTrue(dirents.size() == 5);\r\n    status = nn.getRpcServer().getFileInfo(testdir + \"/f2\");\r\n    long f2Id = status.getFileId();\r\n    xdr_req = new XDR();\r\n    handle = new FileHandle(dirId, namenodeId);\r\n    handle.serialize(xdr_req);\r\n    xdr_req.writeLongAsHyper(f2Id);\r\n    xdr_req.writeLongAsHyper(0);\r\n    xdr_req.writeInt(100);\r\n    response = nfsd.readdir(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    dirents = response.getDirList().getEntries();\r\n    assertTrue(dirents.size() == 1);\r\n    Entry3 entry = dirents.get(0);\r\n    assertTrue(entry.getName().equals(\"f3\"));\r\n    hdfs.delete(new Path(testdir + \"/f2\"), false);\r\n    response = nfsd.readdir(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    dirents = response.getDirList().getEntries();\r\n    assertTrue(dirents.size() == 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testReaddirPlus",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testReaddirPlus() throws IOException\n{\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    handle.serialize(xdr_req);\r\n    xdr_req.writeLongAsHyper(0);\r\n    xdr_req.writeLongAsHyper(0);\r\n    xdr_req.writeInt(100);\r\n    xdr_req.writeInt(1000);\r\n    READDIRPLUS3Response responsePlus = nfsd.readdirplus(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    List<EntryPlus3> direntPlus = responsePlus.getDirListPlus().getEntries();\r\n    assertTrue(direntPlus.size() == 5);\r\n    status = nn.getRpcServer().getFileInfo(testdir + \"/f2\");\r\n    long f2Id = status.getFileId();\r\n    xdr_req = new XDR();\r\n    handle = new FileHandle(dirId, namenodeId);\r\n    handle.serialize(xdr_req);\r\n    xdr_req.writeLongAsHyper(f2Id);\r\n    xdr_req.writeLongAsHyper(0);\r\n    xdr_req.writeInt(100);\r\n    xdr_req.writeInt(1000);\r\n    responsePlus = nfsd.readdirplus(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    direntPlus = responsePlus.getDirListPlus().getEntries();\r\n    assertTrue(direntPlus.size() == 1);\r\n    EntryPlus3 entryPlus = direntPlus.get(0);\r\n    assertTrue(entryPlus.getName().equals(\"f3\"));\r\n    hdfs.delete(new Path(testdir + \"/f2\"), false);\r\n    responsePlus = nfsd.readdirplus(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    direntPlus = responsePlus.getDirListPlus().getEntries();\r\n    assertTrue(direntPlus.size() == 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserGroupConfKey(currentUser), \"*\");\r\n    config.set(DefaultImpersonationProvider.getTestProvider().getProxySuperuserIpConfKey(currentUser), \"*\");\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(config);\r\n    cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\r\n    cluster.waitActive();\r\n    hdfs = cluster.getFileSystem();\r\n    nn = cluster.getNameNode();\r\n    config.setInt(\"nfs3.mountd.port\", 0);\r\n    config.setInt(\"nfs3.server.port\", 0);\r\n    securityHandler = Mockito.mock(SecurityHandler.class);\r\n    Mockito.when(securityHandler.getUser()).thenReturn(System.getProperty(\"user.name\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createFiles() throws IllegalArgumentException, IOException\n{\r\n    hdfs.delete(new Path(testdir), true);\r\n    hdfs.mkdirs(new Path(testdir));\r\n    DFSTestUtil.createFile(hdfs, new Path(testdir + \"/f1\"), 0, (short) 1, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testClientAccessPrivilegeForRemove",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testClientAccessPrivilegeForRemove() throws Exception\n{\r\n    config.set(\"dfs.nfs.exports.allowed.hosts\", \"* ro\");\r\n    Nfs3 nfs = new Nfs3(config);\r\n    nfs.startServiceInternal(false);\r\n    RpcProgramNfs3 nfsd = (RpcProgramNfs3) nfs.getRpcProgram();\r\n    HdfsFileStatus status = nn.getRpcServer().getFileInfo(testdir);\r\n    long dirId = status.getFileId();\r\n    int namenodeId = Nfs3Utils.getNamenodeId(config);\r\n    XDR xdr_req = new XDR();\r\n    FileHandle handle = new FileHandle(dirId, namenodeId);\r\n    handle.serialize(xdr_req);\r\n    xdr_req.writeString(\"f1\");\r\n    REMOVE3Response response = nfsd.remove(xdr_req.asReadOnlyWrap(), securityHandler, new InetSocketAddress(\"localhost\", 1234));\r\n    assertEquals(\"Incorrect return code\", Nfs3Status.NFS3ERR_ACCES, response.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "testGetAccessRightsForUserGroup",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testGetAccessRightsForUserGroup() throws IOException\n{\r\n    Nfs3FileAttributes attr = Mockito.mock(Nfs3FileAttributes.class);\r\n    Mockito.when(attr.getUid()).thenReturn(2);\r\n    Mockito.when(attr.getGid()).thenReturn(3);\r\n    Mockito.when(attr.getMode()).thenReturn(448);\r\n    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());\r\n    assertEquals(\"No access should be allowed as UID does not match attribute over mode 700\", 0, Nfs3Utils.getAccessRightsForUserGroup(3, 3, null, attr));\r\n    Mockito.when(attr.getUid()).thenReturn(2);\r\n    Mockito.when(attr.getGid()).thenReturn(3);\r\n    Mockito.when(attr.getMode()).thenReturn(56);\r\n    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());\r\n    assertEquals(\"No access should be allowed as GID does not match attribute over mode 070\", 0, Nfs3Utils.getAccessRightsForUserGroup(2, 4, null, attr));\r\n    Mockito.when(attr.getUid()).thenReturn(2);\r\n    Mockito.when(attr.getGid()).thenReturn(3);\r\n    Mockito.when(attr.getMode()).thenReturn(7);\r\n    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());\r\n    assertEquals(\"Access should be allowed as mode is 007 and UID/GID do not match\", 61, Nfs3Utils.getAccessRightsForUserGroup(1, 4, new int[] { 5, 6 }, attr));\r\n    Mockito.when(attr.getUid()).thenReturn(2);\r\n    Mockito.when(attr.getGid()).thenReturn(10);\r\n    Mockito.when(attr.getMode()).thenReturn(288);\r\n    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSREG.toValue());\r\n    assertEquals(\"Access should be allowed as mode is 440 and Aux GID does match\", 1, Nfs3Utils.getAccessRightsForUserGroup(3, 4, new int[] { 5, 16, 10 }, attr));\r\n    Mockito.when(attr.getUid()).thenReturn(2);\r\n    Mockito.when(attr.getGid()).thenReturn(10);\r\n    Mockito.when(attr.getMode()).thenReturn(448);\r\n    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSDIR.toValue());\r\n    assertEquals(\"Access should be allowed for dir as mode is 700 and UID does match\", 31, Nfs3Utils.getAccessRightsForUserGroup(2, 4, new int[] { 5, 16, 10 }, attr));\r\n    assertEquals(\"No access should be allowed for dir as mode is 700 even though GID does match\", 0, Nfs3Utils.getAccessRightsForUserGroup(3, 10, new int[] { 5, 16, 4 }, attr));\r\n    assertEquals(\"No access should be allowed for dir as mode is 700 even though AuxGID does match\", 0, Nfs3Utils.getAccessRightsForUserGroup(3, 20, new int[] { 5, 10 }, attr));\r\n    Mockito.when(attr.getUid()).thenReturn(2);\r\n    Mockito.when(attr.getGid()).thenReturn(10);\r\n    Mockito.when(attr.getMode()).thenReturn(457);\r\n    Mockito.when(attr.getType()).thenReturn(NfsFileType.NFSDIR.toValue());\r\n    assertEquals(\"Access should be allowed for dir as mode is 711 and GID matches\", 2, Nfs3Utils.getAccessRightsForUserGroup(3, 10, new int[] { 5, 16, 11 }, attr));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTaskStartTimes",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testTaskStartTimes()\n{\r\n    TaskId taskId = mock(TaskId.class);\r\n    TaskInfo taskInfo = mock(TaskInfo.class);\r\n    Map<TaskAttemptID, TaskAttemptInfo> taskAttempts = new TreeMap<TaskAttemptID, TaskAttemptInfo>();\r\n    TaskAttemptID id = new TaskAttemptID(\"0\", 0, TaskType.MAP, 0, 0);\r\n    TaskAttemptInfo info = mock(TaskAttemptInfo.class);\r\n    when(info.getAttemptId()).thenReturn(id);\r\n    when(info.getStartTime()).thenReturn(10l);\r\n    taskAttempts.put(id, info);\r\n    id = new TaskAttemptID(\"1\", 0, TaskType.MAP, 1, 1);\r\n    info = mock(TaskAttemptInfo.class);\r\n    when(info.getAttemptId()).thenReturn(id);\r\n    when(info.getStartTime()).thenReturn(20l);\r\n    taskAttempts.put(id, info);\r\n    when(taskInfo.getAllTaskAttempts()).thenReturn(taskAttempts);\r\n    CompletedTask task = new CompletedTask(taskId, taskInfo);\r\n    TaskReport report = task.getReport();\r\n    assertTrue(report.getStartTime() == 10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCompletedTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCompletedTaskAttempt()\n{\r\n    TaskAttemptInfo attemptInfo = mock(TaskAttemptInfo.class);\r\n    when(attemptInfo.getRackname()).thenReturn(\"Rackname\");\r\n    when(attemptInfo.getShuffleFinishTime()).thenReturn(11L);\r\n    when(attemptInfo.getSortFinishTime()).thenReturn(12L);\r\n    when(attemptInfo.getShufflePort()).thenReturn(10);\r\n    JobID jobId = new JobID(\"12345\", 0);\r\n    TaskID taskId = new TaskID(jobId, TaskType.REDUCE, 0);\r\n    TaskAttemptID taskAttemptId = new TaskAttemptID(taskId, 0);\r\n    when(attemptInfo.getAttemptId()).thenReturn(taskAttemptId);\r\n    CompletedTaskAttempt taskAttemt = new CompletedTaskAttempt(null, attemptInfo);\r\n    assertEquals(\"Rackname\", taskAttemt.getNodeRackName());\r\n    assertEquals(Phase.CLEANUP, taskAttemt.getPhase());\r\n    assertTrue(taskAttemt.isFinished());\r\n    assertEquals(11L, taskAttemt.getShuffleFinishTime());\r\n    assertEquals(12L, taskAttemt.getSortFinishTime());\r\n    assertEquals(10, taskAttemt.getShufflePort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testAddExisting",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAddExisting()\n{\r\n    JobListCache cache = new JobListCache(2, 1000);\r\n    JobId jobId = MRBuilderUtils.newJobId(1, 1, 1);\r\n    HistoryFileInfo fileInfo = Mockito.mock(HistoryFileInfo.class);\r\n    Mockito.when(fileInfo.getJobId()).thenReturn(jobId);\r\n    cache.addIfAbsent(fileInfo);\r\n    cache.addIfAbsent(fileInfo);\r\n    assertEquals(\"Incorrect number of cache entries\", 1, cache.values().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testEviction",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testEviction() throws InterruptedException\n{\r\n    int maxSize = 2;\r\n    JobListCache cache = new JobListCache(maxSize, 1000);\r\n    JobId jobId1 = MRBuilderUtils.newJobId(1, 1, 1);\r\n    HistoryFileInfo fileInfo1 = Mockito.mock(HistoryFileInfo.class);\r\n    Mockito.when(fileInfo1.getJobId()).thenReturn(jobId1);\r\n    JobId jobId2 = MRBuilderUtils.newJobId(2, 2, 2);\r\n    HistoryFileInfo fileInfo2 = Mockito.mock(HistoryFileInfo.class);\r\n    Mockito.when(fileInfo2.getJobId()).thenReturn(jobId2);\r\n    JobId jobId3 = MRBuilderUtils.newJobId(3, 3, 3);\r\n    HistoryFileInfo fileInfo3 = Mockito.mock(HistoryFileInfo.class);\r\n    Mockito.when(fileInfo3.getJobId()).thenReturn(jobId3);\r\n    cache.addIfAbsent(fileInfo1);\r\n    cache.addIfAbsent(fileInfo2);\r\n    cache.addIfAbsent(fileInfo3);\r\n    Collection<HistoryFileInfo> values;\r\n    for (int i = 0; i < 9; i++) {\r\n        values = cache.values();\r\n        if (values.size() > maxSize) {\r\n            Thread.sleep(100);\r\n        } else {\r\n            assertFalse(\"fileInfo1 should have been evicted\", values.contains(fileInfo1));\r\n            return;\r\n        }\r\n    }\r\n    fail(\"JobListCache didn't delete the extra entry\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup()\n{\r\n    FileUtil.fullyDelete(testDir);\r\n    testDir.mkdirs();\r\n    conf = new Configuration();\r\n    conf.setBoolean(JHAdminConfig.MR_HS_RECOVERY_ENABLE, true);\r\n    conf.setClass(JHAdminConfig.MR_HS_STATE_STORE, HistoryServerLeveldbStateStoreService.class, HistoryServerStateStoreService.class);\r\n    conf.set(JHAdminConfig.MR_HS_LEVELDB_STATE_STORE_PATH, testDir.getAbsoluteFile().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(testDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createAndStartStore",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HistoryServerStateStoreService createAndStartStore() throws IOException\n{\r\n    HistoryServerStateStoreService store = HistoryServerStateStoreServiceFactory.getStore(conf);\r\n    assertTrue(\"Factory did not create a leveldb store\", store instanceof HistoryServerLeveldbStateStoreService);\r\n    store.init(conf);\r\n    store.start();\r\n    return store;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCheckVersion",
  "errType" : [ "ServiceStateException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testCheckVersion() throws IOException\n{\r\n    HistoryServerLeveldbStateStoreService store = new HistoryServerLeveldbStateStoreService();\r\n    store.init(conf);\r\n    store.start();\r\n    Version defaultVersion = store.getCurrentVersion();\r\n    assertEquals(defaultVersion, store.loadVersion());\r\n    Version compatibleVersion = Version.newInstance(defaultVersion.getMajorVersion(), defaultVersion.getMinorVersion() + 2);\r\n    store.dbStoreVersion(compatibleVersion);\r\n    assertEquals(compatibleVersion, store.loadVersion());\r\n    store.close();\r\n    store = new HistoryServerLeveldbStateStoreService();\r\n    store.init(conf);\r\n    store.start();\r\n    assertEquals(defaultVersion, store.loadVersion());\r\n    Version incompatibleVersion = Version.newInstance(defaultVersion.getMajorVersion() + 1, defaultVersion.getMinorVersion());\r\n    store.dbStoreVersion(incompatibleVersion);\r\n    store.close();\r\n    store = new HistoryServerLeveldbStateStoreService();\r\n    try {\r\n        store.init(conf);\r\n        store.start();\r\n        fail(\"Incompatible version, should have thrown before here.\");\r\n    } catch (ServiceStateException e) {\r\n        assertTrue(\"Exception message mismatch\", e.getMessage().contains(\"Incompatible version for state:\"));\r\n    }\r\n    store.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTokenStore",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void testTokenStore() throws IOException\n{\r\n    HistoryServerStateStoreService store = createAndStartStore();\r\n    HistoryServerState state = store.loadState();\r\n    assertTrue(\"token state not empty\", state.tokenState.isEmpty());\r\n    assertTrue(\"key state not empty\", state.tokenMasterKeyState.isEmpty());\r\n    final DelegationKey key1 = new DelegationKey(1, 2, \"keyData1\".getBytes());\r\n    final MRDelegationTokenIdentifier token1 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner1\"), new Text(\"tokenRenewer1\"), new Text(\"tokenUser1\"));\r\n    token1.setSequenceNumber(1);\r\n    final Long tokenDate1 = 1L;\r\n    final MRDelegationTokenIdentifier token2 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner2\"), new Text(\"tokenRenewer2\"), new Text(\"tokenUser2\"));\r\n    token2.setSequenceNumber(12345678);\r\n    final Long tokenDate2 = 87654321L;\r\n    store.storeTokenMasterKey(key1);\r\n    store.storeToken(token1, tokenDate1);\r\n    store.storeToken(token2, tokenDate2);\r\n    store.close();\r\n    store = createAndStartStore();\r\n    state = store.loadState();\r\n    assertEquals(\"incorrect loaded token count\", 2, state.tokenState.size());\r\n    assertTrue(\"missing token 1\", state.tokenState.containsKey(token1));\r\n    assertEquals(\"incorrect token 1 date\", tokenDate1, state.tokenState.get(token1));\r\n    assertTrue(\"missing token 2\", state.tokenState.containsKey(token2));\r\n    assertEquals(\"incorrect token 2 date\", tokenDate2, state.tokenState.get(token2));\r\n    assertEquals(\"incorrect master key count\", 1, state.tokenMasterKeyState.size());\r\n    assertTrue(\"missing master key 1\", state.tokenMasterKeyState.contains(key1));\r\n    final DelegationKey key2 = new DelegationKey(3, 4, \"keyData2\".getBytes());\r\n    final DelegationKey key3 = new DelegationKey(5, 6, \"keyData3\".getBytes());\r\n    final MRDelegationTokenIdentifier token3 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner3\"), new Text(\"tokenRenewer3\"), new Text(\"tokenUser3\"));\r\n    token3.setSequenceNumber(12345679);\r\n    final Long tokenDate3 = 87654321L;\r\n    store.removeToken(token1);\r\n    store.storeTokenMasterKey(key2);\r\n    final Long newTokenDate2 = 975318642L;\r\n    store.updateToken(token2, newTokenDate2);\r\n    store.removeTokenMasterKey(key1);\r\n    store.storeTokenMasterKey(key3);\r\n    store.storeToken(token3, tokenDate3);\r\n    store.close();\r\n    store = createAndStartStore();\r\n    state = store.loadState();\r\n    assertEquals(\"incorrect loaded token count\", 2, state.tokenState.size());\r\n    assertFalse(\"token 1 not removed\", state.tokenState.containsKey(token1));\r\n    assertTrue(\"missing token 2\", state.tokenState.containsKey(token2));\r\n    assertEquals(\"incorrect token 2 date\", newTokenDate2, state.tokenState.get(token2));\r\n    assertTrue(\"missing token 3\", state.tokenState.containsKey(token3));\r\n    assertEquals(\"incorrect token 3 date\", tokenDate3, state.tokenState.get(token3));\r\n    assertEquals(\"incorrect master key count\", 2, state.tokenMasterKeyState.size());\r\n    assertFalse(\"master key 1 not removed\", state.tokenMasterKeyState.contains(key1));\r\n    assertTrue(\"missing master key 2\", state.tokenMasterKeyState.contains(key2));\r\n    assertTrue(\"missing master key 3\", state.tokenMasterKeyState.contains(key3));\r\n    store.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "testAverageMergeTime",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAverageMergeTime() throws IOException\n{\r\n    String historyFileName = \"job_1329348432655_0001-1329348443227-user-Sleep+job-1329348468601-10-1-SUCCEEDED-default.jhist\";\r\n    String confFileName = \"job_1329348432655_0001_conf.xml\";\r\n    Configuration conf = new Configuration();\r\n    JobACLsManager jobAclsMgr = new JobACLsManager(conf);\r\n    Path fulleHistoryPath = new Path(TestJobHistoryEntities.class.getClassLoader().getResource(historyFileName).getFile());\r\n    Path fullConfPath = new Path(TestJobHistoryEntities.class.getClassLoader().getResource(confFileName).getFile());\r\n    HistoryFileInfo info = mock(HistoryFileInfo.class);\r\n    when(info.getConfFile()).thenReturn(fullConfPath);\r\n    when(info.getHistoryFile()).thenReturn(fulleHistoryPath);\r\n    JobId jobId = MRBuilderUtils.newJobId(1329348432655l, 1, 1);\r\n    CompletedJob completedJob = new CompletedJob(conf, jobId, fulleHistoryPath, true, \"user\", info, jobAclsMgr);\r\n    JobInfo jobInfo = new JobInfo(completedJob);\r\n    Assert.assertEquals(50L, jobInfo.getAvgMergeTime().longValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "testAverageReduceTime",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testAverageReduceTime()\n{\r\n    Job job = mock(CompletedJob.class);\r\n    final Task task1 = mock(Task.class);\r\n    final Task task2 = mock(Task.class);\r\n    JobId jobId = MRBuilderUtils.newJobId(1L, 1, 1);\r\n    final TaskId taskId1 = MRBuilderUtils.newTaskId(jobId, 1, TaskType.REDUCE);\r\n    final TaskId taskId2 = MRBuilderUtils.newTaskId(jobId, 2, TaskType.REDUCE);\r\n    final TaskAttemptId taskAttemptId1 = MRBuilderUtils.newTaskAttemptId(taskId1, 1);\r\n    final TaskAttemptId taskAttemptId2 = MRBuilderUtils.newTaskAttemptId(taskId2, 2);\r\n    final TaskAttempt taskAttempt1 = mock(TaskAttempt.class);\r\n    final TaskAttempt taskAttempt2 = mock(TaskAttempt.class);\r\n    JobReport jobReport = mock(JobReport.class);\r\n    when(taskAttempt1.getState()).thenReturn(TaskAttemptState.SUCCEEDED);\r\n    when(taskAttempt1.getLaunchTime()).thenReturn(0L);\r\n    when(taskAttempt1.getShuffleFinishTime()).thenReturn(4L);\r\n    when(taskAttempt1.getSortFinishTime()).thenReturn(6L);\r\n    when(taskAttempt1.getFinishTime()).thenReturn(8L);\r\n    when(taskAttempt2.getState()).thenReturn(TaskAttemptState.SUCCEEDED);\r\n    when(taskAttempt2.getLaunchTime()).thenReturn(5L);\r\n    when(taskAttempt2.getShuffleFinishTime()).thenReturn(10L);\r\n    when(taskAttempt2.getSortFinishTime()).thenReturn(22L);\r\n    when(taskAttempt2.getFinishTime()).thenReturn(42L);\r\n    when(task1.getType()).thenReturn(TaskType.REDUCE);\r\n    when(task2.getType()).thenReturn(TaskType.REDUCE);\r\n    when(task1.getAttempts()).thenReturn(new HashMap<TaskAttemptId, TaskAttempt>() {\r\n\r\n        {\r\n            put(taskAttemptId1, taskAttempt1);\r\n        }\r\n    });\r\n    when(task2.getAttempts()).thenReturn(new HashMap<TaskAttemptId, TaskAttempt>() {\r\n\r\n        {\r\n            put(taskAttemptId2, taskAttempt2);\r\n        }\r\n    });\r\n    when(job.getTasks()).thenReturn(new HashMap<TaskId, Task>() {\r\n\r\n        {\r\n            put(taskId1, task1);\r\n            put(taskId2, task2);\r\n        }\r\n    });\r\n    when(job.getID()).thenReturn(jobId);\r\n    when(job.getReport()).thenReturn(jobReport);\r\n    when(job.getName()).thenReturn(\"TestJobInfo\");\r\n    when(job.getState()).thenReturn(JobState.SUCCEEDED);\r\n    JobInfo jobInfo = new JobInfo(job);\r\n    Assert.assertEquals(11L, jobInfo.getAvgReduceTime().longValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "testGetStartTimeStr",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetStartTimeStr()\n{\r\n    JobReport jobReport = mock(JobReport.class);\r\n    when(jobReport.getStartTime()).thenReturn(-1L);\r\n    Job job = mock(Job.class);\r\n    when(job.getReport()).thenReturn(jobReport);\r\n    when(job.getName()).thenReturn(\"TestJobInfo\");\r\n    when(job.getState()).thenReturn(JobState.SUCCEEDED);\r\n    JobId jobId = MRBuilderUtils.newJobId(1L, 1, 1);\r\n    when(job.getID()).thenReturn(jobId);\r\n    JobInfo jobInfo = new JobInfo(job);\r\n    Assert.assertEquals(JobInfo.NA, jobInfo.getStartTimeStr());\r\n    Date date = new Date();\r\n    when(jobReport.getStartTime()).thenReturn(date.getTime());\r\n    jobInfo = new JobInfo(job);\r\n    Assert.assertEquals(date.toString(), jobInfo.getStartTimeStr());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "testGetFormattedStartTimeStr",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetFormattedStartTimeStr()\n{\r\n    JobReport jobReport = mock(JobReport.class);\r\n    when(jobReport.getStartTime()).thenReturn(-1L);\r\n    Job job = mock(Job.class);\r\n    when(job.getReport()).thenReturn(jobReport);\r\n    when(job.getName()).thenReturn(\"TestJobInfo\");\r\n    when(job.getState()).thenReturn(JobState.SUCCEEDED);\r\n    JobId jobId = MRBuilderUtils.newJobId(1L, 1, 1);\r\n    when(job.getID()).thenReturn(jobId);\r\n    DateFormat dateFormat = new SimpleDateFormat();\r\n    JobInfo jobInfo = new JobInfo(job);\r\n    Assert.assertEquals(JobInfo.NA, jobInfo.getFormattedStartTimeStr(dateFormat));\r\n    Date date = new Date();\r\n    when(jobReport.getStartTime()).thenReturn(date.getTime());\r\n    jobInfo = new JobInfo(job);\r\n    Assert.assertEquals(dateFormat.format(date), jobInfo.getFormattedStartTimeStr(dateFormat));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testStartStopServer",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testStartStopServer() throws Exception\n{\r\n    historyServer = new JobHistoryServer();\r\n    Configuration config = new Configuration();\r\n    historyServer.init(config);\r\n    assertEquals(STATE.INITED, historyServer.getServiceState());\r\n    HistoryClientService historyService = historyServer.getClientService();\r\n    assertNotNull(historyServer.getClientService());\r\n    assertEquals(STATE.INITED, historyService.getServiceState());\r\n    historyServer.start();\r\n    assertEquals(STATE.STARTED, historyServer.getServiceState());\r\n    assertEquals(STATE.STARTED, historyService.getServiceState());\r\n    historyServer.stop();\r\n    assertEquals(STATE.STOPPED, historyServer.getServiceState());\r\n    assertNotNull(historyService.getClientHandler().getConnectAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testReports",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 49,
  "sourceCodeText" : "void testReports() throws Exception\n{\r\n    Configuration config = new Configuration();\r\n    config.setClass(CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n    RackResolver.init(config);\r\n    MRApp app = new MRAppWithHistory(1, 1, true, this.getClass().getName(), true);\r\n    app.submit(config);\r\n    Job job = app.getContext().getAllJobs().values().iterator().next();\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    historyServer = new JobHistoryServer();\r\n    historyServer.init(config);\r\n    historyServer.start();\r\n    JobHistory jobHistory = null;\r\n    for (Service service : historyServer.getServices()) {\r\n        if (service instanceof JobHistory) {\r\n            jobHistory = (JobHistory) service;\r\n        }\r\n    }\r\n    ;\r\n    Map<JobId, Job> jobs = jobHistory.getAllJobs();\r\n    assertEquals(1, jobs.size());\r\n    assertEquals(\"job_0_0000\", jobs.keySet().iterator().next().toString());\r\n    Task task = job.getTasks().values().iterator().next();\r\n    TaskAttempt attempt = task.getAttempts().values().iterator().next();\r\n    HistoryClientService historyService = historyServer.getClientService();\r\n    MRClientProtocol protocol = historyService.getClientHandler();\r\n    GetTaskAttemptReportRequest gtarRequest = recordFactory.newRecordInstance(GetTaskAttemptReportRequest.class);\r\n    TaskAttemptId taId = attempt.getID();\r\n    taId.setTaskId(task.getID());\r\n    taId.getTaskId().setJobId(job.getID());\r\n    gtarRequest.setTaskAttemptId(taId);\r\n    GetTaskAttemptReportResponse response = protocol.getTaskAttemptReport(gtarRequest);\r\n    assertEquals(\"container_0_0000_01_000000\", response.getTaskAttemptReport().getContainerId().toString());\r\n    assertTrue(response.getTaskAttemptReport().getDiagnosticInfo().isEmpty());\r\n    assertNotNull(response.getTaskAttemptReport().getCounters().getCounter(TaskCounter.PHYSICAL_MEMORY_BYTES));\r\n    assertEquals(taId.toString(), response.getTaskAttemptReport().getTaskAttemptId().toString());\r\n    GetTaskReportRequest request = recordFactory.newRecordInstance(GetTaskReportRequest.class);\r\n    TaskId taskId = task.getID();\r\n    taskId.setJobId(job.getID());\r\n    request.setTaskId(taskId);\r\n    GetTaskReportResponse reportResponse = protocol.getTaskReport(request);\r\n    assertEquals(\"\", reportResponse.getTaskReport().getDiagnosticsList().iterator().next());\r\n    assertEquals(1.0f, reportResponse.getTaskReport().getProgress(), 0.01);\r\n    assertEquals(taskId.toString(), reportResponse.getTaskReport().getTaskId().toString());\r\n    assertEquals(TaskState.SUCCEEDED, reportResponse.getTaskReport().getTaskState());\r\n    GetTaskReportsRequest gtreportsRequest = recordFactory.newRecordInstance(GetTaskReportsRequest.class);\r\n    gtreportsRequest.setJobId(TypeConverter.toYarn(JobID.forName(\"job_1415730144495_0001\")));\r\n    gtreportsRequest.setTaskType(TaskType.REDUCE);\r\n    try {\r\n        protocol.getTaskReports(gtreportsRequest);\r\n        fail(\"IOException not thrown for invalid job id\");\r\n    } catch (IOException e) {\r\n    }\r\n    GetTaskAttemptCompletionEventsRequest taskAttemptRequest = recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\r\n    taskAttemptRequest.setJobId(job.getID());\r\n    GetTaskAttemptCompletionEventsResponse taskAttemptCompletionEventsResponse = protocol.getTaskAttemptCompletionEvents(taskAttemptRequest);\r\n    assertEquals(0, taskAttemptCompletionEventsResponse.getCompletionEventCount());\r\n    GetDiagnosticsRequest diagnosticRequest = recordFactory.newRecordInstance(GetDiagnosticsRequest.class);\r\n    diagnosticRequest.setTaskAttemptId(taId);\r\n    GetDiagnosticsResponse diagnosticResponse = protocol.getDiagnostics(diagnosticRequest);\r\n    assertEquals(1, diagnosticResponse.getDiagnosticsCount());\r\n    assertEquals(\"\", diagnosticResponse.getDiagnostics(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testLaunch",
  "errType" : [ "ExitUtil.ExitException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLaunch() throws Exception\n{\r\n    ExitUtil.disableSystemExit();\r\n    try {\r\n        historyServer = JobHistoryServer.launchJobHistoryServer(new String[0]);\r\n    } catch (ExitUtil.ExitException e) {\r\n        assertEquals(0, e.status);\r\n        ExitUtil.resetFirstExitException();\r\n        fail();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop()\n{\r\n    if (historyServer != null) {\r\n        historyServer.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStateNone",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testJobsQueryStateNone() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ArrayList<JobState> JOB_STATES = new ArrayList<JobState>(Arrays.asList(JobState.values()));\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (Map.Entry<JobId, Job> entry : jobsMap.entrySet()) {\r\n        JOB_STATES.remove(entry.getValue().getState());\r\n    }\r\n    assertTrue(\"No unused job states\", JOB_STATES.size() > 0);\r\n    JobState notInUse = JOB_STATES.get(0);\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"state\", notInUse.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    assertEquals(\"jobs is not empty\", new JSONObject().toString(), json.get(\"jobs\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryState",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobsQueryState() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    String queryState = \"BOGUS\";\r\n    JobId jid = null;\r\n    for (Map.Entry<JobId, Job> entry : jobsMap.entrySet()) {\r\n        jid = entry.getValue().getID();\r\n        queryState = entry.getValue().getState().toString();\r\n        break;\r\n    }\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"state\", queryState).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getPartialJob(jid);\r\n    VerifyJobsUtils.verifyHsJobPartial(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStateInvalid",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryStateInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"state\", \"InvalidState\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringContains(\"exception message\", \"org.apache.hadoop.mapreduce.v2.api.records.JobState.InvalidState\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"IllegalArgumentException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"java.lang.IllegalArgumentException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryUserNone",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testJobsQueryUserNone() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"user\", \"bogus\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    assertEquals(\"jobs is not empty\", new JSONObject().toString(), json.get(\"jobs\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryUser",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testJobsQueryUser() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"user\", \"mock\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    System.out.println(json.toString());\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 3, arr.length());\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getPartialJob(MRApps.toJobID(info.getString(\"id\")));\r\n    VerifyJobsUtils.verifyHsJobPartial(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryLimit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testJobsQueryLimit() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"limit\", \"2\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 2, arr.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryLimitInvalid",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryLimitInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"limit\", \"-1\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: limit value must be greater then 0\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryQueue",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testJobsQueryQueue() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"queue\", \"mockqueue\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 3, arr.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryQueueNonExist",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testJobsQueryQueueNonExist() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"queue\", \"bogus\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    assertEquals(\"jobs is not empty\", new JSONObject().toString(), json.get(\"jobs\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeEnd",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobsQueryStartTimeEnd() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Long now = System.currentTimeMillis();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeEnd\", String.valueOf(now)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 3, arr.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeBegin",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testJobsQueryStartTimeBegin() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Long now = System.currentTimeMillis();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeBegin\", String.valueOf(now)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    assertEquals(\"jobs is not empty\", new JSONObject().toString(), json.get(\"jobs\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeBeginEnd",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobsQueryStartTimeBeginEnd() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    int size = jobsMap.size();\r\n    ArrayList<Long> startTime = new ArrayList<Long>(size);\r\n    for (Map.Entry<JobId, Job> entry : jobsMap.entrySet()) {\r\n        startTime.add(entry.getValue().getReport().getStartTime());\r\n    }\r\n    Collections.sort(startTime);\r\n    assertTrue(\"Error we must have atleast 3 jobs\", size >= 3);\r\n    long midStartTime = startTime.get(size - 2);\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeBegin\", String.valueOf(40000)).queryParam(\"startedTimeEnd\", String.valueOf(midStartTime)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", size - 1, arr.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeBeginEndInvalid",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testJobsQueryStartTimeBeginEndInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Long now = System.currentTimeMillis();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeBegin\", String.valueOf(now)).queryParam(\"startedTimeEnd\", String.valueOf(40000)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: startedTimeEnd must be greater than startTimeBegin\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeInvalidformat",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryStartTimeInvalidformat() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeBegin\", \"efsd\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: Invalid number format: For input string: \\\"efsd\\\"\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeEndInvalidformat",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryStartTimeEndInvalidformat() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeEnd\", \"efsd\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: Invalid number format: For input string: \\\"efsd\\\"\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeNegative",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryStartTimeNegative() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeBegin\", String.valueOf(-1000)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: startedTimeBegin must be greater than 0\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryStartTimeEndNegative",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryStartTimeEndNegative() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"startedTimeEnd\", String.valueOf(-1000)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: startedTimeEnd must be greater than 0\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeEndNegative",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryFinishTimeEndNegative() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeEnd\", String.valueOf(-1000)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: finishedTimeEnd must be greater than 0\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeBeginNegative",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryFinishTimeBeginNegative() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeBegin\", String.valueOf(-1000)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: finishedTimeBegin must be greater than 0\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeBeginEndInvalid",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testJobsQueryFinishTimeBeginEndInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Long now = System.currentTimeMillis();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeBegin\", String.valueOf(now)).queryParam(\"finishedTimeEnd\", String.valueOf(40000)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: finishedTimeEnd must be greater than finishedTimeBegin\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeInvalidformat",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryFinishTimeInvalidformat() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeBegin\", \"efsd\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: Invalid number format: For input string: \\\"efsd\\\"\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeEndInvalidformat",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsQueryFinishTimeEndInvalidformat() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeEnd\", \"efsd\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject msg = response.getEntity(JSONObject.class);\r\n    JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n    assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n    String message = exception.getString(\"message\");\r\n    String type = exception.getString(\"exception\");\r\n    String classname = exception.getString(\"javaClassName\");\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: Invalid number format: For input string: \\\"efsd\\\"\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeBegin",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobsQueryFinishTimeBegin() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Long now = System.currentTimeMillis();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeBegin\", String.valueOf(now)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 3, arr.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeEnd",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testJobsQueryFinishTimeEnd() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Long now = System.currentTimeMillis();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeEnd\", String.valueOf(now)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    assertEquals(\"jobs is not empty\", new JSONObject().toString(), json.get(\"jobs\").toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsQueryFinishTimeBeginEnd",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobsQueryFinishTimeBeginEnd() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    int size = jobsMap.size();\r\n    ArrayList<Long> finishTime = new ArrayList<Long>(size);\r\n    for (Map.Entry<JobId, Job> entry : jobsMap.entrySet()) {\r\n        finishTime.add(entry.getValue().getReport().getFinishTime());\r\n    }\r\n    Collections.sort(finishTime);\r\n    assertTrue(\"Error we must have atleast 3 jobs\", size >= 3);\r\n    long midFinishTime = finishTime.get(size - 2);\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").queryParam(\"finishedTimeBegin\", String.valueOf(40000)).queryParam(\"finishedTimeEnd\", String.valueOf(midFinishTime)).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", size - 1, arr.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHS",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testHS() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyHSInfo(json.getJSONObject(\"historyInfo\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHSSlash",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testHSSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyHSInfo(json.getJSONObject(\"historyInfo\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHSDefault",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testHSDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history/\").get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyHSInfo(json.getJSONObject(\"historyInfo\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHSXML",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testHSXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_XML + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    String xml = response.getEntity(String.class);\r\n    verifyHSInfoXML(xml, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testInfo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInfo() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"info\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyHSInfo(json.getJSONObject(\"historyInfo\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testInfoSlash",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInfoSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"info/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyHSInfo(json.getJSONObject(\"historyInfo\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testInfoDefault",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInfoDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"info/\").get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyHSInfo(json.getJSONObject(\"historyInfo\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testInfoXML",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testInfoXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"info/\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_XML + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    String xml = response.getEntity(String.class);\r\n    verifyHSInfoXML(xml, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testInvalidUri",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidUri() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    String responseStr = \"\";\r\n    try {\r\n        responseStr = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"bogus\").accept(MediaType.APPLICATION_JSON).get(String.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        WebServicesTestUtils.checkStringMatch(\"error string exists and shouldn't\", \"\", responseStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testInvalidUri2",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidUri2() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    String responseStr = \"\";\r\n    try {\r\n        responseStr = r.path(\"ws\").path(\"v1\").path(\"invalid\").accept(MediaType.APPLICATION_JSON).get(String.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        WebServicesTestUtils.checkStringMatch(\"error string exists and shouldn't\", \"\", responseStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testInvalidAccept",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidAccept() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    String responseStr = \"\";\r\n    try {\r\n        responseStr = r.path(\"ws\").path(\"v1\").path(\"history\").accept(MediaType.TEXT_PLAIN).get(String.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.INTERNAL_SERVER_ERROR, response.getStatusInfo());\r\n        WebServicesTestUtils.checkStringMatch(\"error string exists and shouldn't\", \"\", responseStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsInfoGeneric",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyHsInfoGeneric(String hadoopVersionBuiltOn, String hadoopBuildVersion, String hadoopVersion, long startedon)\n{\r\n    WebServicesTestUtils.checkStringMatch(\"hadoopVersionBuiltOn\", VersionInfo.getDate(), hadoopVersionBuiltOn);\r\n    WebServicesTestUtils.checkStringEqual(\"hadoopBuildVersion\", VersionInfo.getBuildVersion(), hadoopBuildVersion);\r\n    WebServicesTestUtils.checkStringMatch(\"hadoopVersion\", VersionInfo.getVersion(), hadoopVersion);\r\n    assertEquals(\"startedOn doesn't match: \", JobHistoryServer.historyServerTimeStamp, startedon);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHSInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyHSInfo(JSONObject info, AppContext ctx) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 4, info.length());\r\n    verifyHsInfoGeneric(info.getString(\"hadoopVersionBuiltOn\"), info.getString(\"hadoopBuildVersion\"), info.getString(\"hadoopVersion\"), info.getLong(\"startedOn\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHSInfoXML",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyHSInfoXML(String xml, AppContext ctx) throws JSONException, Exception\n{\r\n    DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n    DocumentBuilder db = dbf.newDocumentBuilder();\r\n    InputSource is = new InputSource();\r\n    is.setCharacterStream(new StringReader(xml));\r\n    Document dom = db.parse(is);\r\n    NodeList nodes = dom.getElementsByTagName(\"historyInfo\");\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        verifyHsInfoGeneric(WebServicesTestUtils.getXmlString(element, \"hadoopVersionBuiltOn\"), WebServicesTestUtils.getXmlString(element, \"hadoopBuildVersion\"), WebServicesTestUtils.getXmlString(element, \"hadoopVersion\"), WebServicesTestUtils.getXmlLong(element, \"startedOn\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setUpClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUpClass() throws Exception\n{\r\n    coreSitePath = \".\" + File.separator + \"target\" + File.separator + \"test-classes\" + File.separator + \"core-site.xml\";\r\n    Configuration conf = new HdfsConfiguration();\r\n    Configuration conf2 = new HdfsConfiguration();\r\n    dfsCluster = new MiniDFSCluster.Builder(conf).build();\r\n    conf2.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, conf.get(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, MiniDFSCluster.getBaseDirectory()) + \"_2\");\r\n    dfsCluster2 = new MiniDFSCluster.Builder(conf2).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "cleanUpClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanUpClass() throws Exception\n{\r\n    dfsCluster.shutdown();\r\n    dfsCluster2.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "cleanTest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanTest() throws Exception\n{\r\n    new File(coreSitePath).delete();\r\n    dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n    dfsCluster2.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getDoneDirNameForTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDoneDirNameForTest()\n{\r\n    return \"/\" + name.getMethodName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getIntermediateDoneDirNameForTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getIntermediateDoneDirNameForTest()\n{\r\n    return \"/intermediate_\" + name.getMethodName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTryCreateHistoryDirs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testTryCreateHistoryDirs(Configuration conf, boolean expected) throws Exception\n{\r\n    conf.set(JHAdminConfig.MR_HISTORY_DONE_DIR, getDoneDirNameForTest());\r\n    conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, getIntermediateDoneDirNameForTest());\r\n    HistoryFileManager hfm = new HistoryFileManager();\r\n    hfm.conf = conf;\r\n    Assert.assertEquals(expected, hfm.tryCreatingHistoryDirs(false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCreateDirsWithoutFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCreateDirsWithoutFileSystem() throws Exception\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:1\");\r\n    testTryCreateHistoryDirs(conf, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCreateDirsWithFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateDirsWithFileSystem() throws Exception\n{\r\n    dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n    Assert.assertFalse(dfsCluster.getFileSystem().isInSafeMode());\r\n    testTryCreateHistoryDirs(dfsCluster.getConfiguration(0), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCreateDirsWithAdditionalFileSystem",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCreateDirsWithAdditionalFileSystem() throws Exception\n{\r\n    dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n    dfsCluster2.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n    Assert.assertFalse(dfsCluster.getFileSystem().isInSafeMode());\r\n    Assert.assertFalse(dfsCluster2.getFileSystem().isInSafeMode());\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, dfsCluster.getURI().toString());\r\n    FileOutputStream os = new FileOutputStream(coreSitePath);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    testTryCreateHistoryDirs(dfsCluster2.getConfiguration(0), true);\r\n    Assert.assertTrue(dfsCluster.getFileSystem().exists(new Path(getDoneDirNameForTest())));\r\n    Assert.assertTrue(dfsCluster.getFileSystem().exists(new Path(getIntermediateDoneDirNameForTest())));\r\n    Assert.assertFalse(dfsCluster2.getFileSystem().exists(new Path(getDoneDirNameForTest())));\r\n    Assert.assertFalse(dfsCluster2.getFileSystem().exists(new Path(getIntermediateDoneDirNameForTest())));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCreateDirsWithFileSystemInSafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateDirsWithFileSystemInSafeMode() throws Exception\n{\r\n    dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);\r\n    Assert.assertTrue(dfsCluster.getFileSystem().isInSafeMode());\r\n    testTryCreateHistoryDirs(dfsCluster.getConfiguration(0), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCreateHistoryDirs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateHistoryDirs(Configuration conf, Clock clock) throws Exception\n{\r\n    conf.set(JHAdminConfig.MR_HISTORY_DONE_DIR, \"/\" + UUID.randomUUID());\r\n    conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, \"/\" + UUID.randomUUID());\r\n    HistoryFileManager hfm = new HistoryFileManager();\r\n    hfm.conf = conf;\r\n    hfm.createHistoryDirs(clock, 500, 2000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCreateDirsWithFileSystemBecomingAvailBeforeTimeout",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCreateDirsWithFileSystemBecomingAvailBeforeTimeout() throws Exception\n{\r\n    dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);\r\n    Assert.assertTrue(dfsCluster.getFileSystem().isInSafeMode());\r\n    new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                Thread.sleep(500);\r\n                dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);\r\n                Assert.assertTrue(dfsCluster.getFileSystem().isInSafeMode());\r\n            } catch (Exception ex) {\r\n                Assert.fail(ex.toString());\r\n            }\r\n        }\r\n    }.start();\r\n    testCreateHistoryDirs(dfsCluster.getConfiguration(0), SystemClock.getInstance());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCreateDirsWithFileSystemNotBecomingAvailBeforeTimeout",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCreateDirsWithFileSystemNotBecomingAvailBeforeTimeout() throws Exception\n{\r\n    dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);\r\n    Assert.assertTrue(dfsCluster.getFileSystem().isInSafeMode());\r\n    final ControlledClock clock = new ControlledClock();\r\n    clock.setTime(1);\r\n    new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                Thread.sleep(500);\r\n                clock.setTime(3000);\r\n            } catch (Exception ex) {\r\n                Assert.fail(ex.toString());\r\n            }\r\n        }\r\n    }.start();\r\n    testCreateHistoryDirs(dfsCluster.getConfiguration(0), clock);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testScanDirectory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testScanDirectory() throws Exception\n{\r\n    Path p = new Path(\"any\");\r\n    FileContext fc = mock(FileContext.class);\r\n    when(fc.makeQualified(p)).thenReturn(p);\r\n    when(fc.listStatus(p)).thenThrow(new FileNotFoundException());\r\n    List<FileStatus> lfs = HistoryFileManager.scanDirectory(p, fc, null);\r\n    Assert.assertNotNull(lfs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryFileInfoSummaryFileNotExist",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testHistoryFileInfoSummaryFileNotExist() throws Exception\n{\r\n    HistoryFileManagerTest hmTest = new HistoryFileManagerTest();\r\n    String job = \"job_1410889000000_123456\";\r\n    Path summaryFile = new Path(job + \".summary\");\r\n    JobIndexInfo jobIndexInfo = new JobIndexInfo();\r\n    jobIndexInfo.setJobId(TypeConverter.toYarn(JobID.forName(job)));\r\n    Configuration conf = dfsCluster.getConfiguration(0);\r\n    conf.set(JHAdminConfig.MR_HISTORY_DONE_DIR, \"/\" + UUID.randomUUID());\r\n    conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, \"/\" + UUID.randomUUID());\r\n    hmTest.serviceInit(conf);\r\n    HistoryFileInfo info = hmTest.getHistoryFileInfo(null, null, summaryFile, jobIndexInfo, false);\r\n    info.moveToDone();\r\n    Assert.assertFalse(info.didMoveFail());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryFileInfoLoadOversizedJobShouldReturnUnParsedJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testHistoryFileInfoLoadOversizedJobShouldReturnUnParsedJob() throws Exception\n{\r\n    HistoryFileManagerTest hmTest = new HistoryFileManagerTest();\r\n    int allowedMaximumTasks = 5;\r\n    Configuration conf = dfsCluster.getConfiguration(0);\r\n    conf.setInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, allowedMaximumTasks);\r\n    hmTest.init(conf);\r\n    String jobId = \"job_1410889000000_123456\";\r\n    JobIndexInfo jobIndexInfo = new JobIndexInfo();\r\n    jobIndexInfo.setJobId(TypeConverter.toYarn(JobID.forName(jobId)));\r\n    jobIndexInfo.setNumMaps(allowedMaximumTasks);\r\n    jobIndexInfo.setNumReduces(allowedMaximumTasks);\r\n    HistoryFileInfo info = hmTest.getHistoryFileInfo(null, null, null, jobIndexInfo, false);\r\n    Job job = info.loadJob();\r\n    Assert.assertTrue(\"Should return an instance of UnparsedJob to indicate\" + \" the job history file is not parsed\", job instanceof UnparsedJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryFileInfoLoadNormalSizedJobShouldReturnCompletedJob",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testHistoryFileInfoLoadNormalSizedJobShouldReturnCompletedJob() throws Exception\n{\r\n    HistoryFileManagerTest hmTest = new HistoryFileManagerTest();\r\n    final int numOfTasks = 100;\r\n    Configuration conf = dfsCluster.getConfiguration(0);\r\n    conf.setInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, numOfTasks + numOfTasks + 1);\r\n    hmTest.init(conf);\r\n    final String jobId = \"job_1416424547277_0002\";\r\n    JobIndexInfo jobIndexInfo = new JobIndexInfo();\r\n    jobIndexInfo.setJobId(TypeConverter.toYarn(JobID.forName(jobId)));\r\n    jobIndexInfo.setNumMaps(numOfTasks);\r\n    jobIndexInfo.setNumReduces(numOfTasks);\r\n    final String historyFile = getClass().getClassLoader().getResource(\"job_2.0.3-alpha-FAILED.jhist\").getFile();\r\n    final Path historyFilePath = FileSystem.getLocal(conf).makeQualified(new Path(historyFile));\r\n    HistoryFileInfo info = hmTest.getHistoryFileInfo(historyFilePath, null, null, jobIndexInfo, false);\r\n    Job job = info.loadJob();\r\n    Assert.assertTrue(\"Should return an instance of CompletedJob as \" + \"a result of parsing the job history file of the job\", job instanceof CompletedJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryFileInfoShouldReturnCompletedJobIfMaxNotConfiged",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testHistoryFileInfoShouldReturnCompletedJobIfMaxNotConfiged() throws Exception\n{\r\n    HistoryFileManagerTest hmTest = new HistoryFileManagerTest();\r\n    Configuration conf = dfsCluster.getConfiguration(0);\r\n    conf.setInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, -1);\r\n    hmTest.init(conf);\r\n    final String jobId = \"job_1416424547277_0002\";\r\n    JobIndexInfo jobIndexInfo = new JobIndexInfo();\r\n    jobIndexInfo.setJobId(TypeConverter.toYarn(JobID.forName(jobId)));\r\n    jobIndexInfo.setNumMaps(100);\r\n    jobIndexInfo.setNumReduces(100);\r\n    final String historyFile = getClass().getClassLoader().getResource(\"job_2.0.3-alpha-FAILED.jhist\").getFile();\r\n    final Path historyFilePath = FileSystem.getLocal(conf).makeQualified(new Path(historyFile));\r\n    HistoryFileInfo info = hmTest.getHistoryFileInfo(historyFilePath, null, null, jobIndexInfo, false);\r\n    Job job = info.loadJob();\r\n    Assert.assertTrue(\"Should return an instance of CompletedJob as \" + \"a result of parsing the job history file of the job\", job instanceof CompletedJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testMoveToDoneAlreadyMovedSucceeds",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testMoveToDoneAlreadyMovedSucceeds() throws Exception\n{\r\n    HistoryFileManagerTest historyFileManager = new HistoryFileManagerTest();\r\n    long jobTimestamp = 1535436603000L;\r\n    String job = \"job_\" + jobTimestamp + \"_123456789\";\r\n    String intermediateDirectory = \"/\" + UUID.randomUUID();\r\n    String doneDirectory = \"/\" + UUID.randomUUID();\r\n    Configuration conf = dfsCluster.getConfiguration(0);\r\n    conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, intermediateDirectory);\r\n    conf.set(JHAdminConfig.MR_HISTORY_DONE_DIR, doneDirectory);\r\n    Path intermediateHistoryFilePath = new Path(intermediateDirectory + \"/\" + job + \".jhist\");\r\n    Path intermediateConfFilePath = new Path(intermediateDirectory + \"/\" + job + \"_conf.xml\");\r\n    Path doneHistoryFilePath = new Path(doneDirectory + \"/\" + JobHistoryUtils.timestampDirectoryComponent(jobTimestamp) + \"/123456/\" + job + \".jhist\");\r\n    Path doneConfFilePath = new Path(doneDirectory + \"/\" + JobHistoryUtils.timestampDirectoryComponent(jobTimestamp) + \"/123456/\" + job + \"_conf.xml\");\r\n    dfsCluster.getFileSystem().createNewFile(doneHistoryFilePath);\r\n    dfsCluster.getFileSystem().createNewFile(doneConfFilePath);\r\n    historyFileManager.serviceInit(conf);\r\n    JobIndexInfo jobIndexInfo = new JobIndexInfo();\r\n    jobIndexInfo.setJobId(TypeConverter.toYarn(JobID.forName(job)));\r\n    jobIndexInfo.setFinishTime(jobTimestamp);\r\n    HistoryFileInfo info = historyFileManager.getHistoryFileInfo(intermediateHistoryFilePath, intermediateConfFilePath, null, jobIndexInfo, false);\r\n    info.moveToDone();\r\n    Assert.assertFalse(info.isMovePending());\r\n    Assert.assertEquals(doneHistoryFilePath.toString(), info.getHistoryFile().toUri().getPath());\r\n    Assert.assertEquals(doneConfFilePath.toString(), info.getConfFile().toUri().getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHsJobBlockForOversizeJobShouldDisplayWarningMessage",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testHsJobBlockForOversizeJobShouldDisplayWarningMessage()\n{\r\n    int maxAllowedTaskNum = 100;\r\n    Configuration config = new Configuration();\r\n    config.setInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, maxAllowedTaskNum);\r\n    JobHistory jobHistory = new JobHistoryStubWithAllOversizeJobs(maxAllowedTaskNum);\r\n    jobHistory.init(config);\r\n    HsJobBlock jobBlock = new HsJobBlock(jobHistory) {\r\n\r\n        @Override\r\n        public Map<String, String> moreParams() {\r\n            Map<String, String> map = new HashMap<>();\r\n            map.put(AMParams.JOB_ID, \"job_0000_0001\");\r\n            return map;\r\n        }\r\n    };\r\n    OutputStream outputStream = new ByteArrayOutputStream();\r\n    HtmlBlock.Block block = createBlockToCreateTo(outputStream);\r\n    jobBlock.render(block);\r\n    block.getWriter().flush();\r\n    String out = outputStream.toString();\r\n    Assert.assertTrue(\"Should display warning message for jobs that have too \" + \"many tasks\", out.contains(\"Any job larger than \" + maxAllowedTaskNum + \" will not be loaded\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHsJobBlockForNormalSizeJobShouldNotDisplayWarningMessage",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testHsJobBlockForNormalSizeJobShouldNotDisplayWarningMessage()\n{\r\n    Configuration config = new Configuration();\r\n    config.setInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, -1);\r\n    JobHistory jobHistory = new JobHitoryStubWithAllNormalSizeJobs();\r\n    jobHistory.init(config);\r\n    HsJobBlock jobBlock = new HsJobBlock(jobHistory) {\r\n\r\n        @Override\r\n        public Map<String, String> moreParams() {\r\n            Map<String, String> map = new HashMap<>();\r\n            map.put(AMParams.JOB_ID, \"job_0000_0001\");\r\n            return map;\r\n        }\r\n\r\n        @Override\r\n        public ResponseInfo info(String about) {\r\n            return new ResponseInfo().about(about);\r\n        }\r\n\r\n        @Override\r\n        public String url(String... parts) {\r\n            return StringHelper.ujoin(\"\", parts);\r\n        }\r\n    };\r\n    OutputStream outputStream = new ByteArrayOutputStream();\r\n    HtmlBlock.Block block = createBlockToCreateTo(outputStream);\r\n    jobBlock.render(block);\r\n    block.getWriter().flush();\r\n    String out = outputStream.toString();\r\n    Assert.assertTrue(\"Should display job overview for the job.\", out.contains(\"ApplicationMaster\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "createBlockToCreateTo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HtmlBlock.Block createBlockToCreateTo(OutputStream outputStream)\n{\r\n    PrintWriter printWriter = new PrintWriter(outputStream);\r\n    HtmlBlock html = new HtmlBlockForTest();\r\n    return new BlockForTest(html, printWriter, 10, false) {\r\n\r\n        @Override\r\n        protected void subView(Class<? extends SubView> cls) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "checkSize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean checkSize(JobIdHistoryFileInfoMap map, int size) throws InterruptedException\n{\r\n    for (int i = 0; i < 100; i++) {\r\n        if (map.size() != size)\r\n            Thread.sleep(20);\r\n        else\r\n            return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testWithSingleElement",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testWithSingleElement() throws InterruptedException\n{\r\n    JobIdHistoryFileInfoMap mapWithSize = new JobIdHistoryFileInfoMap();\r\n    JobId jobId = MRBuilderUtils.newJobId(1, 1, 1);\r\n    HistoryFileInfo fileInfo1 = Mockito.mock(HistoryFileInfo.class);\r\n    Mockito.when(fileInfo1.getJobId()).thenReturn(jobId);\r\n    assertEquals(\"Incorrect return on putIfAbsent()\", null, mapWithSize.putIfAbsent(jobId, fileInfo1));\r\n    assertEquals(\"Incorrect return on putIfAbsent()\", fileInfo1, mapWithSize.putIfAbsent(jobId, fileInfo1));\r\n    assertEquals(\"Incorrect get()\", fileInfo1, mapWithSize.get(jobId));\r\n    assertTrue(\"Incorrect size()\", checkSize(mapWithSize, 1));\r\n    NavigableSet<JobId> set = mapWithSize.navigableKeySet();\r\n    assertEquals(\"Incorrect navigableKeySet()\", 1, set.size());\r\n    assertTrue(\"Incorrect navigableKeySet()\", set.contains(jobId));\r\n    Collection<HistoryFileInfo> values = mapWithSize.values();\r\n    assertEquals(\"Incorrect values()\", 1, values.size());\r\n    assertTrue(\"Incorrect values()\", values.contains(fileInfo1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttempts() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            verifyHsTaskAttempts(json, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptsSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttemptsSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            verifyHsTaskAttempts(json, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptsDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttemptsDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            verifyHsTaskAttempts(json, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testTaskAttemptsXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            String xml = response.getEntity(String.class);\r\n            DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n            DocumentBuilder db = dbf.newDocumentBuilder();\r\n            InputSource is = new InputSource();\r\n            is.setCharacterStream(new StringReader(xml));\r\n            Document dom = db.parse(is);\r\n            NodeList attempts = dom.getElementsByTagName(\"taskAttempts\");\r\n            assertEquals(\"incorrect number of elements\", 1, attempts.getLength());\r\n            NodeList nodes = dom.getElementsByTagName(\"taskAttempt\");\r\n            verifyHsTaskAttemptsXML(nodes, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptId() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"taskAttempt\");\r\n                verifyHsTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdSlash",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptIdSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid + \"/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"taskAttempt\");\r\n                verifyHsTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdDefault",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptIdDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"taskAttempt\");\r\n                verifyHsTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdXML",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTaskAttemptIdXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                String xml = response.getEntity(String.class);\r\n                DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n                DocumentBuilder db = dbf.newDocumentBuilder();\r\n                InputSource is = new InputSource();\r\n                is.setCharacterStream(new StringReader(xml));\r\n                Document dom = db.parse(is);\r\n                NodeList nodes = dom.getElementsByTagName(\"taskAttempt\");\r\n                for (int i = 0; i < nodes.getLength(); i++) {\r\n                    Element element = (Element) nodes.item(i);\r\n                    verifyHsTaskAttemptXML(element, att, task.getType());\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdBogus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdBogus() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"bogusid\", \"java.lang.Exception: TaskAttemptId string : \" + \"bogusid is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdNonExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdNonExist() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_0_1234_m_000000_0\", \"java.lang.Exception: Error getting info on task attempt id attempt_0_1234_m_000000_0\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdInvalid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdInvalid() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_0_1234_d_000000_0\", \"java.lang.Exception: Bad TaskType identifier. TaskAttemptId string : \" + \"attempt_0_1234_d_000000_0 is not properly formed.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdInvalid2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdInvalid2() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_1234_m_000000_0\", \"java.lang.Exception: TaskAttemptId string : \" + \"attempt_1234_m_000000_0 is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdInvalid3",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdInvalid3() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_0_1234_m_000000\", \"java.lang.Exception: TaskAttemptId string : \" + \"attempt_0_1234_m_000000 is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdErrorGeneric",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTaskAttemptIdErrorGeneric(String attid, String error) throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            try {\r\n                r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).accept(MediaType.APPLICATION_JSON).get(JSONObject.class);\r\n                fail(\"should have thrown exception on invalid uri\");\r\n            } catch (UniformInterfaceException ue) {\r\n                ClientResponse response = ue.getResponse();\r\n                assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject msg = response.getEntity(JSONObject.class);\r\n                JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n                assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n                String message = exception.getString(\"message\");\r\n                String type = exception.getString(\"exception\");\r\n                String classname = exception.getString(\"javaClassName\");\r\n                WebServicesTestUtils.checkStringMatch(\"exception message\", error, message);\r\n                WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n                WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTaskAttemptXML",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyHsTaskAttemptXML(Element element, TaskAttempt att, TaskType ttype)\n{\r\n    verifyTaskAttemptGeneric(att, ttype, WebServicesTestUtils.getXmlString(element, \"id\"), WebServicesTestUtils.getXmlString(element, \"state\"), WebServicesTestUtils.getXmlString(element, \"type\"), WebServicesTestUtils.getXmlString(element, \"rack\"), WebServicesTestUtils.getXmlString(element, \"nodeHttpAddress\"), WebServicesTestUtils.getXmlString(element, \"diagnostics\"), WebServicesTestUtils.getXmlString(element, \"assignedContainerId\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlLong(element, \"finishTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedTime\"), WebServicesTestUtils.getXmlFloat(element, \"progress\"));\r\n    if (ttype == TaskType.REDUCE) {\r\n        verifyReduceTaskAttemptGeneric(att, WebServicesTestUtils.getXmlLong(element, \"shuffleFinishTime\"), WebServicesTestUtils.getXmlLong(element, \"mergeFinishTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedShuffleTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedMergeTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedReduceTime\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyHsTaskAttempt(JSONObject info, TaskAttempt att, TaskType ttype) throws JSONException\n{\r\n    if (ttype == TaskType.REDUCE) {\r\n        assertEquals(\"incorrect number of elements\", 17, info.length());\r\n    } else {\r\n        assertEquals(\"incorrect number of elements\", 12, info.length());\r\n    }\r\n    verifyTaskAttemptGeneric(att, ttype, info.getString(\"id\"), info.getString(\"state\"), info.getString(\"type\"), info.getString(\"rack\"), info.getString(\"nodeHttpAddress\"), info.getString(\"diagnostics\"), info.getString(\"assignedContainerId\"), info.getLong(\"startTime\"), info.getLong(\"finishTime\"), info.getLong(\"elapsedTime\"), (float) info.getDouble(\"progress\"));\r\n    if (ttype == TaskType.REDUCE) {\r\n        verifyReduceTaskAttemptGeneric(att, info.getLong(\"shuffleFinishTime\"), info.getLong(\"mergeFinishTime\"), info.getLong(\"elapsedShuffleTime\"), info.getLong(\"elapsedMergeTime\"), info.getLong(\"elapsedReduceTime\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyHsTaskAttempts(JSONObject json, Task task) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject attempts = json.getJSONObject(\"taskAttempts\");\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONArray arr = attempts.getJSONArray(\"taskAttempt\");\r\n    for (TaskAttempt att : task.getAttempts().values()) {\r\n        TaskAttemptId id = att.getID();\r\n        String attid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        for (int i = 0; i < arr.length(); i++) {\r\n            JSONObject info = arr.getJSONObject(i);\r\n            if (attid.matches(info.getString(\"id\"))) {\r\n                found = true;\r\n                verifyHsTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n        assertTrue(\"task attempt with id: \" + attid + \" not in web service output\", found);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTaskAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyHsTaskAttemptsXML(NodeList nodes, Task task)\n{\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (TaskAttempt att : task.getAttempts().values()) {\r\n        TaskAttemptId id = att.getID();\r\n        String attid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        for (int i = 0; i < nodes.getLength(); i++) {\r\n            Element element = (Element) nodes.item(i);\r\n            if (attid.matches(WebServicesTestUtils.getXmlString(element, \"id\"))) {\r\n                found = true;\r\n                verifyHsTaskAttemptXML(element, att, task.getType());\r\n            }\r\n        }\r\n        assertTrue(\"task with id: \" + attid + \" not in web service output\", found);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyTaskAttemptGeneric",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void verifyTaskAttemptGeneric(TaskAttempt ta, TaskType ttype, String id, String state, String type, String rack, String nodeHttpAddress, String diagnostics, String assignedContainerId, long startTime, long finishTime, long elapsedTime, float progress)\n{\r\n    TaskAttemptId attid = ta.getID();\r\n    String attemptId = MRApps.toString(attid);\r\n    WebServicesTestUtils.checkStringMatch(\"id\", attemptId, id);\r\n    WebServicesTestUtils.checkStringMatch(\"type\", ttype.toString(), type);\r\n    WebServicesTestUtils.checkStringMatch(\"state\", ta.getState().toString(), state);\r\n    WebServicesTestUtils.checkStringMatch(\"rack\", ta.getNodeRackName(), rack);\r\n    WebServicesTestUtils.checkStringMatch(\"nodeHttpAddress\", ta.getNodeHttpAddress(), nodeHttpAddress);\r\n    String expectDiag = \"\";\r\n    List<String> diagnosticsList = ta.getDiagnostics();\r\n    if (diagnosticsList != null && !diagnostics.isEmpty()) {\r\n        StringBuffer b = new StringBuffer();\r\n        for (String diag : diagnosticsList) {\r\n            b.append(diag);\r\n        }\r\n        expectDiag = b.toString();\r\n    }\r\n    WebServicesTestUtils.checkStringMatch(\"diagnostics\", expectDiag, diagnostics);\r\n    WebServicesTestUtils.checkStringMatch(\"assignedContainerId\", ta.getAssignedContainerID().toString(), assignedContainerId);\r\n    assertEquals(\"startTime wrong\", ta.getLaunchTime(), startTime);\r\n    assertEquals(\"finishTime wrong\", ta.getFinishTime(), finishTime);\r\n    assertEquals(\"elapsedTime wrong\", finishTime - startTime, elapsedTime);\r\n    assertEquals(\"progress wrong\", ta.getProgress() * 100, progress, 1e-3f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyReduceTaskAttemptGeneric",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyReduceTaskAttemptGeneric(TaskAttempt ta, long shuffleFinishTime, long mergeFinishTime, long elapsedShuffleTime, long elapsedMergeTime, long elapsedReduceTime)\n{\r\n    assertEquals(\"shuffleFinishTime wrong\", ta.getShuffleFinishTime(), shuffleFinishTime);\r\n    assertEquals(\"mergeFinishTime wrong\", ta.getSortFinishTime(), mergeFinishTime);\r\n    assertEquals(\"elapsedShuffleTime wrong\", ta.getShuffleFinishTime() - ta.getLaunchTime(), elapsedShuffleTime);\r\n    assertEquals(\"elapsedMergeTime wrong\", ta.getSortFinishTime() - ta.getShuffleFinishTime(), elapsedMergeTime);\r\n    assertEquals(\"elapsedReduceTime wrong\", ta.getFinishTime() - ta.getSortFinishTime(), elapsedReduceTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdCounters",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptIdCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"counters\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"jobTaskAttemptCounters\");\r\n                verifyHsJobTaskAttemptCounters(info, att);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskAttemptIdXMLCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskAttemptIdXMLCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"counters\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                String xml = response.getEntity(String.class);\r\n                DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n                DocumentBuilder db = dbf.newDocumentBuilder();\r\n                InputSource is = new InputSource();\r\n                is.setCharacterStream(new StringReader(xml));\r\n                Document dom = db.parse(is);\r\n                NodeList nodes = dom.getElementsByTagName(\"jobTaskAttemptCounters\");\r\n                verifyHsTaskCountersXML(nodes, att);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobTaskAttemptCounters",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyHsJobTaskAttemptCounters(JSONObject info, TaskAttempt att) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(att.getID()), info.getString(\"id\"));\r\n    JSONArray counterGroups = info.getJSONArray(\"taskAttemptCounterGroup\");\r\n    for (int i = 0; i < counterGroups.length(); i++) {\r\n        JSONObject counterGroup = counterGroups.getJSONObject(i);\r\n        String name = counterGroup.getString(\"counterGroupName\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        JSONArray counters = counterGroup.getJSONArray(\"counter\");\r\n        for (int j = 0; j < counters.length(); j++) {\r\n            JSONObject counter = counters.getJSONObject(j);\r\n            String counterName = counter.getString(\"name\");\r\n            assertTrue(\"name not set\", (counterName != null && !counterName.isEmpty()));\r\n            long value = counter.getLong(\"value\");\r\n            assertTrue(\"value  >= 0\", value >= 0);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTaskCountersXML",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void verifyHsTaskCountersXML(NodeList nodes, TaskAttempt att)\n{\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(att.getID()), WebServicesTestUtils.getXmlString(element, \"id\"));\r\n        NodeList groups = element.getElementsByTagName(\"taskAttemptCounterGroup\");\r\n        for (int j = 0; j < groups.getLength(); j++) {\r\n            Element counters = (Element) groups.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", counters);\r\n            String name = WebServicesTestUtils.getXmlString(counters, \"counterGroupName\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            NodeList counterArr = counters.getElementsByTagName(\"counter\");\r\n            for (int z = 0; z < counterArr.getLength(); z++) {\r\n                Element counter = (Element) counterArr.item(z);\r\n                String counterName = WebServicesTestUtils.getXmlString(counter, \"name\");\r\n                assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n                long value = WebServicesTestUtils.getXmlLong(counter, \"value\");\r\n                assertTrue(\"value not >= 0\", value >= 0);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "initStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initStorage(Configuration conf) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "startStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startStorage() throws IOException\n{\r\n    state = new HistoryServerState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "closeStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void closeStorage() throws IOException\n{\r\n    state = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadState",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HistoryServerState loadState() throws IOException\n{\r\n    HistoryServerState result = new HistoryServerState();\r\n    result.tokenState.putAll(state.tokenState);\r\n    result.tokenMasterKeyState.addAll(state.tokenMasterKeyState);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n    if (state.tokenState.containsKey(tokenId)) {\r\n        throw new IOException(\"token \" + tokenId + \" was stored twice\");\r\n    }\r\n    state.tokenState.put(tokenId, renewDate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "updateToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n    if (!state.tokenState.containsKey(tokenId)) {\r\n        throw new IOException(\"token \" + tokenId + \" not in store\");\r\n    }\r\n    state.tokenState.put(tokenId, renewDate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeToken(MRDelegationTokenIdentifier tokenId) throws IOException\n{\r\n    state.tokenState.remove(tokenId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void storeTokenMasterKey(DelegationKey key) throws IOException\n{\r\n    if (state.tokenMasterKeyState.contains(key)) {\r\n        throw new IOException(\"token master key \" + key + \" was stored twice\");\r\n    }\r\n    state.tokenMasterKeyState.add(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeTokenMasterKey(DelegationKey key) throws IOException\n{\r\n    state.tokenMasterKeyState.remove(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    testConfDir.mkdir();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop()\n{\r\n    FileUtil.fullyDelete(testConfDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobConf",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobConf() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"conf\");\r\n        verifyHsJobConf(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobConfSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobConfSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"conf\");\r\n        verifyHsJobConf(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobConfDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobConfDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"conf\");\r\n        verifyHsJobConf(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobConfXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobConfXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList info = dom.getElementsByTagName(\"conf\");\r\n        verifyHsJobConfXML(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobConf",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyHsJobConf(JSONObject info, Job job) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"path\", job.getConfFile().toString(), info.getString(\"path\"));\r\n    JSONArray properties = info.getJSONArray(\"property\");\r\n    for (int i = 0; i < properties.length(); i++) {\r\n        JSONObject prop = properties.getJSONObject(i);\r\n        String name = prop.getString(\"name\");\r\n        String value = prop.getString(\"value\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        assertTrue(\"value not set\", (value != null && !value.isEmpty()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobConfXML",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyHsJobConfXML(NodeList nodes, Job job)\n{\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        WebServicesTestUtils.checkStringMatch(\"path\", job.getConfFile().toString(), WebServicesTestUtils.getXmlString(element, \"path\"));\r\n        NodeList properties = element.getElementsByTagName(\"property\");\r\n        for (int j = 0; j < properties.getLength(); j++) {\r\n            Element property = (Element) properties.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", property);\r\n            String name = WebServicesTestUtils.getXmlString(property, \"name\");\r\n            String value = WebServicesTestUtils.getXmlString(property, \"value\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            assertTrue(\"name not set\", (value != null && !value.isEmpty()));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testRefreshLoadedJobCache",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testRefreshLoadedJobCache() throws Exception\n{\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_JOB_CACHE_SIZE, 2);\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    assertFalse(storage.getUseLoadedTasksCache());\r\n    Job[] jobs = new Job[3];\r\n    JobId[] jobIds = new JobId[3];\r\n    for (int i = 0; i < 3; i++) {\r\n        jobs[i] = mock(Job.class);\r\n        jobIds[i] = mock(JobId.class);\r\n        when(jobs[i].getID()).thenReturn(jobIds[i]);\r\n    }\r\n    HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);\r\n    when(historyManager.getFileInfo(any(JobId.class))).thenReturn(fileInfo);\r\n    when(fileInfo.loadJob()).thenReturn(jobs[0]).thenReturn(jobs[1]).thenReturn(jobs[2]);\r\n    for (int i = 0; i < 3; i++) {\r\n        storage.getFullJob(jobs[i].getID());\r\n    }\r\n    Cache<JobId, Job> jobCache = storage.getLoadedJobCache();\r\n    assertTrue(jobCache.size() > 0);\r\n    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_JOB_CACHE_SIZE, 3);\r\n    doReturn(conf).when(storage).createConf();\r\n    when(fileInfo.loadJob()).thenReturn(jobs[0]).thenReturn(jobs[1]).thenReturn(jobs[2]);\r\n    jobHistory.refreshLoadedJobCache();\r\n    for (int i = 0; i < 3; i++) {\r\n        storage.getFullJob(jobs[i].getID());\r\n    }\r\n    jobCache = storage.getLoadedJobCache();\r\n    assertTrue(jobCache.size() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTasksCacheLimit",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testTasksCacheLimit() throws Exception\n{\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE, 50);\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    assertTrue(storage.getUseLoadedTasksCache());\r\n    assertThat(storage.getLoadedTasksCacheSize()).isEqualTo(50);\r\n    Job[] jobs = new Job[10];\r\n    JobId[] jobIds = new JobId[10];\r\n    for (int i = 0; i < jobs.length; i++) {\r\n        jobs[i] = mock(Job.class);\r\n        jobIds[i] = mock(JobId.class);\r\n        when(jobs[i].getID()).thenReturn(jobIds[i]);\r\n        when(jobs[i].getTotalMaps()).thenReturn(10);\r\n        when(jobs[i].getTotalReduces()).thenReturn(2);\r\n    }\r\n    Job[] lgJobs = new Job[3];\r\n    JobId[] lgJobIds = new JobId[3];\r\n    for (int i = 0; i < lgJobs.length; i++) {\r\n        lgJobs[i] = mock(Job.class);\r\n        lgJobIds[i] = mock(JobId.class);\r\n        when(lgJobs[i].getID()).thenReturn(lgJobIds[i]);\r\n        when(lgJobs[i].getTotalMaps()).thenReturn(2000);\r\n        when(lgJobs[i].getTotalReduces()).thenReturn(10);\r\n    }\r\n    HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);\r\n    when(historyManager.getFileInfo(any(JobId.class))).thenReturn(fileInfo);\r\n    when(fileInfo.loadJob()).thenReturn(jobs[0]).thenReturn(jobs[1]).thenReturn(jobs[2]).thenReturn(jobs[3]).thenReturn(jobs[4]).thenReturn(jobs[5]).thenReturn(jobs[6]).thenReturn(jobs[7]).thenReturn(jobs[8]).thenReturn(jobs[9]).thenReturn(lgJobs[0]).thenReturn(lgJobs[1]).thenReturn(lgJobs[2]);\r\n    Cache<JobId, Job> jobCache = storage.getLoadedJobCache();\r\n    for (int i = 0; i < jobs.length; i++) {\r\n        storage.getFullJob(jobs[i].getID());\r\n    }\r\n    long prevSize = jobCache.size();\r\n    for (int i = 0; i < lgJobs.length; i++) {\r\n        storage.getFullJob(lgJobs[i].getID());\r\n    }\r\n    assertTrue(jobCache.size() < prevSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testJobCacheLimitLargerThanMax",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testJobCacheLimitLargerThanMax() throws Exception\n{\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    JobHistory jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE, 500);\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    assertTrue(storage.getUseLoadedTasksCache());\r\n    assertThat(storage.getLoadedTasksCacheSize()).isEqualTo(500);\r\n    Job[] lgJobs = new Job[10];\r\n    JobId[] lgJobIds = new JobId[10];\r\n    for (int i = 0; i < lgJobs.length; i++) {\r\n        lgJobs[i] = mock(Job.class);\r\n        lgJobIds[i] = mock(JobId.class);\r\n        when(lgJobs[i].getID()).thenReturn(lgJobIds[i]);\r\n        when(lgJobs[i].getTotalMaps()).thenReturn(700);\r\n        when(lgJobs[i].getTotalReduces()).thenReturn(50);\r\n    }\r\n    HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);\r\n    when(historyManager.getFileInfo(any(JobId.class))).thenReturn(fileInfo);\r\n    when(fileInfo.loadJob()).thenReturn(lgJobs[0]).thenReturn(lgJobs[1]).thenReturn(lgJobs[2]).thenReturn(lgJobs[3]).thenReturn(lgJobs[4]).thenReturn(lgJobs[5]).thenReturn(lgJobs[6]).thenReturn(lgJobs[7]).thenReturn(lgJobs[8]).thenReturn(lgJobs[9]);\r\n    Cache<JobId, Job> jobCache = storage.getLoadedJobCache();\r\n    long[] cacheSize = new long[10];\r\n    for (int i = 0; i < lgJobs.length; i++) {\r\n        storage.getFullJob(lgJobs[i].getID());\r\n        assertTrue(jobCache.size() > 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testLoadedTasksEmptyConfiguration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testLoadedTasksEmptyConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE, \"\");\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    JobHistory jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    assertFalse(storage.getUseLoadedTasksCache());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testLoadedTasksZeroConfiguration",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLoadedTasksZeroConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE, 0);\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    JobHistory jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    assertTrue(storage.getUseLoadedTasksCache());\r\n    assertThat(storage.getLoadedTasksCacheSize()).isOne();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testLoadedTasksNegativeConfiguration",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLoadedTasksNegativeConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE, -1);\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    JobHistory jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    assertTrue(storage.getUseLoadedTasksCache());\r\n    assertThat(storage.getLoadedTasksCacheSize()).isOne();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testLoadJobErrorCases",
  "errType" : [ "YarnRuntimeException", "YarnRuntimeException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testLoadJobErrorCases() throws IOException\n{\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE, 50);\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    assertTrue(storage.getUseLoadedTasksCache());\r\n    assertThat(storage.getLoadedTasksCacheSize()).isEqualTo(50);\r\n    Job[] jobs = new Job[4];\r\n    JobId[] jobIds = new JobId[4];\r\n    for (int i = 0; i < jobs.length; i++) {\r\n        jobs[i] = mock(Job.class);\r\n        jobIds[i] = mock(JobId.class);\r\n        when(jobs[i].getID()).thenReturn(jobIds[i]);\r\n        when(jobs[i].getTotalMaps()).thenReturn(10);\r\n        when(jobs[i].getTotalReduces()).thenReturn(2);\r\n    }\r\n    HistoryFileInfo loadJobException = mock(HistoryFileInfo.class);\r\n    when(loadJobException.loadJob()).thenThrow(new IOException(\"History file not found\"));\r\n    when(historyManager.getFileInfo(jobIds[0])).thenThrow(new IOException(\"\"));\r\n    when(historyManager.getFileInfo(jobIds[1])).thenReturn(null);\r\n    when(historyManager.getFileInfo(jobIds[2])).thenReturn(loadJobException);\r\n    try {\r\n        storage.getFullJob(jobIds[0]);\r\n        fail(\"Did not get expected YarnRuntimeException for getFileInfo() throwing IOException\");\r\n    } catch (YarnRuntimeException e) {\r\n    }\r\n    Job job = storage.getFullJob(jobIds[1]);\r\n    assertNull(job);\r\n    try {\r\n        storage.getFullJob(jobIds[2]);\r\n        fail(\"Did not get expected YarnRuntimeException for fileInfo.loadJob() throwing IOException\");\r\n    } catch (YarnRuntimeException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testRefreshJobRetentionSettings",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testRefreshJobRetentionSettings() throws IOException, InterruptedException\n{\r\n    String root = \"mockfs://foo/\";\r\n    String historyDoneDir = root + \"mapred/history/done\";\r\n    long now = System.currentTimeMillis();\r\n    long someTimeYesterday = now - (25l * 3600 * 1000);\r\n    long timeBefore200Secs = now - (200l * 1000);\r\n    String timestampComponent = JobHistoryUtils.timestampDirectoryComponent(someTimeYesterday);\r\n    Path donePathYesterday = new Path(historyDoneDir, timestampComponent + \"/\" + \"000000\");\r\n    FileStatus dirCreatedYesterdayStatus = new FileStatus(0, true, 0, 0, someTimeYesterday, donePathYesterday);\r\n    timestampComponent = JobHistoryUtils.timestampDirectoryComponent(timeBefore200Secs);\r\n    Path donePathToday = new Path(historyDoneDir, timestampComponent + \"/\" + \"000000\");\r\n    FileStatus dirCreatedTodayStatus = new FileStatus(0, true, 0, 0, timeBefore200Secs, donePathToday);\r\n    Path fileUnderYesterdayDir = new Path(donePathYesterday.toString(), \"job_1372363578825_0015-\" + someTimeYesterday + \"-user-Sleep+job-\" + someTimeYesterday + \"-1-1-SUCCEEDED-default.jhist\");\r\n    FileStatus fileUnderYesterdayDirStatus = new FileStatus(10, false, 0, 0, someTimeYesterday, fileUnderYesterdayDir);\r\n    Path fileUnderTodayDir = new Path(donePathYesterday.toString(), \"job_1372363578825_0016-\" + timeBefore200Secs + \"-user-Sleep+job-\" + timeBefore200Secs + \"-1-1-SUCCEEDED-default.jhist\");\r\n    FileStatus fileUnderTodayDirStatus = new FileStatus(10, false, 0, 0, timeBefore200Secs, fileUnderTodayDir);\r\n    HistoryFileManager historyManager = spy(new HistoryFileManager());\r\n    jobHistory = spy(new JobHistory());\r\n    List<FileStatus> fileStatusList = new LinkedList<FileStatus>();\r\n    fileStatusList.add(dirCreatedYesterdayStatus);\r\n    fileStatusList.add(dirCreatedTodayStatus);\r\n    doReturn(4).when(jobHistory).getInitDelaySecs();\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    List<FileStatus> list1 = new LinkedList<FileStatus>();\r\n    list1.add(fileUnderYesterdayDirStatus);\r\n    doReturn(list1).when(historyManager).scanDirectoryForHistoryFiles(eq(donePathYesterday), any(FileContext.class));\r\n    List<FileStatus> list2 = new LinkedList<FileStatus>();\r\n    list2.add(fileUnderTodayDirStatus);\r\n    doReturn(list2).when(historyManager).scanDirectoryForHistoryFiles(eq(donePathToday), any(FileContext.class));\r\n    doReturn(fileStatusList).when(historyManager).getHistoryDirsForCleaning(Mockito.anyLong());\r\n    doReturn(true).when(historyManager).deleteDir(any(FileStatus.class));\r\n    JobListCache jobListCache = mock(JobListCache.class);\r\n    HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);\r\n    doReturn(jobListCache).when(historyManager).createJobListCache();\r\n    when(jobListCache.get(any(JobId.class))).thenReturn(fileInfo);\r\n    doNothing().when(fileInfo).delete();\r\n    Configuration conf = new Configuration();\r\n    conf.setLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS, 24l * 3600 * 1000);\r\n    conf.setLong(JHAdminConfig.MR_HISTORY_CLEANER_INTERVAL_MS, 2 * 1000);\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    assertEquals(2 * 1000l, jobHistory.getCleanerInterval());\r\n    verify(fileInfo, timeout(20000).times(1)).delete();\r\n    fileStatusList.remove(dirCreatedYesterdayStatus);\r\n    conf.setLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS, 10 * 1000);\r\n    conf.setLong(JHAdminConfig.MR_HISTORY_CLEANER_INTERVAL_MS, 1 * 1000);\r\n    doReturn(conf).when(jobHistory).createConf();\r\n    jobHistory.refreshJobRetentionSettings();\r\n    assertEquals(1 * 1000l, jobHistory.getCleanerInterval());\r\n    verify(fileInfo, timeout(20000).times(2)).delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCachedStorageWaitsForFileMove",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCachedStorageWaitsForFileMove() throws IOException\n{\r\n    HistoryFileManager historyManager = mock(HistoryFileManager.class);\r\n    jobHistory = spy(new JobHistory());\r\n    doReturn(historyManager).when(jobHistory).createHistoryFileManager();\r\n    Configuration conf = new Configuration();\r\n    jobHistory.init(conf);\r\n    jobHistory.start();\r\n    CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory.getHistoryStorage());\r\n    Job job = mock(Job.class);\r\n    JobId jobId = mock(JobId.class);\r\n    when(job.getID()).thenReturn(jobId);\r\n    when(job.getTotalMaps()).thenReturn(10);\r\n    when(job.getTotalReduces()).thenReturn(2);\r\n    HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);\r\n    when(historyManager.getFileInfo(eq(jobId))).thenReturn(fileInfo);\r\n    when(fileInfo.loadJob()).thenReturn(job);\r\n    storage.getFullJob(jobId);\r\n    verify(fileInfo).waitUntilMoved();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testRefreshLoadedJobCacheUnSupportedOperation",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRefreshLoadedJobCacheUnSupportedOperation()\n{\r\n    jobHistory = spy(new JobHistory());\r\n    HistoryStorage storage = new HistoryStorage() {\r\n\r\n        @Override\r\n        public void setHistoryFileManager(HistoryFileManager hsManager) {\r\n        }\r\n\r\n        @Override\r\n        public JobsInfo getPartialJobs(Long offset, Long count, String user, String queue, Long sBegin, Long sEnd, Long fBegin, Long fEnd, JobState jobState) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Job getFullJob(JobId jobId) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Map<JobId, Job> getAllPartialJobs() {\r\n            return null;\r\n        }\r\n    };\r\n    doReturn(storage).when(jobHistory).createHistoryStorage();\r\n    jobHistory.init(new Configuration());\r\n    jobHistory.start();\r\n    Throwable th = null;\r\n    try {\r\n        jobHistory.refreshLoadedJobCache();\r\n    } catch (Exception e) {\r\n        th = e;\r\n    }\r\n    assertTrue(th instanceof UnsupportedOperationException);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanUp()\n{\r\n    if (jobHistory != null) {\r\n        jobHistory.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testParameters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> testParameters()\n{\r\n    return Arrays.asList(new Object[][] { { false }, { true } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void init() throws HadoopIllegalArgumentException, IOException\n{\r\n    conf = new JobConf();\r\n    conf.set(JHAdminConfig.JHS_ADMIN_ADDRESS, \"0.0.0.0:0\");\r\n    conf.setClass(\"hadoop.security.group.mapping\", MockUnixGroupsMapping.class, GroupMappingServiceProvider.class);\r\n    conf.setLong(\"hadoop.security.groups.cache.secs\", groupRefreshTimeoutSec);\r\n    conf.setBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, securityEnabled);\r\n    Groups.getUserToGroupsMappingService(conf);\r\n    jobHistoryService = mock(JobHistory.class);\r\n    alds = mock(AggregatedLogDeletionService.class);\r\n    hsAdminServer = new HSAdminServer(alds, jobHistoryService) {\r\n\r\n        @Override\r\n        protected Configuration createConf() {\r\n            return conf;\r\n        }\r\n    };\r\n    hsAdminServer.init(conf);\r\n    hsAdminServer.start();\r\n    conf.setSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, hsAdminServer.clientRpcServer.getListenerAddress());\r\n    hsAdminClient = new HSAdmin(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testGetGroups",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetGroups() throws Exception\n{\r\n    String user = UserGroupInformation.getCurrentUser().getUserName();\r\n    String[] args = new String[2];\r\n    args[0] = \"-getGroups\";\r\n    args[1] = user;\r\n    int exitCode = hsAdminClient.run(args);\r\n    assertEquals(\"Exit code should be 0 but was: \" + exitCode, 0, exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testRefreshUserToGroupsMappings",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testRefreshUserToGroupsMappings() throws Exception\n{\r\n    String[] args = new String[] { \"-refreshUserToGroupsMappings\" };\r\n    Groups groups = Groups.getUserToGroupsMappingService(conf);\r\n    String user = UserGroupInformation.getCurrentUser().getUserName();\r\n    System.out.println(\"first attempt:\");\r\n    List<String> g1 = groups.getGroups(user);\r\n    String[] str_groups = new String[g1.size()];\r\n    g1.toArray(str_groups);\r\n    System.out.println(Arrays.toString(str_groups));\r\n    System.out.println(\"second attempt, should be same:\");\r\n    List<String> g2 = groups.getGroups(user);\r\n    g2.toArray(str_groups);\r\n    System.out.println(Arrays.toString(str_groups));\r\n    for (int i = 0; i < g2.size(); i++) {\r\n        assertEquals(\"Should be same group \", g1.get(i), g2.get(i));\r\n    }\r\n    hsAdminClient.run(args);\r\n    System.out.println(\"third attempt(after refresh command), should be different:\");\r\n    List<String> g3 = groups.getGroups(user);\r\n    g3.toArray(str_groups);\r\n    System.out.println(Arrays.toString(str_groups));\r\n    for (int i = 0; i < g3.size(); i++) {\r\n        assertFalse(\"Should be different group: \" + g1.get(i) + \" and \" + g3.get(i), g1.get(i).equals(g3.get(i)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testRefreshSuperUserGroups",
  "errType" : [ "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testRefreshSuperUserGroups() throws Exception\n{\r\n    UserGroupInformation ugi = mock(UserGroupInformation.class);\r\n    UserGroupInformation superUser = mock(UserGroupInformation.class);\r\n    when(ugi.getRealUser()).thenReturn(superUser);\r\n    when(superUser.getShortUserName()).thenReturn(\"superuser\");\r\n    when(superUser.getUserName()).thenReturn(\"superuser\");\r\n    when(ugi.getGroups()).thenReturn(Arrays.asList(new String[] { \"group3\" }));\r\n    when(ugi.getGroupsSet()).thenReturn(Sets.newSet(\"group3\"));\r\n    when(ugi.getUserName()).thenReturn(\"regularUser\");\r\n    conf.set(\"hadoop.proxyuser.superuser.groups\", \"group1,group2\");\r\n    conf.set(\"hadoop.proxyuser.superuser.hosts\", \"127.0.0.1\");\r\n    String[] args = new String[1];\r\n    args[0] = \"-refreshSuperUserGroupsConfiguration\";\r\n    hsAdminClient.run(args);\r\n    Throwable th = null;\r\n    try {\r\n        ProxyUsers.authorize(ugi, \"127.0.0.1\");\r\n    } catch (Exception e) {\r\n        th = e;\r\n    }\r\n    assertTrue(th instanceof AuthorizationException);\r\n    conf.set(\"hadoop.proxyuser.superuser.groups\", \"group1,group2,group3\");\r\n    th = null;\r\n    try {\r\n        ProxyUsers.authorize(ugi, \"127.0.0.1\");\r\n    } catch (Exception e) {\r\n        th = e;\r\n    }\r\n    assertTrue(th instanceof AuthorizationException);\r\n    hsAdminClient.run(args);\r\n    th = null;\r\n    try {\r\n        ProxyUsers.authorize(ugi, \"127.0.0.1\");\r\n    } catch (Exception e) {\r\n        th = e;\r\n    }\r\n    assertNull(\"Unexpected exception thrown: \" + th, th);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testRefreshAdminAcls",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRefreshAdminAcls() throws Exception\n{\r\n    conf.set(JHAdminConfig.JHS_ADMIN_ACL, UserGroupInformation.getCurrentUser().getUserName());\r\n    String[] args = new String[1];\r\n    args[0] = \"-refreshAdminAcls\";\r\n    hsAdminClient.run(args);\r\n    args[0] = \"-refreshSuperUserGroupsConfiguration\";\r\n    hsAdminClient.run(args);\r\n    conf.set(JHAdminConfig.JHS_ADMIN_ACL, \"notCurrentUser\");\r\n    args[0] = \"-refreshAdminAcls\";\r\n    hsAdminClient.run(args);\r\n    Throwable th = null;\r\n    args[0] = \"-refreshSuperUserGroupsConfiguration\";\r\n    try {\r\n        hsAdminClient.run(args);\r\n    } catch (Exception e) {\r\n        th = e;\r\n    }\r\n    assertTrue(th instanceof RemoteException);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testRefreshLoadedJobCache",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRefreshLoadedJobCache() throws Exception\n{\r\n    String[] args = new String[1];\r\n    args[0] = \"-refreshLoadedJobCache\";\r\n    hsAdminClient.run(args);\r\n    verify(jobHistoryService).refreshLoadedJobCache();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testRefreshLogRetentionSettings",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRefreshLogRetentionSettings() throws Exception\n{\r\n    String[] args = new String[1];\r\n    args[0] = \"-refreshLogRetentionSettings\";\r\n    hsAdminClient.run(args);\r\n    verify(alds).refreshLogRetentionSettings();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testRefreshJobRetentionSettings",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRefreshJobRetentionSettings() throws Exception\n{\r\n    String[] args = new String[1];\r\n    args[0] = \"-refreshJobRetentionSettings\";\r\n    hsAdminClient.run(args);\r\n    verify(jobHistoryService).refreshJobRetentionSettings();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "testUGIForLogAndJobRefresh",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testUGIForLogAndJobRefresh() throws Exception\n{\r\n    UserGroupInformation ugi = UserGroupInformation.createUserForTesting(\"test\", new String[] { \"grp\" });\r\n    UserGroupInformation loginUGI = spy(hsAdminServer.getLoginUGI());\r\n    hsAdminServer.setLoginUGI(loginUGI);\r\n    ugi.doAs(new PrivilegedAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() {\r\n            String[] args = new String[1];\r\n            args[0] = \"-refreshLogRetentionSettings\";\r\n            try {\r\n                hsAdminClient.run(args);\r\n            } catch (Exception e) {\r\n                fail(\"refreshLogRetentionSettings should have been successful\");\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n    verify(loginUGI).doAs(any(PrivilegedExceptionAction.class));\r\n    verify(alds).refreshLogRetentionSettings();\r\n    reset(loginUGI);\r\n    ugi.doAs(new PrivilegedAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() {\r\n            String[] args = new String[1];\r\n            args[0] = \"-refreshJobRetentionSettings\";\r\n            try {\r\n                hsAdminClient.run(args);\r\n            } catch (Exception e) {\r\n                fail(\"refreshJobRetentionSettings should have been successful\");\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n    verify(loginUGI).doAs(any(PrivilegedExceptionAction.class));\r\n    verify(jobHistoryService).refreshJobRetentionSettings();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanUp()\n{\r\n    if (hsAdminServer != null)\r\n        hsAdminServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testRecovery",
  "errType" : [ "AccessControlException" ],
  "containingMethodsNum" : 48,
  "sourceCodeText" : "void testRecovery() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    HistoryServerStateStoreService store = new HistoryServerMemStateStoreService();\r\n    store.init(conf);\r\n    store.start();\r\n    Map<MRDelegationTokenIdentifier, Long> tokenState = ((HistoryServerMemStateStoreService) store).state.getTokenState();\r\n    JHSDelegationTokenSecretManagerForTest mgr = new JHSDelegationTokenSecretManagerForTest(store);\r\n    mgr.startThreads();\r\n    MRDelegationTokenIdentifier tokenId1 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner\"), new Text(\"tokenRenewer\"), new Text(\"tokenUser\"));\r\n    Token<MRDelegationTokenIdentifier> token1 = new Token<MRDelegationTokenIdentifier>(tokenId1, mgr);\r\n    MRDelegationTokenIdentifier tokenId2 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner\"), new Text(\"tokenRenewer\"), new Text(\"tokenUser\"));\r\n    Token<MRDelegationTokenIdentifier> token2 = new Token<MRDelegationTokenIdentifier>(tokenId2, mgr);\r\n    DelegationKey[] keys = mgr.getAllKeys();\r\n    long tokenRenewDate1 = mgr.getAllTokens().get(tokenId1).getRenewDate();\r\n    long tokenRenewDate2 = mgr.getAllTokens().get(tokenId2).getRenewDate();\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        public Boolean get() {\r\n            return tokenState.size() == 2;\r\n        }\r\n    }, 10, 2000);\r\n    stopAndCleanSecretManager(mgr);\r\n    mgr.recover(store.loadState());\r\n    List<DelegationKey> recoveredKeys = Arrays.asList(mgr.getAllKeys());\r\n    for (DelegationKey key : keys) {\r\n        assertTrue(\"key missing after recovery\", recoveredKeys.contains(key));\r\n    }\r\n    assertTrue(\"token1 missing\", mgr.getAllTokens().containsKey(tokenId1));\r\n    assertEquals(\"token1 renew date\", tokenRenewDate1, mgr.getAllTokens().get(tokenId1).getRenewDate());\r\n    assertTrue(\"token2 missing\", mgr.getAllTokens().containsKey(tokenId2));\r\n    assertEquals(\"token2 renew date\", tokenRenewDate2, mgr.getAllTokens().get(tokenId2).getRenewDate());\r\n    mgr.startThreads();\r\n    mgr.verifyToken(tokenId1, token1.getPassword());\r\n    mgr.verifyToken(tokenId2, token2.getPassword());\r\n    MRDelegationTokenIdentifier tokenId3 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner\"), new Text(\"tokenRenewer\"), new Text(\"tokenUser\"));\r\n    Token<MRDelegationTokenIdentifier> token3 = new Token<MRDelegationTokenIdentifier>(tokenId3, mgr);\r\n    assertEquals(\"sequence number restore\", tokenId2.getSequenceNumber() + 1, tokenId3.getSequenceNumber());\r\n    mgr.cancelToken(token1, \"tokenOwner\");\r\n    MRDelegationTokenIdentifier tokenIdFull = new MRDelegationTokenIdentifier(new Text(\"tokenOwner/localhost@LOCALHOST\"), new Text(\"tokenRenewer\"), new Text(\"tokenUser\"));\r\n    KerberosName.setRules(\"RULE:[1:$1]\\nRULE:[2:$1]\");\r\n    Token<MRDelegationTokenIdentifier> tokenFull = new Token<MRDelegationTokenIdentifier>(tokenIdFull, mgr);\r\n    try {\r\n        mgr.cancelToken(tokenFull, \"tokenOwner\");\r\n    } catch (AccessControlException ace) {\r\n        assertTrue(ace.getMessage().contains(\"is not authorized to cancel the token\"));\r\n    }\r\n    mgr.cancelToken(tokenFull, tokenIdFull.getOwner().toString());\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        public Boolean get() {\r\n            return tokenState.size() == 2;\r\n        }\r\n    }, 10, 2000);\r\n    long tokenRenewDate3 = mgr.getAllTokens().get(tokenId3).getRenewDate();\r\n    stopAndCleanSecretManager(mgr);\r\n    mgr.recover(store.loadState());\r\n    assertFalse(\"token1 should be missing\", mgr.getAllTokens().containsKey(tokenId1));\r\n    assertTrue(\"token2 missing\", mgr.getAllTokens().containsKey(tokenId2));\r\n    assertEquals(\"token2 renew date incorrect\", tokenRenewDate2, mgr.getAllTokens().get(tokenId2).getRenewDate());\r\n    assertTrue(\"token3 missing from manager\", mgr.getAllTokens().containsKey(tokenId3));\r\n    assertEquals(\"token3 renew date\", tokenRenewDate3, mgr.getAllTokens().get(tokenId3).getRenewDate());\r\n    mgr.startThreads();\r\n    mgr.verifyToken(tokenId2, token2.getPassword());\r\n    mgr.verifyToken(tokenId3, token3.getPassword());\r\n    tokenId3.setMasterKeyId(1000);\r\n    mgr.updateStoredToken(tokenId3, tokenRenewDate3 + 5000);\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        public Boolean get() {\r\n            return tokenState.get(tokenId3).equals(tokenRenewDate3 + 5000);\r\n        }\r\n    }, 10, 2000);\r\n    stopAndCleanSecretManager(mgr);\r\n    Assert.assertTrue(\"Store does not contain token3\", tokenState.containsKey(tokenId3));\r\n    Assert.assertFalse(\"Store does not contain token3\", mgr.getAllTokens().containsKey(tokenId3));\r\n    mgr.recover(store.loadState());\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        public Boolean get() {\r\n            return mgr.getAllTokens().get(tokenId3).getRenewDate() == 0L;\r\n        }\r\n    }, 10, 2000);\r\n    mgr.startThreads();\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        public Boolean get() {\r\n            return !mgr.getAllTokens().containsKey(tokenId3);\r\n        }\r\n    }, 10, 2000);\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        public Boolean get() {\r\n            return !tokenState.containsKey(tokenId3);\r\n        }\r\n    }, 10, 2000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "stopAndCleanSecretManager",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void stopAndCleanSecretManager(JHSDelegationTokenSecretManagerForTest mgr)\n{\r\n    mgr.stopThreads();\r\n    mgr.reset();\r\n    assertThat(mgr.getAllKeys().length).withFailMessage(\"Secret manager should not contain keys\").isZero();\r\n    assertThat(mgr.getAllTokens().size()).withFailMessage(\"Secret manager should not contain tokens\").isZero();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    fs = FileSystem.get(conf);\r\n    createAggregatedFolders();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp()\n{\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "createAggregatedFolders",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void createAggregatedFolders() throws Exception\n{\r\n    Map<ContainerId, String> contentsApp1 = new HashMap<>();\r\n    contentsApp1.put(CONTAINER_1_1_1, \"Hello-\" + CONTAINER_1_1_1);\r\n    contentsApp1.put(CONTAINER_1_1_2, \"Hello-\" + CONTAINER_1_1_2);\r\n    contentsApp1.put(CONTAINER_1_2_1, \"Hello-\" + CONTAINER_1_2_1);\r\n    TestContainerLogsUtils.createContainerLogFileInRemoteFS(conf, fs, LOCAL_ROOT_LOG_DIR, APPID_1, contentsApp1, NM_ID_1, FILE_NAME, USER, false);\r\n    TestContainerLogsUtils.createContainerLogFileInRemoteFS(conf, fs, LOCAL_ROOT_LOG_DIR, APPID_1, Collections.singletonMap(CONTAINER_1_1_3, \"Hello-\" + CONTAINER_1_1_3), NM_ID_2, FILE_NAME, USER, false);\r\n    Map<ContainerId, String> contentsApp2 = new HashMap<>();\r\n    contentsApp2.put(CONTAINER_2_1_1, \"Hello-\" + CONTAINER_2_1_1);\r\n    contentsApp2.put(CONTAINER_2_2_1, \"Hello-\" + CONTAINER_2_2_1);\r\n    TestContainerLogsUtils.createContainerLogFileInRemoteFS(conf, fs, LOCAL_ROOT_LOG_DIR, APPID_2, contentsApp2, NM_ID_1, FILE_NAME, USER, false);\r\n    TestContainerLogsUtils.createContainerLogFileInRemoteFS(conf, fs, LOCAL_ROOT_LOG_DIR, APPID_2, Collections.singletonMap(CONTAINER_2_2_3, \"Hello-\" + CONTAINER_2_2_3), NM_ID_2, FILE_NAME, USER, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "tearDownClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDownClass() throws Exception\n{\r\n    fs.delete(new Path(REMOTE_LOG_ROOT_DIR), true);\r\n    fs.delete(new Path(LOCAL_ROOT_LOG_DIR), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetAggregatedLogsMetaForFinishedApp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetAggregatedLogsMetaForFinishedApp()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APP_ID, APPID_1.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    List<ContainerLogsInfo> responseList = response.getEntity(new GenericType<List<ContainerLogsInfo>>() {\r\n    });\r\n    Set<String> expectedIdStrings = Sets.newHashSet(CONTAINER_1_1_1.toString(), CONTAINER_1_1_2.toString(), CONTAINER_1_1_3.toString(), CONTAINER_1_2_1.toString());\r\n    assertResponseList(responseList, expectedIdStrings, false);\r\n    for (ContainerLogsInfo logsInfo : responseList) {\r\n        String cId = logsInfo.getContainerId();\r\n        assertThat(logsInfo.getLogType()).isEqualTo(ContainerLogAggregationType.AGGREGATED.toString());\r\n        if (cId.equals(CONTAINER_1_1_3.toString())) {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_2));\r\n        } else {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_1));\r\n        }\r\n        assertSimpleContainerLogFileInfo(logsInfo, cId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetAggregatedLogsMetaForRunningApp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetAggregatedLogsMetaForRunningApp()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APP_ID, APPID_2.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    List<ContainerLogsInfo> responseList = response.getEntity(new GenericType<List<ContainerLogsInfo>>() {\r\n    });\r\n    Set<String> expectedIdStrings = Sets.newHashSet(CONTAINER_2_1_1.toString(), CONTAINER_2_2_1.toString(), CONTAINER_2_2_3.toString());\r\n    assertResponseList(responseList, expectedIdStrings, true);\r\n    for (ContainerLogsInfo logsInfo : responseList) {\r\n        String cId = logsInfo.getContainerId();\r\n        if (cId.equals(CONTAINER_2_2_3.toString())) {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_2));\r\n        } else {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_1));\r\n        }\r\n        if (logsInfo.getLogType().equals(ContainerLogAggregationType.AGGREGATED.toString())) {\r\n            assertSimpleContainerLogFileInfo(logsInfo, cId);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetAggregatedLogsMetaForFinishedAppAttempt",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetAggregatedLogsMetaForFinishedAppAttempt()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APPATTEMPT_ID, APP_ATTEMPT_1_1.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    List<ContainerLogsInfo> responseList = response.getEntity(new GenericType<List<ContainerLogsInfo>>() {\r\n    });\r\n    Set<String> expectedIdStrings = Sets.newHashSet(CONTAINER_1_1_1.toString(), CONTAINER_1_1_2.toString(), CONTAINER_1_1_3.toString());\r\n    assertResponseList(responseList, expectedIdStrings, false);\r\n    for (ContainerLogsInfo logsInfo : responseList) {\r\n        String cId = logsInfo.getContainerId();\r\n        assertThat(logsInfo.getLogType()).isEqualTo(ContainerLogAggregationType.AGGREGATED.toString());\r\n        if (cId.equals(CONTAINER_1_1_3.toString())) {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_2));\r\n        } else {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_1));\r\n        }\r\n        assertSimpleContainerLogFileInfo(logsInfo, cId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetAggregatedLogsMetaForRunningAppAttempt",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetAggregatedLogsMetaForRunningAppAttempt()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APPATTEMPT_ID, APP_ATTEMPT_2_2.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    List<ContainerLogsInfo> responseList = response.getEntity(new GenericType<List<ContainerLogsInfo>>() {\r\n    });\r\n    Set<String> expectedIdStrings = Sets.newHashSet(CONTAINER_2_2_1.toString(), CONTAINER_2_2_3.toString());\r\n    assertResponseList(responseList, expectedIdStrings, true);\r\n    for (ContainerLogsInfo logsInfo : responseList) {\r\n        String cId = logsInfo.getContainerId();\r\n        if (cId.equals(CONTAINER_2_2_3.toString())) {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_2));\r\n        } else {\r\n            assertThat(logsInfo.getNodeId()).isEqualTo(formatNodeId(NM_ID_1));\r\n        }\r\n        if (logsInfo.getLogType().equals(ContainerLogAggregationType.AGGREGATED.toString())) {\r\n            assertSimpleContainerLogFileInfo(logsInfo, cId);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetContainerLogsForFinishedContainer",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetContainerLogsForFinishedContainer()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containers\").path(CONTAINER_1_1_2.toString()).path(\"logs\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    List<ContainerLogsInfo> responseText = response.getEntity(new GenericType<List<ContainerLogsInfo>>() {\r\n    });\r\n    assertThat(responseText.size()).isOne();\r\n    ContainerLogsInfo logsInfo = responseText.get(0);\r\n    assertThat(logsInfo.getLogType()).isEqualTo(ContainerLogAggregationType.AGGREGATED.toString());\r\n    assertThat(logsInfo.getContainerId()).isEqualTo(CONTAINER_1_1_2.toString());\r\n    assertSimpleContainerLogFileInfo(logsInfo, CONTAINER_1_1_2.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetContainerLogsForRunningContainer",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testGetContainerLogsForRunningContainer() throws Exception\n{\r\n    WebResource r = resource();\r\n    URI requestURI = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containers\").path(CONTAINER_2_2_2.toString()).path(\"logs\").getURI();\r\n    String redirectURL = getRedirectURL(requestURI.toString());\r\n    assertThat(redirectURL).isNotNull();\r\n    assertThat(redirectURL).contains(NM_WEBADDRESS_1, \"ws/v1/node/containers\", CONTAINER_2_2_2.toString(), \"/logs\");\r\n    requestURI = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containers\").path(CONTAINER_2_2_2.toString()).path(\"logs\").queryParam(YarnWebServiceParams.NM_ID, NM_ID_2.toString()).getURI();\r\n    redirectURL = getRedirectURL(requestURI.toString());\r\n    assertThat(redirectURL).isNotNull();\r\n    assertThat(redirectURL).contains(NM_WEBADDRESS_2, \"ws/v1/node/containers\", CONTAINER_2_2_2.toString(), \"/logs\");\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containers\").path(CONTAINER_2_2_3.toString()).path(\"logs\").queryParam(YarnWebServiceParams.REDIRECTED_FROM_NODE, \"true\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    List<ContainerLogsInfo> responseText = response.getEntity(new GenericType<List<ContainerLogsInfo>>() {\r\n    });\r\n    assertThat(responseText.size()).isEqualTo(2);\r\n    ContainerLogsInfo logsInfo1 = responseText.get(0);\r\n    ContainerLogsInfo logsInfo2 = responseText.get(1);\r\n    assertThat(logsInfo1.getContainerId()).isEqualTo(CONTAINER_2_2_3.toString());\r\n    assertThat(logsInfo2.getContainerId()).isEqualTo(CONTAINER_2_2_3.toString());\r\n    if (logsInfo1.getLogType().equals(ContainerLogAggregationType.AGGREGATED.toString())) {\r\n        assertThat(logsInfo2.getLogType()).isEqualTo(ContainerLogAggregationType.LOCAL.toString());\r\n        assertSimpleContainerLogFileInfo(logsInfo1, CONTAINER_2_2_3.toString());\r\n        assertThat(logsInfo2.getContainerLogsInfo()).isNull();\r\n    } else {\r\n        assertThat(logsInfo1.getLogType()).isEqualTo(ContainerLogAggregationType.LOCAL.toString());\r\n        assertThat(logsInfo2.getLogType()).isEqualTo(ContainerLogAggregationType.AGGREGATED.toString());\r\n        assertThat(logsInfo1.getContainerLogsInfo()).isNull();\r\n        assertSimpleContainerLogFileInfo(logsInfo2, CONTAINER_2_2_3.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetContainerLogFileForFinishedContainer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetContainerLogFileForFinishedContainer()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containerlogs\").path(CONTAINER_1_1_2.toString()).path(FILE_NAME).accept(MediaType.TEXT_PLAIN).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).doesNotContain(\"Can not find logs\", \"Hello-\" + CONTAINER_1_1_1);\r\n    assertThat(responseText).contains(\"Hello-\" + CONTAINER_1_1_2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testNoRedirectForFinishedContainer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNoRedirectForFinishedContainer() throws Exception\n{\r\n    WebResource r = resource();\r\n    URI requestURI = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containerlogs\").path(CONTAINER_2_2_1.toString()).path(FILE_NAME).getURI();\r\n    String redirectURL = getRedirectURL(requestURI.toString());\r\n    assertThat(redirectURL).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetContainerLogFileForRunningContainer",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testGetContainerLogFileForRunningContainer() throws Exception\n{\r\n    WebResource r = resource();\r\n    URI requestURI = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containerlogs\").path(CONTAINER_2_2_2.toString()).path(FILE_NAME).getURI();\r\n    String redirectURL = getRedirectURL(requestURI.toString());\r\n    assertThat(redirectURL).isNotNull();\r\n    assertThat(redirectURL).contains(NM_WEBADDRESS_1, \"ws/v1/node/containers\", \"/logs/\" + FILE_NAME, CONTAINER_2_2_2.toString());\r\n    requestURI = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containerlogs\").path(CONTAINER_2_2_2.toString()).path(FILE_NAME).queryParam(YarnWebServiceParams.NM_ID, NM_ID_2.toString()).getURI();\r\n    redirectURL = getRedirectURL(requestURI.toString());\r\n    assertThat(redirectURL).isNotNull();\r\n    assertThat(redirectURL).contains(NM_WEBADDRESS_2, \"ws/v1/node/containers\", \"/logs/\" + FILE_NAME, CONTAINER_2_2_2.toString());\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containerlogs\").path(CONTAINER_2_2_3.toString()).path(FILE_NAME).queryParam(YarnWebServiceParams.REDIRECTED_FROM_NODE, \"true\").accept(MediaType.TEXT_PLAIN).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).isNotNull();\r\n    assertThat(responseText).contains(\"LogAggregationType: \" + ContainerLogAggregationType.AGGREGATED, \"Hello-\" + CONTAINER_2_2_3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testRemoteLogDirWithUser",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRemoteLogDirWithUser()\n{\r\n    createReconfiguredServlet();\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"remote-log-dir\").queryParam(YarnWebServiceParams.REMOTE_USER, USER).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    RemoteLogPaths res = response.getEntity(new GenericType<RemoteLogPaths>() {\r\n    });\r\n    List<String> collectedControllerNames = new ArrayList<>();\r\n    for (RemoteLogPathEntry entry : res.getPaths()) {\r\n        String path = String.format(\"%s/%s/bucket-%s-%s\", YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR, USER, REMOTE_LOG_DIR_SUFFIX, entry.getFileController().toLowerCase());\r\n        collectedControllerNames.add(entry.getFileController());\r\n        assertEquals(entry.getPath(), path);\r\n    }\r\n    assertTrue(collectedControllerNames.containsAll(Arrays.asList(FILE_FORMATS)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testRemoteLogDir",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRemoteLogDir()\n{\r\n    createReconfiguredServlet();\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(USER);\r\n    UserGroupInformation.setLoginUser(ugi);\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"remote-log-dir\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    RemoteLogPaths res = response.getEntity(new GenericType<RemoteLogPaths>() {\r\n    });\r\n    List<String> collectedControllerNames = new ArrayList<>();\r\n    for (RemoteLogPathEntry entry : res.getPaths()) {\r\n        String path = String.format(\"%s/%s/bucket-%s-%s\", YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR, USER, REMOTE_LOG_DIR_SUFFIX, entry.getFileController().toLowerCase());\r\n        collectedControllerNames.add(entry.getFileController());\r\n        assertEquals(entry.getPath(), path);\r\n    }\r\n    assertTrue(collectedControllerNames.containsAll(Arrays.asList(FILE_FORMATS)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testRemoteLogDirWithUserAndAppId",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRemoteLogDirWithUserAndAppId()\n{\r\n    createReconfiguredServlet();\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"remote-log-dir\").queryParam(YarnWebServiceParams.REMOTE_USER, USER).queryParam(YarnWebServiceParams.APP_ID, APPID_1.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    RemoteLogPaths res = response.getEntity(new GenericType<RemoteLogPaths>() {\r\n    });\r\n    List<String> collectedControllerNames = new ArrayList<>();\r\n    for (RemoteLogPathEntry entry : res.getPaths()) {\r\n        String path = String.format(\"%s/%s/bucket-%s-%s/0001/%s\", YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR, USER, REMOTE_LOG_DIR_SUFFIX, entry.getFileController().toLowerCase(), APPID_1.toString());\r\n        collectedControllerNames.add(entry.getFileController());\r\n        assertEquals(entry.getPath(), path);\r\n    }\r\n    assertTrue(collectedControllerNames.containsAll(Arrays.asList(FILE_FORMATS)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testNonExistingAppId",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNonExistingAppId()\n{\r\n    ApplicationId nonExistingApp = ApplicationId.newInstance(99, 99);\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APP_ID, nonExistingApp.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(WebApplicationException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Can not find\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testBadAppId",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBadAppId()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APP_ID, \"some text\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(BadRequestException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Invalid ApplicationId\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testNonExistingAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testNonExistingAppAttemptId()\n{\r\n    ApplicationId nonExistingApp = ApplicationId.newInstance(99, 99);\r\n    ApplicationAttemptId nonExistingAppAttemptId = ApplicationAttemptId.newInstance(nonExistingApp, 1);\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APPATTEMPT_ID, nonExistingAppAttemptId.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(WebApplicationException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Can not find\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testBadAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBadAppAttemptId()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.APPATTEMPT_ID, \"some text\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(BadRequestException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Invalid AppAttemptId\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testNonExistingContainerId",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNonExistingContainerId()\n{\r\n    ApplicationId nonExistingApp = ApplicationId.newInstance(99, 99);\r\n    ApplicationAttemptId nonExistingAppAttemptId = ApplicationAttemptId.newInstance(nonExistingApp, 1);\r\n    ContainerId nonExistingContainerId = ContainerId.newContainerId(nonExistingAppAttemptId, 1);\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.CONTAINER_ID, nonExistingContainerId.toString()).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(WebApplicationException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Can not find\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testBadContainerId",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBadContainerId()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"aggregatedlogs\").queryParam(YarnWebServiceParams.CONTAINER_ID, \"some text\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(BadRequestException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Invalid ContainerId\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testNonExistingContainerMeta",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNonExistingContainerMeta()\n{\r\n    ApplicationId nonExistingApp = ApplicationId.newInstance(99, 99);\r\n    ApplicationAttemptId nonExistingAppAttemptId = ApplicationAttemptId.newInstance(nonExistingApp, 1);\r\n    ContainerId nonExistingContainerId = ContainerId.newContainerId(nonExistingAppAttemptId, 1);\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containers\").path(nonExistingContainerId.toString()).path(\"logs\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(WebApplicationException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Can not find\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testBadContainerForMeta",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBadContainerForMeta()\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"containers\").path(\"some text\").path(\"logs\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    String responseText = response.getEntity(String.class);\r\n    assertThat(responseText).contains(BadRequestException.class.getSimpleName());\r\n    assertThat(responseText).contains(\"Invalid container id\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "assertSimpleContainerLogFileInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertSimpleContainerLogFileInfo(ContainerLogsInfo logsInfo, String cId)\n{\r\n    assertThat(logsInfo.getContainerLogsInfo()).isNotNull();\r\n    assertThat(logsInfo.getContainerLogsInfo().size()).isEqualTo(1);\r\n    ContainerLogFileInfo fileInfo = logsInfo.getContainerLogsInfo().get(0);\r\n    assertThat(fileInfo.getFileName()).isEqualTo(FILE_NAME);\r\n    assertThat(fileInfo.getFileSize()).isEqualTo(String.valueOf((\"Hello-\" + cId).length()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "assertResponseList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertResponseList(List<ContainerLogsInfo> responseList, Set<String> expectedIdStrings, boolean running)\n{\r\n    Set<String> actualStrings = responseList.stream().map(ContainerLogsInfo::getContainerId).collect(Collectors.toSet());\r\n    assertThat(actualStrings).isEqualTo(expectedIdStrings);\r\n    int expectedSize = expectedIdStrings.size();\r\n    assertThat(responseList.size()).isEqualTo(running ? expectedSize * 2 : expectedSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "formatNodeId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String formatNodeId(NodeId nodeId)\n{\r\n    return nodeId.toString().replace(\":\", \"_\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "newApplicationReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ApplicationReport newApplicationReport(ApplicationId appId, ApplicationAttemptId appAttemptId, boolean running)\n{\r\n    return ApplicationReport.newInstance(appId, appAttemptId, USER, \"fakeQueue\", \"fakeApplicationName\", \"localhost\", 0, null, running ? YarnApplicationState.RUNNING : YarnApplicationState.FINISHED, \"fake an application report\", \"\", 1000L, 1000L, 1000L, null, null, \"\", 50f, \"fakeApplicationType\", null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "newContainerReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerReport newContainerReport(ContainerId containerId, NodeId nodeId, String nmWebAddress)\n{\r\n    return ContainerReport.newInstance(containerId, null, nodeId, Priority.UNDEFINED, 0, 0, null, null, 0, null, nmWebAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getRedirectURL",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getRedirectURL(String url) throws Exception\n{\r\n    HttpURLConnection conn = (HttpURLConnection) new URL(url).openConnection();\r\n    conn.setInstanceFollowRedirects(false);\r\n    if (conn.getResponseCode() == HttpServletResponse.SC_TEMPORARY_REDIRECT) {\r\n        return conn.getHeaderField(\"Location\");\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "createReconfiguredServlet",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createReconfiguredServlet()\n{\r\n    Configuration newConf = new YarnConfiguration();\r\n    newConf.setStrings(YarnConfiguration.LOG_AGGREGATION_FILE_FORMATS, FILE_FORMATS);\r\n    newConf.setClass(String.format(YarnConfiguration.LOG_AGGREGATION_FILE_CONTROLLER_FMT, \"IFile\"), LogAggregationIndexedFileController.class, LogAggregationFileController.class);\r\n    newConf.set(YarnConfiguration.NM_REMOTE_APP_LOG_DIR, YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR);\r\n    newConf.set(YarnConfiguration.NM_REMOTE_APP_LOG_DIR_SUFFIX, REMOTE_LOG_DIR_SUFFIX);\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule(newConf)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryEvents",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testHistoryEvents() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MRApp app = new MRAppWithHistory(2, 1, true, this.getClass().getName(), true);\r\n    app.submit(conf);\r\n    Job job = app.getContext().getAllJobs().values().iterator().next();\r\n    JobId jobId = job.getID();\r\n    LOG.info(\"JOBID is \" + TypeConverter.fromYarn(jobId).toString());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.waitForState(Service.STATE.STOPPED);\r\n    HistoryContext context = new JobHistory();\r\n    ((JobHistory) context).init(conf);\r\n    ((JobHistory) context).start();\r\n    Assert.assertTrue(context.getStartTime() > 0);\r\n    Assert.assertEquals(((JobHistory) context).getServiceState(), Service.STATE.STARTED);\r\n    Job parsedJob = context.getJob(jobId);\r\n    ((JobHistory) context).stop();\r\n    Assert.assertEquals(((JobHistory) context).getServiceState(), Service.STATE.STOPPED);\r\n    Assert.assertEquals(\"CompletedMaps not correct\", 2, parsedJob.getCompletedMaps());\r\n    Assert.assertEquals(System.getProperty(\"user.name\"), parsedJob.getUserName());\r\n    Map<TaskId, Task> tasks = parsedJob.getTasks();\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, tasks.size());\r\n    for (Task task : tasks.values()) {\r\n        verifyTask(task);\r\n    }\r\n    Map<TaskId, Task> maps = parsedJob.getTasks(TaskType.MAP);\r\n    Assert.assertEquals(\"No of maps not correct\", 2, maps.size());\r\n    Map<TaskId, Task> reduces = parsedJob.getTasks(TaskType.REDUCE);\r\n    Assert.assertEquals(\"No of reduces not correct\", 1, reduces.size());\r\n    Assert.assertEquals(\"CompletedReduce not correct\", 1, parsedJob.getCompletedReduces());\r\n    Assert.assertEquals(\"Job state not currect\", JobState.SUCCEEDED, parsedJob.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testEventsFlushOnStop",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testEventsFlushOnStop() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MRApp app = new MRAppWithSpecialHistoryHandler(1, 0, true, this.getClass().getName(), true);\r\n    app.submit(conf);\r\n    Job job = app.getContext().getAllJobs().values().iterator().next();\r\n    JobId jobId = job.getID();\r\n    LOG.info(\"JOBID is \" + TypeConverter.fromYarn(jobId).toString());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.waitForState(Service.STATE.STOPPED);\r\n    HistoryContext context = new JobHistory();\r\n    ((JobHistory) context).init(conf);\r\n    Job parsedJob = context.getJob(jobId);\r\n    Assert.assertEquals(\"CompletedMaps not correct\", 1, parsedJob.getCompletedMaps());\r\n    Map<TaskId, Task> tasks = parsedJob.getTasks();\r\n    Assert.assertEquals(\"No of tasks not correct\", 1, tasks.size());\r\n    verifyTask(tasks.values().iterator().next());\r\n    Map<TaskId, Task> maps = parsedJob.getTasks(TaskType.MAP);\r\n    Assert.assertEquals(\"No of maps not correct\", 1, maps.size());\r\n    Assert.assertEquals(\"Job state not currect\", JobState.SUCCEEDED, parsedJob.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testJobHistoryEventHandlerIsFirstServiceToStop",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testJobHistoryEventHandlerIsFirstServiceToStop()\n{\r\n    MRApp app = new MRAppWithSpecialHistoryHandler(1, 0, true, this.getClass().getName(), true);\r\n    Configuration conf = new Configuration();\r\n    app.init(conf);\r\n    Service[] services = app.getServices().toArray(new Service[0]);\r\n    Assert.assertEquals(\"JobHistoryEventHandler\", services[services.length - 1].getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testAssignedQueue",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAssignedQueue() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MRApp app = new MRAppWithHistory(2, 1, true, this.getClass().getName(), true, \"assignedQueue\");\r\n    app.submit(conf);\r\n    Job job = app.getContext().getAllJobs().values().iterator().next();\r\n    JobId jobId = job.getID();\r\n    LOG.info(\"JOBID is \" + TypeConverter.fromYarn(jobId).toString());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.waitForState(Service.STATE.STOPPED);\r\n    HistoryContext context = new JobHistory();\r\n    ((JobHistory) context).init(conf);\r\n    ((JobHistory) context).start();\r\n    Assert.assertTrue(context.getStartTime() > 0);\r\n    assertThat(((JobHistory) context).getServiceState()).isEqualTo(Service.STATE.STARTED);\r\n    Job parsedJob = context.getJob(jobId);\r\n    ((JobHistory) context).stop();\r\n    assertThat(((JobHistory) context).getServiceState()).isEqualTo(Service.STATE.STOPPED);\r\n    Assert.assertEquals(\"QueueName not correct\", \"assignedQueue\", parsedJob.getQueueName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "verifyTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyTask(Task task)\n{\r\n    Assert.assertEquals(\"Task state not currect\", TaskState.SUCCEEDED, task.getState());\r\n    Map<TaskAttemptId, TaskAttempt> attempts = task.getAttempts();\r\n    Assert.assertEquals(\"No of attempts not correct\", 1, attempts.size());\r\n    for (TaskAttempt attempt : attempts.values()) {\r\n        verifyAttempt(attempt);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "verifyAttempt",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyAttempt(TaskAttempt attempt)\n{\r\n    Assert.assertEquals(\"TaskAttempt state not currect\", TaskAttemptState.SUCCEEDED, attempt.getState());\r\n    Assert.assertNotNull(attempt.getAssignedContainerID());\r\n    ContainerId fakeCid = MRApp.newContainerId(-1, -1, -1, -1);\r\n    Assert.assertFalse(attempt.getAssignedContainerID().equals(fakeCid));\r\n    Assert.assertEquals(MRApp.NM_HOST + \":\" + MRApp.NM_PORT, attempt.getAssignedContainerMgrAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestJobHistoryEvents t = new TestJobHistoryEvents();\r\n    t.testHistoryEvents();\r\n    t.testEventsFlushOnStop();\r\n    t.testJobHistoryEventHandlerIsFirstServiceToStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup()\n{\r\n    FileUtil.fullyDelete(testDir);\r\n    testDir.mkdirs();\r\n    conf = new Configuration();\r\n    conf.setBoolean(JHAdminConfig.MR_HS_RECOVERY_ENABLE, true);\r\n    conf.setClass(JHAdminConfig.MR_HS_STATE_STORE, HistoryServerFileSystemStateStoreService.class, HistoryServerStateStoreService.class);\r\n    conf.set(JHAdminConfig.MR_HS_FS_STATE_STORE_URI, testDir.getAbsoluteFile().toURI().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(testDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createAndStartStore",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HistoryServerStateStoreService createAndStartStore() throws IOException\n{\r\n    HistoryServerStateStoreService store = HistoryServerStateStoreServiceFactory.getStore(conf);\r\n    assertTrue(\"Factory did not create a filesystem store\", store instanceof HistoryServerFileSystemStateStoreService);\r\n    store.init(conf);\r\n    store.start();\r\n    return store;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTokenStore",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 47,
  "sourceCodeText" : "void testTokenStore(String stateStoreUri) throws IOException\n{\r\n    conf.set(JHAdminConfig.MR_HS_FS_STATE_STORE_URI, stateStoreUri);\r\n    HistoryServerStateStoreService store = createAndStartStore();\r\n    HistoryServerState state = store.loadState();\r\n    assertTrue(\"token state not empty\", state.tokenState.isEmpty());\r\n    assertTrue(\"key state not empty\", state.tokenMasterKeyState.isEmpty());\r\n    final DelegationKey key1 = new DelegationKey(1, 2, \"keyData1\".getBytes());\r\n    final MRDelegationTokenIdentifier token1 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner1\"), new Text(\"tokenRenewer1\"), new Text(\"tokenUser1\"));\r\n    token1.setSequenceNumber(1);\r\n    final Long tokenDate1 = 1L;\r\n    final MRDelegationTokenIdentifier token2 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner2\"), new Text(\"tokenRenewer2\"), new Text(\"tokenUser2\"));\r\n    token2.setSequenceNumber(12345678);\r\n    final Long tokenDate2 = 87654321L;\r\n    store.storeTokenMasterKey(key1);\r\n    try {\r\n        store.storeTokenMasterKey(key1);\r\n        fail(\"redundant store of key undetected\");\r\n    } catch (IOException e) {\r\n    }\r\n    store.storeToken(token1, tokenDate1);\r\n    store.storeToken(token2, tokenDate2);\r\n    try {\r\n        store.storeToken(token1, tokenDate1);\r\n        fail(\"redundant store of token undetected\");\r\n    } catch (IOException e) {\r\n    }\r\n    store.close();\r\n    store = createAndStartStore();\r\n    state = store.loadState();\r\n    assertEquals(\"incorrect loaded token count\", 2, state.tokenState.size());\r\n    assertTrue(\"missing token 1\", state.tokenState.containsKey(token1));\r\n    assertEquals(\"incorrect token 1 date\", tokenDate1, state.tokenState.get(token1));\r\n    assertTrue(\"missing token 2\", state.tokenState.containsKey(token2));\r\n    assertEquals(\"incorrect token 2 date\", tokenDate2, state.tokenState.get(token2));\r\n    assertEquals(\"incorrect master key count\", 1, state.tokenMasterKeyState.size());\r\n    assertTrue(\"missing master key 1\", state.tokenMasterKeyState.contains(key1));\r\n    final DelegationKey key2 = new DelegationKey(3, 4, \"keyData2\".getBytes());\r\n    final DelegationKey key3 = new DelegationKey(5, 6, \"keyData3\".getBytes());\r\n    final MRDelegationTokenIdentifier token3 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner3\"), new Text(\"tokenRenewer3\"), new Text(\"tokenUser3\"));\r\n    token3.setSequenceNumber(12345679);\r\n    final Long tokenDate3 = 87654321L;\r\n    store.removeToken(token1);\r\n    store.storeTokenMasterKey(key2);\r\n    final Long newTokenDate2 = 975318642L;\r\n    store.updateToken(token2, newTokenDate2);\r\n    store.removeTokenMasterKey(key1);\r\n    store.storeTokenMasterKey(key3);\r\n    store.storeToken(token3, tokenDate3);\r\n    store.close();\r\n    store = createAndStartStore();\r\n    state = store.loadState();\r\n    assertEquals(\"incorrect loaded token count\", 2, state.tokenState.size());\r\n    assertFalse(\"token 1 not removed\", state.tokenState.containsKey(token1));\r\n    assertTrue(\"missing token 2\", state.tokenState.containsKey(token2));\r\n    assertEquals(\"incorrect token 2 date\", newTokenDate2, state.tokenState.get(token2));\r\n    assertTrue(\"missing token 3\", state.tokenState.containsKey(token3));\r\n    assertEquals(\"incorrect token 3 date\", tokenDate3, state.tokenState.get(token3));\r\n    assertEquals(\"incorrect master key count\", 2, state.tokenMasterKeyState.size());\r\n    assertFalse(\"master key 1 not removed\", state.tokenMasterKeyState.contains(key1));\r\n    assertTrue(\"missing master key 2\", state.tokenMasterKeyState.contains(key2));\r\n    assertTrue(\"missing master key 3\", state.tokenMasterKeyState.contains(key3));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTokenStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTokenStore() throws IOException\n{\r\n    testTokenStore(testDir.getAbsoluteFile().toURI().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTokenStoreHdfs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTokenStoreHdfs() throws IOException\n{\r\n    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();\r\n    conf = cluster.getConfiguration(0);\r\n    try {\r\n        testTokenStore(\"/tmp/historystore\");\r\n    } finally {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testUpdatedTokenRecovery",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testUpdatedTokenRecovery() throws IOException\n{\r\n    IOException intentionalErr = new IOException(\"intentional error\");\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    final FileSystem spyfs = spy(fs);\r\n    ArgumentMatcher<Path> updateTmpMatcher = arg -> arg.getName().startsWith(\"update\");\r\n    doThrow(intentionalErr).when(spyfs).rename(argThat(updateTmpMatcher), isA(Path.class));\r\n    conf.set(JHAdminConfig.MR_HS_FS_STATE_STORE_URI, testDir.getAbsoluteFile().toURI().toString());\r\n    HistoryServerStateStoreService store = new HistoryServerFileSystemStateStoreService() {\r\n\r\n        @Override\r\n        FileSystem createFileSystem() throws IOException {\r\n            return spyfs;\r\n        }\r\n    };\r\n    store.init(conf);\r\n    store.start();\r\n    final MRDelegationTokenIdentifier token1 = new MRDelegationTokenIdentifier(new Text(\"tokenOwner1\"), new Text(\"tokenRenewer1\"), new Text(\"tokenUser1\"));\r\n    token1.setSequenceNumber(1);\r\n    final Long tokenDate1 = 1L;\r\n    store.storeToken(token1, tokenDate1);\r\n    final Long newTokenDate1 = 975318642L;\r\n    try {\r\n        store.updateToken(token1, newTokenDate1);\r\n        fail(\"intentional error not thrown\");\r\n    } catch (IOException e) {\r\n        assertEquals(intentionalErr, e);\r\n    }\r\n    store.close();\r\n    store = createAndStartStore();\r\n    HistoryServerState state = store.loadState();\r\n    assertEquals(\"incorrect loaded token count\", 1, state.tokenState.size());\r\n    assertTrue(\"missing token 1\", state.tokenState.containsKey(token1));\r\n    assertEquals(\"incorrect token 1 date\", newTokenDate1, state.tokenState.get(token1));\r\n    store.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "newHistoryJobs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobsPair newHistoryJobs(int numJobs, int numTasksPerJob, int numAttemptsPerTask) throws IOException\n{\r\n    Map<JobId, Job> mocked = newJobs(numJobs, numTasksPerJob, numAttemptsPerTask);\r\n    return split(mocked);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "newHistoryJobs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobsPair newHistoryJobs(ApplicationId appID, int numJobsPerApp, int numTasksPerJob, int numAttemptsPerTask) throws IOException\n{\r\n    Map<JobId, Job> mocked = newJobs(appID, numJobsPerApp, numTasksPerJob, numAttemptsPerTask);\r\n    return split(mocked);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "newHistoryJobs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobsPair newHistoryJobs(ApplicationId appID, int numJobsPerApp, int numTasksPerJob, int numAttemptsPerTask, boolean hasFailedTasks) throws IOException\n{\r\n    Map<JobId, Job> mocked = newJobs(appID, numJobsPerApp, numTasksPerJob, numAttemptsPerTask, hasFailedTasks);\r\n    return split(mocked);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "JobsPair split(Map<JobId, Job> mocked) throws IOException\n{\r\n    JobsPair ret = new JobsPair();\r\n    ret.full = Maps.newHashMap();\r\n    ret.partial = Maps.newHashMap();\r\n    for (Map.Entry<JobId, Job> entry : mocked.entrySet()) {\r\n        JobId id = entry.getKey();\r\n        Job j = entry.getValue();\r\n        MockCompletedJob mockJob = new MockCompletedJob(j);\r\n        ret.full.put(id, mockJob);\r\n        JobReport report = mockJob.getReport();\r\n        JobIndexInfo info = new JobIndexInfo(report.getStartTime(), report.getFinishTime(), mockJob.getUserName(), mockJob.getName(), id, mockJob.getCompletedMaps(), mockJob.getCompletedReduces(), String.valueOf(mockJob.getState()));\r\n        info.setJobStartTime(report.getStartTime());\r\n        info.setQueueName(mockJob.getQueueName());\r\n        ret.partial.put(id, new PartialJob(info, id));\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobs",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobs() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getPartialJob(MRApps.toJobID(info.getString(\"id\")));\r\n    VerifyJobsUtils.verifyHsJobPartial(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsSlash",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobsSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getPartialJob(MRApps.toJobID(info.getString(\"id\")));\r\n    VerifyJobsUtils.verifyHsJobPartial(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsDefault",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobsDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getPartialJob(MRApps.toJobID(info.getString(\"id\")));\r\n    VerifyJobsUtils.verifyHsJobPartial(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobsXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    String xml = response.getEntity(String.class);\r\n    DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n    DocumentBuilder db = dbf.newDocumentBuilder();\r\n    InputSource is = new InputSource();\r\n    is.setCharacterStream(new StringReader(xml));\r\n    Document dom = db.parse(is);\r\n    NodeList jobs = dom.getElementsByTagName(\"jobs\");\r\n    assertEquals(\"incorrect number of elements\", 1, jobs.getLength());\r\n    NodeList job = dom.getElementsByTagName(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 1, job.getLength());\r\n    verifyHsJobPartialXML(job, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobPartialXML",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyHsJobPartialXML(NodeList nodes, MockHistoryContext appContext)\n{\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        Job job = appContext.getPartialJob(MRApps.toJobID(WebServicesTestUtils.getXmlString(element, \"id\")));\r\n        assertNotNull(\"Job not found - output incorrect\", job);\r\n        VerifyJobsUtils.verifyHsJobGeneric(job, WebServicesTestUtils.getXmlString(element, \"id\"), WebServicesTestUtils.getXmlString(element, \"user\"), WebServicesTestUtils.getXmlString(element, \"name\"), WebServicesTestUtils.getXmlString(element, \"state\"), WebServicesTestUtils.getXmlString(element, \"queue\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlLong(element, \"finishTime\"), WebServicesTestUtils.getXmlInt(element, \"mapsTotal\"), WebServicesTestUtils.getXmlInt(element, \"mapsCompleted\"), WebServicesTestUtils.getXmlInt(element, \"reducesTotal\"), WebServicesTestUtils.getXmlInt(element, \"reducesCompleted\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobXML",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyHsJobXML(NodeList nodes, AppContext appContext)\n{\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        Job job = appContext.getJob(MRApps.toJobID(WebServicesTestUtils.getXmlString(element, \"id\")));\r\n        assertNotNull(\"Job not found - output incorrect\", job);\r\n        VerifyJobsUtils.verifyHsJobGeneric(job, WebServicesTestUtils.getXmlString(element, \"id\"), WebServicesTestUtils.getXmlString(element, \"user\"), WebServicesTestUtils.getXmlString(element, \"name\"), WebServicesTestUtils.getXmlString(element, \"state\"), WebServicesTestUtils.getXmlString(element, \"queue\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlLong(element, \"finishTime\"), WebServicesTestUtils.getXmlInt(element, \"mapsTotal\"), WebServicesTestUtils.getXmlInt(element, \"mapsCompleted\"), WebServicesTestUtils.getXmlInt(element, \"reducesTotal\"), WebServicesTestUtils.getXmlInt(element, \"reducesCompleted\"));\r\n        VerifyJobsUtils.verifyHsJobGenericSecure(job, WebServicesTestUtils.getXmlBoolean(element, \"uberized\"), WebServicesTestUtils.getXmlString(element, \"diagnostics\"), WebServicesTestUtils.getXmlLong(element, \"avgMapTime\"), WebServicesTestUtils.getXmlLong(element, \"avgReduceTime\"), WebServicesTestUtils.getXmlLong(element, \"avgShuffleTime\"), WebServicesTestUtils.getXmlLong(element, \"avgMergeTime\"), WebServicesTestUtils.getXmlInt(element, \"failedReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"killedReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"successfulReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"failedMapAttempts\"), WebServicesTestUtils.getXmlInt(element, \"killedMapAttempts\"), WebServicesTestUtils.getXmlInt(element, \"successfulMapAttempts\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobId",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobId() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"job\");\r\n        VerifyJobsUtils.verifyHsJob(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobIdSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId + \"/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"job\");\r\n        VerifyJobsUtils.verifyHsJob(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobIdDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"job\");\r\n        VerifyJobsUtils.verifyHsJob(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdNonExist",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobIdNonExist() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(\"job_0_1234\").get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: job, job_0_1234, is not found\", message);\r\n        WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n        WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdInvalid",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobIdInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(\"job_foo\").accept(MediaType.APPLICATION_JSON).get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        verifyJobIdInvalid(message, type, classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdInvalidDefault",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobIdInvalidDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(\"job_foo\").get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        verifyJobIdInvalid(message, type, classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdInvalidXML",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testJobIdInvalidXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(\"job_foo\").accept(MediaType.APPLICATION_XML).get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String msg = response.getEntity(String.class);\r\n        System.out.println(msg);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(msg));\r\n        Document dom = db.parse(is);\r\n        NodeList nodes = dom.getElementsByTagName(\"RemoteException\");\r\n        Element element = (Element) nodes.item(0);\r\n        String message = WebServicesTestUtils.getXmlString(element, \"message\");\r\n        String type = WebServicesTestUtils.getXmlString(element, \"exception\");\r\n        String classname = WebServicesTestUtils.getXmlString(element, \"javaClassName\");\r\n        verifyJobIdInvalid(message, type, classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyJobIdInvalid",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyJobIdInvalid(String message, String type, String classname)\n{\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: JobId string : job_foo is not properly formed\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdInvalidBogus",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobIdInvalidBogus() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(\"bogusfoo\").get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: JobId string : \" + \"bogusfoo is not properly formed\", message);\r\n        WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n        WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobIdXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobIdXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList job = dom.getElementsByTagName(\"job\");\r\n        verifyHsJobXML(job, appContext);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobCounters",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobCounters\");\r\n        verifyHsJobCounters(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobCountersSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobCountersSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobCounters\");\r\n        verifyHsJobCounters(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobCountersForKilledJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testJobCountersForKilledJob() throws Exception\n{\r\n    WebResource r = resource();\r\n    appContext = new MockHistoryContext(0, 1, 1, 1, true);\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new ServletModule() {\r\n\r\n        @Override\r\n        protected void configureServlets() {\r\n            webApp = mock(HsWebApp.class);\r\n            when(webApp.name()).thenReturn(\"hsmockwebapp\");\r\n            bind(JAXBContextResolver.class);\r\n            bind(HsWebServices.class);\r\n            bind(GenericExceptionHandler.class);\r\n            bind(WebApp.class).toInstance(webApp);\r\n            bind(AppContext.class).toInstance(appContext);\r\n            bind(HistoryContext.class).toInstance(appContext);\r\n            bind(Configuration.class).toInstance(conf);\r\n            bind(ApplicationClientProtocol.class).toProvider(Providers.of(null));\r\n            serve(\"/*\").with(GuiceContainer.class);\r\n        }\r\n    }));\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobCounters\");\r\n        WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(id), info.getString(\"id\"));\r\n        assertTrue(\"Job shouldn't contain any counters\", info.length() == 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobCountersDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobCountersDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters/\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobCounters\");\r\n        verifyHsJobCounters(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobCountersXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobCountersXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList info = dom.getElementsByTagName(\"jobCounters\");\r\n        verifyHsJobCountersXML(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void verifyHsJobCounters(JSONObject info, Job job) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(job.getID()), info.getString(\"id\"));\r\n    JSONArray counterGroups = info.getJSONArray(\"counterGroup\");\r\n    for (int i = 0; i < counterGroups.length(); i++) {\r\n        JSONObject counterGroup = counterGroups.getJSONObject(i);\r\n        String name = counterGroup.getString(\"counterGroupName\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        JSONArray counters = counterGroup.getJSONArray(\"counter\");\r\n        for (int j = 0; j < counters.length(); j++) {\r\n            JSONObject counter = counters.getJSONObject(j);\r\n            String counterName = counter.getString(\"name\");\r\n            assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n            long mapValue = counter.getLong(\"mapCounterValue\");\r\n            assertTrue(\"mapCounterValue  >= 0\", mapValue >= 0);\r\n            long reduceValue = counter.getLong(\"reduceCounterValue\");\r\n            assertTrue(\"reduceCounterValue  >= 0\", reduceValue >= 0);\r\n            long totalValue = counter.getLong(\"totalCounterValue\");\r\n            assertTrue(\"totalCounterValue  >= 0\", totalValue >= 0);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobCountersXML",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void verifyHsJobCountersXML(NodeList nodes, Job job)\n{\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        assertNotNull(\"Job not found - output incorrect\", job);\r\n        WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(job.getID()), WebServicesTestUtils.getXmlString(element, \"id\"));\r\n        NodeList groups = element.getElementsByTagName(\"counterGroup\");\r\n        for (int j = 0; j < groups.getLength(); j++) {\r\n            Element counters = (Element) groups.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", counters);\r\n            String name = WebServicesTestUtils.getXmlString(counters, \"counterGroupName\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            NodeList counterArr = counters.getElementsByTagName(\"counter\");\r\n            for (int z = 0; z < counterArr.getLength(); z++) {\r\n                Element counter = (Element) counterArr.item(z);\r\n                String counterName = WebServicesTestUtils.getXmlString(counter, \"name\");\r\n                assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n                long mapValue = WebServicesTestUtils.getXmlLong(counter, \"mapCounterValue\");\r\n                assertTrue(\"mapCounterValue not >= 0\", mapValue >= 0);\r\n                long reduceValue = WebServicesTestUtils.getXmlLong(counter, \"reduceCounterValue\");\r\n                assertTrue(\"reduceCounterValue  >= 0\", reduceValue >= 0);\r\n                long totalValue = WebServicesTestUtils.getXmlLong(counter, \"totalCounterValue\");\r\n                assertTrue(\"totalCounterValue  >= 0\", totalValue >= 0);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobAttempts",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobAttempts() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobAttempts\");\r\n        verifyHsJobAttempts(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobAttemptsSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobAttemptsSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobAttempts\");\r\n        verifyHsJobAttempts(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobAttemptsDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobAttemptsDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobAttempts\");\r\n        verifyHsJobAttempts(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobAttemptsXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList attempts = dom.getElementsByTagName(\"jobAttempts\");\r\n        assertEquals(\"incorrect number of elements\", 1, attempts.getLength());\r\n        NodeList info = dom.getElementsByTagName(\"jobAttempt\");\r\n        verifyHsJobAttemptsXML(info, appContext.getJob(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobAttempts",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyHsJobAttempts(JSONObject info, Job job) throws JSONException\n{\r\n    JSONArray attempts = info.getJSONArray(\"jobAttempt\");\r\n    assertEquals(\"incorrect number of elements\", 2, attempts.length());\r\n    for (int i = 0; i < attempts.length(); i++) {\r\n        JSONObject attempt = attempts.getJSONObject(i);\r\n        verifyHsJobAttemptsGeneric(job, attempt.getString(\"nodeHttpAddress\"), attempt.getString(\"nodeId\"), attempt.getInt(\"id\"), attempt.getLong(\"startTime\"), attempt.getString(\"containerId\"), attempt.getString(\"logsLink\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyHsJobAttemptsXML(NodeList nodes, Job job)\n{\r\n    assertEquals(\"incorrect number of elements\", 2, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        verifyHsJobAttemptsGeneric(job, WebServicesTestUtils.getXmlString(element, \"nodeHttpAddress\"), WebServicesTestUtils.getXmlString(element, \"nodeId\"), WebServicesTestUtils.getXmlInt(element, \"id\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlString(element, \"containerId\"), WebServicesTestUtils.getXmlString(element, \"logsLink\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobAttemptsGeneric",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void verifyHsJobAttemptsGeneric(Job job, String nodeHttpAddress, String nodeId, int id, long startTime, String containerId, String logsLink)\n{\r\n    boolean attemptFound = false;\r\n    for (AMInfo amInfo : job.getAMInfos()) {\r\n        if (amInfo.getAppAttemptId().getAttemptId() == id) {\r\n            attemptFound = true;\r\n            String nmHost = amInfo.getNodeManagerHost();\r\n            int nmHttpPort = amInfo.getNodeManagerHttpPort();\r\n            int nmPort = amInfo.getNodeManagerPort();\r\n            WebServicesTestUtils.checkStringMatch(\"nodeHttpAddress\", nmHost + \":\" + nmHttpPort, nodeHttpAddress);\r\n            assertTrue(\"startime not greater than 0\", startTime > 0);\r\n            WebServicesTestUtils.checkStringMatch(\"containerId\", amInfo.getContainerId().toString(), containerId);\r\n            String localLogsLink = join(\"hsmockwebapp\", ujoin(\"logs\", nodeId, containerId, MRApps.toString(job.getID()), job.getUserName()));\r\n            assertTrue(\"logsLink\", logsLink.contains(localLogsLink));\r\n        }\r\n    }\r\n    assertTrue(\"attempt: \" + id + \" was not found\", attemptFound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobPartial",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyHsJobPartial(JSONObject info, Job job) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 12, info.length());\r\n    verifyHsJobGeneric(job, info.getString(\"id\"), info.getString(\"user\"), info.getString(\"name\"), info.getString(\"state\"), info.getString(\"queue\"), info.getLong(\"startTime\"), info.getLong(\"finishTime\"), info.getInt(\"mapsTotal\"), info.getInt(\"mapsCompleted\"), info.getInt(\"reducesTotal\"), info.getInt(\"reducesCompleted\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyHsJob(JSONObject info, Job job) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 25, info.length());\r\n    verifyHsJobGeneric(job, info.getString(\"id\"), info.getString(\"user\"), info.getString(\"name\"), info.getString(\"state\"), info.getString(\"queue\"), info.getLong(\"startTime\"), info.getLong(\"finishTime\"), info.getInt(\"mapsTotal\"), info.getInt(\"mapsCompleted\"), info.getInt(\"reducesTotal\"), info.getInt(\"reducesCompleted\"));\r\n    String diagnostics = \"\";\r\n    if (info.has(\"diagnostics\")) {\r\n        diagnostics = info.getString(\"diagnostics\");\r\n    }\r\n    verifyHsJobGenericSecure(job, info.getBoolean(\"uberized\"), diagnostics, info.getLong(\"avgMapTime\"), info.getLong(\"avgReduceTime\"), info.getLong(\"avgShuffleTime\"), info.getLong(\"avgMergeTime\"), info.getInt(\"failedReduceAttempts\"), info.getInt(\"killedReduceAttempts\"), info.getInt(\"successfulReduceAttempts\"), info.getInt(\"failedMapAttempts\"), info.getInt(\"killedMapAttempts\"), info.getInt(\"successfulMapAttempts\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobGeneric",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyHsJobGeneric(Job job, String id, String user, String name, String state, String queue, long startTime, long finishTime, int mapsTotal, int mapsCompleted, int reducesTotal, int reducesCompleted)\n{\r\n    JobReport report = job.getReport();\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(job.getID()), id);\r\n    WebServicesTestUtils.checkStringMatch(\"user\", job.getUserName().toString(), user);\r\n    WebServicesTestUtils.checkStringMatch(\"name\", job.getName(), name);\r\n    WebServicesTestUtils.checkStringMatch(\"state\", job.getState().toString(), state);\r\n    WebServicesTestUtils.checkStringMatch(\"queue\", job.getQueueName(), queue);\r\n    assertEquals(\"startTime incorrect\", report.getStartTime(), startTime);\r\n    assertEquals(\"finishTime incorrect\", report.getFinishTime(), finishTime);\r\n    assertEquals(\"mapsTotal incorrect\", job.getTotalMaps(), mapsTotal);\r\n    assertEquals(\"mapsCompleted incorrect\", job.getCompletedMaps(), mapsCompleted);\r\n    assertEquals(\"reducesTotal incorrect\", job.getTotalReduces(), reducesTotal);\r\n    assertEquals(\"reducesCompleted incorrect\", job.getCompletedReduces(), reducesCompleted);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobGenericSecure",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void verifyHsJobGenericSecure(Job job, Boolean uberized, String diagnostics, long avgMapTime, long avgReduceTime, long avgShuffleTime, long avgMergeTime, int failedReduceAttempts, int killedReduceAttempts, int successfulReduceAttempts, int failedMapAttempts, int killedMapAttempts, int successfulMapAttempts)\n{\r\n    String diagString = \"\";\r\n    List<String> diagList = job.getDiagnostics();\r\n    if (diagList != null && !diagList.isEmpty()) {\r\n        StringBuffer b = new StringBuffer();\r\n        for (String diag : diagList) {\r\n            b.append(diag);\r\n        }\r\n        diagString = b.toString();\r\n    }\r\n    WebServicesTestUtils.checkStringMatch(\"diagnostics\", diagString, diagnostics);\r\n    assertEquals(\"isUber incorrect\", job.isUber(), uberized);\r\n    assertTrue(\"failedReduceAttempts not >= 0\", failedReduceAttempts >= 0);\r\n    assertTrue(\"killedReduceAttempts not >= 0\", killedReduceAttempts >= 0);\r\n    assertTrue(\"successfulReduceAttempts not >= 0\", successfulReduceAttempts >= 0);\r\n    assertTrue(\"failedMapAttempts not >= 0\", failedMapAttempts >= 0);\r\n    assertTrue(\"killedMapAttempts not >= 0\", killedMapAttempts >= 0);\r\n    assertTrue(\"successfulMapAttempts not >= 0\", successfulMapAttempts >= 0);\r\n    assertTrue(\"avgMapTime not >= 0\", avgMapTime >= 0);\r\n    assertTrue(\"avgReduceTime not >= 0\", avgReduceTime >= 0);\r\n    assertTrue(\"avgShuffleTime not >= 0\", avgShuffleTime >= 0);\r\n    assertTrue(\"avgMergeTime not >= 0\", avgMergeTime >= 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    if (USER_DIR.exists()) {\r\n        FileUtils.cleanDirectory(USER_DIR);\r\n    }\r\n    USER_DIR.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    FileUtils.deleteDirectory(INTERMEDIATE_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTwoThreadsQueryingDifferentJobOfSameUser",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTwoThreadsQueryingDifferentJobOfSameUser() throws InterruptedException, IOException\n{\r\n    final Configuration config = new Configuration();\r\n    config.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, INTERMEDIATE_DIR.getPath());\r\n    config.setLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS, Long.MAX_VALUE);\r\n    final JobId job1 = createJobId(0);\r\n    final JobId job2 = createJobId(1);\r\n    final HistoryFileManagerUnderContention historyFileManager = createHistoryFileManager(config, job1, job2);\r\n    Thread webRequest1 = null;\r\n    Thread webRequest2 = null;\r\n    try {\r\n        createJhistFile(job1);\r\n        webRequest1 = new Thread(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    HistoryFileManager.HistoryFileInfo historyFileInfo = historyFileManager.getFileInfo(job1);\r\n                    historyFileInfo.loadJob();\r\n                } catch (IOException e) {\r\n                    e.printStackTrace();\r\n                }\r\n            }\r\n        });\r\n        webRequest1.start();\r\n        historyFileManager.waitUntilIntermediateDirIsScanned(job1);\r\n        createJhistFile(job2);\r\n        webRequest2 = new Thread(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    HistoryFileManager.HistoryFileInfo historyFileInfo = historyFileManager.getFileInfo(job2);\r\n                    historyFileInfo.loadJob();\r\n                } catch (IOException e) {\r\n                    e.printStackTrace();\r\n                }\r\n            }\r\n        });\r\n        webRequest2.start();\r\n        historyFileManager.waitUntilIntermediateDirIsScanned(job2);\r\n        Assert.assertTrue(\"Thread 2 is blocked while it is trying to \" + \"load job2 by Thread 1 which is loading job1.\", webRequest2.getState() != Thread.State.BLOCKED);\r\n    } finally {\r\n        if (webRequest1 != null) {\r\n            webRequest1.interrupt();\r\n        }\r\n        if (webRequest2 != null) {\r\n            webRequest2.interrupt();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHistoryFileManager",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HistoryFileManagerUnderContention createHistoryFileManager(Configuration config, JobId... jobIds)\n{\r\n    HistoryFileManagerUnderContention historyFileManager = new HistoryFileManagerUnderContention(jobIds);\r\n    historyFileManager.init(config);\r\n    historyFileManager.start();\r\n    return historyFileManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHistoryStorage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CachedHistoryStorage createHistoryStorage(Configuration config, HistoryFileManager historyFileManager)\n{\r\n    CachedHistoryStorage historyStorage = new CachedHistoryStorage();\r\n    historyStorage.setHistoryFileManager(historyFileManager);\r\n    historyStorage.init(config);\r\n    historyStorage.start();\r\n    return historyStorage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createJobId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobId createJobId(int id)\n{\r\n    JobId jobId = new JobIdPBImpl();\r\n    jobId.setId(id);\r\n    jobId.setAppId(ApplicationIdPBImpl.newInstance(0, id));\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createJhistFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean createJhistFile(JobId jobId) throws IOException\n{\r\n    StringBuilder fileName = new StringBuilder(jobId.toString());\r\n    long finishTime = System.currentTimeMillis();\r\n    fileName.append(\"-\").append(finishTime - 1000).append(\"-\").append(\"test\").append(\"-\").append(jobId.getId()).append(\"-\").append(finishTime).append(\".jhist\");\r\n    File jhistFile = new File(USER_DIR, fileName.toString());\r\n    return jhistFile.createNewFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testKeepRetryingWhileNameNodeInSafeMode",
  "errType" : [ "YarnRuntimeException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testKeepRetryingWhileNameNodeInSafeMode() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    final long maxJhsWaitTime = 500;\r\n    conf.setLong(JHAdminConfig.MR_HISTORY_MAX_START_WAIT_TIME, maxJhsWaitTime);\r\n    conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, CLUSTER_BASE_DIR);\r\n    MiniDFSCluster dfsCluster = new MiniDFSCluster.Builder(conf).build();\r\n    try {\r\n        dfsCluster.getFileSystem().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);\r\n        Assert.assertTrue(dfsCluster.getFileSystem().isInSafeMode());\r\n        HistoryFileManager hfm = new HistoryFileManager();\r\n        hfm.serviceInit(conf);\r\n        Assert.fail(\"History File Manager did not retry to connect to name node\");\r\n    } catch (YarnRuntimeException yex) {\r\n        String expectedExceptionMsg = \"Timed out '\" + maxJhsWaitTime + \"ms' waiting for FileSystem to become available\";\r\n        Assert.assertEquals(\"Unexpected reconnect timeout exception message\", expectedExceptionMsg, yex.getMessage());\r\n    } finally {\r\n        dfsCluster.shutdown(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testAppControllerIndex",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAppControllerIndex()\n{\r\n    MockAppContext ctx = new MockAppContext(0, 1, 1, 1);\r\n    Injector injector = WebAppTests.createMockInjector(AppContext.class, ctx);\r\n    HsController controller = injector.getInstance(HsController.class);\r\n    controller.index();\r\n    assertEquals(ctx.getApplicationID().toString(), controller.get(APP_ID, \"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobView",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJobView()\n{\r\n    LOG.info(\"HsJobPage\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = TestAMWebApp.getJobParams(appContext);\r\n    WebAppTests.testPage(HsJobPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksView",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testTasksView()\n{\r\n    LOG.info(\"HsTasksPage\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = TestAMWebApp.getTaskParams(appContext);\r\n    WebAppTests.testPage(HsTasksPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksViewNaturalSortType",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTasksViewNaturalSortType()\n{\r\n    LOG.info(\"HsTasksPage\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = TestAMWebApp.getTaskParams(appContext);\r\n    Injector testPage = WebAppTests.testPage(HsTasksPage.class, AppContext.class, appContext, params);\r\n    View viewInstance = testPage.getInstance(HsTasksPage.class);\r\n    Map<String, String> moreParams = viewInstance.context().requestContext().moreParams();\r\n    String appTableColumnsMeta = moreParams.get(\"ui.dataTables.selector.init\");\r\n    Assert.assertTrue(appTableColumnsMeta.indexOf(\"natural\") != -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskView",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testTaskView()\n{\r\n    LOG.info(\"HsTaskPage\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = TestAMWebApp.getTaskParams(appContext);\r\n    WebAppTests.testPage(HsTaskPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskViewNaturalSortType",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTaskViewNaturalSortType()\n{\r\n    LOG.info(\"HsTaskPage\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = TestAMWebApp.getTaskParams(appContext);\r\n    Injector testPage = WebAppTests.testPage(HsTaskPage.class, AppContext.class, appContext, params);\r\n    View viewInstance = testPage.getInstance(HsTaskPage.class);\r\n    Map<String, String> moreParams = viewInstance.context().requestContext().moreParams();\r\n    String appTableColumnsMeta = moreParams.get(\"ui.dataTables.attempts.init\");\r\n    Assert.assertTrue(appTableColumnsMeta.indexOf(\"natural\") != -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testAttemptsWithJobView",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAttemptsWithJobView()\n{\r\n    LOG.info(\"HsAttemptsPage with data\");\r\n    MockAppContext ctx = new MockAppContext(0, 1, 1, 1);\r\n    JobId id = ctx.getAllJobs().keySet().iterator().next();\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(JOB_ID, id.toString());\r\n    params.put(TASK_TYPE, \"m\");\r\n    params.put(ATTEMPT_STATE, \"SUCCESSFUL\");\r\n    WebAppTests.testPage(HsAttemptsPage.class, AppContext.class, ctx, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testAttemptsView",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAttemptsView()\n{\r\n    LOG.info(\"HsAttemptsPage\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = TestAMWebApp.getTaskParams(appContext);\r\n    WebAppTests.testPage(HsAttemptsPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testConfView",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConfView()\n{\r\n    LOG.info(\"HsConfPage\");\r\n    WebAppTests.testPage(HsConfPage.class, AppContext.class, new MockAppContext(0, 1, 1, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testAboutView",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAboutView()\n{\r\n    LOG.info(\"HsAboutPage\");\r\n    WebAppTests.testPage(HsAboutPage.class, AppContext.class, new MockAppContext(0, 1, 1, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobCounterView",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJobCounterView()\n{\r\n    LOG.info(\"JobCounterView\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = TestAMWebApp.getJobParams(appContext);\r\n    WebAppTests.testPage(HsCountersPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobCounterViewForKilledJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJobCounterViewForKilledJob()\n{\r\n    LOG.info(\"JobCounterViewForKilledJob\");\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1, true);\r\n    Map<String, String> params = TestAMWebApp.getJobParams(appContext);\r\n    WebAppTests.testPage(HsCountersPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testSingleCounterView",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleCounterView()\n{\r\n    LOG.info(\"HsSingleCounterPage\");\r\n    WebAppTests.testPage(HsSingleCounterPage.class, AppContext.class, new MockAppContext(0, 1, 1, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testLogsView1",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testLogsView1() throws IOException\n{\r\n    LOG.info(\"HsLogsPage\");\r\n    Injector injector = WebAppTests.testPage(AggregatedLogsPage.class, AppContext.class, new MockAppContext(0, 1, 1, 1));\r\n    PrintWriter spyPw = WebAppTests.getPrintWriter(injector);\r\n    verify(spyPw).write(\"Cannot get container logs without a ContainerId\");\r\n    verify(spyPw).write(\"Cannot get container logs without a NodeId\");\r\n    verify(spyPw).write(\"Cannot get container logs without an app owner\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testLogsView2",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testLogsView2() throws IOException\n{\r\n    LOG.info(\"HsLogsPage with data\");\r\n    MockAppContext ctx = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(CONTAINER_ID, MRApp.newContainerId(1, 1, 333, 1).toString());\r\n    params.put(NM_NODENAME, NodeId.newInstance(MockJobs.NM_HOST, MockJobs.NM_PORT).toString());\r\n    params.put(ENTITY_STRING, \"container_10_0001_01_000001\");\r\n    params.put(APP_OWNER, \"owner\");\r\n    Injector injector = WebAppTests.testPage(AggregatedLogsPage.class, AppContext.class, ctx, params);\r\n    PrintWriter spyPw = WebAppTests.getPrintWriter(injector);\r\n    verify(spyPw).write(\"Aggregation is not enabled. Try the nodemanager at \" + MockJobs.NM_HOST + \":\" + MockJobs.NM_PORT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testLogsViewSingle",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testLogsViewSingle() throws IOException\n{\r\n    LOG.info(\"HsLogsPage with params for single log and data limits\");\r\n    MockAppContext ctx = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    final Configuration conf = new YarnConfiguration();\r\n    conf.setBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED, true);\r\n    params.put(\"start\", \"-2048\");\r\n    params.put(\"end\", \"-1024\");\r\n    params.put(CONTAINER_LOG_TYPE, \"syslog\");\r\n    params.put(CONTAINER_ID, MRApp.newContainerId(1, 1, 333, 1).toString());\r\n    params.put(NM_NODENAME, NodeId.newInstance(MockJobs.NM_HOST, MockJobs.NM_PORT).toString());\r\n    params.put(ENTITY_STRING, \"container_10_0001_01_000001\");\r\n    params.put(APP_OWNER, \"owner\");\r\n    Injector injector = WebAppTests.testPage(AggregatedLogsPage.class, AppContext.class, ctx, params, new AbstractModule() {\r\n\r\n        @Override\r\n        protected void configure() {\r\n            bind(Configuration.class).toInstance(conf);\r\n        }\r\n    });\r\n    PrintWriter spyPw = WebAppTests.getPrintWriter(injector);\r\n    verify(spyPw).write(\"Logs not available for container_10_0001_01_000001.\" + \" Aggregation may not be complete, Check back later or try to\" + \" find the container logs in the local directory of nodemanager \" + MockJobs.NM_HOST + \":\" + MockJobs.NM_PORT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testLogsViewBadStartEnd",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testLogsViewBadStartEnd() throws IOException\n{\r\n    LOG.info(\"HsLogsPage with bad start/end params\");\r\n    MockAppContext ctx = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(\"start\", \"foo\");\r\n    params.put(\"end\", \"bar\");\r\n    params.put(CONTAINER_ID, MRApp.newContainerId(1, 1, 333, 1).toString());\r\n    params.put(NM_NODENAME, NodeId.newInstance(MockJobs.NM_HOST, MockJobs.NM_PORT).toString());\r\n    params.put(ENTITY_STRING, \"container_10_0001_01_000001\");\r\n    params.put(APP_OWNER, \"owner\");\r\n    Injector injector = WebAppTests.testPage(AggregatedLogsPage.class, AppContext.class, ctx, params);\r\n    PrintWriter spyPw = WebAppTests.getPrintWriter(injector);\r\n    verify(spyPw).write(\"Invalid log start value: foo\");\r\n    verify(spyPw).write(\"Invalid log end value: bar\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    List<Object[]> list = new ArrayList<Object[]>(2);\r\n    list.add(new Object[] { true });\r\n    list.add(new Object[] { false });\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCompletedJob",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCompletedJob() throws Exception\n{\r\n    HistoryFileInfo info = mock(HistoryFileInfo.class);\r\n    when(info.getConfFile()).thenReturn(fullConfPath);\r\n    when(info.getHistoryFile()).thenReturn(fullHistoryPath);\r\n    completedJob = new CompletedJob(conf, jobId, fullHistoryPath, loadTasks, \"user\", info, jobAclsManager);\r\n    assertEquals(loadTasks, completedJob.tasksLoaded.get());\r\n    assertEquals(1, completedJob.getAMInfos().size());\r\n    assertEquals(10, completedJob.getCompletedMaps());\r\n    assertEquals(1, completedJob.getCompletedReduces());\r\n    assertEquals(12, completedJob.getTasks().size());\r\n    assertThat(completedJob.tasksLoaded.get()).isTrue();\r\n    assertEquals(10, completedJob.getTasks(TaskType.MAP).size());\r\n    assertEquals(2, completedJob.getTasks(TaskType.REDUCE).size());\r\n    assertEquals(\"user\", completedJob.getUserName());\r\n    assertEquals(JobState.SUCCEEDED, completedJob.getState());\r\n    JobReport jobReport = completedJob.getReport();\r\n    assertEquals(\"user\", jobReport.getUser());\r\n    assertEquals(JobState.SUCCEEDED, jobReport.getJobState());\r\n    assertEquals(fullHistoryPath.toString(), jobReport.getHistoryFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCopmletedJobReportWithZeroTasks",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCopmletedJobReportWithZeroTasks() throws Exception\n{\r\n    HistoryFileInfo info = mock(HistoryFileInfo.class);\r\n    when(info.getConfFile()).thenReturn(fullConfPath);\r\n    when(info.getHistoryFile()).thenReturn(fullHistoryPathZeroReduces);\r\n    completedJob = new CompletedJob(conf, jobId, fullHistoryPathZeroReduces, loadTasks, \"user\", info, jobAclsManager);\r\n    JobReport jobReport = completedJob.getReport();\r\n    assertEquals(0, completedJob.getTotalReduces());\r\n    assertEquals(0, completedJob.getCompletedReduces());\r\n    assertEquals(1.0, jobReport.getReduceProgress(), 0.001);\r\n    assertEquals(fullHistoryPathZeroReduces.toString(), jobReport.getHistoryFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCompletedTask",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testCompletedTask() throws Exception\n{\r\n    HistoryFileInfo info = mock(HistoryFileInfo.class);\r\n    when(info.getConfFile()).thenReturn(fullConfPath);\r\n    completedJob = new CompletedJob(conf, jobId, fullHistoryPath, loadTasks, \"user\", info, jobAclsManager);\r\n    TaskId mt1Id = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\r\n    TaskId rt1Id = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\r\n    Map<TaskId, Task> mapTasks = completedJob.getTasks(TaskType.MAP);\r\n    Map<TaskId, Task> reduceTasks = completedJob.getTasks(TaskType.REDUCE);\r\n    assertEquals(10, mapTasks.size());\r\n    assertEquals(2, reduceTasks.size());\r\n    Task mt1 = mapTasks.get(mt1Id);\r\n    assertEquals(1, mt1.getAttempts().size());\r\n    assertEquals(TaskState.SUCCEEDED, mt1.getState());\r\n    TaskReport mt1Report = mt1.getReport();\r\n    assertEquals(TaskState.SUCCEEDED, mt1Report.getTaskState());\r\n    assertEquals(mt1Id, mt1Report.getTaskId());\r\n    Task rt1 = reduceTasks.get(rt1Id);\r\n    assertEquals(1, rt1.getAttempts().size());\r\n    assertEquals(TaskState.SUCCEEDED, rt1.getState());\r\n    TaskReport rt1Report = rt1.getReport();\r\n    assertEquals(TaskState.SUCCEEDED, rt1Report.getTaskState());\r\n    assertEquals(rt1Id, rt1Report.getTaskId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCompletedTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testCompletedTaskAttempt() throws Exception\n{\r\n    HistoryFileInfo info = mock(HistoryFileInfo.class);\r\n    when(info.getConfFile()).thenReturn(fullConfPath);\r\n    completedJob = new CompletedJob(conf, jobId, fullHistoryPath, loadTasks, \"user\", info, jobAclsManager);\r\n    TaskId mt1Id = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\r\n    TaskId rt1Id = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\r\n    TaskAttemptId mta1Id = MRBuilderUtils.newTaskAttemptId(mt1Id, 0);\r\n    TaskAttemptId rta1Id = MRBuilderUtils.newTaskAttemptId(rt1Id, 0);\r\n    Task mt1 = completedJob.getTask(mt1Id);\r\n    Task rt1 = completedJob.getTask(rt1Id);\r\n    TaskAttempt mta1 = mt1.getAttempt(mta1Id);\r\n    assertEquals(TaskAttemptState.SUCCEEDED, mta1.getState());\r\n    assertEquals(\"localhost:45454\", mta1.getAssignedContainerMgrAddress());\r\n    assertEquals(\"localhost:9999\", mta1.getNodeHttpAddress());\r\n    TaskAttemptReport mta1Report = mta1.getReport();\r\n    assertEquals(TaskAttemptState.SUCCEEDED, mta1Report.getTaskAttemptState());\r\n    assertEquals(\"localhost\", mta1Report.getNodeManagerHost());\r\n    assertEquals(45454, mta1Report.getNodeManagerPort());\r\n    assertEquals(9999, mta1Report.getNodeManagerHttpPort());\r\n    TaskAttempt rta1 = rt1.getAttempt(rta1Id);\r\n    assertEquals(TaskAttemptState.SUCCEEDED, rta1.getState());\r\n    assertEquals(\"localhost:45454\", rta1.getAssignedContainerMgrAddress());\r\n    assertEquals(\"localhost:9999\", rta1.getNodeHttpAddress());\r\n    TaskAttemptReport rta1Report = rta1.getReport();\r\n    assertEquals(TaskAttemptState.SUCCEEDED, rta1Report.getTaskAttemptState());\r\n    assertEquals(\"localhost\", rta1Report.getNodeManagerHost());\r\n    assertEquals(45454, rta1Report.getNodeManagerPort());\r\n    assertEquals(9999, rta1Report.getNodeManagerHttpPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testGetTaskAttemptCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testGetTaskAttemptCompletionEvent() throws Exception\n{\r\n    HistoryFileInfo info = mock(HistoryFileInfo.class);\r\n    when(info.getConfFile()).thenReturn(fullConfPath);\r\n    completedJob = new CompletedJob(conf, jobId, fullHistoryPath, loadTasks, \"user\", info, jobAclsManager);\r\n    TaskCompletionEvent[] events = completedJob.getMapAttemptCompletionEvents(0, 1000);\r\n    assertEquals(10, completedJob.getMapAttemptCompletionEvents(0, 10).length);\r\n    int currentEventId = 0;\r\n    for (TaskCompletionEvent taskAttemptCompletionEvent : events) {\r\n        int eventId = taskAttemptCompletionEvent.getEventId();\r\n        assertTrue(eventId >= currentEventId);\r\n        currentEventId = eventId;\r\n    }\r\n    assertNull(completedJob.loadConfFile());\r\n    assertEquals(\"Sleep job\", completedJob.getName());\r\n    assertEquals(\"default\", completedJob.getQueueName());\r\n    assertEquals(1.0, completedJob.getProgress(), 0.001);\r\n    assertEquals(12, completedJob.getTaskAttemptCompletionEvents(0, 1000).length);\r\n    assertEquals(10, completedJob.getTaskAttemptCompletionEvents(0, 10).length);\r\n    assertEquals(7, completedJob.getTaskAttemptCompletionEvents(5, 10).length);\r\n    assertEquals(1, completedJob.getDiagnostics().size());\r\n    assertEquals(\"\", completedJob.getDiagnostics().get(0));\r\n    assertEquals(0, completedJob.getJobACLs().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCompletedJobWithDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCompletedJobWithDiagnostics() throws Exception\n{\r\n    final String jobError = \"Job Diagnostics\";\r\n    JobInfo jobInfo = spy(new JobInfo());\r\n    when(jobInfo.getErrorInfo()).thenReturn(jobError);\r\n    when(jobInfo.getJobStatus()).thenReturn(JobState.FAILED.toString());\r\n    when(jobInfo.getAMInfos()).thenReturn(Collections.<JobHistoryParser.AMInfo>emptyList());\r\n    final JobHistoryParser mockParser = mock(JobHistoryParser.class);\r\n    when(mockParser.parse()).thenReturn(jobInfo);\r\n    HistoryFileInfo info = mock(HistoryFileInfo.class);\r\n    when(info.getConfFile()).thenReturn(fullConfPath);\r\n    when(info.getHistoryFile()).thenReturn(fullHistoryPath);\r\n    CompletedJob job = new CompletedJob(conf, jobId, fullHistoryPath, loadTasks, \"user\", info, jobAclsManager) {\r\n\r\n        @Override\r\n        protected JobHistoryParser createJobHistoryParser(Path historyFileAbsolute) throws IOException {\r\n            return mockParser;\r\n        }\r\n    };\r\n    assertEquals(jobError, job.getReport().getDiagnostics());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasks",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasks() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 2, arr.length());\r\n        verifyHsTask(arr, jobsMap.get(id), null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 2, arr.length());\r\n        verifyHsTask(arr, jobsMap.get(id), null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksSlash",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 2, arr.length());\r\n        verifyHsTask(arr, jobsMap.get(id), null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksXML",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTasksXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList tasks = dom.getElementsByTagName(\"tasks\");\r\n        assertEquals(\"incorrect number of elements\", 1, tasks.getLength());\r\n        NodeList task = dom.getElementsByTagName(\"task\");\r\n        verifyHsTaskXML(task, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksQueryMap",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksQueryMap() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String type = \"m\";\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").queryParam(\"type\", type).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n        verifyHsTask(arr, jobsMap.get(id), type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksQueryReduce",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksQueryReduce() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String type = \"r\";\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").queryParam(\"type\", type).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n        verifyHsTask(arr, jobsMap.get(id), type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTasksQueryInvalid",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTasksQueryInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tasktype = \"reduce\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").queryParam(\"type\", tasktype).accept(MediaType.APPLICATION_JSON).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: tasktype must be either m or r\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskId",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskId() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"task\");\r\n            verifyHsSingleTask(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdSlash",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid + \"/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"task\");\r\n            verifyHsSingleTask(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"task\");\r\n            verifyHsSingleTask(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdBogus",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdBogus() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"bogustaskid\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"bogustaskid is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,11 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdNonExist",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdNonExist() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0_0000_m_000000\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: task not found with id task_0_0000_m_000000\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdInvalid",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0_0000_d_000000\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"task_0_0000_d_000000 is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,20 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdInvalid2",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdInvalid2() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0000_m_000000\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"task_0000_m_000000 is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,18 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdInvalid3",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdInvalid3() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0_0000_m\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"task_0_0000_m is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,13 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdXML",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testTaskIdXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            String xml = response.getEntity(String.class);\r\n            DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n            DocumentBuilder db = dbf.newDocumentBuilder();\r\n            InputSource is = new InputSource();\r\n            is.setCharacterStream(new StringReader(xml));\r\n            Document dom = db.parse(is);\r\n            NodeList nodes = dom.getElementsByTagName(\"task\");\r\n            for (int i = 0; i < nodes.getLength(); i++) {\r\n                Element element = (Element) nodes.item(i);\r\n                verifyHsSingleTaskXML(element, task);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsSingleTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyHsSingleTask(JSONObject info, Task task) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 9, info.length());\r\n    verifyTaskGeneric(task, info.getString(\"id\"), info.getString(\"state\"), info.getString(\"type\"), info.getString(\"successfulAttempt\"), info.getLong(\"startTime\"), info.getLong(\"finishTime\"), info.getLong(\"elapsedTime\"), (float) info.getDouble(\"progress\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTask",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyHsTask(JSONArray arr, Job job, String type) throws JSONException\n{\r\n    for (Task task : job.getTasks().values()) {\r\n        TaskId id = task.getID();\r\n        String tid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        if (type != null && task.getType() == MRApps.taskType(type)) {\r\n            for (int i = 0; i < arr.length(); i++) {\r\n                JSONObject info = arr.getJSONObject(i);\r\n                if (tid.matches(info.getString(\"id\"))) {\r\n                    found = true;\r\n                    verifyHsSingleTask(info, task);\r\n                }\r\n            }\r\n            assertTrue(\"task with id: \" + tid + \" not in web service output\", found);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyTaskGeneric",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void verifyTaskGeneric(Task task, String id, String state, String type, String successfulAttempt, long startTime, long finishTime, long elapsedTime, float progress)\n{\r\n    TaskId taskid = task.getID();\r\n    String tid = MRApps.toString(taskid);\r\n    TaskReport report = task.getReport();\r\n    WebServicesTestUtils.checkStringMatch(\"id\", tid, id);\r\n    WebServicesTestUtils.checkStringMatch(\"type\", task.getType().toString(), type);\r\n    WebServicesTestUtils.checkStringMatch(\"state\", report.getTaskState().toString(), state);\r\n    assertNotNull(\"successfulAttempt null\", successfulAttempt);\r\n    assertEquals(\"startTime wrong\", report.getStartTime(), startTime);\r\n    assertEquals(\"finishTime wrong\", report.getFinishTime(), finishTime);\r\n    assertEquals(\"elapsedTime wrong\", finishTime - startTime, elapsedTime);\r\n    assertEquals(\"progress wrong\", report.getProgress() * 100, progress, 1e-3f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsSingleTaskXML",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyHsSingleTaskXML(Element element, Task task)\n{\r\n    verifyTaskGeneric(task, WebServicesTestUtils.getXmlString(element, \"id\"), WebServicesTestUtils.getXmlString(element, \"state\"), WebServicesTestUtils.getXmlString(element, \"type\"), WebServicesTestUtils.getXmlString(element, \"successfulAttempt\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlLong(element, \"finishTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedTime\"), WebServicesTestUtils.getXmlFloat(element, \"progress\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTaskXML",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyHsTaskXML(NodeList nodes, Job job)\n{\r\n    assertEquals(\"incorrect number of elements\", 2, nodes.getLength());\r\n    for (Task task : job.getTasks().values()) {\r\n        TaskId id = task.getID();\r\n        String tid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        for (int i = 0; i < nodes.getLength(); i++) {\r\n            Element element = (Element) nodes.item(i);\r\n            if (tid.matches(WebServicesTestUtils.getXmlString(element, \"id\"))) {\r\n                found = true;\r\n                verifyHsSingleTaskXML(element, task);\r\n            }\r\n        }\r\n        assertTrue(\"task with id: \" + tid + \" not in web service output\", found);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdCounters",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"jobTaskCounters\");\r\n            verifyHsJobTaskCounters(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdCountersSlash",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdCountersSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"jobTaskCounters\");\r\n            verifyHsJobTaskCounters(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testTaskIdCountersDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdCountersDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters\").get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"jobTaskCounters\");\r\n            verifyHsJobTaskCounters(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testJobTaskCountersXML",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobTaskCountersXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"history\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            String xml = response.getEntity(String.class);\r\n            DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n            DocumentBuilder db = dbf.newDocumentBuilder();\r\n            InputSource is = new InputSource();\r\n            is.setCharacterStream(new StringReader(xml));\r\n            Document dom = db.parse(is);\r\n            NodeList info = dom.getElementsByTagName(\"jobTaskCounters\");\r\n            verifyHsTaskCountersXML(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsJobTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyHsJobTaskCounters(JSONObject info, Task task) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(task.getID()), info.getString(\"id\"));\r\n    JSONArray counterGroups = info.getJSONArray(\"taskCounterGroup\");\r\n    for (int i = 0; i < counterGroups.length(); i++) {\r\n        JSONObject counterGroup = counterGroups.getJSONObject(i);\r\n        String name = counterGroup.getString(\"counterGroupName\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        JSONArray counters = counterGroup.getJSONArray(\"counter\");\r\n        for (int j = 0; j < counters.length(); j++) {\r\n            JSONObject counter = counters.getJSONObject(j);\r\n            String counterName = counter.getString(\"name\");\r\n            assertTrue(\"name not set\", (counterName != null && !counterName.isEmpty()));\r\n            long value = counter.getLong(\"value\");\r\n            assertTrue(\"value  >= 0\", value >= 0);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "verifyHsTaskCountersXML",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void verifyHsTaskCountersXML(NodeList nodes, Task task)\n{\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(task.getID()), WebServicesTestUtils.getXmlString(element, \"id\"));\r\n        NodeList groups = element.getElementsByTagName(\"taskCounterGroup\");\r\n        for (int j = 0; j < groups.getLength(); j++) {\r\n            Element counters = (Element) groups.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", counters);\r\n            String name = WebServicesTestUtils.getXmlString(counters, \"counterGroupName\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            NodeList counterArr = counters.getElementsByTagName(\"counter\");\r\n            for (int z = 0; z < counterArr.getLength(); z++) {\r\n                Element counter = (Element) counterArr.item(z);\r\n                String counterName = WebServicesTestUtils.getXmlString(counter, \"name\");\r\n                assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n                long value = WebServicesTestUtils.getXmlLong(counter, \"value\");\r\n                assertTrue(\"value not >= 0\", value >= 0);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testPullTaskLink",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPullTaskLink()\n{\r\n    Task task = getTask(0);\r\n    String taskId = task.getID().toString();\r\n    Assert.assertEquals(\"pull links doesn't work correctly\", \"Task failed <a href=\\\"/jobhistory/task/\" + taskId + \"\\\">\" + taskId + \"</a>\", HsJobBlock.addTaskLinks(\"Task failed \" + taskId));\r\n    Assert.assertEquals(\"pull links doesn't work correctly\", \"Task failed <a href=\\\"/jobhistory/task/\" + taskId + \"\\\">\" + taskId + \"</a>\\n Job failed as tasks failed. failedMaps:1 failedReduces:0\", HsJobBlock.addTaskLinks(\"Task failed \" + taskId + \"\\n \" + \"Job failed as tasks failed. failedMaps:1 failedReduces:0\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHsTasksBlock",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testHsTasksBlock()\n{\r\n    Task task = getTask(0);\r\n    Map<TaskId, Task> tasks = new HashMap<TaskId, Task>();\r\n    tasks.put(task.getID(), task);\r\n    AppContext ctx = mock(AppContext.class);\r\n    AppForTest app = new AppForTest(ctx);\r\n    Job job = mock(Job.class);\r\n    when(job.getTasks()).thenReturn(tasks);\r\n    app.setJob(job);\r\n    HsTasksBlockForTest block = new HsTasksBlockForTest(app);\r\n    block.addParameter(AMParams.TASK_TYPE, \"r\");\r\n    PrintWriter pWriter = new PrintWriter(data);\r\n    Block html = new BlockForTest(new HtmlBlockForTest(), pWriter, 0, false);\r\n    block.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"task_0_0001_r_000000\"));\r\n    assertTrue(data.toString().contains(\"SUCCEEDED\"));\r\n    assertTrue(data.toString().contains(\"100001\"));\r\n    assertTrue(data.toString().contains(\"100011\"));\r\n    assertTrue(data.toString().contains(\"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testAttemptsBlock",
  "errType" : null,
  "containingMethodsNum" : 62,
  "sourceCodeText" : "void testAttemptsBlock()\n{\r\n    AppContext ctx = mock(AppContext.class);\r\n    AppForTest app = new AppForTest(ctx);\r\n    Task task = getTask(0);\r\n    Map<TaskAttemptId, TaskAttempt> attempts = new HashMap<TaskAttemptId, TaskAttempt>();\r\n    TaskAttempt attempt = mock(TaskAttempt.class);\r\n    TaskAttemptId taId = new TaskAttemptIdPBImpl();\r\n    taId.setId(0);\r\n    taId.setTaskId(task.getID());\r\n    when(attempt.getID()).thenReturn(taId);\r\n    when(attempt.getNodeHttpAddress()).thenReturn(\"Node address\");\r\n    ApplicationId appId = ApplicationIdPBImpl.newInstance(0, 5);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptIdPBImpl.newInstance(appId, 1);\r\n    ContainerId containerId = ContainerIdPBImpl.newContainerId(appAttemptId, 1);\r\n    when(attempt.getAssignedContainerID()).thenReturn(containerId);\r\n    when(attempt.getAssignedContainerMgrAddress()).thenReturn(\"assignedContainerMgrAddress\");\r\n    when(attempt.getNodeRackName()).thenReturn(\"nodeRackName\");\r\n    final long taStartTime = 100002L;\r\n    final long taFinishTime = 100012L;\r\n    final long taShuffleFinishTime = 100010L;\r\n    final long taSortFinishTime = 100011L;\r\n    final TaskAttemptState taState = TaskAttemptState.SUCCEEDED;\r\n    when(attempt.getLaunchTime()).thenReturn(taStartTime);\r\n    when(attempt.getFinishTime()).thenReturn(taFinishTime);\r\n    when(attempt.getShuffleFinishTime()).thenReturn(taShuffleFinishTime);\r\n    when(attempt.getSortFinishTime()).thenReturn(taSortFinishTime);\r\n    when(attempt.getState()).thenReturn(taState);\r\n    TaskAttemptReport taReport = mock(TaskAttemptReport.class);\r\n    when(taReport.getStartTime()).thenReturn(taStartTime);\r\n    when(taReport.getFinishTime()).thenReturn(taFinishTime);\r\n    when(taReport.getShuffleFinishTime()).thenReturn(taShuffleFinishTime);\r\n    when(taReport.getSortFinishTime()).thenReturn(taSortFinishTime);\r\n    when(taReport.getContainerId()).thenReturn(containerId);\r\n    when(taReport.getProgress()).thenReturn(1.0f);\r\n    when(taReport.getStateString()).thenReturn(\"Processed 128/128 records <p> \\n\");\r\n    when(taReport.getTaskAttemptState()).thenReturn(taState);\r\n    when(taReport.getDiagnosticInfo()).thenReturn(\"\");\r\n    when(attempt.getReport()).thenReturn(taReport);\r\n    attempts.put(taId, attempt);\r\n    when(task.getAttempts()).thenReturn(attempts);\r\n    app.setTask(task);\r\n    Job job = mock(Job.class);\r\n    when(job.getUserName()).thenReturn(\"User\");\r\n    app.setJob(job);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED, true);\r\n    AttemptsBlockForTest block = new AttemptsBlockForTest(app, conf);\r\n    block.addParameter(AMParams.TASK_TYPE, \"r\");\r\n    PrintWriter pWriter = new PrintWriter(data);\r\n    Block html = new BlockForTest(new HtmlBlockForTest(), pWriter, 0, false);\r\n    block.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"attempt_0_0001_r_000000_0\"));\r\n    assertTrue(data.toString().contains(\"SUCCEEDED\"));\r\n    assertFalse(data.toString().contains(\"Processed 128/128 records <p> \\n\"));\r\n    assertTrue(data.toString().contains(\"Processed 128\\\\/128 records &lt;p&gt; \\\\n\"));\r\n    assertTrue(data.toString().contains(\"_0005_01_000001:attempt_0_0001_r_000000_0:User:\"));\r\n    assertTrue(data.toString().contains(\"100002\"));\r\n    assertTrue(data.toString().contains(\"100010\"));\r\n    assertTrue(data.toString().contains(\"100011\"));\r\n    assertTrue(data.toString().contains(\"100012\"));\r\n    data.reset();\r\n    conf.setBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED, false);\r\n    block = new AttemptsBlockForTest(app, conf);\r\n    block.addParameter(AMParams.TASK_TYPE, \"r\");\r\n    pWriter = new PrintWriter(data);\r\n    html = new BlockForTest(new HtmlBlockForTest(), pWriter, 0, false);\r\n    block.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"attempt_0_0001_r_000000_0\"));\r\n    assertTrue(data.toString().contains(\"SUCCEEDED\"));\r\n    assertFalse(data.toString().contains(\"Processed 128/128 records <p> \\n\"));\r\n    assertTrue(data.toString().contains(\"Processed 128\\\\/128 records &lt;p&gt; \\\\n\"));\r\n    assertTrue(data.toString().contains(\"Node address:node:containerlogs:container_0_0005_01_000001:User:\"));\r\n    assertTrue(data.toString().contains(\"100002\"));\r\n    assertTrue(data.toString().contains(\"100010\"));\r\n    assertTrue(data.toString().contains(\"100011\"));\r\n    assertTrue(data.toString().contains(\"100012\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHsJobsBlock",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testHsJobsBlock()\n{\r\n    AppContext ctx = mock(AppContext.class);\r\n    Map<JobId, Job> jobs = new HashMap<JobId, Job>();\r\n    Job job = getJob();\r\n    jobs.put(job.getID(), job);\r\n    when(ctx.getAllJobs()).thenReturn(jobs);\r\n    Controller.RequestContext rc = mock(Controller.RequestContext.class);\r\n    ViewContext view = mock(ViewContext.class);\r\n    HttpServletRequest req = mock(HttpServletRequest.class);\r\n    when(rc.getRequest()).thenReturn(req);\r\n    when(view.requestContext()).thenReturn(rc);\r\n    Configuration conf = new Configuration();\r\n    HsJobsBlock block = new HsJobsBlockForTest(conf, ctx, view);\r\n    PrintWriter pWriter = new PrintWriter(data);\r\n    Block html = new BlockForTest(new HtmlBlockForTest(), pWriter, 0, false);\r\n    block.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"JobName\"));\r\n    assertTrue(data.toString().contains(\"UserName\"));\r\n    assertTrue(data.toString().contains(\"QueueName\"));\r\n    assertTrue(data.toString().contains(\"SUCCEEDED\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testHsController",
  "errType" : null,
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void testHsController() throws Exception\n{\r\n    AppContext ctx = mock(AppContext.class);\r\n    ApplicationId appId = ApplicationIdPBImpl.newInstance(0, 5);\r\n    when(ctx.getApplicationID()).thenReturn(appId);\r\n    AppForTest app = new AppForTest(ctx);\r\n    Configuration config = new Configuration();\r\n    RequestContext requestCtx = mock(RequestContext.class);\r\n    HsControllerForTest controller = new HsControllerForTest(app, config, requestCtx);\r\n    controller.index();\r\n    assertEquals(\"JobHistory\", controller.get(Params.TITLE, \"\"));\r\n    assertEquals(HsJobPage.class, controller.jobPage());\r\n    assertEquals(HsCountersPage.class, controller.countersPage());\r\n    assertEquals(HsTasksPage.class, controller.tasksPage());\r\n    assertEquals(HsTaskPage.class, controller.taskPage());\r\n    assertEquals(HsAttemptsPage.class, controller.attemptsPage());\r\n    controller.set(AMParams.JOB_ID, \"job_01_01\");\r\n    controller.set(AMParams.TASK_ID, \"task_01_01_m_01\");\r\n    controller.set(AMParams.TASK_TYPE, \"m\");\r\n    controller.set(AMParams.ATTEMPT_STATE, \"State\");\r\n    Job job = mock(Job.class);\r\n    Task task = mock(Task.class);\r\n    when(job.getTask(any(TaskId.class))).thenReturn(task);\r\n    JobId jobID = MRApps.toJobID(\"job_01_01\");\r\n    when(ctx.getJob(jobID)).thenReturn(job);\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(true);\r\n    controller.job();\r\n    assertEquals(HsJobPage.class, controller.getClazz());\r\n    controller.jobCounters();\r\n    assertEquals(HsCountersPage.class, controller.getClazz());\r\n    controller.taskCounters();\r\n    assertEquals(HsCountersPage.class, controller.getClazz());\r\n    controller.tasks();\r\n    assertEquals(HsTasksPage.class, controller.getClazz());\r\n    controller.task();\r\n    assertEquals(HsTaskPage.class, controller.getClazz());\r\n    controller.attempts();\r\n    assertEquals(HsAttemptsPage.class, controller.getClazz());\r\n    assertEquals(HsConfPage.class, controller.confPage());\r\n    assertEquals(HsAboutPage.class, controller.aboutPage());\r\n    controller.about();\r\n    assertEquals(HsAboutPage.class, controller.getClazz());\r\n    controller.logs();\r\n    assertEquals(HsLogsPage.class, controller.getClazz());\r\n    controller.nmlogs();\r\n    assertEquals(AggregatedLogsPage.class, controller.getClazz());\r\n    assertEquals(HsSingleCounterPage.class, controller.singleCounterPage());\r\n    controller.singleJobCounter();\r\n    assertEquals(HsSingleCounterPage.class, controller.getClazz());\r\n    controller.singleTaskCounter();\r\n    assertEquals(HsSingleCounterPage.class, controller.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "Job getJob()\n{\r\n    Job job = mock(Job.class);\r\n    JobId jobId = new JobIdPBImpl();\r\n    ApplicationId appId = ApplicationIdPBImpl.newInstance(System.currentTimeMillis(), 4);\r\n    jobId.setAppId(appId);\r\n    jobId.setId(1);\r\n    when(job.getID()).thenReturn(jobId);\r\n    JobReport report = mock(JobReport.class);\r\n    when(report.getStartTime()).thenReturn(100010L);\r\n    when(report.getFinishTime()).thenReturn(100015L);\r\n    when(job.getReport()).thenReturn(report);\r\n    when(job.getName()).thenReturn(\"JobName\");\r\n    when(job.getUserName()).thenReturn(\"UserName\");\r\n    when(job.getQueueName()).thenReturn(\"QueueName\");\r\n    when(job.getState()).thenReturn(JobState.SUCCEEDED);\r\n    when(job.getTotalMaps()).thenReturn(3);\r\n    when(job.getCompletedMaps()).thenReturn(2);\r\n    when(job.getTotalReduces()).thenReturn(2);\r\n    when(job.getCompletedReduces()).thenReturn(1);\r\n    when(job.getCompletedReduces()).thenReturn(1);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Task getTask(long timestamp)\n{\r\n    JobId jobId = new JobIdPBImpl();\r\n    jobId.setId(0);\r\n    jobId.setAppId(ApplicationIdPBImpl.newInstance(timestamp, 1));\r\n    TaskId taskId = new TaskIdPBImpl();\r\n    taskId.setId(0);\r\n    taskId.setTaskType(TaskType.REDUCE);\r\n    taskId.setJobId(jobId);\r\n    Task task = mock(Task.class);\r\n    when(task.getID()).thenReturn(taskId);\r\n    TaskReport report = mock(TaskReport.class);\r\n    when(report.getProgress()).thenReturn(0.7f);\r\n    when(report.getTaskState()).thenReturn(TaskState.SUCCEEDED);\r\n    when(report.getStartTime()).thenReturn(100001L);\r\n    when(report.getFinishTime()).thenReturn(100011L);\r\n    when(task.getReport()).thenReturn(report);\r\n    when(task.getType()).thenReturn(TaskType.REDUCE);\r\n    return task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testJobInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testJobInfo() throws Exception\n{\r\n    JobInfo info = new JobInfo();\r\n    Assert.assertEquals(\"NORMAL\", info.getPriority());\r\n    info.printAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryParsing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testHistoryParsing() throws Exception\n{\r\n    LOG.info(\"STARTING testHistoryParsing()\");\r\n    try {\r\n        checkHistoryParsing(2, 1, 2);\r\n    } finally {\r\n        LOG.info(\"FINISHED testHistoryParsing()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryParsingWithParseErrors",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testHistoryParsingWithParseErrors() throws Exception\n{\r\n    LOG.info(\"STARTING testHistoryParsingWithParseErrors()\");\r\n    try {\r\n        checkHistoryParsing(3, 0, 2);\r\n    } finally {\r\n        LOG.info(\"FINISHED testHistoryParsingWithParseErrors()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJobSummary",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getJobSummary(FileContext fc, Path path) throws IOException\n{\r\n    Path qPath = fc.makeQualified(path);\r\n    FSDataInputStream in = fc.open(qPath);\r\n    String jobSummaryString = in.readUTF();\r\n    in.close();\r\n    return jobSummaryString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "checkHistoryParsing",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 92,
  "sourceCodeText" : "void checkHistoryParsing(final int numMaps, final int numReduces, final int numSuccessfulMaps) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.USER_NAME, System.getProperty(\"user.name\"));\r\n    long amStartTimeEst = System.currentTimeMillis();\r\n    conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n    RackResolver.init(conf);\r\n    MRApp app = new MRAppWithHistory(numMaps, numReduces, true, this.getClass().getName(), true);\r\n    app.submit(conf);\r\n    Job job = app.getContext().getAllJobs().values().iterator().next();\r\n    JobId jobId = job.getID();\r\n    LOG.info(\"JOBID is \" + TypeConverter.fromYarn(jobId).toString());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.waitForState(Service.STATE.STOPPED);\r\n    String jobhistoryDir = JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);\r\n    FileContext fc = null;\r\n    try {\r\n        fc = FileContext.getFileContext(conf);\r\n    } catch (IOException ioe) {\r\n        LOG.info(\"Can not get FileContext\", ioe);\r\n        throw (new Exception(\"Can not get File Context\"));\r\n    }\r\n    if (numMaps == numSuccessfulMaps) {\r\n        String summaryFileName = JobHistoryUtils.getIntermediateSummaryFileName(jobId);\r\n        Path summaryFile = new Path(jobhistoryDir, summaryFileName);\r\n        String jobSummaryString = getJobSummary(fc, summaryFile);\r\n        Assert.assertNotNull(jobSummaryString);\r\n        Assert.assertTrue(jobSummaryString.contains(\"resourcesPerMap=100\"));\r\n        Assert.assertTrue(jobSummaryString.contains(\"resourcesPerReduce=100\"));\r\n        Map<String, String> jobSummaryElements = new HashMap<String, String>();\r\n        StringTokenizer strToken = new StringTokenizer(jobSummaryString, \",\");\r\n        while (strToken.hasMoreTokens()) {\r\n            String keypair = strToken.nextToken();\r\n            jobSummaryElements.put(keypair.split(\"=\")[0], keypair.split(\"=\")[1]);\r\n        }\r\n        Assert.assertEquals(\"JobId does not match\", jobId.toString(), jobSummaryElements.get(\"jobId\"));\r\n        Assert.assertEquals(\"JobName does not match\", \"test\", jobSummaryElements.get(\"jobName\"));\r\n        Assert.assertTrue(\"submitTime should not be 0\", Long.parseLong(jobSummaryElements.get(\"submitTime\")) != 0);\r\n        Assert.assertTrue(\"launchTime should not be 0\", Long.parseLong(jobSummaryElements.get(\"launchTime\")) != 0);\r\n        Assert.assertTrue(\"firstMapTaskLaunchTime should not be 0\", Long.parseLong(jobSummaryElements.get(\"firstMapTaskLaunchTime\")) != 0);\r\n        Assert.assertTrue(\"firstReduceTaskLaunchTime should not be 0\", Long.parseLong(jobSummaryElements.get(\"firstReduceTaskLaunchTime\")) != 0);\r\n        Assert.assertTrue(\"finishTime should not be 0\", Long.parseLong(jobSummaryElements.get(\"finishTime\")) != 0);\r\n        Assert.assertEquals(\"Mismatch in num map slots\", numSuccessfulMaps, Integer.parseInt(jobSummaryElements.get(\"numMaps\")));\r\n        Assert.assertEquals(\"Mismatch in num reduce slots\", numReduces, Integer.parseInt(jobSummaryElements.get(\"numReduces\")));\r\n        Assert.assertEquals(\"User does not match\", System.getProperty(\"user.name\"), jobSummaryElements.get(\"user\"));\r\n        Assert.assertEquals(\"Queue does not match\", \"default\", jobSummaryElements.get(\"queue\"));\r\n        Assert.assertEquals(\"Status does not match\", \"SUCCEEDED\", jobSummaryElements.get(\"status\"));\r\n    }\r\n    JobHistory jobHistory = new JobHistory();\r\n    jobHistory.init(conf);\r\n    HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);\r\n    JobInfo jobInfo;\r\n    long numFinishedMaps;\r\n    synchronized (fileInfo) {\r\n        Path historyFilePath = fileInfo.getHistoryFile();\r\n        FSDataInputStream in = null;\r\n        LOG.info(\"JobHistoryFile is: \" + historyFilePath);\r\n        try {\r\n            in = fc.open(fc.makeQualified(historyFilePath));\r\n        } catch (IOException ioe) {\r\n            LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\r\n            throw (new Exception(\"Can not open History File\"));\r\n        }\r\n        JobHistoryParser parser = new JobHistoryParser(in);\r\n        final EventReader realReader = new EventReader(in);\r\n        EventReader reader = Mockito.mock(EventReader.class);\r\n        if (numMaps == numSuccessfulMaps) {\r\n            reader = realReader;\r\n        } else {\r\n            final AtomicInteger numFinishedEvents = new AtomicInteger(0);\r\n            Mockito.when(reader.getNextEvent()).thenAnswer(new Answer<HistoryEvent>() {\r\n\r\n                public HistoryEvent answer(InvocationOnMock invocation) throws IOException {\r\n                    HistoryEvent event = realReader.getNextEvent();\r\n                    if (event instanceof TaskFinishedEvent) {\r\n                        numFinishedEvents.incrementAndGet();\r\n                    }\r\n                    if (numFinishedEvents.get() <= numSuccessfulMaps) {\r\n                        return event;\r\n                    } else {\r\n                        throw new IOException(\"test\");\r\n                    }\r\n                }\r\n            });\r\n        }\r\n        jobInfo = parser.parse(reader);\r\n        numFinishedMaps = computeFinishedMaps(jobInfo, numMaps, numSuccessfulMaps);\r\n        if (numFinishedMaps != numMaps) {\r\n            Exception parseException = parser.getParseException();\r\n            Assert.assertNotNull(\"Didn't get expected parse exception\", parseException);\r\n        }\r\n    }\r\n    Assert.assertEquals(\"Incorrect username \", System.getProperty(\"user.name\"), jobInfo.getUsername());\r\n    Assert.assertEquals(\"Incorrect jobName \", \"test\", jobInfo.getJobname());\r\n    Assert.assertEquals(\"Incorrect queuename \", \"default\", jobInfo.getJobQueueName());\r\n    Assert.assertEquals(\"incorrect conf path\", \"test\", jobInfo.getJobConfPath());\r\n    Assert.assertEquals(\"incorrect finishedMap \", numSuccessfulMaps, numFinishedMaps);\r\n    Assert.assertEquals(\"incorrect finishedReduces \", numReduces, jobInfo.getSucceededReduces());\r\n    Assert.assertEquals(\"incorrect uberized \", job.isUber(), jobInfo.getUberized());\r\n    Map<TaskID, TaskInfo> allTasks = jobInfo.getAllTasks();\r\n    int totalTasks = allTasks.size();\r\n    Assert.assertEquals(\"total number of tasks is incorrect  \", (numMaps + numReduces), totalTasks);\r\n    Assert.assertEquals(1, jobInfo.getAMInfos().size());\r\n    Assert.assertEquals(MRApp.NM_HOST, jobInfo.getAMInfos().get(0).getNodeManagerHost());\r\n    AMInfo amInfo = jobInfo.getAMInfos().get(0);\r\n    Assert.assertEquals(MRApp.NM_PORT, amInfo.getNodeManagerPort());\r\n    Assert.assertEquals(MRApp.NM_HTTP_PORT, amInfo.getNodeManagerHttpPort());\r\n    Assert.assertEquals(1, amInfo.getAppAttemptId().getAttemptId());\r\n    Assert.assertEquals(amInfo.getAppAttemptId(), amInfo.getContainerId().getApplicationAttemptId());\r\n    Assert.assertTrue(amInfo.getStartTime() <= System.currentTimeMillis() && amInfo.getStartTime() >= amStartTimeEst);\r\n    ContainerId fakeCid = MRApp.newContainerId(-1, -1, -1, -1);\r\n    for (TaskInfo taskInfo : allTasks.values()) {\r\n        int taskAttemptCount = taskInfo.getAllTaskAttempts().size();\r\n        Assert.assertEquals(\"total number of task attempts \", 1, taskAttemptCount);\r\n        TaskAttemptInfo taInfo = taskInfo.getAllTaskAttempts().values().iterator().next();\r\n        Assert.assertNotNull(taInfo.getContainerId());\r\n        Assert.assertFalse(taInfo.getContainerId().equals(fakeCid));\r\n    }\r\n    for (Task task : job.getTasks().values()) {\r\n        TaskInfo taskInfo = allTasks.get(TypeConverter.fromYarn(task.getID()));\r\n        Assert.assertNotNull(\"TaskInfo not found\", taskInfo);\r\n        for (TaskAttempt taskAttempt : task.getAttempts().values()) {\r\n            TaskAttemptInfo taskAttemptInfo = taskInfo.getAllTaskAttempts().get(TypeConverter.fromYarn((taskAttempt.getID())));\r\n            Assert.assertNotNull(\"TaskAttemptInfo not found\", taskAttemptInfo);\r\n            Assert.assertEquals(\"Incorrect shuffle port for task attempt\", taskAttempt.getShufflePort(), taskAttemptInfo.getShufflePort());\r\n            if (numMaps == numSuccessfulMaps) {\r\n                Assert.assertEquals(MRApp.NM_HOST, taskAttemptInfo.getHostname());\r\n                Assert.assertEquals(MRApp.NM_PORT, taskAttemptInfo.getPort());\r\n                Assert.assertEquals(\"rack-name is incorrect\", taskAttemptInfo.getRackname(), RACK_NAME);\r\n            }\r\n        }\r\n    }\r\n    PrintStream stdps = System.out;\r\n    try {\r\n        System.setOut(new PrintStream(outContent));\r\n        HistoryViewer viewer;\r\n        synchronized (fileInfo) {\r\n            viewer = new HistoryViewer(fc.makeQualified(fileInfo.getHistoryFile()).toString(), conf, true);\r\n        }\r\n        viewer.print();\r\n        for (TaskInfo taskInfo : allTasks.values()) {\r\n            String test = (taskInfo.getTaskStatus() == null ? \"\" : taskInfo.getTaskStatus()) + \" \" + taskInfo.getTaskType() + \" task list for \" + taskInfo.getTaskId().getJobID();\r\n            Assert.assertTrue(outContent.toString().indexOf(test) > 0);\r\n            Assert.assertTrue(outContent.toString().indexOf(taskInfo.getTaskId().toString()) > 0);\r\n        }\r\n    } finally {\r\n        System.setOut(stdps);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "computeFinishedMaps",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long computeFinishedMaps(JobInfo jobInfo, int numMaps, int numSuccessfulMaps)\n{\r\n    if (numMaps == numSuccessfulMaps) {\r\n        return jobInfo.getSucceededMaps();\r\n    }\r\n    long numFinishedMaps = 0;\r\n    Map<org.apache.hadoop.mapreduce.TaskID, TaskInfo> taskInfos = jobInfo.getAllTasks();\r\n    for (TaskInfo taskInfo : taskInfos.values()) {\r\n        if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {\r\n            ++numFinishedMaps;\r\n        }\r\n    }\r\n    return numFinishedMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryParsingForFailedAttempts",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testHistoryParsingForFailedAttempts() throws Exception\n{\r\n    LOG.info(\"STARTING testHistoryParsingForFailedAttempts\");\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n        RackResolver.init(conf);\r\n        MRApp app = new MRAppWithHistoryWithFailedAttempt(2, 1, true, this.getClass().getName(), true);\r\n        app.submit(conf);\r\n        Job job = app.getContext().getAllJobs().values().iterator().next();\r\n        JobId jobId = job.getID();\r\n        app.waitForState(job, JobState.SUCCEEDED);\r\n        app.waitForState(Service.STATE.STOPPED);\r\n        JobHistory jobHistory = new JobHistory();\r\n        jobHistory.init(conf);\r\n        HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);\r\n        JobHistoryParser parser;\r\n        JobInfo jobInfo;\r\n        synchronized (fileInfo) {\r\n            Path historyFilePath = fileInfo.getHistoryFile();\r\n            FSDataInputStream in = null;\r\n            FileContext fc = null;\r\n            try {\r\n                fc = FileContext.getFileContext(conf);\r\n                in = fc.open(fc.makeQualified(historyFilePath));\r\n            } catch (IOException ioe) {\r\n                LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\r\n                throw (new Exception(\"Can not open History File\"));\r\n            }\r\n            parser = new JobHistoryParser(in);\r\n            jobInfo = parser.parse();\r\n        }\r\n        Exception parseException = parser.getParseException();\r\n        Assert.assertNull(\"Caught an expected exception \" + parseException, parseException);\r\n        int noOffailedAttempts = 0;\r\n        Map<TaskID, TaskInfo> allTasks = jobInfo.getAllTasks();\r\n        for (Task task : job.getTasks().values()) {\r\n            TaskInfo taskInfo = allTasks.get(TypeConverter.fromYarn(task.getID()));\r\n            for (TaskAttempt taskAttempt : task.getAttempts().values()) {\r\n                TaskAttemptInfo taskAttemptInfo = taskInfo.getAllTaskAttempts().get(TypeConverter.fromYarn((taskAttempt.getID())));\r\n                assertThat(taskAttemptInfo.getRackname()).withFailMessage(\"rack-name is incorrect\").isEqualTo(RACK_NAME);\r\n                if (taskAttemptInfo.getTaskStatus().equals(\"FAILED\")) {\r\n                    noOffailedAttempts++;\r\n                }\r\n            }\r\n        }\r\n        Assert.assertEquals(\"No of Failed tasks doesn't match.\", 2, noOffailedAttempts);\r\n    } finally {\r\n        LOG.info(\"FINISHED testHistoryParsingForFailedAttempts\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testHistoryParsingForKilledAndFailedAttempts",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testHistoryParsingForKilledAndFailedAttempts() throws Exception\n{\r\n    MRApp app = null;\r\n    JobHistory jobHistory = null;\r\n    LOG.info(\"STARTING testHistoryParsingForKilledAndFailedAttempts\");\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n        conf.set(JHAdminConfig.MR_HS_JHIST_FORMAT, \"json\");\r\n        conf.setInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 50);\r\n        conf.setInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT, 50);\r\n        RackResolver.init(conf);\r\n        app = new MRAppWithHistoryWithFailedAndKilledTask(3, 3, true, this.getClass().getName(), true);\r\n        app.submit(conf);\r\n        Job job = app.getContext().getAllJobs().values().iterator().next();\r\n        JobId jobId = job.getID();\r\n        app.waitForState(job, JobState.SUCCEEDED);\r\n        app.waitForState(Service.STATE.STOPPED);\r\n        jobHistory = new JobHistory();\r\n        jobHistory.init(conf);\r\n        HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);\r\n        JobHistoryParser parser;\r\n        JobInfo jobInfo;\r\n        synchronized (fileInfo) {\r\n            Path historyFilePath = fileInfo.getHistoryFile();\r\n            FSDataInputStream in = null;\r\n            FileContext fc = null;\r\n            try {\r\n                fc = FileContext.getFileContext(conf);\r\n                in = fc.open(fc.makeQualified(historyFilePath));\r\n            } catch (IOException ioe) {\r\n                LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\r\n                throw (new Exception(\"Can not open History File\"));\r\n            }\r\n            parser = new JobHistoryParser(in);\r\n            jobInfo = parser.parse();\r\n        }\r\n        Exception parseException = parser.getParseException();\r\n        Assert.assertNull(\"Caught an expected exception \" + parseException, parseException);\r\n        assertEquals(\"FailedMaps\", 1, jobInfo.getFailedMaps());\r\n        assertEquals(\"KilledMaps\", 1, jobInfo.getKilledMaps());\r\n        assertEquals(\"FailedReduces\", 1, jobInfo.getFailedReduces());\r\n        assertEquals(\"KilledReduces\", 1, jobInfo.getKilledReduces());\r\n    } finally {\r\n        LOG.info(\"FINISHED testHistoryParsingForKilledAndFailedAttempts\");\r\n        if (app != null) {\r\n            app.close();\r\n        }\r\n        if (jobHistory != null) {\r\n            jobHistory.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testCountersForFailedTask",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testCountersForFailedTask() throws Exception\n{\r\n    LOG.info(\"STARTING testCountersForFailedTask\");\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n        RackResolver.init(conf);\r\n        MRApp app = new MRAppWithHistoryWithFailedTask(2, 1, true, this.getClass().getName(), true);\r\n        app.submit(conf);\r\n        Job job = app.getContext().getAllJobs().values().iterator().next();\r\n        JobId jobId = job.getID();\r\n        app.waitForState(job, JobState.FAILED);\r\n        app.waitForState(Service.STATE.STOPPED);\r\n        JobHistory jobHistory = new JobHistory();\r\n        jobHistory.init(conf);\r\n        HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);\r\n        JobHistoryParser parser;\r\n        JobInfo jobInfo;\r\n        synchronized (fileInfo) {\r\n            Path historyFilePath = fileInfo.getHistoryFile();\r\n            FSDataInputStream in = null;\r\n            FileContext fc = null;\r\n            try {\r\n                fc = FileContext.getFileContext(conf);\r\n                in = fc.open(fc.makeQualified(historyFilePath));\r\n            } catch (IOException ioe) {\r\n                LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\r\n                throw (new Exception(\"Can not open History File\"));\r\n            }\r\n            parser = new JobHistoryParser(in);\r\n            jobInfo = parser.parse();\r\n        }\r\n        Exception parseException = parser.getParseException();\r\n        Assert.assertNull(\"Caught an expected exception \" + parseException, parseException);\r\n        for (Map.Entry<TaskID, TaskInfo> entry : jobInfo.getAllTasks().entrySet()) {\r\n            TaskId yarnTaskID = TypeConverter.toYarn(entry.getKey());\r\n            CompletedTask ct = new CompletedTask(yarnTaskID, entry.getValue());\r\n            Assert.assertNotNull(\"completed task report has null counters\", ct.getReport().getCounters());\r\n        }\r\n        final List<String> originalDiagnostics = job.getDiagnostics();\r\n        final String historyError = jobInfo.getErrorInfo();\r\n        assertTrue(\"No original diagnostics for a failed job\", originalDiagnostics != null && !originalDiagnostics.isEmpty());\r\n        assertNotNull(\"No history error info for a failed job \", historyError);\r\n        for (String diagString : originalDiagnostics) {\r\n            assertTrue(historyError.contains(diagString));\r\n        }\r\n    } finally {\r\n        LOG.info(\"FINISHED testCountersForFailedTask\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testDiagnosticsForKilledJob",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testDiagnosticsForKilledJob() throws Exception\n{\r\n    LOG.info(\"STARTING testDiagnosticsForKilledJob\");\r\n    try {\r\n        final Configuration conf = new Configuration();\r\n        conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n        RackResolver.init(conf);\r\n        MRApp app = new MRAppWithHistoryWithJobKilled(2, 1, true, this.getClass().getName(), true);\r\n        app.submit(conf);\r\n        Job job = app.getContext().getAllJobs().values().iterator().next();\r\n        JobId jobId = job.getID();\r\n        app.waitForState(job, JobState.KILLED);\r\n        app.waitForState(Service.STATE.STOPPED);\r\n        JobHistory jobHistory = new JobHistory();\r\n        jobHistory.init(conf);\r\n        HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);\r\n        JobHistoryParser parser;\r\n        JobInfo jobInfo;\r\n        synchronized (fileInfo) {\r\n            Path historyFilePath = fileInfo.getHistoryFile();\r\n            FSDataInputStream in = null;\r\n            FileContext fc = null;\r\n            try {\r\n                fc = FileContext.getFileContext(conf);\r\n                in = fc.open(fc.makeQualified(historyFilePath));\r\n            } catch (IOException ioe) {\r\n                LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\r\n                throw (new Exception(\"Can not open History File\"));\r\n            }\r\n            parser = new JobHistoryParser(in);\r\n            jobInfo = parser.parse();\r\n        }\r\n        Exception parseException = parser.getParseException();\r\n        assertNull(\"Caught an expected exception \" + parseException, parseException);\r\n        final List<String> originalDiagnostics = job.getDiagnostics();\r\n        final String historyError = jobInfo.getErrorInfo();\r\n        assertTrue(\"No original diagnostics for a failed job\", originalDiagnostics != null && !originalDiagnostics.isEmpty());\r\n        assertNotNull(\"No history error info for a failed job \", historyError);\r\n        for (String diagString : originalDiagnostics) {\r\n            assertTrue(historyError.contains(diagString));\r\n        }\r\n        assertTrue(\"No killed message in diagnostics\", historyError.contains(JobImpl.JOB_KILLED_DIAG));\r\n    } finally {\r\n        LOG.info(\"FINISHED testDiagnosticsForKilledJob\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testScanningOldDirs",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testScanningOldDirs() throws Exception\n{\r\n    LOG.info(\"STARTING testScanningOldDirs\");\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n        RackResolver.init(conf);\r\n        MRApp app = new MRAppWithHistory(1, 1, true, this.getClass().getName(), true);\r\n        app.submit(conf);\r\n        Job job = app.getContext().getAllJobs().values().iterator().next();\r\n        JobId jobId = job.getID();\r\n        LOG.info(\"JOBID is \" + TypeConverter.fromYarn(jobId).toString());\r\n        app.waitForState(job, JobState.SUCCEEDED);\r\n        app.waitForState(Service.STATE.STOPPED);\r\n        HistoryFileManagerForTest hfm = new HistoryFileManagerForTest();\r\n        hfm.init(conf);\r\n        HistoryFileInfo fileInfo = hfm.getFileInfo(jobId);\r\n        Assert.assertNotNull(\"Unable to locate job history\", fileInfo);\r\n        hfm.deleteJobFromJobListCache(fileInfo);\r\n        final int msecPerSleep = 10;\r\n        int msecToSleep = 10 * 1000;\r\n        while (fileInfo.isMovePending() && msecToSleep > 0) {\r\n            Assert.assertTrue(!fileInfo.didMoveFail());\r\n            msecToSleep -= msecPerSleep;\r\n            Thread.sleep(msecPerSleep);\r\n        }\r\n        Assert.assertTrue(\"Timeout waiting for history move\", msecToSleep > 0);\r\n        fileInfo = hfm.getFileInfo(jobId);\r\n        hfm.stop();\r\n        Assert.assertNotNull(\"Unable to locate old job history\", fileInfo);\r\n        Assert.assertTrue(\"HistoryFileManager not shutdown properly\", hfm.moveToDoneExecutor.isTerminated());\r\n    } finally {\r\n        LOG.info(\"FINISHED testScanningOldDirs\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestJobHistoryParsing t = new TestJobHistoryParsing();\r\n    t.testHistoryParsing();\r\n    t.testHistoryParsingForFailedAttempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testDeleteFileInfo",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testDeleteFileInfo() throws Exception\n{\r\n    LOG.info(\"STARTING testDeleteFileInfo\");\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n        RackResolver.init(conf);\r\n        MRApp app = new MRAppWithHistory(1, 1, true, this.getClass().getName(), true);\r\n        app.submit(conf);\r\n        Job job = app.getContext().getAllJobs().values().iterator().next();\r\n        JobId jobId = job.getID();\r\n        app.waitForState(job, JobState.SUCCEEDED);\r\n        app.waitForState(Service.STATE.STOPPED);\r\n        HistoryFileManager hfm = new HistoryFileManager();\r\n        hfm.init(conf);\r\n        HistoryFileInfo fileInfo = hfm.getFileInfo(jobId);\r\n        hfm.initExisting();\r\n        while (fileInfo.isMovePending()) {\r\n            Thread.sleep(300);\r\n        }\r\n        Assert.assertNotNull(hfm.jobListCache.values());\r\n        hfm.clean();\r\n        Assert.assertFalse(fileInfo.isDeleted());\r\n        hfm.setMaxHistoryAge(-1);\r\n        hfm.clean();\r\n        hfm.stop();\r\n        Assert.assertTrue(\"Thread pool shutdown\", hfm.moveToDoneExecutor.isTerminated());\r\n        Assert.assertTrue(\"file should be deleted \", fileInfo.isDeleted());\r\n    } finally {\r\n        LOG.info(\"FINISHED testDeleteFileInfo\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testJobHistoryMethods",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testJobHistoryMethods() throws Exception\n{\r\n    LOG.info(\"STARTING testJobHistoryMethods\");\r\n    try {\r\n        Configuration configuration = new Configuration();\r\n        configuration.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, MyResolver.class, DNSToSwitchMapping.class);\r\n        RackResolver.init(configuration);\r\n        MRApp app = new MRAppWithHistory(1, 1, true, this.getClass().getName(), true);\r\n        app.submit(configuration);\r\n        Job job = app.getContext().getAllJobs().values().iterator().next();\r\n        JobId jobId = job.getID();\r\n        LOG.info(\"JOBID is \" + TypeConverter.fromYarn(jobId).toString());\r\n        app.waitForState(job, JobState.SUCCEEDED);\r\n        app.waitForState(Service.STATE.STOPPED);\r\n        JobHistory jobHistory = new JobHistory();\r\n        jobHistory.init(configuration);\r\n        Assert.assertEquals(1, jobHistory.getAllJobs().size());\r\n        Assert.assertEquals(1, jobHistory.getAllJobs(app.getAppID()).size());\r\n        JobsInfo jobsinfo = jobHistory.getPartialJobs(0L, 10L, null, \"default\", 0L, System.currentTimeMillis() + 1, 0L, System.currentTimeMillis() + 1, JobState.SUCCEEDED);\r\n        Assert.assertEquals(1, jobsinfo.getJobs().size());\r\n        Assert.assertNotNull(jobHistory.getApplicationAttemptId());\r\n        Assert.assertEquals(\"application_0_0000\", jobHistory.getApplicationID().toString());\r\n        Assert.assertEquals(\"Job History Server\", jobHistory.getApplicationName());\r\n        Assert.assertNull(jobHistory.getEventHandler());\r\n        Assert.assertNull(jobHistory.getClock());\r\n        Assert.assertNull(jobHistory.getClusterInfo());\r\n    } finally {\r\n        LOG.info(\"FINISHED testJobHistoryMethods\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testPartialJob",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testPartialJob() throws Exception\n{\r\n    JobId jobId = new JobIdPBImpl();\r\n    jobId.setId(0);\r\n    JobIndexInfo jii = new JobIndexInfo(0L, System.currentTimeMillis(), \"user\", \"jobName\", jobId, 3, 2, \"JobStatus\");\r\n    PartialJob test = new PartialJob(jii, jobId);\r\n    Assert.assertEquals(1.0f, test.getProgress(), 0.001f);\r\n    assertNull(test.getAllCounters());\r\n    assertNull(test.getTasks());\r\n    assertNull(test.getTasks(TaskType.MAP));\r\n    assertNull(test.getTask(new TaskIdPBImpl()));\r\n    assertNull(test.getTaskAttemptCompletionEvents(0, 100));\r\n    assertNull(test.getMapAttemptCompletionEvents(0, 100));\r\n    assertTrue(test.checkAccess(UserGroupInformation.getCurrentUser(), null));\r\n    assertNull(test.getAMInfos());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testMultipleFailedTasks",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultipleFailedTasks() throws Exception\n{\r\n    JobHistoryParser parser = new JobHistoryParser(Mockito.mock(FSDataInputStream.class));\r\n    EventReader reader = Mockito.mock(EventReader.class);\r\n    final AtomicInteger numEventsRead = new AtomicInteger(0);\r\n    final org.apache.hadoop.mapreduce.TaskType taskType = org.apache.hadoop.mapreduce.TaskType.MAP;\r\n    final TaskID[] tids = new TaskID[2];\r\n    final JobID jid = new JobID(\"1\", 1);\r\n    tids[0] = new TaskID(jid, taskType, 0);\r\n    tids[1] = new TaskID(jid, taskType, 1);\r\n    Mockito.when(reader.getNextEvent()).thenAnswer(new Answer<HistoryEvent>() {\r\n\r\n        public HistoryEvent answer(InvocationOnMock invocation) throws IOException {\r\n            int eventId = numEventsRead.getAndIncrement();\r\n            TaskID tid = tids[eventId & 0x1];\r\n            if (eventId < 2) {\r\n                return new TaskStartedEvent(tid, 0, taskType, \"\");\r\n            }\r\n            if (eventId < 4) {\r\n                TaskFailedEvent tfe = new TaskFailedEvent(tid, 0, taskType, \"failed\", \"FAILED\", null, new Counters());\r\n                tfe.setDatum(tfe.getDatum());\r\n                return tfe;\r\n            }\r\n            if (eventId < 5) {\r\n                JobUnsuccessfulCompletionEvent juce = new JobUnsuccessfulCompletionEvent(jid, 100L, 2, 0, 0, 0, 0, 0, \"JOB_FAILED\", Collections.singletonList(\"Task failed: \" + tids[0].toString()));\r\n                return juce;\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n    JobInfo info = parser.parse(reader);\r\n    assertTrue(\"Task 0 not implicated\", info.getErrorInfo().contains(tids[0].toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testFailedJobHistoryWithoutDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testFailedJobHistoryWithoutDiagnostics() throws Exception\n{\r\n    final Path histPath = new Path(getClass().getClassLoader().getResource(\"job_1393307629410_0001-1393307687476-user-Sleep+job-1393307723835-0-0-FAILED-default-1393307693920.jhist\").getFile());\r\n    final FileSystem lfs = FileSystem.getLocal(new Configuration());\r\n    final FSDataInputStream fsdis = lfs.open(histPath);\r\n    try {\r\n        JobHistoryParser parser = new JobHistoryParser(fsdis);\r\n        JobInfo info = parser.parse();\r\n        assertEquals(\"History parsed jobId incorrectly\", info.getJobId(), JobID.forName(\"job_1393307629410_0001\"));\r\n        assertEquals(\"Default diagnostics incorrect \", \"\", info.getErrorInfo());\r\n    } finally {\r\n        fsdis.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTaskAttemptUnsuccessfulCompletionWithoutCounters203",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTaskAttemptUnsuccessfulCompletionWithoutCounters203() throws IOException\n{\r\n    Path histPath = new Path(getClass().getClassLoader().getResource(\"job_2.0.3-alpha-FAILED.jhist\").getFile());\r\n    JobHistoryParser parser = new JobHistoryParser(FileSystem.getLocal(new Configuration()), histPath);\r\n    JobInfo jobInfo = parser.parse();\r\n    LOG.info(\" job info: \" + jobInfo.getJobname() + \" \" + jobInfo.getSucceededMaps() + \" \" + jobInfo.getTotalMaps() + \" \" + jobInfo.getJobId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTaskAttemptUnsuccessfulCompletionWithoutCounters240",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTaskAttemptUnsuccessfulCompletionWithoutCounters240() throws IOException\n{\r\n    Path histPath = new Path(getClass().getClassLoader().getResource(\"job_2.4.0-FAILED.jhist\").getFile());\r\n    JobHistoryParser parser = new JobHistoryParser(FileSystem.getLocal(new Configuration()), histPath);\r\n    JobInfo jobInfo = parser.parse();\r\n    LOG.info(\" job info: \" + jobInfo.getJobname() + \" \" + jobInfo.getSucceededMaps() + \" \" + jobInfo.getTotalMaps() + \" \" + jobInfo.getJobId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "testTaskAttemptUnsuccessfulCompletionWithoutCounters0239",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTaskAttemptUnsuccessfulCompletionWithoutCounters0239() throws IOException\n{\r\n    Path histPath = new Path(getClass().getClassLoader().getResource(\"job_0.23.9-FAILED.jhist\").getFile());\r\n    JobHistoryParser parser = new JobHistoryParser(FileSystem.getLocal(new Configuration()), histPath);\r\n    JobInfo jobInfo = parser.parse();\r\n    LOG.info(\" job info: \" + jobInfo.getJobname() + \" \" + jobInfo.getSucceededMaps() + \" \" + jobInfo.getTotalMaps() + \" \" + jobInfo.getJobId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getJob(JobId jobID)\n{\r\n    return fullJobs.get(jobID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getPartialJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getPartialJob(JobId jobID)\n{\r\n    return partialJobs.get(jobID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<JobId, Job> getAllJobs()\n{\r\n    return fullJobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<JobId, Job> getAllJobs(ApplicationId appID)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getPartialJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobsInfo getPartialJobs(Long offset, Long count, String user, String queue, Long sBegin, Long sEnd, Long fBegin, Long fEnd, JobState jobState)\n{\r\n    return CachedHistoryStorage.getPartialJobs(this.partialJobs.values(), offset, count, user, queue, sBegin, sEnd, fBegin, fEnd, jobState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    this.conf = new JobConf();\r\n    this.conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING, NullGroupsProvider.class.getName());\r\n    this.conf.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    Groups.getUserToGroupsMappingService(conf);\r\n    this.ctx = buildHistoryContext(this.conf);\r\n    WebApp webApp = mock(HsWebApp.class);\r\n    when(webApp.name()).thenReturn(\"hsmockwebapp\");\r\n    this.hsWebServices = new HsWebServices(ctx, conf, webApp, null);\r\n    this.hsWebServices.setResponse(mock(HttpServletResponse.class));\r\n    Job job = ctx.getAllJobs().values().iterator().next();\r\n    this.jobIdStr = job.getID().toString();\r\n    Task task = job.getTasks().values().iterator().next();\r\n    this.taskIdStr = task.getID().toString();\r\n    this.taskAttemptIdStr = task.getAttempts().keySet().iterator().next().toString();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJob(hsr, jobIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJob(hsr, jobIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobCountersAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobCountersAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJobCounters(hsr, jobIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJobCounters(hsr, jobIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobConfAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobConfAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJobConf(hsr, jobIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJobConf(hsr, jobIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobTasksAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobTasksAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJobTasks(hsr, jobIdStr, \"m\");\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJobTasks(hsr, jobIdStr, \"m\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobTaskAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobTaskAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJobTask(hsr, jobIdStr, this.taskIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJobTask(hsr, this.jobIdStr, this.taskIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetSingleTaskCountersAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetSingleTaskCountersAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getSingleTaskCounters(hsr, this.jobIdStr, this.taskIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getSingleTaskCounters(hsr, this.jobIdStr, this.taskIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobTaskAttemptsAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobTaskAttemptsAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJobTaskAttempts(hsr, this.jobIdStr, this.taskIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJobTaskAttempts(hsr, this.jobIdStr, this.taskIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobTaskAttemptIdAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobTaskAttemptIdAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJobTaskAttemptId(hsr, this.jobIdStr, this.taskIdStr, this.taskAttemptIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJobTaskAttemptId(hsr, this.jobIdStr, this.taskIdStr, this.taskAttemptIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "testGetJobTaskAttemptIdCountersAcls",
  "errType" : [ "WebApplicationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobTaskAttemptIdCountersAcls()\n{\r\n    HttpServletRequest hsr = mock(HttpServletRequest.class);\r\n    when(hsr.getRemoteUser()).thenReturn(ENEMY_USER);\r\n    try {\r\n        hsWebServices.getJobTaskAttemptIdCounters(hsr, this.jobIdStr, this.taskIdStr, this.taskAttemptIdStr);\r\n        fail(\"enemy can access job\");\r\n    } catch (WebApplicationException e) {\r\n        assertEquals(Status.UNAUTHORIZED, Status.fromStatusCode(e.getResponse().getStatus()));\r\n    }\r\n    when(hsr.getRemoteUser()).thenReturn(FRIENDLY_USER);\r\n    hsWebServices.getJobTaskAttemptIdCounters(hsr, this.jobIdStr, this.taskIdStr, this.taskAttemptIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "buildHistoryContext",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HistoryContext buildHistoryContext(final Configuration conf) throws IOException\n{\r\n    HistoryContext ctx = new MockHistoryContext(1, 1, 1);\r\n    Map<JobId, Job> jobs = ctx.getAllJobs();\r\n    JobId jobId = jobs.keySet().iterator().next();\r\n    Job mockJob = new MockJobForAcls(jobs.get(jobId), conf);\r\n    jobs.put(jobId, mockJob);\r\n    return ctx;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    set(DATATABLES_ID, \"jobs\");\r\n    set(initID(DATATABLES, \"jobs\"), jobsTableInit());\r\n    set(postInitID(DATATABLES, \"jobs\"), jobsPostTableInit());\r\n    setTableStyles(html, \"jobs\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "commonPreHead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void commonPreHead(Page.HTML<__> html)\n{\r\n    set(ACCORDION_ID, \"nav\");\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:0}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setActiveNavColumnForTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setActiveNavColumnForTask()\n{\r\n    String tid = $(TASK_ID);\r\n    String activeNav = \"2\";\r\n    if ((tid == null || tid.isEmpty())) {\r\n        activeNav = \"1\";\r\n    }\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:\" + activeNav + \"}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "nav",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> nav()\n{\r\n    return HsNavBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return HsJobsBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "jobsTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String jobsTableInit()\n{\r\n    return tableInit().append(\", 'aaData': jobsTableData\").append(\", bDeferRender: true\").append(\", bProcessing: true\").append(\", aaSorting: [[3, 'desc']]\").append(\", aoColumnDefs:[\").append(\"{'sType':'numeric', 'bSearchable': false\" + \", 'aTargets': [ 8, 9, 10, 11 ] }\").append(\"]}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "jobsPostTableInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String jobsPostTableInit()\n{\r\n    return \"var asInitVals = new Array();\\n\" + \"$('tfoot input').keyup( function () \\n{\" + \"  jobsDataTable.fnFilter( this.value, $('tfoot input').index(this) );\\n\" + \"} );\\n\" + \"$('tfoot input').each( function (i) {\\n\" + \"  asInitVals[i] = this.value;\\n\" + \"} );\\n\" + \"$('tfoot input').focus( function () {\\n\" + \"  if ( this.className == 'search_init' )\\n\" + \"  {\\n\" + \"    this.className = '';\\n\" + \"    this.value = '';\\n\" + \"  }\\n\" + \"} );\\n\" + \"$('tfoot input').blur( function (i) {\\n\" + \"  if ( this.value == '' )\\n\" + \"  {\\n\" + \"    this.className = 'search_init';\\n\" + \"    this.value = asInitVals[$('tfoot input').index(this)];\\n\" + \"  }\\n\" + \"} );\\n\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setHistoryFileManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHistoryFileManager(HistoryFileManager hsManager)\n{\r\n    this.hsManager = hsManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    LOG.info(\"CachedHistoryStorage Init\");\r\n    createLoadedJobCache(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createLoadedJobCache",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void createLoadedJobCache(Configuration conf)\n{\r\n    loadedJobCacheSize = conf.getInt(JHAdminConfig.MR_HISTORY_LOADED_JOB_CACHE_SIZE, JHAdminConfig.DEFAULT_MR_HISTORY_LOADED_JOB_CACHE_SIZE);\r\n    useLoadedTasksCache = false;\r\n    try {\r\n        String taskSizeString = conf.get(JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE);\r\n        if (taskSizeString != null) {\r\n            loadedTasksCacheSize = Math.max(Integer.parseInt(taskSizeString), 1);\r\n            useLoadedTasksCache = true;\r\n        }\r\n    } catch (NumberFormatException nfe) {\r\n        LOG.error(\"The property \" + JHAdminConfig.MR_HISTORY_LOADED_TASKS_CACHE_SIZE + \" is not an integer value.  Please set it to a positive\" + \" integer value.\");\r\n    }\r\n    CacheLoader<JobId, Job> loader;\r\n    loader = new CacheLoader<JobId, Job>() {\r\n\r\n        @Override\r\n        public Job load(JobId key) throws Exception {\r\n            return loadJob(key);\r\n        }\r\n    };\r\n    if (!useLoadedTasksCache) {\r\n        loadedJobCache = CacheBuilder.newBuilder().maximumSize(loadedJobCacheSize).initialCapacity(loadedJobCacheSize).concurrencyLevel(1).build(loader);\r\n    } else {\r\n        Weigher<JobId, Job> weightByTasks;\r\n        weightByTasks = new Weigher<JobId, Job>() {\r\n\r\n            @Override\r\n            public int weigh(JobId key, Job value) {\r\n                int taskCount = Math.min(loadedTasksCacheSize, value.getTotalMaps() + value.getTotalReduces());\r\n                return taskCount;\r\n            }\r\n        };\r\n        loadedJobCache = CacheBuilder.newBuilder().maximumWeight(loadedTasksCacheSize).weigher(weightByTasks).concurrencyLevel(1).build(loader);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "refreshLoadedJobCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void refreshLoadedJobCache()\n{\r\n    if (getServiceState() == STATE.STARTED) {\r\n        setConfig(createConf());\r\n        createLoadedJobCache(getConfig());\r\n    } else {\r\n        LOG.warn(\"Failed to execute refreshLoadedJobCache: CachedHistoryStorage is not started\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration createConf()\n{\r\n    return new Configuration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Job loadJob(JobId jobId) throws RuntimeException, IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Looking for Job \" + jobId);\r\n    }\r\n    HistoryFileInfo fileInfo;\r\n    fileInfo = hsManager.getFileInfo(jobId);\r\n    if (fileInfo == null) {\r\n        throw new HSFileRuntimeException(\"Unable to find job \" + jobId);\r\n    }\r\n    fileInfo.waitUntilMoved();\r\n    if (fileInfo.isDeleted()) {\r\n        throw new HSFileRuntimeException(\"Cannot load deleted job \" + jobId);\r\n    } else {\r\n        return fileInfo.loadJob();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getLoadedJobCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Cache<JobId, Job> getLoadedJobCache()\n{\r\n    return loadedJobCache;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFullJob",
  "errType" : [ "UncheckedExecutionException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job getFullJob(JobId jobId)\n{\r\n    Job retVal = null;\r\n    try {\r\n        retVal = loadedJobCache.getUnchecked(jobId);\r\n    } catch (UncheckedExecutionException e) {\r\n        if (e.getCause() instanceof HSFileRuntimeException) {\r\n            LOG.error(e.getCause().getMessage());\r\n            return null;\r\n        } else {\r\n            throw new YarnRuntimeException(e.getCause());\r\n        }\r\n    }\r\n    return retVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllPartialJobs",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<JobId, Job> getAllPartialJobs()\n{\r\n    LOG.debug(\"Called getAllPartialJobs()\");\r\n    SortedMap<JobId, Job> result = new TreeMap<JobId, Job>();\r\n    try {\r\n        for (HistoryFileInfo mi : hsManager.getAllFileInfo()) {\r\n            if (mi != null) {\r\n                JobId id = mi.getJobId();\r\n                mi.waitUntilMoved();\r\n                result.put(id, new PartialJob(mi.getJobIndexInfo(), id));\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Error trying to scan for all FileInfos\", e);\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getPartialJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobsInfo getPartialJobs(Long offset, Long count, String user, String queue, Long sBegin, Long sEnd, Long fBegin, Long fEnd, JobState jobState)\n{\r\n    return getPartialJobs(getAllPartialJobs().values(), offset, count, user, queue, sBegin, sEnd, fBegin, fEnd, jobState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getPartialJobs",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "JobsInfo getPartialJobs(Collection<Job> jobs, Long offset, Long count, String user, String queue, Long sBegin, Long sEnd, Long fBegin, Long fEnd, JobState jobState)\n{\r\n    JobsInfo allJobs = new JobsInfo();\r\n    if (sBegin == null || sBegin < 0)\r\n        sBegin = 0l;\r\n    if (sEnd == null)\r\n        sEnd = Long.MAX_VALUE;\r\n    if (fBegin == null || fBegin < 0)\r\n        fBegin = 0l;\r\n    if (fEnd == null)\r\n        fEnd = Long.MAX_VALUE;\r\n    if (offset == null || offset < 0)\r\n        offset = 0l;\r\n    if (count == null)\r\n        count = Long.MAX_VALUE;\r\n    if (offset > jobs.size()) {\r\n        return allJobs;\r\n    }\r\n    long at = 0;\r\n    long end = offset + count - 1;\r\n    if (end < 0) {\r\n        end = Long.MAX_VALUE;\r\n    }\r\n    for (Job job : jobs) {\r\n        if (at > end) {\r\n            break;\r\n        }\r\n        if (queue != null && !queue.isEmpty()) {\r\n            if (!job.getQueueName().equals(queue)) {\r\n                continue;\r\n            }\r\n        }\r\n        if (user != null && !user.isEmpty()) {\r\n            if (!job.getUserName().equals(user)) {\r\n                continue;\r\n            }\r\n        }\r\n        JobReport report = job.getReport();\r\n        if (report.getStartTime() < sBegin || report.getStartTime() > sEnd) {\r\n            continue;\r\n        }\r\n        if (report.getFinishTime() < fBegin || report.getFinishTime() > fEnd) {\r\n            continue;\r\n        }\r\n        if (jobState != null && jobState != report.getJobState()) {\r\n            continue;\r\n        }\r\n        at++;\r\n        if ((at - 1) < offset) {\r\n            continue;\r\n        }\r\n        JobInfo jobInfo = new JobInfo(job);\r\n        allJobs.add(jobInfo);\r\n    }\r\n    return allJobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getUseLoadedTasksCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getUseLoadedTasksCache()\n{\r\n    return useLoadedTasksCache;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getLoadedTasksCacheSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLoadedTasksCacheSize()\n{\r\n    return loadedTasksCacheSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    setActiveNavColumnForTask();\r\n    set(DATATABLES_ID, \"singleCounter\");\r\n    set(initID(DATATABLES, \"singleCounter\"), counterTableInit());\r\n    setTableStyles(html, \"singleCounter\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "counterTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String counterTableInit()\n{\r\n    return tableInit().append(\", aoColumnDefs:[\").append(\"{'sType':'title-numeric', 'aTargets': [ 1 ] }\").append(\"]}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return SingleCounterBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceInit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    Configuration config = new YarnConfiguration(conf);\r\n    MRWebAppUtil.initialize(getConfig());\r\n    try {\r\n        doSecureLogin(conf);\r\n    } catch (IOException ie) {\r\n        throw new YarnRuntimeException(\"History Server Failed to login\", ie);\r\n    }\r\n    jobHistoryService = new JobHistory();\r\n    stateStore = createStateStore(conf);\r\n    this.jhsDTSecretManager = createJHSSecretManager(conf, stateStore);\r\n    clientService = createHistoryClientService();\r\n    aggLogDelService = new AggregatedLogDeletionService();\r\n    hsAdminServer = new HSAdminServer(aggLogDelService, jobHistoryService);\r\n    addService(stateStore);\r\n    addService(new HistoryServerSecretManagerService());\r\n    addService(jobHistoryService);\r\n    addService(clientService);\r\n    addService(aggLogDelService);\r\n    addService(hsAdminServer);\r\n    DefaultMetricsSystem.initialize(\"JobHistoryServer\");\r\n    JvmMetrics jm = JvmMetrics.initSingleton(\"JobHistoryServer\", null);\r\n    pauseMonitor = new JvmPauseMonitor();\r\n    addService(pauseMonitor);\r\n    jm.setPauseMonitor(pauseMonitor);\r\n    super.serviceInit(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHistoryClientService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryClientService createHistoryClientService()\n{\r\n    return new HistoryClientService(jobHistoryService, this.jhsDTSecretManager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createJHSSecretManager",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JHSDelegationTokenSecretManager createJHSSecretManager(Configuration conf, HistoryServerStateStoreService store)\n{\r\n    long secretKeyInterval = conf.getLong(MRConfig.DELEGATION_KEY_UPDATE_INTERVAL_KEY, MRConfig.DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT);\r\n    long tokenMaxLifetime = conf.getLong(MRConfig.DELEGATION_TOKEN_MAX_LIFETIME_KEY, MRConfig.DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT);\r\n    long tokenRenewInterval = conf.getLong(MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_KEY, MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT);\r\n    return new JHSDelegationTokenSecretManager(secretKeyInterval, tokenMaxLifetime, tokenRenewInterval, 3600000, store);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createStateStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HistoryServerStateStoreService createStateStore(Configuration conf)\n{\r\n    return HistoryServerStateStoreServiceFactory.getStore(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "doSecureLogin",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void doSecureLogin(Configuration conf) throws IOException\n{\r\n    InetSocketAddress socAddr = getBindAddress(conf);\r\n    SecurityUtil.login(conf, JHAdminConfig.MR_HISTORY_KEYTAB, JHAdminConfig.MR_HISTORY_PRINCIPAL, socAddr.getHostName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getBindAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InetSocketAddress getBindAddress(Configuration conf)\n{\r\n    return conf.getSocketAddr(JHAdminConfig.MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_PORT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    DefaultMetricsSystem.shutdown();\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getClientService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryClientService getClientService()\n{\r\n    return this.clientService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "launchJobHistoryServer",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "JobHistoryServer launchJobHistoryServer(String[] args)\n{\r\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\r\n    StringUtils.startupShutdownMessage(JobHistoryServer.class, args, LOG);\r\n    JobHistoryServer jobHistoryServer = null;\r\n    try {\r\n        jobHistoryServer = new JobHistoryServer();\r\n        ShutdownHookManager.get().addShutdownHook(new CompositeServiceShutdownHook(jobHistoryServer), SHUTDOWN_HOOK_PRIORITY);\r\n        YarnConfiguration conf = new YarnConfiguration(new JobConf());\r\n        new GenericOptionsParser(conf, args);\r\n        jobHistoryServer.init(conf);\r\n        jobHistoryServer.start();\r\n    } catch (Throwable t) {\r\n        LOG.error(\"Error starting JobHistoryServer\", t);\r\n        ExitUtil.terminate(-1, \"Error starting JobHistoryServer\");\r\n    }\r\n    return jobHistoryServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    launchJobHistoryServer(args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:0}\");\r\n    setTitle(\"About History Server\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    HistoryInfo info = new HistoryInfo();\r\n    info(\"History Server\").__(\"BuildVersion\", info.getHadoopBuildVersion() + \" on \" + info.getHadoopVersionBuiltOn()).__(\"History Server started on\", Times.format(info.getStartedOn()));\r\n    return InfoBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    RPC.setProtocolEngine(conf, RefreshUserMappingsProtocolPB.class, ProtobufRpcEngine2.class);\r\n    RefreshUserMappingsProtocolServerSideTranslatorPB refreshUserMappingXlator = new RefreshUserMappingsProtocolServerSideTranslatorPB(this);\r\n    BlockingService refreshUserMappingService = RefreshUserMappingsProtocolService.newReflectiveBlockingService(refreshUserMappingXlator);\r\n    GetUserMappingsProtocolServerSideTranslatorPB getUserMappingXlator = new GetUserMappingsProtocolServerSideTranslatorPB(this);\r\n    BlockingService getUserMappingService = GetUserMappingsProtocolService.newReflectiveBlockingService(getUserMappingXlator);\r\n    HSAdminRefreshProtocolServerSideTranslatorPB refreshHSAdminProtocolXlator = new HSAdminRefreshProtocolServerSideTranslatorPB(this);\r\n    BlockingService refreshHSAdminProtocolService = HSAdminRefreshProtocolService.newReflectiveBlockingService(refreshHSAdminProtocolXlator);\r\n    clientRpcAddress = conf.getSocketAddr(JHAdminConfig.MR_HISTORY_BIND_HOST, JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    clientRpcServer = new RPC.Builder(conf).setProtocol(RefreshUserMappingsProtocolPB.class).setInstance(refreshUserMappingService).setBindAddress(clientRpcAddress.getHostName()).setPort(clientRpcAddress.getPort()).setVerbose(false).build();\r\n    addProtocol(conf, GetUserMappingsProtocolPB.class, getUserMappingService);\r\n    addProtocol(conf, HSAdminRefreshProtocolPB.class, refreshHSAdminProtocolService);\r\n    if (conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {\r\n        clientRpcServer.refreshServiceAcl(conf, new ClientHSPolicyProvider());\r\n    }\r\n    adminAcl = new AccessControlList(conf.get(JHAdminConfig.JHS_ADMIN_ACL, JHAdminConfig.DEFAULT_JHS_ADMIN_ACL));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    if (UserGroupInformation.isSecurityEnabled()) {\r\n        loginUGI = UserGroupInformation.getLoginUser();\r\n    } else {\r\n        loginUGI = UserGroupInformation.getCurrentUser();\r\n    }\r\n    clientRpcServer.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "getLoginUGI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserGroupInformation getLoginUGI()\n{\r\n    return loginUGI;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "setLoginUGI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLoginUGI(UserGroupInformation ugi)\n{\r\n    loginUGI = ugi;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (clientRpcServer != null) {\r\n        clientRpcServer.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "addProtocol",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addProtocol(Configuration conf, Class<?> protocol, BlockingService blockingService) throws IOException\n{\r\n    RPC.setProtocolEngine(conf, protocol, ProtobufRpcEngine2.class);\r\n    clientRpcServer.addProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, protocol, blockingService);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "checkAcls",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "UserGroupInformation checkAcls(String method) throws IOException\n{\r\n    UserGroupInformation user;\r\n    try {\r\n        user = UserGroupInformation.getCurrentUser();\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"Couldn't get current user\", ioe);\r\n        HSAuditLogger.logFailure(\"UNKNOWN\", method, adminAcl.toString(), HISTORY_ADMIN_SERVER, \"Couldn't get current user\");\r\n        throw ioe;\r\n    }\r\n    if (!adminAcl.isUserAllowed(user)) {\r\n        LOG.warn(\"User \" + user.getShortUserName() + \" doesn't have permission\" + \" to call '\" + method + \"'\");\r\n        HSAuditLogger.logFailure(user.getShortUserName(), method, adminAcl.toString(), HISTORY_ADMIN_SERVER, AuditConstants.UNAUTHORIZED_USER);\r\n        throw new AccessControlException(\"User \" + user.getShortUserName() + \" doesn't have permission\" + \" to call '\" + method + \"'\");\r\n    }\r\n    LOG.info(\"HS Admin: \" + method + \" invoked by user \" + user.getShortUserName());\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "getGroupsForUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getGroupsForUser(String user) throws IOException\n{\r\n    return UserGroupInformation.createRemoteUser(user).getGroupNames();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "refreshUserToGroupsMappings",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void refreshUserToGroupsMappings() throws IOException\n{\r\n    UserGroupInformation user = checkAcls(\"refreshUserToGroupsMappings\");\r\n    Groups.getUserToGroupsMappingService().refresh();\r\n    HSAuditLogger.logSuccess(user.getShortUserName(), \"refreshUserToGroupsMappings\", HISTORY_ADMIN_SERVER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void refreshSuperUserGroupsConfiguration() throws IOException\n{\r\n    UserGroupInformation user = checkAcls(\"refreshSuperUserGroupsConfiguration\");\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(createConf());\r\n    HSAuditLogger.logSuccess(user.getShortUserName(), \"refreshSuperUserGroupsConfiguration\", HISTORY_ADMIN_SERVER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "createConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration createConf()\n{\r\n    return new Configuration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "refreshAdminAcls",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void refreshAdminAcls() throws IOException\n{\r\n    UserGroupInformation user = checkAcls(\"refreshAdminAcls\");\r\n    Configuration conf = createConf();\r\n    adminAcl = new AccessControlList(conf.get(JHAdminConfig.JHS_ADMIN_ACL, JHAdminConfig.DEFAULT_JHS_ADMIN_ACL));\r\n    HSAuditLogger.logSuccess(user.getShortUserName(), \"refreshAdminAcls\", HISTORY_ADMIN_SERVER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "refreshLoadedJobCache",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void refreshLoadedJobCache() throws IOException\n{\r\n    UserGroupInformation user = checkAcls(\"refreshLoadedJobCache\");\r\n    try {\r\n        jobHistoryService.refreshLoadedJobCache();\r\n    } catch (UnsupportedOperationException e) {\r\n        HSAuditLogger.logFailure(user.getShortUserName(), \"refreshLoadedJobCache\", adminAcl.toString(), HISTORY_ADMIN_SERVER, e.getMessage());\r\n        throw e;\r\n    }\r\n    HSAuditLogger.logSuccess(user.getShortUserName(), \"refreshLoadedJobCache\", HISTORY_ADMIN_SERVER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "refreshLogRetentionSettings",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void refreshLogRetentionSettings() throws IOException\n{\r\n    UserGroupInformation user = checkAcls(\"refreshLogRetentionSettings\");\r\n    try {\r\n        loginUGI.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws IOException {\r\n                aggLogDelService.refreshLogRetentionSettings();\r\n                return null;\r\n            }\r\n        });\r\n    } catch (InterruptedException e) {\r\n        throw new IOException(e);\r\n    }\r\n    HSAuditLogger.logSuccess(user.getShortUserName(), \"refreshLogRetentionSettings\", \"HSAdminServer\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\server",
  "methodName" : "refreshJobRetentionSettings",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void refreshJobRetentionSettings() throws IOException\n{\r\n    UserGroupInformation user = checkAcls(\"refreshJobRetentionSettings\");\r\n    try {\r\n        loginUGI.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n            @Override\r\n            public Void run() throws IOException {\r\n                jobHistoryService.refreshJobRetentionSettings();\r\n                return null;\r\n            }\r\n        });\r\n    } catch (InterruptedException e) {\r\n        throw new IOException(e);\r\n    }\r\n    HSAuditLogger.logSuccess(user.getShortUserName(), \"refreshJobRetentionSettings\", HISTORY_ADMIN_SERVER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCompletedMaps",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getCompletedMaps()\n{\r\n    int killedMaps = (int) jobInfo.getKilledMaps();\r\n    int failedMaps = (int) jobInfo.getFailedMaps();\r\n    if (killedMaps == UNDEFINED_VALUE) {\r\n        killedMaps = 0;\r\n    }\r\n    if (failedMaps == UNDEFINED_VALUE) {\r\n        failedMaps = 0;\r\n    }\r\n    return (int) (jobInfo.getSucceededMaps() + killedMaps + failedMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCompletedReduces",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getCompletedReduces()\n{\r\n    int killedReduces = (int) jobInfo.getKilledReduces();\r\n    int failedReduces = (int) jobInfo.getFailedReduces();\r\n    if (killedReduces == UNDEFINED_VALUE) {\r\n        killedReduces = 0;\r\n    }\r\n    if (failedReduces == UNDEFINED_VALUE) {\r\n        failedReduces = 0;\r\n    }\r\n    return (int) (jobInfo.getSucceededReduces() + killedReduces + failedReduces);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters getAllCounters()\n{\r\n    return jobInfo.getTotalCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getID()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobReport getReport()\n{\r\n    if (report == null) {\r\n        constructJobReport();\r\n    }\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "constructJobReport",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void constructJobReport()\n{\r\n    report = Records.newRecord(JobReport.class);\r\n    report.setJobId(jobId);\r\n    report.setJobState(JobState.valueOf(jobInfo.getJobStatus()));\r\n    report.setSubmitTime(jobInfo.getSubmitTime());\r\n    report.setStartTime(jobInfo.getLaunchTime());\r\n    report.setFinishTime(jobInfo.getFinishTime());\r\n    report.setJobName(jobInfo.getJobname());\r\n    report.setUser(jobInfo.getUsername());\r\n    report.setDiagnostics(jobInfo.getErrorInfo());\r\n    if (getTotalMaps() == 0) {\r\n        report.setMapProgress(1.0f);\r\n    } else {\r\n        report.setMapProgress((float) getCompletedMaps() / getTotalMaps());\r\n    }\r\n    if (getTotalReduces() == 0) {\r\n        report.setReduceProgress(1.0f);\r\n    } else {\r\n        report.setReduceProgress((float) getCompletedReduces() / getTotalReduces());\r\n    }\r\n    report.setJobFile(getConfFile().toString());\r\n    String historyUrl = \"N/A\";\r\n    try {\r\n        historyUrl = MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(conf, jobId.getAppId());\r\n    } catch (UnknownHostException e) {\r\n        LOG.error(\"Problem determining local host: \" + e.getMessage());\r\n    }\r\n    report.setTrackingUrl(historyUrl);\r\n    report.setAMInfos(getAMInfos());\r\n    report.setIsUber(isUber());\r\n    report.setHistoryFile(info.getHistoryFile().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return 1.0f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobState getState()\n{\r\n    return JobState.valueOf(jobInfo.getJobStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Task getTask(TaskId taskId)\n{\r\n    if (tasksLoaded.get()) {\r\n        return tasks.get(taskId);\r\n    } else {\r\n        TaskID oldTaskId = TypeConverter.fromYarn(taskId);\r\n        CompletedTask completedTask = new CompletedTask(taskId, jobInfo.getAllTasks().get(oldTaskId));\r\n        return completedTask;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTaskAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskAttemptCompletionEvent[] getTaskAttemptCompletionEvents(int fromEventId, int maxEvents)\n{\r\n    if (completionEvents == null) {\r\n        constructTaskAttemptCompletionEvents();\r\n    }\r\n    return getAttemptCompletionEvents(completionEvents, fromEventId, maxEvents);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getMapAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskCompletionEvent[] getMapAttemptCompletionEvents(int startIndex, int maxEvents)\n{\r\n    if (mapCompletionEvents == null) {\r\n        constructTaskAttemptCompletionEvents();\r\n    }\r\n    return TypeConverter.fromYarn(getAttemptCompletionEvents(mapCompletionEvents, startIndex, maxEvents));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskAttemptCompletionEvent[] getAttemptCompletionEvents(List<TaskAttemptCompletionEvent> eventList, int startIndex, int maxEvents)\n{\r\n    TaskAttemptCompletionEvent[] events = new TaskAttemptCompletionEvent[0];\r\n    if (eventList.size() > startIndex) {\r\n        int actualMax = Math.min(maxEvents, (eventList.size() - startIndex));\r\n        events = eventList.subList(startIndex, actualMax + startIndex).toArray(events);\r\n    }\r\n    return events;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "constructTaskAttemptCompletionEvents",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void constructTaskAttemptCompletionEvents()\n{\r\n    loadAllTasks();\r\n    completionEvents = new LinkedList<TaskAttemptCompletionEvent>();\r\n    List<TaskAttempt> allTaskAttempts = new LinkedList<TaskAttempt>();\r\n    int numMapAttempts = 0;\r\n    for (Map.Entry<TaskId, Task> taskEntry : tasks.entrySet()) {\r\n        Task task = taskEntry.getValue();\r\n        for (Map.Entry<TaskAttemptId, TaskAttempt> taskAttemptEntry : task.getAttempts().entrySet()) {\r\n            TaskAttempt taskAttempt = taskAttemptEntry.getValue();\r\n            allTaskAttempts.add(taskAttempt);\r\n            if (task.getType() == TaskType.MAP) {\r\n                ++numMapAttempts;\r\n            }\r\n        }\r\n    }\r\n    Collections.sort(allTaskAttempts, new Comparator<TaskAttempt>() {\r\n\r\n        @Override\r\n        public int compare(TaskAttempt o1, TaskAttempt o2) {\r\n            if (o1.getFinishTime() == 0 || o2.getFinishTime() == 0) {\r\n                if (o1.getFinishTime() == 0 && o2.getFinishTime() == 0) {\r\n                    if (o1.getLaunchTime() == 0 || o2.getLaunchTime() == 0) {\r\n                        if (o1.getLaunchTime() == 0 && o2.getLaunchTime() == 0) {\r\n                            return 0;\r\n                        } else {\r\n                            long res = o1.getLaunchTime() - o2.getLaunchTime();\r\n                            return res > 0 ? -1 : 1;\r\n                        }\r\n                    } else {\r\n                        return (int) (o1.getLaunchTime() - o2.getLaunchTime());\r\n                    }\r\n                } else {\r\n                    long res = o1.getFinishTime() - o2.getFinishTime();\r\n                    return res > 0 ? -1 : 1;\r\n                }\r\n            } else {\r\n                return (int) (o1.getFinishTime() - o2.getFinishTime());\r\n            }\r\n        }\r\n    });\r\n    mapCompletionEvents = new ArrayList<TaskAttemptCompletionEvent>(numMapAttempts);\r\n    int eventId = 0;\r\n    for (TaskAttempt taskAttempt : allTaskAttempts) {\r\n        TaskAttemptCompletionEvent tace = Records.newRecord(TaskAttemptCompletionEvent.class);\r\n        int attemptRunTime = -1;\r\n        if (taskAttempt.getLaunchTime() != 0 && taskAttempt.getFinishTime() != 0) {\r\n            attemptRunTime = (int) (taskAttempt.getFinishTime() - taskAttempt.getLaunchTime());\r\n        }\r\n        TaskAttemptCompletionEventStatus taceStatus = TaskAttemptCompletionEventStatus.KILLED;\r\n        String taStateString = taskAttempt.getState().toString();\r\n        try {\r\n            taceStatus = TaskAttemptCompletionEventStatus.valueOf(taStateString);\r\n        } catch (Exception e) {\r\n            LOG.warn(\"Cannot constuct TACEStatus from TaskAtemptState: [\" + taStateString + \"] for taskAttemptId: [\" + taskAttempt.getID() + \"]. Defaulting to KILLED\");\r\n        }\r\n        tace.setAttemptId(taskAttempt.getID());\r\n        tace.setAttemptRunTime(attemptRunTime);\r\n        tace.setEventId(eventId++);\r\n        tace.setMapOutputServerAddress(taskAttempt.getAssignedContainerMgrAddress());\r\n        tace.setStatus(taceStatus);\r\n        completionEvents.add(tace);\r\n        if (taskAttempt.getID().getTaskId().getTaskType() == TaskType.MAP) {\r\n            mapCompletionEvents.add(tace);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<TaskId, Task> getTasks()\n{\r\n    loadAllTasks();\r\n    return tasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadAllTasks",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void loadAllTasks()\n{\r\n    if (tasksLoaded.get()) {\r\n        return;\r\n    }\r\n    tasksLock.lock();\r\n    try {\r\n        if (tasksLoaded.get()) {\r\n            return;\r\n        }\r\n        for (Map.Entry<TaskID, TaskInfo> entry : jobInfo.getAllTasks().entrySet()) {\r\n            TaskId yarnTaskID = TypeConverter.toYarn(entry.getKey());\r\n            TaskInfo taskInfo = entry.getValue();\r\n            Task task = new CompletedTask(yarnTaskID, taskInfo);\r\n            tasks.put(yarnTaskID, task);\r\n            if (task.getType() == TaskType.MAP) {\r\n                mapTasks.put(task.getID(), task);\r\n            } else if (task.getType() == TaskType.REDUCE) {\r\n                reduceTasks.put(task.getID(), task);\r\n            }\r\n        }\r\n        tasksLoaded.set(true);\r\n    } finally {\r\n        tasksLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createJobHistoryParser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobHistoryParser createJobHistoryParser(Path historyFileAbsolute) throws IOException\n{\r\n    return new JobHistoryParser(historyFileAbsolute.getFileSystem(conf), historyFileAbsolute);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadFullHistoryData",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void loadFullHistoryData(boolean loadTasks, Path historyFileAbsolute) throws IOException\n{\r\n    LOG.info(\"Loading history file: [\" + historyFileAbsolute + \"]\");\r\n    if (this.jobInfo != null) {\r\n        return;\r\n    }\r\n    if (historyFileAbsolute != null) {\r\n        JobHistoryParser parser = null;\r\n        try {\r\n            parser = createJobHistoryParser(historyFileAbsolute);\r\n            this.jobInfo = parser.parse();\r\n        } catch (IOException e) {\r\n            String errorMsg = \"Could not load history file \" + historyFileAbsolute;\r\n            LOG.warn(errorMsg, e);\r\n            throw new YarnRuntimeException(errorMsg, e);\r\n        }\r\n        IOException parseException = parser.getParseException();\r\n        if (parseException != null) {\r\n            String errorMsg = \"Could not parse history file \" + historyFileAbsolute;\r\n            LOG.warn(errorMsg, parseException);\r\n            throw new YarnRuntimeException(errorMsg, parseException);\r\n        }\r\n    } else {\r\n        String errorMsg = \"History file not found\";\r\n        LOG.warn(errorMsg);\r\n        throw new IOException(errorMsg);\r\n    }\r\n    if (loadTasks) {\r\n        loadAllTasks();\r\n        LOG.info(\"TaskInfo loaded\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> getDiagnostics()\n{\r\n    return Collections.singletonList(jobInfo.getErrorInfo());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getName()\n{\r\n    return jobInfo.getJobname();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return jobInfo.getJobQueueName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTotalMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalMaps()\n{\r\n    return (int) jobInfo.getTotalMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTotalReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalReduces()\n{\r\n    return (int) jobInfo.getTotalReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "isUber",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isUber()\n{\r\n    return jobInfo.getUberized();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<TaskId, Task> getTasks(TaskType taskType)\n{\r\n    loadAllTasks();\r\n    if (TaskType.MAP.equals(taskType)) {\r\n        return mapTasks;\r\n    } else {\r\n        return reduceTasks;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation)\n{\r\n    Map<JobACL, AccessControlList> jobACLs = jobInfo.getJobACLs();\r\n    AccessControlList jobACL = jobACLs.get(jobOperation);\r\n    if (jobACL == null) {\r\n        return true;\r\n    }\r\n    return aclsMgr.checkAccess(callerUGI, jobOperation, jobInfo.getUsername(), jobACL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJobACLs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<JobACL, AccessControlList> getJobACLs()\n{\r\n    return jobInfo.getJobACLs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getUserName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUserName()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getConfFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getConfFile()\n{\r\n    return info.getConfFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadConfFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration loadConfFile() throws IOException\n{\r\n    return info.loadConfFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAMInfos",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<AMInfo> getAMInfos()\n{\r\n    List<AMInfo> amInfos = new LinkedList<AMInfo>();\r\n    for (org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.AMInfo jhAmInfo : jobInfo.getAMInfos()) {\r\n        AMInfo amInfo = MRBuilderUtils.newAMInfo(jhAmInfo.getAppAttemptId(), jhAmInfo.getStartTime(), jhAmInfo.getContainerId(), jhAmInfo.getNodeManagerHost(), jhAmInfo.getNodeManagerPort(), jhAmInfo.getNodeManagerHttpPort());\r\n        amInfos.add(amInfo);\r\n    }\r\n    return amInfos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    throw new UnsupportedOperationException(\"Can't set job's queue name in history\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setJobPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobPriority(Priority priority)\n{\r\n    throw new UnsupportedOperationException(\"Can't set job's priority in history\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getFailedMaps()\n{\r\n    return (int) jobInfo.getFailedMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getFailedReduces()\n{\r\n    return (int) jobInfo.getFailedReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getKilledMaps()\n{\r\n    return (int) jobInfo.getKilledMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getKilledReduces()\n{\r\n    return (int) jobInfo.getKilledReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MRDelegationTokenIdentifier createIdentifier()\n{\r\n    return new MRDelegationTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeNewMasterKey",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void storeNewMasterKey(DelegationKey key) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Storing master key \" + key.getKeyId());\r\n    }\r\n    try {\r\n        store.storeTokenMasterKey(key);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to store master key \" + key.getKeyId(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeStoredMasterKey",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeStoredMasterKey(DelegationKey key)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Removing master key \" + key.getKeyId());\r\n    }\r\n    try {\r\n        store.removeTokenMasterKey(key);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to remove master key \" + key.getKeyId(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeNewToken",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void storeNewToken(MRDelegationTokenIdentifier tokenId, long renewDate)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Storing token \" + tokenId.getSequenceNumber());\r\n    }\r\n    try {\r\n        store.storeToken(tokenId, renewDate);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to store token \" + tokenId.getSequenceNumber(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeStoredToken",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeStoredToken(MRDelegationTokenIdentifier tokenId) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Storing token \" + tokenId.getSequenceNumber());\r\n    }\r\n    try {\r\n        store.removeToken(tokenId);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to remove token \" + tokenId.getSequenceNumber(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "updateStoredToken",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateStoredToken(MRDelegationTokenIdentifier tokenId, long renewDate)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Updating token \" + tokenId.getSequenceNumber());\r\n    }\r\n    try {\r\n        store.updateToken(tokenId, renewDate);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to update token \" + tokenId.getSequenceNumber(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "recover",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void recover(HistoryServerState state) throws IOException\n{\r\n    LOG.info(\"Recovering \" + getClass().getSimpleName());\r\n    for (DelegationKey key : state.tokenMasterKeyState) {\r\n        addKey(key);\r\n    }\r\n    for (Entry<MRDelegationTokenIdentifier, Long> entry : state.tokenState.entrySet()) {\r\n        addPersistedDelegationToken(entry.getKey(), entry.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "index",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void index()\n{\r\n    setTitle(\"JobHistory\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "jobPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> jobPage()\n{\r\n    return HsJobPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "countersPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> countersPage()\n{\r\n    return HsCountersPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "tasksPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> tasksPage()\n{\r\n    return HsTasksPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "taskPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> taskPage()\n{\r\n    return HsTaskPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "attemptsPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> attemptsPage()\n{\r\n    return HsAttemptsPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "job",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void job()\n{\r\n    super.job();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "jobCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void jobCounters()\n{\r\n    super.jobCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "taskCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void taskCounters()\n{\r\n    super.taskCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "tasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tasks()\n{\r\n    super.tasks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "task",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void task()\n{\r\n    super.task();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "attempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void attempts()\n{\r\n    super.attempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "confPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> confPage()\n{\r\n    return HsConfPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "aboutPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> aboutPage()\n{\r\n    return HsAboutPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "about",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void about()\n{\r\n    render(aboutPage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "logs",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void logs()\n{\r\n    String logEntity = $(ENTITY_STRING);\r\n    JobID jid = null;\r\n    try {\r\n        jid = JobID.forName(logEntity);\r\n        set(JOB_ID, logEntity);\r\n        requireJob();\r\n    } catch (Exception e) {\r\n    }\r\n    if (jid == null) {\r\n        try {\r\n            TaskAttemptID taskAttemptId = TaskAttemptID.forName(logEntity);\r\n            set(TASK_ID, taskAttemptId.getTaskID().toString());\r\n            set(JOB_ID, taskAttemptId.getJobID().toString());\r\n            requireTask();\r\n            requireJob();\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n    render(HsLogsPage.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "nmlogs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nmlogs()\n{\r\n    render(AggregatedLogsPage.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "singleCounterPage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends View> singleCounterPage()\n{\r\n    return HsSingleCounterPage.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "singleJobCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void singleJobCounter() throws IOException\n{\r\n    super.singleJobCounter();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "singleTaskCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void singleTaskCounter() throws IOException\n{\r\n    super.singleTaskCounter();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobId getID()\n{\r\n    return this.jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getName()\n{\r\n    return jobIndexInfo.getJobName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return jobIndexInfo.getQueueName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getState",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobState getState()\n{\r\n    JobState js = null;\r\n    try {\r\n        js = JobState.valueOf(jobIndexInfo.getJobStatus());\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Exception while parsing job state. Defaulting to KILLED\", e);\r\n        js = JobState.KILLED;\r\n    }\r\n    return js;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobReport getReport()\n{\r\n    return jobReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return 1.0f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getAllCounters()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<TaskId, Task> getTasks()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<TaskId, Task> getTasks(TaskType taskType)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Task getTask(TaskId taskID)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getDiagnostics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTotalMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalMaps()\n{\r\n    return jobIndexInfo.getNumMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTotalReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalReduces()\n{\r\n    return jobIndexInfo.getNumReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCompletedMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCompletedMaps()\n{\r\n    return jobIndexInfo.getNumMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCompletedReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCompletedReduces()\n{\r\n    return jobIndexInfo.getNumReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "isUber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUber()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTaskAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptCompletionEvent[] getTaskAttemptCompletionEvents(int fromEventId, int maxEvents)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getMapAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskCompletionEvent[] getMapAttemptCompletionEvents(int startIndex, int maxEvents)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getUserName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUserName()\n{\r\n    return jobIndexInfo.getUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getConfFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getConfFile()\n{\r\n    throw new IllegalStateException(\"Not implemented yet\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadConfFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration loadConfFile()\n{\r\n    throw new IllegalStateException(\"Not implemented yet\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJobACLs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<JobACL, AccessControlList> getJobACLs()\n{\r\n    throw new IllegalStateException(\"Not implemented yet\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAMInfos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<AMInfo> getAMInfos()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    throw new UnsupportedOperationException(\"Can't set job's queue name in history\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setJobPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobPriority(Priority priority)\n{\r\n    throw new UnsupportedOperationException(\"Can't set job's priority in history\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedMaps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedReduces()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledMaps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledReduces()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "initStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initStorage(Configuration conf) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "startStorage",
  "errType" : [ "NativeDB.DBException", "DBException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void startStorage() throws IOException\n{\r\n    Path storeRoot = createStorageDir(getConfig());\r\n    Options options = new Options();\r\n    options.createIfMissing(false);\r\n    LOG.info(\"Using state database at \" + storeRoot + \" for recovery\");\r\n    File dbfile = new File(storeRoot.toString());\r\n    try {\r\n        db = JniDBFactory.factory.open(dbfile, options);\r\n    } catch (NativeDB.DBException e) {\r\n        if (e.isNotFound() || e.getMessage().contains(\" does not exist \")) {\r\n            LOG.info(\"Creating state database at \" + dbfile);\r\n            options.createIfMissing(true);\r\n            try {\r\n                db = JniDBFactory.factory.open(dbfile, options);\r\n                storeVersion();\r\n            } catch (DBException dbErr) {\r\n                throw new IOException(dbErr.getMessage(), dbErr);\r\n            }\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n    checkVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "closeStorage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeStorage() throws IOException\n{\r\n    if (db != null) {\r\n        db.close();\r\n        db = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadState",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HistoryServerState loadState() throws IOException\n{\r\n    HistoryServerState state = new HistoryServerState();\r\n    int numKeys = loadTokenMasterKeys(state);\r\n    LOG.info(\"Recovered \" + numKeys + \" token master keys\");\r\n    int numTokens = loadTokens(state);\r\n    LOG.info(\"Recovered \" + numTokens + \" tokens\");\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokenMasterKeys",
  "errType" : [ "DBException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int loadTokenMasterKeys(HistoryServerState state) throws IOException\n{\r\n    int numKeys = 0;\r\n    LeveldbIterator iter = null;\r\n    try {\r\n        iter = new LeveldbIterator(db);\r\n        iter.seek(bytes(TOKEN_MASTER_KEY_KEY_PREFIX));\r\n        while (iter.hasNext()) {\r\n            Entry<byte[], byte[]> entry = iter.next();\r\n            String key = asString(entry.getKey());\r\n            if (!key.startsWith(TOKEN_MASTER_KEY_KEY_PREFIX)) {\r\n                break;\r\n            }\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Loading master key from \" + key);\r\n            }\r\n            try {\r\n                loadTokenMasterKey(state, entry.getValue());\r\n            } catch (IOException e) {\r\n                throw new IOException(\"Error loading token master key from \" + key, e);\r\n            }\r\n            ++numKeys;\r\n        }\r\n    } catch (DBException e) {\r\n        throw new IOException(e);\r\n    } finally {\r\n        if (iter != null) {\r\n            iter.close();\r\n        }\r\n    }\r\n    return numKeys;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void loadTokenMasterKey(HistoryServerState state, byte[] data) throws IOException\n{\r\n    DelegationKey key = new DelegationKey();\r\n    DataInputStream in = new DataInputStream(new ByteArrayInputStream(data));\r\n    try {\r\n        key.readFields(in);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, in);\r\n    }\r\n    state.tokenMasterKeyState.add(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokens",
  "errType" : [ "DBException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int loadTokens(HistoryServerState state) throws IOException\n{\r\n    int numTokens = 0;\r\n    LeveldbIterator iter = null;\r\n    try {\r\n        iter = new LeveldbIterator(db);\r\n        iter.seek(bytes(TOKEN_STATE_KEY_PREFIX));\r\n        while (iter.hasNext()) {\r\n            Entry<byte[], byte[]> entry = iter.next();\r\n            String key = asString(entry.getKey());\r\n            if (!key.startsWith(TOKEN_STATE_KEY_PREFIX)) {\r\n                break;\r\n            }\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Loading token from \" + key);\r\n            }\r\n            try {\r\n                loadToken(state, entry.getValue());\r\n            } catch (IOException e) {\r\n                throw new IOException(\"Error loading token state from \" + key, e);\r\n            }\r\n            ++numTokens;\r\n        }\r\n    } catch (DBException e) {\r\n        throw new IOException(e);\r\n    } finally {\r\n        if (iter != null) {\r\n            iter.close();\r\n        }\r\n    }\r\n    return numTokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void loadToken(HistoryServerState state, byte[] data) throws IOException\n{\r\n    MRDelegationTokenIdentifier tokenId = new MRDelegationTokenIdentifier();\r\n    long renewDate;\r\n    DataInputStream in = new DataInputStream(new ByteArrayInputStream(data));\r\n    try {\r\n        tokenId.readFields(in);\r\n        renewDate = in.readLong();\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, in);\r\n    }\r\n    state.tokenState.put(tokenId, renewDate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeToken",
  "errType" : [ "DBException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Storing token \" + tokenId.getSequenceNumber());\r\n    }\r\n    ByteArrayOutputStream memStream = new ByteArrayOutputStream();\r\n    DataOutputStream dataStream = new DataOutputStream(memStream);\r\n    try {\r\n        tokenId.write(dataStream);\r\n        dataStream.writeLong(renewDate);\r\n        dataStream.close();\r\n        dataStream = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, dataStream);\r\n    }\r\n    String dbKey = getTokenDatabaseKey(tokenId);\r\n    try {\r\n        db.put(bytes(dbKey), memStream.toByteArray());\r\n    } catch (DBException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "updateToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n    storeToken(tokenId, renewDate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeToken",
  "errType" : [ "DBException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeToken(MRDelegationTokenIdentifier tokenId) throws IOException\n{\r\n    String dbKey = getTokenDatabaseKey(tokenId);\r\n    try {\r\n        db.delete(bytes(dbKey));\r\n    } catch (DBException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTokenDatabaseKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTokenDatabaseKey(MRDelegationTokenIdentifier tokenId)\n{\r\n    return TOKEN_STATE_KEY_PREFIX + tokenId.getSequenceNumber();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeTokenMasterKey",
  "errType" : [ "DBException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void storeTokenMasterKey(DelegationKey masterKey) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Storing master key \" + masterKey.getKeyId());\r\n    }\r\n    ByteArrayOutputStream memStream = new ByteArrayOutputStream();\r\n    DataOutputStream dataStream = new DataOutputStream(memStream);\r\n    try {\r\n        masterKey.write(dataStream);\r\n        dataStream.close();\r\n        dataStream = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, dataStream);\r\n    }\r\n    String dbKey = getTokenMasterKeyDatabaseKey(masterKey);\r\n    try {\r\n        db.put(bytes(dbKey), memStream.toByteArray());\r\n    } catch (DBException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeTokenMasterKey",
  "errType" : [ "DBException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeTokenMasterKey(DelegationKey masterKey) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Removing master key \" + masterKey.getKeyId());\r\n    }\r\n    String dbKey = getTokenMasterKeyDatabaseKey(masterKey);\r\n    try {\r\n        db.delete(bytes(dbKey));\r\n    } catch (DBException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTokenMasterKeyDatabaseKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTokenMasterKeyDatabaseKey(DelegationKey masterKey)\n{\r\n    return TOKEN_MASTER_KEY_KEY_PREFIX + masterKey.getKeyId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createStorageDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path createStorageDir(Configuration conf) throws IOException\n{\r\n    String confPath = conf.get(JHAdminConfig.MR_HS_LEVELDB_STATE_STORE_PATH);\r\n    if (confPath == null) {\r\n        throw new IOException(\"No store location directory configured in \" + JHAdminConfig.MR_HS_LEVELDB_STATE_STORE_PATH);\r\n    }\r\n    Path root = new Path(confPath, DB_NAME);\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    fs.mkdirs(root, new FsPermission((short) 0700));\r\n    return root;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadVersion",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Version loadVersion() throws IOException\n{\r\n    byte[] data = db.get(bytes(DB_SCHEMA_VERSION_KEY));\r\n    if (data == null || data.length == 0) {\r\n        return Version.newInstance(1, 0);\r\n    }\r\n    Version version = new VersionPBImpl(VersionProto.parseFrom(data));\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void storeVersion() throws IOException\n{\r\n    dbStoreVersion(CURRENT_VERSION_INFO);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "dbStoreVersion",
  "errType" : [ "DBException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void dbStoreVersion(Version state) throws IOException\n{\r\n    String key = DB_SCHEMA_VERSION_KEY;\r\n    byte[] data = ((VersionPBImpl) state).getProto().toByteArray();\r\n    try {\r\n        db.put(bytes(key), data);\r\n    } catch (DBException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCurrentVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Version getCurrentVersion()\n{\r\n    return CURRENT_VERSION_INFO;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "checkVersion",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkVersion() throws IOException\n{\r\n    Version loadedVersion = loadVersion();\r\n    LOG.info(\"Loaded state version info \" + loadedVersion);\r\n    if (loadedVersion.equals(getCurrentVersion())) {\r\n        return;\r\n    }\r\n    if (loadedVersion.isCompatibleTo(getCurrentVersion())) {\r\n        LOG.info(\"Storing state version info \" + getCurrentVersion());\r\n        storeVersion();\r\n    } else {\r\n        throw new IOException(\"Incompatible version for state: expecting state version \" + getCurrentVersion() + \", but loading version \" + loadedVersion);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void render(Block html)\n{\r\n    String jid = $(JOB_ID);\r\n    if (jid.isEmpty()) {\r\n        html.p().__(\"Sorry, can't do anything without a JobID.\").__();\r\n        return;\r\n    }\r\n    JobId jobID = MRApps.toJobID(jid);\r\n    Job j = appContext.getJob(jobID);\r\n    if (j == null) {\r\n        html.p().__(\"Sorry, \", jid, \" not found.\").__();\r\n        return;\r\n    }\r\n    if (j instanceof UnparsedJob) {\r\n        final int taskCount = j.getTotalMaps() + j.getTotalReduces();\r\n        UnparsedJob oversizedJob = (UnparsedJob) j;\r\n        html.p().__(\"The job has a total of \" + taskCount + \" tasks. \").__(\"Any job larger than \" + oversizedJob.getMaxTasksAllowed() + \" will not be loaded.\").__();\r\n        html.p().__(\"You can either use the CLI tool: 'mapred job -history'\" + \" to view large jobs or adjust the property \" + JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX + \".\").__();\r\n        return;\r\n    }\r\n    List<AMInfo> amInfos = j.getAMInfos();\r\n    JobInfo job = new JobInfo(j);\r\n    ResponseInfo infoBlock = info(\"Job Overview\").__(\"Job Name:\", job.getName()).__(\"User Name:\", job.getUserName()).__(\"Queue:\", job.getQueueName()).__(\"State:\", job.getState()).__(\"Uberized:\", job.isUber()).__(\"Submitted:\", new Date(job.getSubmitTime())).__(\"Started:\", job.getStartTimeStr()).__(\"Finished:\", new Date(job.getFinishTime())).__(\"Elapsed:\", StringUtils.formatTime(Times.elapsed(job.getStartTime(), job.getFinishTime(), false)));\r\n    String amString = amInfos.size() == 1 ? \"ApplicationMaster\" : \"ApplicationMasters\";\r\n    List<String> diagnostics = j.getDiagnostics();\r\n    if (diagnostics != null && !diagnostics.isEmpty()) {\r\n        StringBuffer b = new StringBuffer();\r\n        for (String diag : diagnostics) {\r\n            b.append(addTaskLinks(diag));\r\n        }\r\n        infoBlock._r(\"Diagnostics:\", b.toString());\r\n    }\r\n    if (job.getNumMaps() > 0) {\r\n        infoBlock.__(\"Average Map Time\", StringUtils.formatTime(job.getAvgMapTime()));\r\n    }\r\n    if (job.getNumReduces() > 0) {\r\n        infoBlock.__(\"Average Shuffle Time\", StringUtils.formatTime(job.getAvgShuffleTime()));\r\n        infoBlock.__(\"Average Merge Time\", StringUtils.formatTime(job.getAvgMergeTime()));\r\n        infoBlock.__(\"Average Reduce Time\", StringUtils.formatTime(job.getAvgReduceTime()));\r\n    }\r\n    for (ConfEntryInfo entry : job.getAcls()) {\r\n        infoBlock.__(\"ACL \" + entry.getName() + \":\", entry.getValue());\r\n    }\r\n    DIV<Hamlet> div = html.__(InfoBlock.class).div(_INFO_WRAP);\r\n    TABLE<DIV<Hamlet>> table = div.table(\"#job\");\r\n    table.tr().th(amString).__().tr().th(_TH, \"Attempt Number\").th(_TH, \"Start Time\").th(_TH, \"Node\").th(_TH, \"Logs\").__();\r\n    boolean odd = false;\r\n    for (AMInfo amInfo : amInfos) {\r\n        AMAttemptInfo attempt = new AMAttemptInfo(amInfo, job.getId(), job.getUserName(), \"\", \"\");\r\n        table.tr((odd = !odd) ? _ODD : _EVEN).td(String.valueOf(attempt.getAttemptId())).td(new Date(attempt.getStartTime()).toString()).td().a(\".nodelink\", url(MRWebAppUtil.getYARNWebappScheme(), attempt.getNodeHttpAddress()), attempt.getNodeHttpAddress()).__().td().a(\".logslink\", url(attempt.getLogsLink()), \"logs\").__().__();\r\n    }\r\n    table.__();\r\n    div.__();\r\n    html.div(_INFO_WRAP).table(\"#job\").tr().th(_TH, \"Task Type\").th(_TH, \"Total\").th(_TH, \"Complete\").__().tr(_ODD).th().a(url(\"tasks\", jid, \"m\"), \"Map\").__().td(String.valueOf(String.valueOf(job.getMapsTotal()))).td(String.valueOf(String.valueOf(job.getMapsCompleted()))).__().tr(_EVEN).th().a(url(\"tasks\", jid, \"r\"), \"Reduce\").__().td(String.valueOf(String.valueOf(job.getReducesTotal()))).td(String.valueOf(String.valueOf(job.getReducesCompleted()))).__().__().table(\"#job\").tr().th(_TH, \"Attempt Type\").th(_TH, \"Failed\").th(_TH, \"Killed\").th(_TH, \"Successful\").__().tr(_ODD).th(\"Maps\").td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.FAILED.toString()), String.valueOf(job.getFailedMapAttempts())).__().td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.KILLED.toString()), String.valueOf(job.getKilledMapAttempts())).__().td().a(url(\"attempts\", jid, \"m\", TaskAttemptStateUI.SUCCESSFUL.toString()), String.valueOf(job.getSuccessfulMapAttempts())).__().__().tr(_EVEN).th(\"Reduces\").td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.FAILED.toString()), String.valueOf(job.getFailedReduceAttempts())).__().td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.KILLED.toString()), String.valueOf(job.getKilledReduceAttempts())).__().td().a(url(\"attempts\", jid, \"r\", TaskAttemptStateUI.SUCCESSFUL.toString()), String.valueOf(job.getSuccessfulReduceAttempts())).__().__().__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "addTaskLinks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String addTaskLinks(String text)\n{\r\n    return TaskID.taskIdPattern.matcher(text).replaceAll(\"<a href=\\\"/jobhistory/task/$0\\\">$0</a>\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshAdminAcls",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshAdminAclsResponseProto refreshAdminAcls(RpcController controller, RefreshAdminAclsRequestProto request) throws ServiceException\n{\r\n    try {\r\n        impl.refreshAdminAcls();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n    return VOID_REFRESH_ADMIN_ACLS_RESPONSE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshLoadedJobCache",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshLoadedJobCacheResponseProto refreshLoadedJobCache(RpcController controller, RefreshLoadedJobCacheRequestProto request) throws ServiceException\n{\r\n    try {\r\n        impl.refreshLoadedJobCache();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n    return VOID_REFRESH_LOADED_JOB_CACHE_RESPONSE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshJobRetentionSettings",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshJobRetentionSettingsResponseProto refreshJobRetentionSettings(RpcController controller, RefreshJobRetentionSettingsRequestProto request) throws ServiceException\n{\r\n    try {\r\n        impl.refreshJobRetentionSettings();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n    return VOID_REFRESH_JOB_RETENTION_SETTINGS_RESPONSE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshLogRetentionSettings",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshLogRetentionSettingsResponseProto refreshLogRetentionSettings(RpcController controller, RefreshLogRetentionSettingsRequestProto request) throws ServiceException\n{\r\n    try {\r\n        impl.refreshLogRetentionSettings();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n    return VOID_REFRESH_LOG_RETENTION_SETTINGS_RESPONSE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 51,
  "sourceCodeText" : "void render(Block html)\n{\r\n    if (app.getJob() == null) {\r\n        html.h2($(TITLE));\r\n        return;\r\n    }\r\n    TaskType type = null;\r\n    String symbol = $(TASK_TYPE);\r\n    if (!symbol.isEmpty()) {\r\n        type = MRApps.taskType(symbol);\r\n    }\r\n    THEAD<TABLE<Hamlet>> thead;\r\n    if (type != null)\r\n        thead = html.table(\"#\" + app.getJob().getID() + type).$class(\"dt-tasks\").thead();\r\n    else\r\n        thead = html.table(\"#tasks\").thead();\r\n    int attemptColSpan = type == TaskType.REDUCE ? 8 : 3;\r\n    thead.tr().th().$colspan(5).$class(\"ui-state-default\").__(\"Task\").__().th().$colspan(attemptColSpan).$class(\"ui-state-default\").__(\"Successful Attempt\").__().__();\r\n    TR<THEAD<TABLE<Hamlet>>> theadRow = thead.tr().th(\"Name\").th(\"State\").th(\"Start Time\").th(\"Finish Time\").th(\"Elapsed Time\").th(\"Start Time\");\r\n    if (type == TaskType.REDUCE) {\r\n        theadRow.th(\"Shuffle Finish Time\");\r\n        theadRow.th(\"Merge Finish Time\");\r\n    }\r\n    theadRow.th(\"Finish Time\");\r\n    if (type == TaskType.REDUCE) {\r\n        theadRow.th(\"Elapsed Time Shuffle\");\r\n        theadRow.th(\"Elapsed Time Merge\");\r\n        theadRow.th(\"Elapsed Time Reduce\");\r\n    }\r\n    theadRow.th(\"Elapsed Time\");\r\n    TBODY<TABLE<Hamlet>> tbody = theadRow.__().__().tbody();\r\n    StringBuilder tasksTableData = new StringBuilder(\"[\\n\");\r\n    for (Task task : app.getJob().getTasks().values()) {\r\n        if (type != null && task.getType() != type) {\r\n            continue;\r\n        }\r\n        TaskInfo info = new TaskInfo(task);\r\n        String tid = info.getId();\r\n        long startTime = info.getStartTime();\r\n        long finishTime = info.getFinishTime();\r\n        long elapsed = info.getElapsedTime();\r\n        long attemptStartTime = -1;\r\n        long shuffleFinishTime = -1;\r\n        long sortFinishTime = -1;\r\n        long attemptFinishTime = -1;\r\n        long elapsedShuffleTime = -1;\r\n        long elapsedSortTime = -1;\r\n        ;\r\n        long elapsedReduceTime = -1;\r\n        long attemptElapsed = -1;\r\n        TaskAttempt successful = info.getSuccessful();\r\n        if (successful != null) {\r\n            TaskAttemptInfo ta;\r\n            if (type == TaskType.REDUCE) {\r\n                ReduceTaskAttemptInfo rta = new ReduceTaskAttemptInfo(successful);\r\n                shuffleFinishTime = rta.getShuffleFinishTime();\r\n                sortFinishTime = rta.getMergeFinishTime();\r\n                elapsedShuffleTime = rta.getElapsedShuffleTime();\r\n                elapsedSortTime = rta.getElapsedMergeTime();\r\n                elapsedReduceTime = rta.getElapsedReduceTime();\r\n                ta = rta;\r\n            } else {\r\n                ta = new MapTaskAttemptInfo(successful, false);\r\n            }\r\n            attemptStartTime = ta.getStartTime();\r\n            attemptFinishTime = ta.getFinishTime();\r\n            attemptElapsed = ta.getElapsedTime();\r\n        }\r\n        tasksTableData.append(\"[\\\"\").append(\"<a href='\" + url(\"task\", tid)).append(\"'>\").append(tid).append(\"</a>\\\",\\\"\").append(info.getState()).append(\"\\\",\\\"\").append(startTime).append(\"\\\",\\\"\").append(finishTime).append(\"\\\",\\\"\").append(elapsed).append(\"\\\",\\\"\").append(attemptStartTime).append(\"\\\",\\\"\");\r\n        if (type == TaskType.REDUCE) {\r\n            tasksTableData.append(shuffleFinishTime).append(\"\\\",\\\"\").append(sortFinishTime).append(\"\\\",\\\"\");\r\n        }\r\n        tasksTableData.append(attemptFinishTime).append(\"\\\",\\\"\");\r\n        if (type == TaskType.REDUCE) {\r\n            tasksTableData.append(elapsedShuffleTime).append(\"\\\",\\\"\").append(elapsedSortTime).append(\"\\\",\\\"\").append(elapsedReduceTime).append(\"\\\",\\\"\");\r\n        }\r\n        tasksTableData.append(attemptElapsed).append(\"\\\"],\\n\");\r\n    }\r\n    if (tasksTableData.charAt(tasksTableData.length() - 2) == ',') {\r\n        tasksTableData.delete(tasksTableData.length() - 2, tasksTableData.length() - 1);\r\n    }\r\n    tasksTableData.append(\"]\");\r\n    html.script().$type(\"text/javascript\").__(\"var tasksTableData=\" + tasksTableData).__();\r\n    TR<TFOOT<TABLE<Hamlet>>> footRow = tbody.__().tfoot().tr();\r\n    footRow.th().input(\"search_init\").$type(InputType.text).$name(\"task\").$value(\"ID\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"state\").$value(\"State\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"start_time\").$value(\"Start Time\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"finish_time\").$value(\"Finish Time\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"elapsed_time\").$value(\"Elapsed Time\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"attempt_start_time\").$value(\"Start Time\").__().__();\r\n    if (type == TaskType.REDUCE) {\r\n        footRow.th().input(\"search_init\").$type(InputType.text).$name(\"shuffle_time\").$value(\"Shuffle Time\").__().__();\r\n        footRow.th().input(\"search_init\").$type(InputType.text).$name(\"merge_time\").$value(\"Merge Time\").__().__();\r\n    }\r\n    footRow.th().input(\"search_init\").$type(InputType.text).$name(\"attempt_finish\").$value(\"Finish Time\").__().__();\r\n    if (type == TaskType.REDUCE) {\r\n        footRow.th().input(\"search_init\").$type(InputType.text).$name(\"elapsed_shuffle_time\").$value(\"Elapsed Shuffle Time\").__().__();\r\n        footRow.th().input(\"search_init\").$type(InputType.text).$name(\"elapsed_merge_time\").$value(\"Elapsed Merge Time\").__().__();\r\n        footRow.th().input(\"search_init\").$type(InputType.text).$name(\"elapsed_reduce_time\").$value(\"Elapsed Reduce Time\").__().__();\r\n    }\r\n    footRow.th().input(\"search_init\").$type(InputType.text).$name(\"attempt_elapsed\").$value(\"Elapsed Time\").__().__();\r\n    footRow.__().__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JAXBContext getContext(Class<?> objectType)\n{\r\n    return (types.contains(objectType)) ? context : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    setActiveNavColumnForTask();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return AggregatedLogsBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "logSuccess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void logSuccess(String user, String operation, String target)\n{\r\n    if (LOG.isInfoEnabled()) {\r\n        LOG.info(createSuccessLog(user, operation, target));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createSuccessLog",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String createSuccessLog(String user, String operation, String target)\n{\r\n    StringBuilder b = new StringBuilder();\r\n    start(Keys.USER, user, b);\r\n    addRemoteIP(b);\r\n    add(Keys.OPERATION, operation, b);\r\n    add(Keys.TARGET, target, b);\r\n    add(Keys.RESULT, AuditConstants.SUCCESS, b);\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "addRemoteIP",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addRemoteIP(StringBuilder b)\n{\r\n    InetAddress ip = Server.getRemoteIp();\r\n    if (ip != null) {\r\n        add(Keys.IP, ip.getHostAddress(), b);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(Keys key, String value, StringBuilder b)\n{\r\n    b.append(AuditConstants.PAIR_SEPARATOR).append(key.name()).append(AuditConstants.KEY_VAL_SEPARATOR).append(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void start(Keys key, String value, StringBuilder b)\n{\r\n    b.append(key.name()).append(AuditConstants.KEY_VAL_SEPARATOR).append(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "logFailure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void logFailure(String user, String operation, String perm, String target, String description)\n{\r\n    if (LOG.isWarnEnabled()) {\r\n        LOG.warn(createFailureLog(user, operation, perm, target, description));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createFailureLog",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String createFailureLog(String user, String operation, String perm, String target, String description)\n{\r\n    StringBuilder b = new StringBuilder();\r\n    start(Keys.USER, user, b);\r\n    addRemoteIP(b);\r\n    add(Keys.OPERATION, operation, b);\r\n    add(Keys.TARGET, target, b);\r\n    add(Keys.RESULT, AuditConstants.FAILURE, b);\r\n    add(Keys.DESCRIPTION, description, b);\r\n    add(Keys.PERMISSIONS, perm, b);\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "hasAccess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasAccess(Job job, HttpServletRequest request)\n{\r\n    String remoteUser = request.getRemoteUser();\r\n    if (remoteUser != null) {\r\n        return job.checkAccess(UserGroupInformation.createRemoteUser(remoteUser), JobACL.VIEW_JOB);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkAccess(Job job, HttpServletRequest request)\n{\r\n    if (!hasAccess(job, request)) {\r\n        throw new WebApplicationException(Status.UNAUTHORIZED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init()\n{\r\n    response.setContentType(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setResponse",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResponse(HttpServletResponse response)\n{\r\n    this.response = response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HistoryInfo get()\n{\r\n    return getHistoryInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getHistoryInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HistoryInfo getHistoryInfo()\n{\r\n    init();\r\n    return new HistoryInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobs",
  "errType" : [ "NumberFormatException", "NumberFormatException", "NumberFormatException", "NumberFormatException", "NumberFormatException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "JobsInfo getJobs(@QueryParam(\"user\") String userQuery, @QueryParam(\"limit\") String count, @QueryParam(\"state\") String stateQuery, @QueryParam(\"queue\") String queueQuery, @QueryParam(\"startedTimeBegin\") String startedBegin, @QueryParam(\"startedTimeEnd\") String startedEnd, @QueryParam(\"finishedTimeBegin\") String finishBegin, @QueryParam(\"finishedTimeEnd\") String finishEnd)\n{\r\n    Long countParam = null;\r\n    init();\r\n    if (count != null && !count.isEmpty()) {\r\n        try {\r\n            countParam = Long.parseLong(count);\r\n        } catch (NumberFormatException e) {\r\n            throw new BadRequestException(e.getMessage());\r\n        }\r\n        if (countParam <= 0) {\r\n            throw new BadRequestException(\"limit value must be greater then 0\");\r\n        }\r\n    }\r\n    Long sBegin = null;\r\n    if (startedBegin != null && !startedBegin.isEmpty()) {\r\n        try {\r\n            sBegin = Long.parseLong(startedBegin);\r\n        } catch (NumberFormatException e) {\r\n            throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\r\n        }\r\n        if (sBegin < 0) {\r\n            throw new BadRequestException(\"startedTimeBegin must be greater than 0\");\r\n        }\r\n    }\r\n    Long sEnd = null;\r\n    if (startedEnd != null && !startedEnd.isEmpty()) {\r\n        try {\r\n            sEnd = Long.parseLong(startedEnd);\r\n        } catch (NumberFormatException e) {\r\n            throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\r\n        }\r\n        if (sEnd < 0) {\r\n            throw new BadRequestException(\"startedTimeEnd must be greater than 0\");\r\n        }\r\n    }\r\n    if (sBegin != null && sEnd != null && sBegin > sEnd) {\r\n        throw new BadRequestException(\"startedTimeEnd must be greater than startTimeBegin\");\r\n    }\r\n    Long fBegin = null;\r\n    if (finishBegin != null && !finishBegin.isEmpty()) {\r\n        try {\r\n            fBegin = Long.parseLong(finishBegin);\r\n        } catch (NumberFormatException e) {\r\n            throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\r\n        }\r\n        if (fBegin < 0) {\r\n            throw new BadRequestException(\"finishedTimeBegin must be greater than 0\");\r\n        }\r\n    }\r\n    Long fEnd = null;\r\n    if (finishEnd != null && !finishEnd.isEmpty()) {\r\n        try {\r\n            fEnd = Long.parseLong(finishEnd);\r\n        } catch (NumberFormatException e) {\r\n            throw new BadRequestException(\"Invalid number format: \" + e.getMessage());\r\n        }\r\n        if (fEnd < 0) {\r\n            throw new BadRequestException(\"finishedTimeEnd must be greater than 0\");\r\n        }\r\n    }\r\n    if (fBegin != null && fEnd != null && fBegin > fEnd) {\r\n        throw new BadRequestException(\"finishedTimeEnd must be greater than finishedTimeBegin\");\r\n    }\r\n    JobState jobState = null;\r\n    if (stateQuery != null) {\r\n        jobState = JobState.valueOf(stateQuery);\r\n    }\r\n    return ctx.getPartialJobs(0l, countParam, userQuery, queueQuery, sBegin, sEnd, fBegin, fEnd, jobState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobInfo getJob(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    return new JobInfo(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobAttempts",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "AMAttemptsInfo getJobAttempts(@PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    AMAttemptsInfo amAttempts = new AMAttemptsInfo();\r\n    for (AMInfo amInfo : job.getAMInfos()) {\r\n        AMAttemptInfo attempt = new AMAttemptInfo(amInfo, MRApps.toString(job.getID()), job.getUserName(), uriInfo.getBaseUri().toString(), webapp.name());\r\n        amAttempts.add(attempt);\r\n    }\r\n    return amAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobCounterInfo getJobCounters(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    return new JobCounterInfo(this.ctx, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ConfInfo getJobConf(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    ConfInfo info;\r\n    try {\r\n        info = new ConfInfo(job);\r\n    } catch (IOException e) {\r\n        throw new NotFoundException(\"unable to load configuration for job: \" + jid);\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobTasks",
  "errType" : [ "YarnRuntimeException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TasksInfo getJobTasks(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @QueryParam(\"type\") String type)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    TasksInfo allTasks = new TasksInfo();\r\n    for (Task task : job.getTasks().values()) {\r\n        TaskType ttype = null;\r\n        if (type != null && !type.isEmpty()) {\r\n            try {\r\n                ttype = MRApps.taskType(type);\r\n            } catch (YarnRuntimeException e) {\r\n                throw new BadRequestException(\"tasktype must be either m or r\");\r\n            }\r\n        }\r\n        if (ttype != null && task.getType() != ttype) {\r\n            continue;\r\n        }\r\n        allTasks.add(new TaskInfo(task));\r\n    }\r\n    return allTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskInfo getJobTask(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    Task task = AMWebServices.getTaskFromTaskIdString(tid, job);\r\n    return new TaskInfo(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getSingleTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "JobTaskCounterInfo getSingleTaskCounters(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    TaskId taskID = MRApps.toTaskID(tid);\r\n    if (taskID == null) {\r\n        throw new NotFoundException(\"taskid \" + tid + \" not found or invalid\");\r\n    }\r\n    Task task = job.getTask(taskID);\r\n    if (task == null) {\r\n        throw new NotFoundException(\"task not found with id \" + tid);\r\n    }\r\n    return new JobTaskCounterInfo(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskAttemptsInfo getJobTaskAttempts(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid)\n{\r\n    init();\r\n    TaskAttemptsInfo attempts = new TaskAttemptsInfo();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    Task task = AMWebServices.getTaskFromTaskIdString(tid, job);\r\n    for (TaskAttempt ta : task.getAttempts().values()) {\r\n        if (ta != null) {\r\n            if (task.getType() == TaskType.REDUCE) {\r\n                attempts.add(new ReduceTaskAttemptInfo(ta));\r\n            } else {\r\n                attempts.add(new MapTaskAttemptInfo(ta, false));\r\n            }\r\n        }\r\n    }\r\n    return attempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TaskAttemptInfo getJobTaskAttemptId(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid, @PathParam(\"attemptid\") String attId)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    Task task = AMWebServices.getTaskFromTaskIdString(tid, job);\r\n    TaskAttempt ta = AMWebServices.getTaskAttemptFromTaskAttemptString(attId, task);\r\n    if (task.getType() == TaskType.REDUCE) {\r\n        return new ReduceTaskAttemptInfo(ta);\r\n    } else {\r\n        return new MapTaskAttemptInfo(ta, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getJobTaskAttemptIdCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "JobTaskAttemptCounterInfo getJobTaskAttemptIdCounters(@Context HttpServletRequest hsr, @PathParam(\"jobid\") String jid, @PathParam(\"taskid\") String tid, @PathParam(\"attemptid\") String attId)\n{\r\n    init();\r\n    Job job = AMWebServices.getJobFromJobIdString(jid, ctx);\r\n    checkAccess(job, hsr);\r\n    Task task = AMWebServices.getTaskFromTaskIdString(tid, job);\r\n    TaskAttempt ta = AMWebServices.getTaskAttemptFromTaskAttemptString(attId, task);\r\n    return new JobTaskAttemptCounterInfo(ta);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getRemoteLogDirPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Response getRemoteLogDirPath(@Context HttpServletRequest req, @QueryParam(YarnWebServiceParams.REMOTE_USER) String user, @QueryParam(YarnWebServiceParams.APP_ID) String appIdStr) throws IOException\n{\r\n    init();\r\n    return logServlet.getRemoteLogDirPath(user, appIdStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getAggregatedLogsMeta",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Response getAggregatedLogsMeta(@Context HttpServletRequest hsr, @QueryParam(YarnWebServiceParams.CONTAINER_LOG_FILE_NAME) String fileName, @QueryParam(YarnWebServiceParams.FILESIZE) Set<String> fileSize, @QueryParam(YarnWebServiceParams.MODIFICATION_TIME) Set<String> modificationTime, @QueryParam(YarnWebServiceParams.APP_ID) String appIdStr, @QueryParam(YarnWebServiceParams.CONTAINER_ID) String containerIdStr, @QueryParam(YarnWebServiceParams.NM_ID) String nmId) throws IOException\n{\r\n    init();\r\n    ExtendedLogMetaRequest.ExtendedLogMetaRequestBuilder logsRequest = new ExtendedLogMetaRequest.ExtendedLogMetaRequestBuilder();\r\n    logsRequest.setAppId(appIdStr);\r\n    logsRequest.setFileName(fileName);\r\n    logsRequest.setContainerId(containerIdStr);\r\n    logsRequest.setFileSize(fileSize);\r\n    logsRequest.setModificationTime(modificationTime);\r\n    logsRequest.setNodeId(nmId);\r\n    return logServlet.getContainerLogsInfo(hsr, logsRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getAggregatedLogsMeta",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Response getAggregatedLogsMeta(@Context HttpServletRequest hsr, @QueryParam(YarnWebServiceParams.APP_ID) String appIdStr, @QueryParam(YarnWebServiceParams.APPATTEMPT_ID) String appAttemptIdStr, @QueryParam(YarnWebServiceParams.CONTAINER_ID) String containerIdStr, @QueryParam(YarnWebServiceParams.NM_ID) String nmId, @QueryParam(YarnWebServiceParams.REDIRECTED_FROM_NODE) @DefaultValue(\"false\") boolean redirectedFromNode, @QueryParam(YarnWebServiceParams.MANUAL_REDIRECTION) @DefaultValue(\"false\") boolean manualRedirection)\n{\r\n    init();\r\n    return logServlet.getLogsInfo(hsr, appIdStr, appAttemptIdStr, containerIdStr, nmId, redirectedFromNode, manualRedirection);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getContainerLogs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Response getContainerLogs(@Context HttpServletRequest hsr, @PathParam(YarnWebServiceParams.CONTAINER_ID) String containerIdStr, @QueryParam(YarnWebServiceParams.NM_ID) String nmId, @QueryParam(YarnWebServiceParams.REDIRECTED_FROM_NODE) @DefaultValue(\"false\") boolean redirectedFromNode, @QueryParam(YarnWebServiceParams.MANUAL_REDIRECTION) @DefaultValue(\"false\") boolean manualRedirection)\n{\r\n    init();\r\n    WrappedLogMetaRequest.Builder logMetaRequestBuilder = LogServlet.createRequestFromContainerId(containerIdStr);\r\n    return logServlet.getContainerLogsInfo(hsr, logMetaRequestBuilder, nmId, redirectedFromNode, null, manualRedirection);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getContainerLogFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Response getContainerLogFile(@Context HttpServletRequest req, @PathParam(YarnWebServiceParams.CONTAINER_ID) String containerIdStr, @PathParam(YarnWebServiceParams.CONTAINER_LOG_FILE_NAME) String filename, @QueryParam(YarnWebServiceParams.RESPONSE_CONTENT_FORMAT) String format, @QueryParam(YarnWebServiceParams.RESPONSE_CONTENT_SIZE) String size, @QueryParam(YarnWebServiceParams.NM_ID) String nmId, @QueryParam(YarnWebServiceParams.REDIRECTED_FROM_NODE) @DefaultValue(\"false\") boolean redirectedFromNode, @QueryParam(YarnWebServiceParams.MANUAL_REDIRECTION) @DefaultValue(\"false\") boolean manualRedirection)\n{\r\n    init();\r\n    return logServlet.getLogFile(req, containerIdStr, filename, format, size, nmId, redirectedFromNode, null, manualRedirection);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "getLogServlet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LogServlet getLogServlet()\n{\r\n    return this.logServlet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setLogServlet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLogServlet(LogServlet logServlet)\n{\r\n    this.logServlet = logServlet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "initStorage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initStorage(Configuration conf) throws IOException\n{\r\n    final String storeUri = conf.get(JHAdminConfig.MR_HS_FS_STATE_STORE_URI);\r\n    if (storeUri == null) {\r\n        throw new IOException(\"No store location URI configured in \" + JHAdminConfig.MR_HS_FS_STATE_STORE_URI);\r\n    }\r\n    LOG.info(\"Using \" + storeUri + \" for history server state storage\");\r\n    rootStatePath = new Path(storeUri, ROOT_STATE_DIR_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "startStorage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void startStorage() throws IOException\n{\r\n    fs = createFileSystem();\r\n    createDir(rootStatePath);\r\n    tokenStatePath = new Path(rootStatePath, TOKEN_STATE_DIR_NAME);\r\n    createDir(tokenStatePath);\r\n    tokenKeysStatePath = new Path(tokenStatePath, TOKEN_KEYS_DIR_NAME);\r\n    createDir(tokenKeysStatePath);\r\n    for (int i = 0; i < NUM_TOKEN_BUCKETS; ++i) {\r\n        createDir(getTokenBucketPath(i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem createFileSystem() throws IOException\n{\r\n    return rootStatePath.getFileSystem(getConfig());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "closeStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void closeStorage() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadState",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HistoryServerState loadState() throws IOException\n{\r\n    LOG.info(\"Loading history server state from \" + rootStatePath);\r\n    HistoryServerState state = new HistoryServerState();\r\n    loadTokenState(state);\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Storing token \" + tokenId.getSequenceNumber());\r\n    }\r\n    Path tokenPath = getTokenPath(tokenId);\r\n    if (fs.exists(tokenPath)) {\r\n        throw new IOException(tokenPath + \" already exists\");\r\n    }\r\n    createNewFile(tokenPath, buildTokenData(tokenId, renewDate));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "updateToken",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void updateToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Updating token \" + tokenId.getSequenceNumber());\r\n    }\r\n    Path tokenPath = getTokenPath(tokenId);\r\n    Path tmp = new Path(tokenPath.getParent(), UPDATE_TMP_FILE_PREFIX + tokenPath.getName());\r\n    writeFile(tmp, buildTokenData(tokenId, renewDate));\r\n    try {\r\n        deleteFile(tokenPath);\r\n    } catch (IOException e) {\r\n        fs.delete(tmp, false);\r\n        throw e;\r\n    }\r\n    if (!fs.rename(tmp, tokenPath)) {\r\n        throw new IOException(\"Could not rename \" + tmp + \" to \" + tokenPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeToken(MRDelegationTokenIdentifier tokenId) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Removing token \" + tokenId.getSequenceNumber());\r\n    }\r\n    deleteFile(getTokenPath(tokenId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void storeTokenMasterKey(DelegationKey key) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Storing master key \" + key.getKeyId());\r\n    }\r\n    Path keyPath = new Path(tokenKeysStatePath, TOKEN_MASTER_KEY_FILE_PREFIX + key.getKeyId());\r\n    if (fs.exists(keyPath)) {\r\n        throw new FileAlreadyExistsException(keyPath + \" already exists\");\r\n    }\r\n    ByteArrayOutputStream memStream = new ByteArrayOutputStream();\r\n    DataOutputStream dataStream = new DataOutputStream(memStream);\r\n    try {\r\n        key.write(dataStream);\r\n        dataStream.close();\r\n        dataStream = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, dataStream);\r\n    }\r\n    createNewFile(keyPath, memStream.toByteArray());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeTokenMasterKey(DelegationKey key) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Removing master key \" + key.getKeyId());\r\n    }\r\n    Path keyPath = new Path(tokenKeysStatePath, TOKEN_MASTER_KEY_FILE_PREFIX + key.getKeyId());\r\n    deleteFile(keyPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getBucketId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getBucketId(MRDelegationTokenIdentifier tokenId)\n{\r\n    return tokenId.getSequenceNumber() % NUM_TOKEN_BUCKETS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTokenBucketPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTokenBucketPath(int bucketId)\n{\r\n    return new Path(tokenStatePath, String.format(TOKEN_BUCKET_NAME_FORMAT, bucketId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTokenPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTokenPath(MRDelegationTokenIdentifier tokenId)\n{\r\n    Path bucketPath = getTokenBucketPath(getBucketId(tokenId));\r\n    return new Path(bucketPath, TOKEN_FILE_PREFIX + tokenId.getSequenceNumber());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createDir",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createDir(Path dir) throws IOException\n{\r\n    try {\r\n        FileStatus status = fs.getFileStatus(dir);\r\n        if (!status.isDirectory()) {\r\n            throw new FileAlreadyExistsException(\"Unexpected file in store: \" + dir);\r\n        }\r\n        if (!status.getPermission().equals(DIR_PERMISSIONS)) {\r\n            fs.setPermission(dir, DIR_PERMISSIONS);\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        fs.mkdirs(dir, DIR_PERMISSIONS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createNewFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createNewFile(Path file, byte[] data) throws IOException\n{\r\n    Path tmp = new Path(file.getParent(), TMP_FILE_PREFIX + file.getName());\r\n    writeFile(tmp, data);\r\n    try {\r\n        if (!fs.rename(tmp, file)) {\r\n            throw new IOException(\"Could not rename \" + tmp + \" to \" + file);\r\n        }\r\n    } catch (IOException e) {\r\n        fs.delete(tmp, false);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "writeFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeFile(Path file, byte[] data) throws IOException\n{\r\n    final int WRITE_BUFFER_SIZE = 4096;\r\n    FSDataOutputStream out = fs.create(file, FILE_PERMISSIONS, true, WRITE_BUFFER_SIZE, fs.getDefaultReplication(file), fs.getDefaultBlockSize(file), null);\r\n    try {\r\n        try {\r\n            out.write(data);\r\n            out.close();\r\n            out = null;\r\n        } finally {\r\n            IOUtils.cleanupWithLogger(LOG, out);\r\n        }\r\n    } catch (IOException e) {\r\n        fs.delete(file, false);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "readFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] readFile(Path file, long numBytes) throws IOException\n{\r\n    byte[] data = new byte[(int) numBytes];\r\n    FSDataInputStream in = fs.open(file);\r\n    try {\r\n        in.readFully(data);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, in);\r\n    }\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "deleteFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteFile(Path file) throws IOException\n{\r\n    boolean deleted;\r\n    try {\r\n        deleted = fs.delete(file, false);\r\n    } catch (FileNotFoundException e) {\r\n        deleted = true;\r\n    }\r\n    if (!deleted) {\r\n        throw new IOException(\"Unable to delete \" + file);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "buildTokenData",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "byte[] buildTokenData(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n    ByteArrayOutputStream memStream = new ByteArrayOutputStream();\r\n    DataOutputStream dataStream = new DataOutputStream(memStream);\r\n    try {\r\n        tokenId.write(dataStream);\r\n        dataStream.writeLong(renewDate);\r\n        dataStream.close();\r\n        dataStream = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, dataStream);\r\n    }\r\n    return memStream.toByteArray();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void loadTokenMasterKey(HistoryServerState state, Path keyFile, long numKeyFileBytes) throws IOException\n{\r\n    DelegationKey key = new DelegationKey();\r\n    byte[] keyData = readFile(keyFile, numKeyFileBytes);\r\n    DataInputStream in = new DataInputStream(new ByteArrayInputStream(keyData));\r\n    try {\r\n        key.readFields(in);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, in);\r\n    }\r\n    state.tokenMasterKeyState.add(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokenFromBucket",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void loadTokenFromBucket(int bucketId, HistoryServerState state, Path tokenFile, long numTokenFileBytes) throws IOException\n{\r\n    MRDelegationTokenIdentifier token = loadToken(state, tokenFile, numTokenFileBytes);\r\n    int tokenBucketId = getBucketId(token);\r\n    if (tokenBucketId != bucketId) {\r\n        throw new IOException(\"Token \" + tokenFile + \" should be in bucket \" + tokenBucketId + \", found in bucket \" + bucketId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "MRDelegationTokenIdentifier loadToken(HistoryServerState state, Path tokenFile, long numTokenFileBytes) throws IOException\n{\r\n    MRDelegationTokenIdentifier tokenId = new MRDelegationTokenIdentifier();\r\n    long renewDate;\r\n    byte[] tokenData = readFile(tokenFile, numTokenFileBytes);\r\n    DataInputStream in = new DataInputStream(new ByteArrayInputStream(tokenData));\r\n    try {\r\n        tokenId.readFields(in);\r\n        renewDate = in.readLong();\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, in);\r\n    }\r\n    state.tokenState.put(tokenId, renewDate);\r\n    return tokenId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokensFromBucket",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "int loadTokensFromBucket(HistoryServerState state, Path bucket) throws IOException\n{\r\n    String numStr = bucket.getName().substring(TOKEN_BUCKET_DIR_PREFIX.length());\r\n    final int bucketId = Integer.parseInt(numStr);\r\n    int numTokens = 0;\r\n    FileStatus[] tokenStats = fs.listStatus(bucket);\r\n    Set<String> loadedTokens = new HashSet<String>(tokenStats.length);\r\n    for (FileStatus stat : tokenStats) {\r\n        String name = stat.getPath().getName();\r\n        if (name.startsWith(TOKEN_FILE_PREFIX)) {\r\n            loadTokenFromBucket(bucketId, state, stat.getPath(), stat.getLen());\r\n            loadedTokens.add(name);\r\n            ++numTokens;\r\n        } else if (name.startsWith(UPDATE_TMP_FILE_PREFIX)) {\r\n            String tokenName = name.substring(UPDATE_TMP_FILE_PREFIX.length());\r\n            if (loadedTokens.contains(tokenName)) {\r\n                fs.delete(stat.getPath(), false);\r\n            } else {\r\n                loadTokenFromBucket(bucketId, state, stat.getPath(), stat.getLen());\r\n                fs.rename(stat.getPath(), new Path(stat.getPath().getParent(), tokenName));\r\n                loadedTokens.add(tokenName);\r\n                ++numTokens;\r\n            }\r\n        } else if (name.startsWith(TMP_FILE_PREFIX)) {\r\n            fs.delete(stat.getPath(), false);\r\n        } else {\r\n            LOG.warn(\"Skipping unexpected file in history server token bucket: \" + stat.getPath());\r\n        }\r\n    }\r\n    return numTokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadKeys",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int loadKeys(HistoryServerState state) throws IOException\n{\r\n    FileStatus[] stats = fs.listStatus(tokenKeysStatePath);\r\n    int numKeys = 0;\r\n    for (FileStatus stat : stats) {\r\n        String name = stat.getPath().getName();\r\n        if (name.startsWith(TOKEN_MASTER_KEY_FILE_PREFIX)) {\r\n            loadTokenMasterKey(state, stat.getPath(), stat.getLen());\r\n            ++numKeys;\r\n        } else {\r\n            LOG.warn(\"Skipping unexpected file in history server token state: \" + stat.getPath());\r\n        }\r\n    }\r\n    return numKeys;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokens",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int loadTokens(HistoryServerState state) throws IOException\n{\r\n    FileStatus[] stats = fs.listStatus(tokenStatePath);\r\n    int numTokens = 0;\r\n    for (FileStatus stat : stats) {\r\n        String name = stat.getPath().getName();\r\n        if (name.startsWith(TOKEN_BUCKET_DIR_PREFIX)) {\r\n            numTokens += loadTokensFromBucket(state, stat.getPath());\r\n        } else if (name.equals(TOKEN_KEYS_DIR_NAME)) {\r\n            continue;\r\n        } else {\r\n            LOG.warn(\"Skipping unexpected file in history server token state: \" + stat.getPath());\r\n        }\r\n    }\r\n    return numTokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadTokenState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void loadTokenState(HistoryServerState state) throws IOException\n{\r\n    int numKeys = loadKeys(state);\r\n    int numTokens = loadTokens(state);\r\n    LOG.info(\"Loaded \" + numKeys + \" master keys and \" + numTokens + \" tokens from \" + tokenStatePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    set(DATATABLES_ID, \"tasks\");\r\n    set(DATATABLES_SELECTOR, \".dt-tasks\");\r\n    set(initSelector(DATATABLES), tasksTableInit());\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:1}\");\r\n    set(initID(DATATABLES, \"tasks\"), tasksTableInit());\r\n    set(postInitID(DATATABLES, \"tasks\"), jobsPostTableInit());\r\n    setTableStyles(html, \"tasks\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return HsTasksBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "tasksTableInit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String tasksTableInit()\n{\r\n    TaskType type = null;\r\n    String symbol = $(TASK_TYPE);\r\n    if (!symbol.isEmpty()) {\r\n        type = MRApps.taskType(symbol);\r\n    }\r\n    StringBuilder b = tableInit().append(\", 'aaData': tasksTableData\").append(\", bDeferRender: true\").append(\", bProcessing: true\").append(\"\\n, aoColumnDefs: [\\n\").append(\"{'sType':'natural', 'aTargets': [ 0 ]\").append(\", 'mRender': parseHadoopID }\").append(\", {'sType':'numeric', 'aTargets': [ 4\").append(type == TaskType.REDUCE ? \", 9, 10, 11, 12\" : \", 7\").append(\" ], 'mRender': renderHadoopElapsedTime }\").append(\"\\n, {'sType':'numeric', 'aTargets': [ 2, 3, 5\").append(type == TaskType.REDUCE ? \", 6, 7, 8\" : \", 6\").append(\" ], 'mRender': renderHadoopDate }]\").append(\"\\n, aaSorting: [[0, 'asc']]\").append(\"}\");\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "jobsPostTableInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String jobsPostTableInit()\n{\r\n    return \"var asInitVals = new Array();\\n\" + \"$('tfoot input').keyup( function () \\n{\" + \"  $('.dt-tasks').dataTable().fnFilter(\" + \" this.value, $('tfoot input').index(this) );\\n\" + \"} );\\n\" + \"$('tfoot input').each( function (i) {\\n\" + \"  asInitVals[i] = this.value;\\n\" + \"} );\\n\" + \"$('tfoot input').focus( function () {\\n\" + \"  if ( this.className == 'search_init' )\\n\" + \"  {\\n\" + \"    this.className = '';\\n\" + \"    this.value = '';\\n\" + \"  }\\n\" + \"} );\\n\" + \"$('tfoot input').blur( function (i) {\\n\" + \"  if ( this.value == '' )\\n\" + \"  {\\n\" + \"    this.className = 'search_init';\\n\" + \"    this.value = asInitVals[$('tfoot input').index(this)];\\n\" + \"  }\\n\" + \"} );\\n\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws IOException\n{\r\n    initStorage(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStart() throws IOException\n{\r\n    startStorage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStop() throws IOException\n{\r\n    closeStorage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "initStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initStorage(Configuration conf) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "startStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startStorage() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "closeStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void closeStorage() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryServerState loadState() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "updateToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeToken(MRDelegationTokenIdentifier tokenId) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void storeTokenMasterKey(DelegationKey key) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeTokenMasterKey(DelegationKey key) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(AMAttemptInfo info)\n{\r\n    this.attempt.add(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<AMAttemptInfo> getAttempts()\n{\r\n    return this.attempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (conf != null) {\r\n        conf = addSecurityConfiguration(conf);\r\n    }\r\n    super.setConf(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "addSecurityConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration addSecurityConfiguration(Configuration conf)\n{\r\n    conf = new JobConf(conf);\r\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, conf.get(JHAdminConfig.MR_HISTORY_PRINCIPAL, \"\"));\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void printUsage(String cmd)\n{\r\n    if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\r\n        System.err.println(\"Usage: mapred hsadmin [-refreshUserToGroupsMappings]\");\r\n    } else if (\"-refreshSuperUserGroupsConfiguration\".equals(cmd)) {\r\n        System.err.println(\"Usage: mapred hsadmin [-refreshSuperUserGroupsConfiguration]\");\r\n    } else if (\"-refreshAdminAcls\".equals(cmd)) {\r\n        System.err.println(\"Usage: mapred hsadmin [-refreshAdminAcls]\");\r\n    } else if (\"-refreshLoadedJobCache\".equals(cmd)) {\r\n        System.err.println(\"Usage: mapred hsadmin [-refreshLoadedJobCache]\");\r\n    } else if (\"-refreshJobRetentionSettings\".equals(cmd)) {\r\n        System.err.println(\"Usage: mapred hsadmin [-refreshJobRetentionSettings]\");\r\n    } else if (\"-refreshLogRetentionSettings\".equals(cmd)) {\r\n        System.err.println(\"Usage: mapred hsadmin [-refreshLogRetentionSettings]\");\r\n    } else if (\"-getGroups\".equals(cmd)) {\r\n        System.err.println(\"Usage: mapred hsadmin\" + \" [-getGroups [username]]\");\r\n    } else {\r\n        System.err.println(\"Usage: mapred hsadmin\");\r\n        System.err.println(\"           [-refreshUserToGroupsMappings]\");\r\n        System.err.println(\"           [-refreshSuperUserGroupsConfiguration]\");\r\n        System.err.println(\"           [-refreshAdminAcls]\");\r\n        System.err.println(\"           [-refreshLoadedJobCache]\");\r\n        System.err.println(\"           [-refreshJobRetentionSettings]\");\r\n        System.err.println(\"           [-refreshLogRetentionSettings]\");\r\n        System.err.println(\"           [-getGroups [username]]\");\r\n        System.err.println(\"           [-help [cmd]]\");\r\n        System.err.println();\r\n        ToolRunner.printGenericCommandUsage(System.err);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "printHelp",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void printHelp(String cmd)\n{\r\n    String summary = \"hsadmin is the command to execute Job History server administrative commands.\\n\" + \"The full syntax is: \\n\\n\" + \"mapred hsadmin\" + \" [-refreshUserToGroupsMappings]\" + \" [-refreshSuperUserGroupsConfiguration]\" + \" [-refreshAdminAcls]\" + \" [-refreshLoadedJobCache]\" + \" [-refreshLogRetentionSettings]\" + \" [-refreshJobRetentionSettings]\" + \" [-getGroups [username]]\" + \" [-help [cmd]]\\n\";\r\n    String refreshUserToGroupsMappings = \"-refreshUserToGroupsMappings: Refresh user-to-groups mappings\\n\";\r\n    String refreshSuperUserGroupsConfiguration = \"-refreshSuperUserGroupsConfiguration: Refresh superuser proxy groups mappings\\n\";\r\n    String refreshAdminAcls = \"-refreshAdminAcls: Refresh acls for administration of Job history server\\n\";\r\n    String refreshLoadedJobCache = \"-refreshLoadedJobCache: Refresh loaded job cache of Job history server\\n\";\r\n    String refreshJobRetentionSettings = \"-refreshJobRetentionSettings:\" + \"Refresh job history period,job cleaner settings\\n\";\r\n    String refreshLogRetentionSettings = \"-refreshLogRetentionSettings:\" + \"Refresh log retention period and log retention check interval\\n\";\r\n    String getGroups = \"-getGroups [username]: Get the groups which given user belongs to\\n\";\r\n    String help = \"-help [cmd]: \\tDisplays help for the given command or all commands if none\\n\" + \"\\t\\tis specified.\\n\";\r\n    if (\"refreshUserToGroupsMappings\".equals(cmd)) {\r\n        System.out.println(refreshUserToGroupsMappings);\r\n    } else if (\"help\".equals(cmd)) {\r\n        System.out.println(help);\r\n    } else if (\"refreshSuperUserGroupsConfiguration\".equals(cmd)) {\r\n        System.out.println(refreshSuperUserGroupsConfiguration);\r\n    } else if (\"refreshAdminAcls\".equals(cmd)) {\r\n        System.out.println(refreshAdminAcls);\r\n    } else if (\"refreshLoadedJobCache\".equals(cmd)) {\r\n        System.out.println(refreshLoadedJobCache);\r\n    } else if (\"refreshJobRetentionSettings\".equals(cmd)) {\r\n        System.out.println(refreshJobRetentionSettings);\r\n    } else if (\"refreshLogRetentionSettings\".equals(cmd)) {\r\n        System.out.println(refreshLogRetentionSettings);\r\n    } else if (\"getGroups\".equals(cmd)) {\r\n        System.out.println(getGroups);\r\n    } else {\r\n        System.out.println(summary);\r\n        System.out.println(refreshUserToGroupsMappings);\r\n        System.out.println(refreshSuperUserGroupsConfiguration);\r\n        System.out.println(refreshAdminAcls);\r\n        System.out.println(refreshLoadedJobCache);\r\n        System.out.println(refreshJobRetentionSettings);\r\n        System.out.println(refreshLogRetentionSettings);\r\n        System.out.println(getGroups);\r\n        System.out.println(help);\r\n        System.out.println();\r\n        ToolRunner.printGenericCommandUsage(System.out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "getGroups",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int getGroups(String[] usernames) throws IOException\n{\r\n    if (usernames.length == 0) {\r\n        usernames = new String[] { UserGroupInformation.getCurrentUser().getUserName() };\r\n    }\r\n    Configuration conf = getConf();\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    GetUserMappingsProtocol getUserMappingProtocol = HSProxies.createProxy(conf, address, GetUserMappingsProtocol.class, UserGroupInformation.getCurrentUser());\r\n    for (String username : usernames) {\r\n        StringBuilder sb = new StringBuilder();\r\n        sb.append(username + \" :\");\r\n        for (String group : getUserMappingProtocol.getGroupsForUser(username)) {\r\n            sb.append(\" \");\r\n            sb.append(group);\r\n        }\r\n        System.out.println(sb);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "refreshUserToGroupsMappings",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int refreshUserToGroupsMappings() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    RefreshUserMappingsProtocol refreshProtocol = HSProxies.createProxy(conf, address, RefreshUserMappingsProtocol.class, UserGroupInformation.getCurrentUser());\r\n    refreshProtocol.refreshUserToGroupsMappings();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int refreshSuperUserGroupsConfiguration() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    RefreshUserMappingsProtocol refreshProtocol = HSProxies.createProxy(conf, address, RefreshUserMappingsProtocol.class, UserGroupInformation.getCurrentUser());\r\n    refreshProtocol.refreshSuperUserGroupsConfiguration();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "refreshAdminAcls",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int refreshAdminAcls() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    HSAdminRefreshProtocol refreshProtocol = HSProxies.createProxy(conf, address, HSAdminRefreshProtocol.class, UserGroupInformation.getCurrentUser());\r\n    refreshProtocol.refreshAdminAcls();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "refreshLoadedJobCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int refreshLoadedJobCache() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    HSAdminRefreshProtocol refreshProtocol = HSProxies.createProxy(conf, address, HSAdminRefreshProtocol.class, UserGroupInformation.getCurrentUser());\r\n    refreshProtocol.refreshLoadedJobCache();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "refreshJobRetentionSettings",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int refreshJobRetentionSettings() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    HSAdminRefreshProtocol refreshProtocol = HSProxies.createProxy(conf, address, HSAdminRefreshProtocol.class, UserGroupInformation.getCurrentUser());\r\n    refreshProtocol.refreshJobRetentionSettings();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "refreshLogRetentionSettings",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int refreshLogRetentionSettings() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_ADDRESS, JHAdminConfig.DEFAULT_JHS_ADMIN_PORT);\r\n    HSAdminRefreshProtocol refreshProtocol = HSProxies.createProxy(conf, address, HSAdminRefreshProtocol.class, UserGroupInformation.getCurrentUser());\r\n    refreshProtocol.refreshLogRetentionSettings();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length < 1) {\r\n        printUsage(\"\");\r\n        return -1;\r\n    }\r\n    int exitCode = -1;\r\n    int i = 0;\r\n    String cmd = args[i++];\r\n    if (\"-refreshUserToGroupsMappings\".equals(cmd) || \"-refreshSuperUserGroupsConfiguration\".equals(cmd) || \"-refreshAdminAcls\".equals(cmd) || \"-refreshLoadedJobCache\".equals(cmd) || \"-refreshJobRetentionSettings\".equals(cmd) || \"-refreshLogRetentionSettings\".equals(cmd)) {\r\n        if (args.length != 1) {\r\n            printUsage(cmd);\r\n            return exitCode;\r\n        }\r\n    }\r\n    exitCode = 0;\r\n    if (\"-refreshUserToGroupsMappings\".equals(cmd)) {\r\n        exitCode = refreshUserToGroupsMappings();\r\n    } else if (\"-refreshSuperUserGroupsConfiguration\".equals(cmd)) {\r\n        exitCode = refreshSuperUserGroupsConfiguration();\r\n    } else if (\"-refreshAdminAcls\".equals(cmd)) {\r\n        exitCode = refreshAdminAcls();\r\n    } else if (\"-refreshLoadedJobCache\".equals(cmd)) {\r\n        exitCode = refreshLoadedJobCache();\r\n    } else if (\"-refreshJobRetentionSettings\".equals(cmd)) {\r\n        exitCode = refreshJobRetentionSettings();\r\n    } else if (\"-refreshLogRetentionSettings\".equals(cmd)) {\r\n        exitCode = refreshLogRetentionSettings();\r\n    } else if (\"-getGroups\".equals(cmd)) {\r\n        String[] usernames = Arrays.copyOfRange(args, i, args.length);\r\n        exitCode = getGroups(usernames);\r\n    } else if (\"-help\".equals(cmd)) {\r\n        if (i < args.length) {\r\n            printHelp(args[i]);\r\n        } else {\r\n            printHelp(\"\");\r\n        }\r\n    } else {\r\n        exitCode = -1;\r\n        System.err.println(cmd.substring(1) + \": Unknown command\");\r\n        printUsage(\"\");\r\n    }\r\n    return exitCode;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\client",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    int result = ToolRunner.run(new HSAdmin(conf), args);\r\n    System.exit(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "initStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initStorage(Configuration conf) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "startStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startStorage() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "closeStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void closeStorage() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryServerState loadState() throws IOException\n{\r\n    throw new UnsupportedOperationException(\"Cannot load state from null store\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void storeToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "updateToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateToken(MRDelegationTokenIdentifier tokenId, Long renewDate) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeToken(MRDelegationTokenIdentifier tokenId) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "storeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void storeTokenMasterKey(DelegationKey key) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeTokenMasterKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeTokenMasterKey(DelegationKey key) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceInit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    LOG.info(\"JobHistory Init\");\r\n    this.conf = conf;\r\n    this.appID = ApplicationId.newInstance(0, 0);\r\n    this.appAttemptID = RecordFactoryProvider.getRecordFactory(conf).newRecordInstance(ApplicationAttemptId.class);\r\n    moveThreadInterval = conf.getLong(JHAdminConfig.MR_HISTORY_MOVE_INTERVAL_MS, JHAdminConfig.DEFAULT_MR_HISTORY_MOVE_INTERVAL_MS);\r\n    hsManager = createHistoryFileManager();\r\n    hsManager.init(conf);\r\n    try {\r\n        hsManager.initExisting();\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(\"Failed to initialize existing directories\", e);\r\n    }\r\n    storage = createHistoryStorage();\r\n    if (storage instanceof Service) {\r\n        ((Service) storage).init(conf);\r\n    }\r\n    storage.setHistoryFileManager(hsManager);\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHistoryStorage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HistoryStorage createHistoryStorage()\n{\r\n    return ReflectionUtils.newInstance(conf.getClass(JHAdminConfig.MR_HISTORY_STORAGE, CachedHistoryStorage.class, HistoryStorage.class), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHistoryFileManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryFileManager createHistoryFileManager()\n{\r\n    return new HistoryFileManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    hsManager.start();\r\n    if (storage instanceof Service) {\r\n        ((Service) storage).start();\r\n    }\r\n    scheduledExecutor = new HadoopScheduledThreadPoolExecutor(2, new ThreadFactoryBuilder().setNameFormat(\"Log Scanner/Cleaner #%d\").build());\r\n    scheduledExecutor.scheduleAtFixedRate(new MoveIntermediateToDoneRunnable(), moveThreadInterval, moveThreadInterval, TimeUnit.MILLISECONDS);\r\n    scheduleHistoryCleaner();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getInitDelaySecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getInitDelaySecs()\n{\r\n    return 30;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStop",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    LOG.info(\"Stopping JobHistory\");\r\n    if (scheduledExecutor != null) {\r\n        LOG.info(\"Stopping History Cleaner/Move To Done\");\r\n        scheduledExecutor.shutdown();\r\n        int retryCnt = 50;\r\n        try {\r\n            while (!scheduledExecutor.awaitTermination(20, TimeUnit.MILLISECONDS)) {\r\n                if (--retryCnt == 0) {\r\n                    scheduledExecutor.shutdownNow();\r\n                    break;\r\n                }\r\n            }\r\n        } catch (InterruptedException iex) {\r\n            LOG.warn(\"HistoryCleanerService/move to done shutdown may not have \" + \"succeeded, Forcing a shutdown\", iex);\r\n            if (!scheduledExecutor.isShutdown()) {\r\n                scheduledExecutor.shutdownNow();\r\n            }\r\n        }\r\n        scheduledExecutor = null;\r\n    }\r\n    if (storage != null && storage instanceof Service) {\r\n        ((Service) storage).stop();\r\n    }\r\n    if (hsManager != null) {\r\n        hsManager.stop();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getApplicationName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getApplicationName()\n{\r\n    return \"Job History Server\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJobFileInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HistoryFileInfo getJobFileInfo(JobId jobId) throws IOException\n{\r\n    return hsManager.getFileInfo(jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getJob(JobId jobId)\n{\r\n    return storage.getFullJob(jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllJobs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<JobId, Job> getAllJobs(ApplicationId appID)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Called getAllJobs(AppId): \" + appID);\r\n    }\r\n    org.apache.hadoop.mapreduce.JobID oldJobID = TypeConverter.fromYarn(appID);\r\n    Map<JobId, Job> jobs = new HashMap<JobId, Job>();\r\n    JobId jobID = TypeConverter.toYarn(oldJobID);\r\n    jobs.put(jobID, getJob(jobID));\r\n    return jobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<JobId, Job> getAllJobs()\n{\r\n    return storage.getAllPartialJobs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "refreshLoadedJobCache",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void refreshLoadedJobCache()\n{\r\n    if (getServiceState() == STATE.STARTED) {\r\n        if (storage instanceof CachedHistoryStorage) {\r\n            ((CachedHistoryStorage) storage).refreshLoadedJobCache();\r\n        } else {\r\n            throw new UnsupportedOperationException(storage.getClass().getName() + \" is expected to be an instance of \" + CachedHistoryStorage.class.getName());\r\n        }\r\n    } else {\r\n        LOG.warn(\"Failed to execute refreshLoadedJobCache: JobHistory service is not started\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getHistoryStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryStorage getHistoryStorage()\n{\r\n    return storage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getPartialJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobsInfo getPartialJobs(Long offset, Long count, String user, String queue, Long sBegin, Long sEnd, Long fBegin, Long fEnd, JobState jobState)\n{\r\n    return storage.getPartialJobs(offset, count, user, queue, sBegin, sEnd, fBegin, fEnd, jobState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "refreshJobRetentionSettings",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void refreshJobRetentionSettings()\n{\r\n    if (getServiceState() == STATE.STARTED) {\r\n        conf = createConf();\r\n        long maxHistoryAge = conf.getLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS, JHAdminConfig.DEFAULT_MR_HISTORY_MAX_AGE);\r\n        hsManager.setMaxHistoryAge(maxHistoryAge);\r\n        if (futureHistoryCleaner != null) {\r\n            futureHistoryCleaner.cancel(false);\r\n        }\r\n        futureHistoryCleaner = null;\r\n        scheduleHistoryCleaner();\r\n    } else {\r\n        LOG.warn(\"Failed to execute refreshJobRetentionSettings : Job History service is not started\");\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "scheduleHistoryCleaner",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void scheduleHistoryCleaner()\n{\r\n    boolean startCleanerService = conf.getBoolean(JHAdminConfig.MR_HISTORY_CLEANER_ENABLE, true);\r\n    if (startCleanerService) {\r\n        cleanerInterval = conf.getLong(JHAdminConfig.MR_HISTORY_CLEANER_INTERVAL_MS, JHAdminConfig.DEFAULT_MR_HISTORY_CLEANER_INTERVAL_MS);\r\n        futureHistoryCleaner = scheduledExecutor.scheduleAtFixedRate(new HistoryCleaner(), getInitDelaySecs() * 1000l, cleanerInterval, TimeUnit.MILLISECONDS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration createConf()\n{\r\n    return new Configuration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCleanerInterval",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCleanerInterval()\n{\r\n    return cleanerInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getApplicationAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ApplicationAttemptId getApplicationAttemptId()\n{\r\n    return appAttemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getApplicationID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ApplicationId getApplicationID()\n{\r\n    return appID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getEventHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventHandler<Event> getEventHandler()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CharSequence getUser()\n{\r\n    if (userName != null) {\r\n        userName = conf.get(MRJobConfig.USER_NAME, \"history-user\");\r\n    }\r\n    return userName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getClock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Clock getClock()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getClusterInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClusterInfo getClusterInfo()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getBlacklistedNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getBlacklistedNodes()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getClientToAMTokenSecretManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientToAMTokenSecretManager getClientToAMTokenSecretManager()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "isLastAMRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLastAMRetry()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "hasSuccessfullyUnregistered",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasSuccessfullyUnregistered()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getNMHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNMHostname()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTaskAttemptFinishingMonitor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptFinishingMonitor getTaskAttemptFinishingMonitor()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getHistoryUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHistoryUrl()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setHistoryUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHistoryUrl(String historyUrl)\n{\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    String jobID = $(JOB_ID);\r\n    set(TITLE, jobID.isEmpty() ? \"Bad request: missing job ID\" : join(\"Configuration for MapReduce Job \", $(JOB_ID)));\r\n    commonPreHead(html);\r\n    set(DATATABLES_ID, \"conf\");\r\n    set(initID(DATATABLES, \"conf\"), confTableInit());\r\n    set(postInitID(DATATABLES, \"conf\"), confPostTableInit());\r\n    setTableStyles(html, \"conf\");\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:1}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return ConfBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "confTableInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String confTableInit()\n{\r\n    return tableInit().append(\"}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "confPostTableInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String confPostTableInit()\n{\r\n    return \"var confInitVals = new Array();\\n\" + \"$('tfoot input').keyup( function () \\n{\" + \"  confDataTable.fnFilter( this.value, $('tfoot input').index(this) );\\n\" + \"} );\\n\" + \"$('tfoot input').each( function (i) {\\n\" + \"  confInitVals[i] = this.value;\\n\" + \"} );\\n\" + \"$('tfoot input').focus( function () {\\n\" + \"  if ( this.className == 'search_init' )\\n\" + \"  {\\n\" + \"    this.className = '';\\n\" + \"    this.value = '';\\n\" + \"  }\\n\" + \"} );\\n\" + \"$('tfoot input').blur( function (i) {\\n\" + \"  if ( this.value == '' )\\n\" + \"  {\\n\" + \"    this.className = 'search_init';\\n\" + \"    this.value = confInitVals[$('tfoot input').index(this)];\\n\" + \"  }\\n\" + \"} );\\n\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "canCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean canCommit(TaskAttemptId taskAttemptID)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskAttempt getAttempt(TaskAttemptId attemptID)\n{\r\n    loadAllTaskAttempts();\r\n    return attempts.get(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<TaskAttemptId, TaskAttempt> getAttempts()\n{\r\n    loadAllTaskAttempts();\r\n    return attempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return taskInfo.getCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskId getID()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return 1.0f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskReport getReport()\n{\r\n    if (report == null) {\r\n        constructTaskReport();\r\n    }\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskType getType()\n{\r\n    return TypeConverter.toYarn(taskInfo.getTaskType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "isFinished",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFinished()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskState getState()\n{\r\n    return taskInfo.getTaskStatus() == null ? TaskState.KILLED : TaskState.valueOf(taskInfo.getTaskStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "constructTaskReport",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void constructTaskReport()\n{\r\n    loadAllTaskAttempts();\r\n    this.report = Records.newRecord(TaskReport.class);\r\n    report.setTaskId(taskId);\r\n    long minLaunchTime = Long.MAX_VALUE;\r\n    for (TaskAttempt attempt : attempts.values()) {\r\n        minLaunchTime = Math.min(minLaunchTime, attempt.getLaunchTime());\r\n    }\r\n    minLaunchTime = minLaunchTime == Long.MAX_VALUE ? -1 : minLaunchTime;\r\n    report.setStartTime(minLaunchTime);\r\n    report.setFinishTime(taskInfo.getFinishTime());\r\n    report.setTaskState(getState());\r\n    report.setProgress(getProgress());\r\n    Counters counters = getCounters();\r\n    if (counters == null) {\r\n        counters = EMPTY_COUNTERS;\r\n    }\r\n    report.setRawCounters(counters);\r\n    if (successfulAttempt != null) {\r\n        report.setSuccessfulAttempt(successfulAttempt);\r\n    }\r\n    report.addAllDiagnostics(reportDiagnostics);\r\n    report.addAllRunningAttempts(new ArrayList<TaskAttemptId>(attempts.keySet()));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadAllTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void loadAllTaskAttempts()\n{\r\n    if (taskAttemptsLoaded.get()) {\r\n        return;\r\n    }\r\n    taskAttemptsLock.lock();\r\n    try {\r\n        if (taskAttemptsLoaded.get()) {\r\n            return;\r\n        }\r\n        for (TaskAttemptInfo attemptHistory : taskInfo.getAllTaskAttempts().values()) {\r\n            CompletedTaskAttempt attempt = new CompletedTaskAttempt(taskId, attemptHistory);\r\n            reportDiagnostics.addAll(attempt.getDiagnostics());\r\n            attempts.put(attempt.getID(), attempt);\r\n            if (successfulAttempt == null && attemptHistory.getTaskStatus() != null && attemptHistory.getTaskStatus().equals(TaskState.SUCCEEDED.toString())) {\r\n                successfulAttempt = TypeConverter.toYarn(attemptHistory.getAttemptId());\r\n            }\r\n        }\r\n        taskAttemptsLoaded.set(true);\r\n    } finally {\r\n        taskAttemptsLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    Configuration conf = getConfig();\r\n    YarnRPC rpc = YarnRPC.create(conf);\r\n    initializeWebApp(conf);\r\n    InetSocketAddress address = conf.getSocketAddr(JHAdminConfig.MR_HISTORY_BIND_HOST, JHAdminConfig.MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_PORT);\r\n    server = rpc.getServer(HSClientProtocol.class, protocolHandler, address, conf, jhsDTSecretManager, conf.getInt(JHAdminConfig.MR_HISTORY_CLIENT_THREAD_COUNT, JHAdminConfig.DEFAULT_MR_HISTORY_CLIENT_THREAD_COUNT));\r\n    if (conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {\r\n        server.refreshServiceAcl(conf, new ClientHSPolicyProvider());\r\n    }\r\n    server.start();\r\n    this.bindAddress = conf.updateConnectAddr(JHAdminConfig.MR_HISTORY_BIND_HOST, JHAdminConfig.MR_HISTORY_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_ADDRESS, server.getListenerAddress());\r\n    LOG.info(\"Instantiated HistoryClientService at \" + this.bindAddress);\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "initializeWebApp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initializeWebApp(Configuration conf) throws IOException\n{\r\n    webApp = new HsWebApp(history);\r\n    setupFilters(conf);\r\n    InetSocketAddress bindAddress = MRWebAppUtil.getJHSWebBindAddress(conf);\r\n    ApplicationClientProtocol appClientProtocol = ClientRMProxy.createRMProxy(conf, ApplicationClientProtocol.class);\r\n    WebApps.$for(\"jobhistory\", HistoryClientService.class, this, \"ws\").with(conf).withHttpSpnegoKeytabKey(JHAdminConfig.MR_WEBAPP_SPNEGO_KEYTAB_FILE_KEY).withHttpSpnegoPrincipalKey(JHAdminConfig.MR_WEBAPP_SPNEGO_USER_NAME_KEY).withCSRFProtection(JHAdminConfig.MR_HISTORY_CSRF_PREFIX).withXFSProtection(JHAdminConfig.MR_HISTORY_XFS_PREFIX).withAppClientProtocol(appClientProtocol).at(NetUtils.getHostPortString(bindAddress)).start(webApp);\r\n    String connectHost = MRWebAppUtil.getJHSWebappURLWithoutScheme(conf).split(\":\")[0];\r\n    MRWebAppUtil.setJHSWebappURLWithoutScheme(conf, connectHost + \":\" + webApp.getListenerAddress().getPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (server != null) {\r\n        server.stop();\r\n    }\r\n    if (webApp != null) {\r\n        webApp.stop();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getClientHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MRClientProtocol getClientHandler()\n{\r\n    return this.protocolHandler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getBindAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getBindAddress()\n{\r\n    return this.bindAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setupFilters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupFilters(Configuration conf)\n{\r\n    boolean enableCorsFilter = conf.getBoolean(JHAdminConfig.MR_HISTORY_ENABLE_CORS_FILTER, JHAdminConfig.DEFAULT_MR_HISTORY_ENABLE_CORS_FILTER);\r\n    if (enableCorsFilter) {\r\n        conf.setBoolean(HttpCrossOriginFilterInitializer.PREFIX + HttpCrossOriginFilterInitializer.ENABLED_SUFFIX, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(JobInfo jobInfo)\n{\r\n    this.job.add(jobInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<JobInfo> getJobs()\n{\r\n    return this.job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    RPC.stopProxy(rpcProxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshAdminAcls",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshAdminAcls() throws IOException\n{\r\n    try {\r\n        rpcProxy.refreshAdminAcls(NULL_CONTROLLER, VOID_REFRESH_ADMIN_ACLS_REQUEST);\r\n    } catch (ServiceException se) {\r\n        throw ProtobufHelper.getRemoteException(se);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshLoadedJobCache",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshLoadedJobCache() throws IOException\n{\r\n    try {\r\n        rpcProxy.refreshLoadedJobCache(NULL_CONTROLLER, VOID_REFRESH_LOADED_JOB_CACHE_REQUEST);\r\n    } catch (ServiceException se) {\r\n        throw ProtobufHelper.getRemoteException(se);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshJobRetentionSettings",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshJobRetentionSettings() throws IOException\n{\r\n    try {\r\n        rpcProxy.refreshJobRetentionSettings(NULL_CONTROLLER, VOID_REFRESH_JOB_RETENTION_SETTINGS_REQUEST);\r\n    } catch (ServiceException se) {\r\n        throw ProtobufHelper.getRemoteException(se);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "refreshLogRetentionSettings",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshLogRetentionSettings() throws IOException\n{\r\n    try {\r\n        rpcProxy.refreshLogRetentionSettings(NULL_CONTROLLER, VOID_REFRESH_LOG_RETENTION_SETTINGS_REQUEST);\r\n    } catch (ServiceException se) {\r\n        throw ProtobufHelper.getRemoteException(se);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\protocolPB",
  "methodName" : "isMethodSupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMethodSupported(String methodName) throws IOException\n{\r\n    return RpcClientUtil.isMethodSupported(rpcProxy, HSAdminRefreshProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(HSAdminRefreshProtocolPB.class), methodName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void render(Block html)\n{\r\n    DIV<Hamlet> nav = html.div(\"#nav\").h3(\"Application\").ul().li().a(url(\"about\"), \"About\").__().li().a(url(\"app\"), \"Jobs\").__().__();\r\n    if (app.getJob() != null) {\r\n        String jobid = MRApps.toString(app.getJob().getID());\r\n        nav.h3(\"Job\").ul().li().a(url(\"job\", jobid), \"Overview\").__().li().a(url(\"jobcounters\", jobid), \"Counters\").__().li().a(url(\"conf\", jobid), \"Configuration\").__().li().a(url(\"tasks\", jobid, \"m\"), \"Map tasks\").__().li().a(url(\"tasks\", jobid, \"r\"), \"Reduce tasks\").__().__();\r\n        if (app.getTask() != null) {\r\n            String taskid = MRApps.toString(app.getTask().getID());\r\n            nav.h3(\"Task\").ul().li().a(url(\"task\", taskid), \"Task Overview\").__().li().a(url(\"taskcounters\", taskid), \"Counters\").__().__();\r\n        }\r\n    }\r\n    Hamlet.UL<DIV<Hamlet>> tools = WebPageUtils.appendToolSection(nav, conf);\r\n    if (tools != null) {\r\n        tools.__().__();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    String jobID = $(JOB_ID);\r\n    set(TITLE, jobID.isEmpty() ? \"Bad request: missing job ID\" : join(\"MapReduce Job \", $(JOB_ID)));\r\n    commonPreHead(html);\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:1}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return HsJobBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return FewAttemptsBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getHadoopVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHadoopVersion()\n{\r\n    return this.hadoopVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getHadoopBuildVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHadoopBuildVersion()\n{\r\n    return this.hadoopBuildVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getHadoopVersionBuiltOn",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHadoopVersionBuiltOn()\n{\r\n    return this.hadoopVersionBuiltOn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getStartedOn",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartedOn()\n{\r\n    return this.startedOn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    setActiveNavColumnForTask();\r\n    set(DATATABLES_SELECTOR, \"#counters .dt-counters\");\r\n    set(initSelector(DATATABLES), \"{bJQueryUI:true, sDom:'t', iDisplayLength:-1}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "postHead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void postHead(Page.HTML<__> html)\n{\r\n    html.style(\"#counters, .dt-counters { table-layout: fixed }\", \"#counters th { overflow: hidden; vertical-align: middle }\", \"#counters .dataTables_wrapper { min-height: 1em }\", \"#counters .group { width: 15em }\", \"#counters .name { width: 30em }\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return CountersBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void setup()\n{\r\n    bind(HsWebServices.class);\r\n    bind(JAXBContextResolver.class);\r\n    bind(GenericExceptionHandler.class);\r\n    bind(AppContext.class).toInstance(history);\r\n    bind(HistoryContext.class).toInstance(history);\r\n    route(\"/\", HsController.class);\r\n    route(\"/app\", HsController.class);\r\n    route(pajoin(\"/job\", JOB_ID), HsController.class, \"job\");\r\n    route(pajoin(\"/conf\", JOB_ID), HsController.class, \"conf\");\r\n    routeWithoutDefaultView(pajoin(\"/downloadconf\", JOB_ID), HsController.class, \"downloadConf\");\r\n    route(pajoin(\"/jobcounters\", JOB_ID), HsController.class, \"jobCounters\");\r\n    route(pajoin(\"/singlejobcounter\", JOB_ID, COUNTER_GROUP, COUNTER_NAME), HsController.class, \"singleJobCounter\");\r\n    route(pajoin(\"/tasks\", JOB_ID, TASK_TYPE), HsController.class, \"tasks\");\r\n    route(pajoin(\"/attempts\", JOB_ID, TASK_TYPE, ATTEMPT_STATE), HsController.class, \"attempts\");\r\n    route(pajoin(\"/task\", TASK_ID), HsController.class, \"task\");\r\n    route(pajoin(\"/taskcounters\", TASK_ID), HsController.class, \"taskCounters\");\r\n    route(pajoin(\"/singletaskcounter\", TASK_ID, COUNTER_GROUP, COUNTER_NAME), HsController.class, \"singleTaskCounter\");\r\n    route(\"/about\", HsController.class, \"about\");\r\n    route(pajoin(\"/logs\", NM_NODENAME, CONTAINER_ID, ENTITY_STRING, APP_OWNER, CONTAINER_LOG_TYPE), HsController.class, \"logs\");\r\n    route(pajoin(\"/nmlogs\", NM_NODENAME, CONTAINER_ID, ENTITY_STRING, APP_OWNER, CONTAINER_LOG_TYPE), HsController.class, \"nmlogs\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getNumMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumMaps()\n{\r\n    return numMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getNumReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumReduces()\n{\r\n    return numReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getAvgMapTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getAvgMapTime()\n{\r\n    return avgMapTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getAvgReduceTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getAvgReduceTime()\n{\r\n    return avgReduceTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getAvgShuffleTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getAvgShuffleTime()\n{\r\n    return avgShuffleTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getAvgMergeTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long getAvgMergeTime()\n{\r\n    return avgMergeTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getFailedReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getFailedReduceAttempts()\n{\r\n    return failedReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getKilledReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getKilledReduceAttempts()\n{\r\n    return killedReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getSuccessfulReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getSuccessfulReduceAttempts()\n{\r\n    return successfulReduceAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getFailedMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getFailedMapAttempts()\n{\r\n    return failedMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getKilledMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getKilledMapAttempts()\n{\r\n    return killedMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getSuccessfulMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Integer getSuccessfulMapAttempts()\n{\r\n    return successfulMapAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getAcls",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<ConfEntryInfo> getAcls()\n{\r\n    return acls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getReducesCompleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReducesCompleted()\n{\r\n    return this.reducesCompleted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getReducesTotal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReducesTotal()\n{\r\n    return this.reducesTotal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getMapsCompleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapsCompleted()\n{\r\n    return this.mapsCompleted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getMapsTotal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapsTotal()\n{\r\n    return this.mapsTotal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getState()\n{\r\n    return this.state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getUserName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUserName()\n{\r\n    return this.user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return this.name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return this.queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getId()\n{\r\n    return this.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSubmitTime()\n{\r\n    return this.submitTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getFormattedStartTimeStr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFormattedStartTimeStr(final DateFormat dateFormat)\n{\r\n    String str = NA;\r\n    if (startTime >= 0) {\r\n        str = dateFormat.format(new Date(startTime));\r\n    }\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getStartTimeStr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStartTimeStr()\n{\r\n    String str = NA;\r\n    if (startTime >= 0) {\r\n        str = new Date(startTime).toString();\r\n    }\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return this.finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "isUber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Boolean isUber()\n{\r\n    return this.uberized;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDiagnostics()\n{\r\n    return this.diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "countTasksAndAttempts",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void countTasksAndAttempts(Job job)\n{\r\n    numReduces = 0;\r\n    numMaps = 0;\r\n    final Map<TaskId, Task> tasks = job.getTasks();\r\n    if (tasks == null) {\r\n        return;\r\n    }\r\n    for (Task task : tasks.values()) {\r\n        Map<TaskAttemptId, TaskAttempt> attempts = task.getAttempts();\r\n        int successful, failed, killed;\r\n        for (TaskAttempt attempt : attempts.values()) {\r\n            successful = 0;\r\n            failed = 0;\r\n            killed = 0;\r\n            if (TaskAttemptStateUI.NEW.correspondsTo(attempt.getState())) {\r\n            } else if (TaskAttemptStateUI.RUNNING.correspondsTo(attempt.getState())) {\r\n            } else if (TaskAttemptStateUI.SUCCESSFUL.correspondsTo(attempt.getState())) {\r\n                ++successful;\r\n            } else if (TaskAttemptStateUI.FAILED.correspondsTo(attempt.getState())) {\r\n                ++failed;\r\n            } else if (TaskAttemptStateUI.KILLED.correspondsTo(attempt.getState())) {\r\n                ++killed;\r\n            }\r\n            switch(task.getType()) {\r\n                case MAP:\r\n                    successfulMapAttempts += successful;\r\n                    failedMapAttempts += failed;\r\n                    killedMapAttempts += killed;\r\n                    if (attempt.getState() == TaskAttemptState.SUCCEEDED) {\r\n                        numMaps++;\r\n                        avgMapTime += (attempt.getFinishTime() - attempt.getLaunchTime());\r\n                    }\r\n                    break;\r\n                case REDUCE:\r\n                    successfulReduceAttempts += successful;\r\n                    failedReduceAttempts += failed;\r\n                    killedReduceAttempts += killed;\r\n                    if (attempt.getState() == TaskAttemptState.SUCCEEDED) {\r\n                        numReduces++;\r\n                        avgShuffleTime += (attempt.getShuffleFinishTime() - attempt.getLaunchTime());\r\n                        avgMergeTime += attempt.getSortFinishTime() - attempt.getShuffleFinishTime();\r\n                        avgReduceTime += (attempt.getFinishTime() - attempt.getSortFinishTime());\r\n                    }\r\n                    break;\r\n            }\r\n        }\r\n    }\r\n    if (numMaps > 0) {\r\n        avgMapTime = avgMapTime / numMaps;\r\n    }\r\n    if (numReduces > 0) {\r\n        avgReduceTime = avgReduceTime / numReduces;\r\n        avgShuffleTime = avgShuffleTime / numReduces;\r\n        avgMergeTime = avgMergeTime / numReduces;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getNodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NodeId getNodeId() throws UnsupportedOperationException\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAssignedContainerID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerId getAssignedContainerID()\n{\r\n    return attemptInfo.getContainerId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAssignedContainerMgrAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getAssignedContainerMgrAddress()\n{\r\n    return attemptInfo.getHostname() + \":\" + attemptInfo.getPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getNodeHttpAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getNodeHttpAddress()\n{\r\n    return attemptInfo.getTrackerName() + \":\" + attemptInfo.getHttpPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getNodeRackName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNodeRackName()\n{\r\n    return attemptInfo.getRackname();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return attemptInfo.getCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptId getID()\n{\r\n    return attemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return 1.0f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptReport getReport()\n{\r\n    if (report == null) {\r\n        constructTaskAttemptReport();\r\n    }\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getPhase",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Phase getPhase()\n{\r\n    return Phase.CLEANUP;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptState getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "isFinished",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFinished()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getDiagnostics()\n{\r\n    return diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLaunchTime()\n{\r\n    return attemptInfo.getStartTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return attemptInfo.getFinishTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getShuffleFinishTime()\n{\r\n    return attemptInfo.getShuffleFinishTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getSortFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSortFinishTime()\n{\r\n    return attemptInfo.getSortFinishTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getShufflePort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getShufflePort()\n{\r\n    return attemptInfo.getShufflePort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "constructTaskAttemptReport",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void constructTaskAttemptReport()\n{\r\n    report = Records.newRecord(TaskAttemptReport.class);\r\n    report.setTaskAttemptId(attemptId);\r\n    report.setTaskAttemptState(state);\r\n    report.setProgress(getProgress());\r\n    report.setStartTime(attemptInfo.getStartTime());\r\n    report.setFinishTime(attemptInfo.getFinishTime());\r\n    report.setShuffleFinishTime(attemptInfo.getShuffleFinishTime());\r\n    report.setSortFinishTime(attemptInfo.getSortFinishTime());\r\n    if (localDiagMessage != null) {\r\n        report.setDiagnosticInfo(attemptInfo.getError() + \", \" + localDiagMessage);\r\n    } else {\r\n        report.setDiagnosticInfo(attemptInfo.getError());\r\n    }\r\n    report.setStateString(attemptInfo.getState());\r\n    report.setRawCounters(getCounters());\r\n    report.setContainerId(attemptInfo.getContainerId());\r\n    if (attemptInfo.getHostname() == null) {\r\n        report.setNodeManagerHost(\"UNKNOWN\");\r\n    } else {\r\n        report.setNodeManagerHost(attemptInfo.getHostname());\r\n        report.setNodeManagerPort(attemptInfo.getPort());\r\n    }\r\n    report.setNodeManagerHttpPort(attemptInfo.getHttpPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getStore",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HistoryServerStateStoreService getStore(Configuration conf)\n{\r\n    Class<? extends HistoryServerStateStoreService> storeClass = HistoryServerNullStateStoreService.class;\r\n    boolean recoveryEnabled = conf.getBoolean(JHAdminConfig.MR_HS_RECOVERY_ENABLE, JHAdminConfig.DEFAULT_MR_HS_RECOVERY_ENABLE);\r\n    if (recoveryEnabled) {\r\n        storeClass = conf.getClass(JHAdminConfig.MR_HS_STATE_STORE, null, HistoryServerStateStoreService.class);\r\n        if (storeClass == null) {\r\n            throw new RuntimeException(\"Unable to locate storage class, check \" + JHAdminConfig.MR_HS_STATE_STORE);\r\n        }\r\n    }\r\n    return ReflectionUtils.newInstance(storeClass, conf);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "preHead",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void preHead(Page.HTML<__> html)\n{\r\n    commonPreHead(html);\r\n    set(initID(ACCORDION, \"nav\"), \"{autoHeight:false, active:2}\");\r\n    set(DATATABLES_ID, \"attempts\");\r\n    set(initID(DATATABLES, \"attempts\"), attemptsTableInit());\r\n    set(postInitID(DATATABLES, \"attempts\"), attemptsPostTableInit());\r\n    setTableStyles(html, \"attempts\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "content",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends SubView> content()\n{\r\n    return AttemptsBlock.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "attemptsTableInit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String attemptsTableInit()\n{\r\n    TaskType type = null;\r\n    String symbol = $(TASK_TYPE);\r\n    if (!symbol.isEmpty()) {\r\n        type = MRApps.taskType(symbol);\r\n    } else {\r\n        TaskId taskID = MRApps.toTaskID($(TASK_ID));\r\n        type = taskID.getTaskType();\r\n    }\r\n    StringBuilder b = tableInit().append(\", 'aaData': attemptsTableData\").append(\", bDeferRender: true\").append(\", bProcessing: true\").append(\"\\n,aoColumnDefs:[\\n\").append(\"\\n{'aTargets': [ 4 ]\").append(\", 'bSearchable': false }\").append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\").append(\", 'mRender': parseHadoopID }\").append(\"\\n, {'sType':'numeric', 'aTargets': [ 5, 6\").append(type == TaskType.REDUCE ? \", 7, 8\" : \"\").append(\" ], 'mRender': renderHadoopDate }\").append(\"\\n, {'sType':'numeric', 'aTargets': [\").append(type == TaskType.REDUCE ? \"9, 10, 11, 12\" : \"7\").append(\" ], 'mRender': renderHadoopElapsedTime }]\").append(\"\\n, aaSorting: [[0, 'asc']]\").append(\"}\");\r\n    return b.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "attemptsPostTableInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String attemptsPostTableInit()\n{\r\n    return \"var asInitVals = new Array();\\n\" + \"$('tfoot input').keyup( function () \\n{\" + \"  attemptsDataTable.fnFilter( this.value, $('tfoot input').index(this) );\\n\" + \"} );\\n\" + \"$('tfoot input').each( function (i) {\\n\" + \"  asInitVals[i] = this.value;\\n\" + \"} );\\n\" + \"$('tfoot input').focus( function () {\\n\" + \"  if ( this.className == 'search_init' )\\n\" + \"  {\\n\" + \"    this.className = '';\\n\" + \"    this.value = '';\\n\" + \"  }\\n\" + \"} );\\n\" + \"$('tfoot input').blur( function (i) {\\n\" + \"  if ( this.value == '' )\\n\" + \"  {\\n\" + \"    this.className = 'search_init';\\n\" + \"    this.value = asInitVals[$('tfoot input').index(this)];\\n\" + \"  }\\n\" + \"} );\\n\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createProxy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "T createProxy(Configuration conf, InetSocketAddress hsaddr, Class<T> xface, UserGroupInformation ugi) throws IOException\n{\r\n    T proxy;\r\n    if (xface == RefreshUserMappingsProtocol.class) {\r\n        proxy = (T) createHSProxyWithRefreshUserMappingsProtocol(hsaddr, conf, ugi);\r\n    } else if (xface == GetUserMappingsProtocol.class) {\r\n        proxy = (T) createHSProxyWithGetUserMappingsProtocol(hsaddr, conf, ugi);\r\n    } else if (xface == HSAdminRefreshProtocol.class) {\r\n        proxy = (T) createHSProxyWithHSAdminRefreshProtocol(hsaddr, conf, ugi);\r\n    } else {\r\n        String message = \"Unsupported protocol found when creating the proxy \" + \"connection to History server: \" + ((xface != null) ? xface.getClass().getName() : \"null\");\r\n        LOG.error(message);\r\n        throw new IllegalStateException(message);\r\n    }\r\n    return proxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHSProxyWithRefreshUserMappingsProtocol",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshUserMappingsProtocol createHSProxyWithRefreshUserMappingsProtocol(InetSocketAddress address, Configuration conf, UserGroupInformation ugi) throws IOException\n{\r\n    RefreshUserMappingsProtocolPB proxy = (RefreshUserMappingsProtocolPB) createHSProxy(address, conf, ugi, RefreshUserMappingsProtocolPB.class, 0);\r\n    return new RefreshUserMappingsProtocolClientSideTranslatorPB(proxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHSProxyWithGetUserMappingsProtocol",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetUserMappingsProtocol createHSProxyWithGetUserMappingsProtocol(InetSocketAddress address, Configuration conf, UserGroupInformation ugi) throws IOException\n{\r\n    GetUserMappingsProtocolPB proxy = (GetUserMappingsProtocolPB) createHSProxy(address, conf, ugi, GetUserMappingsProtocolPB.class, 0);\r\n    return new GetUserMappingsProtocolClientSideTranslatorPB(proxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHSProxyWithHSAdminRefreshProtocol",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HSAdminRefreshProtocol createHSProxyWithHSAdminRefreshProtocol(InetSocketAddress hsaddr, Configuration conf, UserGroupInformation ugi) throws IOException\n{\r\n    HSAdminRefreshProtocolPB proxy = (HSAdminRefreshProtocolPB) createHSProxy(hsaddr, conf, ugi, HSAdminRefreshProtocolPB.class, 0);\r\n    return new HSAdminRefreshProtocolClientSideTranslatorPB(proxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHSProxy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object createHSProxy(InetSocketAddress address, Configuration conf, UserGroupInformation ugi, Class<?> xface, int rpcTimeout) throws IOException\n{\r\n    RPC.setProtocolEngine(conf, xface, ProtobufRpcEngine2.class);\r\n    Object proxy = RPC.getProxy(xface, RPC.getProtocolVersion(xface), address, ugi, conf, NetUtils.getDefaultSocketFactory(conf), rpcTimeout);\r\n    return proxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getNodeHttpAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNodeHttpAddress()\n{\r\n    return this.nodeHttpAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getNodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNodeId()\n{\r\n    return this.nodeId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getAttemptId()\n{\r\n    return this.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getContainerId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getContainerId()\n{\r\n    return this.containerId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getLogsLink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLogsLink()\n{\r\n    return this.logsLink;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp\\dao",
  "methodName" : "getShortLogsLink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getShortLogsLink()\n{\r\n    return this.shortLogsLink;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    this.conf = conf;\r\n    int serialNumberLowDigits = 3;\r\n    serialNumberFormat = (\"%0\" + (JobHistoryUtils.SERIAL_NUMBER_DIRECTORY_DIGITS + serialNumberLowDigits) + \"d\");\r\n    long maxFSWaitTime = conf.getLong(JHAdminConfig.MR_HISTORY_MAX_START_WAIT_TIME, JHAdminConfig.DEFAULT_MR_HISTORY_MAX_START_WAIT_TIME);\r\n    createHistoryDirs(SystemClock.getInstance(), 10 * 1000, maxFSWaitTime);\r\n    maxTasksForLoadedJob = conf.getInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, JHAdminConfig.DEFAULT_MR_HS_LOADED_JOBS_TASKS_MAX);\r\n    this.aclsMgr = new JobACLsManager(conf);\r\n    maxHistoryAge = conf.getLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS, JHAdminConfig.DEFAULT_MR_HISTORY_MAX_AGE);\r\n    jobListCache = createJobListCache();\r\n    serialNumberIndex = new SerialNumberIndex(conf.getInt(JHAdminConfig.MR_HISTORY_DATESTRING_CACHE_SIZE, JHAdminConfig.DEFAULT_MR_HISTORY_DATESTRING_CACHE_SIZE));\r\n    int numMoveThreads = conf.getInt(JHAdminConfig.MR_HISTORY_MOVE_THREAD_COUNT, JHAdminConfig.DEFAULT_MR_HISTORY_MOVE_THREAD_COUNT);\r\n    moveToDoneExecutor = createMoveToDoneThreadPool(numMoveThreads);\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createMoveToDoneThreadPool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ThreadPoolExecutor createMoveToDoneThreadPool(int numMoveThreads)\n{\r\n    ThreadFactory tf = new ThreadFactoryBuilder().setNameFormat(\"MoveIntermediateToDone Thread #%d\").build();\r\n    return new HadoopThreadPoolExecutor(numMoveThreads, numMoveThreads, 1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHistoryDirs",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createHistoryDirs(Clock clock, long intervalCheckMillis, long timeOutMillis) throws IOException\n{\r\n    long start = clock.getTime();\r\n    boolean done = false;\r\n    int counter = 0;\r\n    while (!done && ((timeOutMillis == -1) || (clock.getTime() - start < timeOutMillis))) {\r\n        done = tryCreatingHistoryDirs(counter++ % 3 == 0);\r\n        if (done) {\r\n            break;\r\n        }\r\n        try {\r\n            Thread.sleep(intervalCheckMillis);\r\n        } catch (InterruptedException ex) {\r\n            throw new YarnRuntimeException(ex);\r\n        }\r\n    }\r\n    if (!done) {\r\n        throw new YarnRuntimeException(\"Timed out '\" + timeOutMillis + \"ms' waiting for FileSystem to become available\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "isNameNodeStillNotStarted",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isNameNodeStillNotStarted(Exception ex)\n{\r\n    String nameNodeNotStartedMsg = NameNode.composeNotStartedMessage(HdfsServerConstants.NamenodeRole.NAMENODE);\r\n    return ex.toString().contains(\"SafeModeException\") || (ex instanceof RetriableException && ex.getMessage().contains(nameNodeNotStartedMsg));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "tryCreatingHistoryDirs",
  "errType" : [ "ConnectException", "IOException", "ConnectException", "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "boolean tryCreatingHistoryDirs(boolean logWait) throws IOException\n{\r\n    boolean succeeded = true;\r\n    String doneDirPrefix = JobHistoryUtils.getConfiguredHistoryServerDoneDirPrefix(conf);\r\n    try {\r\n        doneDirPrefixPath = FileContext.getFileContext(conf).makeQualified(new Path(doneDirPrefix));\r\n        doneDirFc = FileContext.getFileContext(doneDirPrefixPath.toUri(), conf);\r\n        doneDirFc.setUMask(JobHistoryUtils.HISTORY_DONE_DIR_UMASK);\r\n        mkdir(doneDirFc, doneDirPrefixPath, new FsPermission(JobHistoryUtils.HISTORY_DONE_DIR_PERMISSION));\r\n    } catch (ConnectException ex) {\r\n        if (logWait) {\r\n            LOG.info(\"Waiting for FileSystem at \" + doneDirPrefixPath.toUri().getAuthority() + \"to be available\");\r\n        }\r\n        succeeded = false;\r\n    } catch (IOException e) {\r\n        if (isNameNodeStillNotStarted(e)) {\r\n            succeeded = false;\r\n            if (logWait) {\r\n                LOG.info(\"Waiting for FileSystem at \" + doneDirPrefixPath.toUri().getAuthority() + \"to be out of safe mode\");\r\n            }\r\n        } else {\r\n            throw new YarnRuntimeException(\"Error creating done directory: [\" + doneDirPrefixPath + \"]\", e);\r\n        }\r\n    }\r\n    if (succeeded) {\r\n        String intermediateDoneDirPrefix = JobHistoryUtils.getConfiguredHistoryIntermediateDoneDirPrefix(conf);\r\n        try {\r\n            intermediateDoneDirPath = FileContext.getFileContext(conf).makeQualified(new Path(intermediateDoneDirPrefix));\r\n            intermediateDoneDirFc = FileContext.getFileContext(intermediateDoneDirPath.toUri(), conf);\r\n            mkdir(intermediateDoneDirFc, intermediateDoneDirPath, new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS.toShort()));\r\n        } catch (ConnectException ex) {\r\n            succeeded = false;\r\n            if (logWait) {\r\n                LOG.info(\"Waiting for FileSystem at \" + intermediateDoneDirPath.toUri().getAuthority() + \"to be available\");\r\n            }\r\n        } catch (IOException e) {\r\n            if (isNameNodeStillNotStarted(e)) {\r\n                succeeded = false;\r\n                if (logWait) {\r\n                    LOG.info(\"Waiting for FileSystem at \" + intermediateDoneDirPath.toUri().getAuthority() + \"to be out of safe mode\");\r\n                }\r\n            } else {\r\n                throw new YarnRuntimeException(\"Error creating intermediate done directory: [\" + intermediateDoneDirPath + \"]\", e);\r\n            }\r\n        }\r\n    }\r\n    return succeeded;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    ShutdownThreadsHelper.shutdownExecutorService(moveToDoneExecutor);\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createJobListCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobListCache createJobListCache()\n{\r\n    return new JobListCache(conf.getInt(JHAdminConfig.MR_HISTORY_JOBLIST_CACHE_SIZE, JHAdminConfig.DEFAULT_MR_HISTORY_JOBLIST_CACHE_SIZE), maxHistoryAge);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "mkdir",
  "errType" : [ "FileAlreadyExistsException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void mkdir(FileContext fc, Path path, FsPermission fsp) throws IOException\n{\r\n    if (!fc.util().exists(path)) {\r\n        try {\r\n            fc.mkdir(path, fsp, true);\r\n            FileStatus fsStatus = fc.getFileStatus(path);\r\n            LOG.info(\"Perms after creating \" + fsStatus.getPermission().toShort() + \", Expected: \" + fsp.toShort());\r\n            if (fsStatus.getPermission().toShort() != fsp.toShort()) {\r\n                LOG.info(\"Explicitly setting permissions to : \" + fsp.toShort() + \", \" + fsp);\r\n                fc.setPermission(path, fsp);\r\n            }\r\n        } catch (FileAlreadyExistsException e) {\r\n            LOG.info(\"Directory: [\" + path + \"] already exists.\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "createHistoryFileInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HistoryFileInfo createHistoryFileInfo(Path historyFile, Path confFile, Path summaryFile, JobIndexInfo jobIndexInfo, boolean isInDone)\n{\r\n    return new HistoryFileInfo(historyFile, confFile, summaryFile, jobIndexInfo, isInDone);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "initExisting",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void initExisting() throws IOException\n{\r\n    LOG.info(\"Initializing Existing Jobs...\");\r\n    List<FileStatus> timestampedDirList = findTimestampedDirectories();\r\n    Collections.sort(timestampedDirList);\r\n    LOG.info(\"Found \" + timestampedDirList.size() + \" directories to load\");\r\n    for (FileStatus fs : timestampedDirList) {\r\n        addDirectoryToSerialNumberIndex(fs.getPath());\r\n    }\r\n    final double maxCacheSize = (double) jobListCache.maxSize;\r\n    int prevCacheSize = jobListCache.size();\r\n    for (int i = timestampedDirList.size() - 1; i >= 0 && !jobListCache.isFull(); i--) {\r\n        FileStatus fs = timestampedDirList.get(i);\r\n        addDirectoryToJobListCache(fs.getPath());\r\n        int currCacheSize = jobListCache.size();\r\n        if ((currCacheSize - prevCacheSize) / maxCacheSize >= 0.05) {\r\n            LOG.info(currCacheSize * 100.0 / maxCacheSize + \"% of cache is loaded.\");\r\n        }\r\n        prevCacheSize = currCacheSize;\r\n    }\r\n    final double loadedPercent = maxCacheSize == 0.0 ? 100 : prevCacheSize * 100.0 / maxCacheSize;\r\n    LOG.info(\"Existing job initialization finished. \" + loadedPercent + \"% of cache is occupied.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "removeDirectoryFromSerialNumberIndex",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeDirectoryFromSerialNumberIndex(Path serialDirPath)\n{\r\n    String serialPart = serialDirPath.getName();\r\n    String timeStampPart = JobHistoryUtils.getTimestampPartFromPath(serialDirPath.toString());\r\n    if (timeStampPart == null) {\r\n        LOG.warn(\"Could not find timestamp portion from path: \" + serialDirPath.toString() + \". Continuing with next\");\r\n        return;\r\n    }\r\n    if (serialPart == null) {\r\n        LOG.warn(\"Could not find serial portion from path: \" + serialDirPath.toString() + \". Continuing with next\");\r\n        return;\r\n    }\r\n    serialNumberIndex.remove(serialPart, timeStampPart);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "addDirectoryToSerialNumberIndex",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addDirectoryToSerialNumberIndex(Path serialDirPath)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Adding \" + serialDirPath + \" to serial index\");\r\n    }\r\n    String serialPart = serialDirPath.getName();\r\n    String timestampPart = JobHistoryUtils.getTimestampPartFromPath(serialDirPath.toString());\r\n    if (timestampPart == null) {\r\n        LOG.warn(\"Could not find timestamp portion from path: \" + serialDirPath + \". Continuing with next\");\r\n        return;\r\n    }\r\n    if (serialPart == null) {\r\n        LOG.warn(\"Could not find serial portion from path: \" + serialDirPath.toString() + \". Continuing with next\");\r\n    } else {\r\n        serialNumberIndex.add(serialPart, timestampPart);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "addDirectoryToJobListCache",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void addDirectoryToJobListCache(Path path) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Adding \" + path + \" to job list cache.\");\r\n    }\r\n    List<FileStatus> historyFileList = scanDirectoryForHistoryFiles(path, doneDirFc);\r\n    for (FileStatus fs : historyFileList) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Adding in history for \" + fs.getPath());\r\n        }\r\n        JobIndexInfo jobIndexInfo = FileNameIndexUtils.getIndexInfo(fs.getPath().getName());\r\n        String confFileName = JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\r\n        String summaryFileName = JobHistoryUtils.getIntermediateSummaryFileName(jobIndexInfo.getJobId());\r\n        HistoryFileInfo fileInfo = createHistoryFileInfo(fs.getPath(), new Path(fs.getPath().getParent(), confFileName), new Path(fs.getPath().getParent(), summaryFileName), jobIndexInfo, true);\r\n        jobListCache.addIfAbsent(fileInfo);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "scanDirectory",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<FileStatus> scanDirectory(Path path, FileContext fc, PathFilter pathFilter) throws IOException\n{\r\n    path = fc.makeQualified(path);\r\n    List<FileStatus> jhStatusList = new ArrayList<FileStatus>();\r\n    try {\r\n        RemoteIterator<FileStatus> fileStatusIter = fc.listStatus(path);\r\n        while (fileStatusIter.hasNext()) {\r\n            FileStatus fileStatus = fileStatusIter.next();\r\n            Path filePath = fileStatus.getPath();\r\n            if (fileStatus.isFile() && pathFilter.accept(filePath)) {\r\n                jhStatusList.add(fileStatus);\r\n            }\r\n        }\r\n    } catch (FileNotFoundException fe) {\r\n        LOG.error(\"Error while scanning directory \" + path, fe);\r\n    }\r\n    return jhStatusList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "scanDirectoryForHistoryFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<FileStatus> scanDirectoryForHistoryFiles(Path path, FileContext fc) throws IOException\n{\r\n    return scanDirectory(path, fc, JobHistoryUtils.getHistoryFileFilter());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "findTimestampedDirectories",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<FileStatus> findTimestampedDirectories() throws IOException\n{\r\n    List<FileStatus> fsList = JobHistoryUtils.localGlobber(doneDirFc, doneDirPrefixPath, DONE_BEFORE_SERIAL_TAIL);\r\n    return fsList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "scanIntermediateDirectory",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void scanIntermediateDirectory() throws IOException\n{\r\n    if (UserGroupInformation.isSecurityEnabled()) {\r\n        UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\r\n    }\r\n    List<FileStatus> userDirList = JobHistoryUtils.localGlobber(intermediateDoneDirFc, intermediateDoneDirPath, \"\");\r\n    LOG.debug(\"Scanning intermediate dirs\");\r\n    for (FileStatus userDir : userDirList) {\r\n        String name = userDir.getPath().getName();\r\n        UserLogDir dir = userDirModificationTimeMap.get(name);\r\n        if (dir == null) {\r\n            dir = new UserLogDir();\r\n            UserLogDir old = userDirModificationTimeMap.putIfAbsent(name, dir);\r\n            if (old != null) {\r\n                dir = old;\r\n            }\r\n        }\r\n        dir.scanIfNeeded(userDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "scanIntermediateDirectory",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void scanIntermediateDirectory(final Path absPath) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Scanning intermediate dir \" + absPath);\r\n    }\r\n    List<FileStatus> fileStatusList = scanDirectoryForHistoryFiles(absPath, intermediateDoneDirFc);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Found \" + fileStatusList.size() + \" files\");\r\n    }\r\n    for (FileStatus fs : fileStatusList) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"scanning file: \" + fs.getPath());\r\n        }\r\n        JobIndexInfo jobIndexInfo = FileNameIndexUtils.getIndexInfo(fs.getPath().getName());\r\n        String confFileName = JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\r\n        String summaryFileName = JobHistoryUtils.getIntermediateSummaryFileName(jobIndexInfo.getJobId());\r\n        HistoryFileInfo fileInfo = createHistoryFileInfo(fs.getPath(), new Path(fs.getPath().getParent(), confFileName), new Path(fs.getPath().getParent(), summaryFileName), jobIndexInfo, false);\r\n        final HistoryFileInfo old = jobListCache.addIfAbsent(fileInfo);\r\n        if (old == null || old.didMoveFail()) {\r\n            final HistoryFileInfo found = (old == null) ? fileInfo : old;\r\n            long cutoff = System.currentTimeMillis() - maxHistoryAge;\r\n            if (found.getJobIndexInfo().getFinishTime() <= cutoff) {\r\n                try {\r\n                    found.delete();\r\n                } catch (IOException e) {\r\n                    LOG.warn(\"Error cleaning up a HistoryFile that is out of date.\", e);\r\n                }\r\n            } else {\r\n                if (LOG.isDebugEnabled()) {\r\n                    LOG.debug(\"Scheduling move to done of \" + found);\r\n                }\r\n                moveToDoneExecutor.execute(new Runnable() {\r\n\r\n                    @Override\r\n                    public void run() {\r\n                        try {\r\n                            found.moveToDone();\r\n                        } catch (IOException e) {\r\n                            LOG.info(\"Failed to process fileInfo for job: \" + found.getJobId(), e);\r\n                        }\r\n                    }\r\n                });\r\n            }\r\n        } else if (!old.isMovePending()) {\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Duplicate: deleting\");\r\n            }\r\n            fileInfo.delete();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJobFileInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HistoryFileInfo getJobFileInfo(List<FileStatus> fileStatusList, JobId jobId) throws IOException\n{\r\n    for (FileStatus fs : fileStatusList) {\r\n        JobIndexInfo jobIndexInfo = FileNameIndexUtils.getIndexInfo(fs.getPath().getName());\r\n        if (jobIndexInfo.getJobId().equals(jobId)) {\r\n            String confFileName = JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\r\n            String summaryFileName = JobHistoryUtils.getIntermediateSummaryFileName(jobIndexInfo.getJobId());\r\n            HistoryFileInfo fileInfo = createHistoryFileInfo(fs.getPath(), new Path(fs.getPath().getParent(), confFileName), new Path(fs.getPath().getParent(), summaryFileName), jobIndexInfo, true);\r\n            return fileInfo;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "scanOldDirsForJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HistoryFileInfo scanOldDirsForJob(JobId jobId) throws IOException\n{\r\n    String boxedSerialNumber = JobHistoryUtils.serialNumberDirectoryComponent(jobId, serialNumberFormat);\r\n    Set<String> dateStringSet = serialNumberIndex.get(boxedSerialNumber);\r\n    if (dateStringSet == null) {\r\n        return null;\r\n    }\r\n    for (String timestampPart : dateStringSet) {\r\n        Path logDir = canonicalHistoryLogPath(jobId, timestampPart);\r\n        List<FileStatus> fileStatusList = scanDirectoryForHistoryFiles(logDir, doneDirFc);\r\n        HistoryFileInfo fileInfo = getJobFileInfo(fileStatusList, jobId);\r\n        if (fileInfo != null) {\r\n            return fileInfo;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllFileInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Collection<HistoryFileInfo> getAllFileInfo() throws IOException\n{\r\n    scanIntermediateDirectory();\r\n    return jobListCache.values();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFileInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HistoryFileInfo getFileInfo(JobId jobId) throws IOException\n{\r\n    HistoryFileInfo fileInfo = jobListCache.get(jobId);\r\n    if (fileInfo != null) {\r\n        return fileInfo;\r\n    }\r\n    scanIntermediateDirectory();\r\n    fileInfo = jobListCache.get(jobId);\r\n    if (fileInfo != null) {\r\n        return fileInfo;\r\n    }\r\n    fileInfo = scanOldDirsForJob(jobId);\r\n    if (fileInfo != null) {\r\n        return fileInfo;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "moveToDoneNow",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void moveToDoneNow(final Path src, final Path target) throws IOException\n{\r\n    LOG.info(\"Moving \" + src.toString() + \" to \" + target.toString());\r\n    try {\r\n        intermediateDoneDirFc.rename(src, target, Options.Rename.NONE);\r\n    } catch (FileNotFoundException e) {\r\n        if (doneDirFc.util().exists(target)) {\r\n            LOG.info(\"Source file \" + src.toString() + \" not found, but target \" + \"file \" + target.toString() + \" already exists. Move already \" + \"happened.\");\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJobSummary",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getJobSummary(FileContext fc, Path path) throws IOException\n{\r\n    Path qPath = fc.makeQualified(path);\r\n    FSDataInputStream in = null;\r\n    String jobSummaryString = null;\r\n    try {\r\n        in = fc.open(qPath);\r\n        jobSummaryString = in.readUTF();\r\n    } finally {\r\n        if (in != null) {\r\n            in.close();\r\n        }\r\n    }\r\n    return jobSummaryString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "makeDoneSubdir",
  "errType" : [ "FileNotFoundException", "FileAlreadyExistsException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void makeDoneSubdir(Path path) throws IOException\n{\r\n    try {\r\n        doneDirFc.getFileStatus(path);\r\n        existingDoneSubdirs.add(path);\r\n    } catch (FileNotFoundException fnfE) {\r\n        try {\r\n            FsPermission fsp = new FsPermission(JobHistoryUtils.HISTORY_DONE_DIR_PERMISSION);\r\n            doneDirFc.mkdir(path, fsp, true);\r\n            FileStatus fsStatus = doneDirFc.getFileStatus(path);\r\n            LOG.info(\"Perms after creating \" + fsStatus.getPermission().toShort() + \", Expected: \" + fsp.toShort());\r\n            if (fsStatus.getPermission().toShort() != fsp.toShort()) {\r\n                LOG.info(\"Explicitly setting permissions to : \" + fsp.toShort() + \", \" + fsp);\r\n                doneDirFc.setPermission(path, fsp);\r\n            }\r\n            existingDoneSubdirs.add(path);\r\n        } catch (FileAlreadyExistsException faeE) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "canonicalHistoryLogPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path canonicalHistoryLogPath(JobId id, String timestampComponent)\n{\r\n    return new Path(doneDirPrefixPath, JobHistoryUtils.historyLogSubdirectory(id, timestampComponent, serialNumberFormat));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "canonicalHistoryLogPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path canonicalHistoryLogPath(JobId id, long millisecondTime)\n{\r\n    String timestampComponent = JobHistoryUtils.timestampDirectoryComponent(millisecondTime);\r\n    return new Path(doneDirPrefixPath, JobHistoryUtils.historyLogSubdirectory(id, timestampComponent, serialNumberFormat));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getEffectiveTimestamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getEffectiveTimestamp(long finishTime, FileStatus fileStatus)\n{\r\n    if (finishTime == 0) {\r\n        return fileStatus.getModificationTime();\r\n    }\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "deleteJobFromDone",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteJobFromDone(HistoryFileInfo fileInfo) throws IOException\n{\r\n    jobListCache.delete(fileInfo);\r\n    fileInfo.delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getHistoryDirsForCleaning",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<FileStatus> getHistoryDirsForCleaning(long cutoff) throws IOException\n{\r\n    return JobHistoryUtils.getHistoryDirsForCleaning(doneDirFc, doneDirPrefixPath, cutoff);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "clean",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void clean() throws IOException\n{\r\n    long cutoff = System.currentTimeMillis() - maxHistoryAge;\r\n    boolean halted = false;\r\n    List<FileStatus> serialDirList = getHistoryDirsForCleaning(cutoff);\r\n    Collections.sort(serialDirList);\r\n    for (FileStatus serialDir : serialDirList) {\r\n        List<FileStatus> historyFileList = scanDirectoryForHistoryFiles(serialDir.getPath(), doneDirFc);\r\n        for (FileStatus historyFile : historyFileList) {\r\n            JobIndexInfo jobIndexInfo = FileNameIndexUtils.getIndexInfo(historyFile.getPath().getName());\r\n            long effectiveTimestamp = getEffectiveTimestamp(jobIndexInfo.getFinishTime(), historyFile);\r\n            if (effectiveTimestamp <= cutoff) {\r\n                HistoryFileInfo fileInfo = this.jobListCache.get(jobIndexInfo.getJobId());\r\n                if (fileInfo == null) {\r\n                    String confFileName = JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());\r\n                    fileInfo = createHistoryFileInfo(historyFile.getPath(), new Path(historyFile.getPath().getParent(), confFileName), null, jobIndexInfo, true);\r\n                }\r\n                deleteJobFromDone(fileInfo);\r\n            } else {\r\n                halted = true;\r\n                break;\r\n            }\r\n        }\r\n        if (!halted) {\r\n            deleteDir(serialDir);\r\n            removeDirectoryFromSerialNumberIndex(serialDir.getPath());\r\n            existingDoneSubdirs.remove(serialDir.getPath());\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "deleteDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean deleteDir(FileStatus serialDir) throws AccessControlException, FileNotFoundException, UnsupportedFileSystemException, IOException\n{\r\n    return doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setMaxHistoryAge",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaxHistoryAge(long newValue)\n{\r\n    maxHistoryAge = newValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getMaxTasksAllowed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxTasksAllowed()\n{\r\n    return maxTasksAllowed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobId getID()\n{\r\n    return jobIndexInfo.getJobId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getName()\n{\r\n    return jobIndexInfo.getJobName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobState getState()\n{\r\n    return JobState.valueOf(jobIndexInfo.getJobStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobReport getReport()\n{\r\n    if (jobReport == null) {\r\n        jobReport = constructJobReport();\r\n    }\r\n    return jobReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "constructJobReport",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "JobReport constructJobReport()\n{\r\n    JobReport report = Records.newRecord(JobReport.class);\r\n    report.setJobId(getID());\r\n    report.setJobState(getState());\r\n    report.setSubmitTime(jobIndexInfo.getSubmitTime());\r\n    report.setStartTime(jobIndexInfo.getJobStartTime());\r\n    report.setFinishTime(jobIndexInfo.getFinishTime());\r\n    report.setJobName(jobIndexInfo.getJobName());\r\n    report.setUser(jobIndexInfo.getUser());\r\n    report.setJobFile(getConfFile().toString());\r\n    report.setHistoryFile(jhfInfo.getHistoryFile().toString());\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAllCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getAllCounters()\n{\r\n    return new Counters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<TaskId, Task> getTasks()\n{\r\n    return new HashMap<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<TaskId, Task> getTasks(TaskType taskType)\n{\r\n    return new HashMap<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Task getTask(TaskId taskID)\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getDiagnostics()\n{\r\n    return new ArrayList<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTotalMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalMaps()\n{\r\n    return jobIndexInfo.getNumMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTotalReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalReduces()\n{\r\n    return jobIndexInfo.getNumReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCompletedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCompletedMaps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getCompletedReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCompletedReduces()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return 1.0f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "isUber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUber()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getUserName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUserName()\n{\r\n    return jobIndexInfo.getUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return jobIndexInfo.getQueueName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getConfFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getConfFile()\n{\r\n    return jhfInfo.getConfFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "loadConfFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration loadConfFile() throws IOException\n{\r\n    return jhfInfo.loadConfFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getJobACLs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<JobACL, AccessControlList> getJobACLs()\n{\r\n    return new HashMap<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getTaskAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptCompletionEvent[] getTaskAttemptCompletionEvents(int fromEventId, int maxEvents)\n{\r\n    return new TaskAttemptCompletionEvent[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getMapAttemptCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskCompletionEvent[] getMapAttemptCompletionEvents(int startIndex, int maxEvents)\n{\r\n    return TaskCompletionEvent.EMPTY_ARRAY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getAMInfos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<AMInfo> getAMInfos()\n{\r\n    return new ArrayList<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    throw new UnsupportedOperationException(\"Can't set job's \" + \"queue name in history\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "setJobPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobPriority(Priority priority)\n{\r\n    throw new UnsupportedOperationException(\"Can't set job's priority in history\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedMaps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedReduces()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledMaps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs",
  "methodName" : "getKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledReduces()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-hs\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2\\hs\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void render(Block html)\n{\r\n    TBODY<TABLE<Hamlet>> tbody = html.h2(\"Retired Jobs\").table(\"#jobs\").thead().tr().th(\"Submit Time\").th(\"Start Time\").th(\"Finish Time\").th(\".id\", \"Job ID\").th(\".name\", \"Name\").th(\"User\").th(\"Queue\").th(\".state\", \"State\").th(\"Maps Total\").th(\"Maps Completed\").th(\"Reduces Total\").th(\"Reduces Completed\").th(\"Elapsed Time\").__().__().tbody();\r\n    LOG.info(\"Getting list of all Jobs.\");\r\n    StringBuilder jobsTableData = new StringBuilder(\"[\\n\");\r\n    for (Job j : appContext.getAllJobs().values()) {\r\n        JobInfo job = new JobInfo(j);\r\n        ugi = getCallerUGI();\r\n        if (isFilterAppListByUserEnabled && ugi != null && !aclsManager.checkAccess(ugi, JobACL.VIEW_JOB, job.getUserName(), null)) {\r\n            continue;\r\n        }\r\n        jobsTableData.append(\"[\\\"\").append(dateFormat.format(new Date(job.getSubmitTime()))).append(\"\\\",\\\"\").append(job.getFormattedStartTimeStr(dateFormat)).append(\"\\\",\\\"\").append(dateFormat.format(new Date(job.getFinishTime()))).append(\"\\\",\\\"\").append(\"<a href='\").append(url(\"job\", job.getId())).append(\"'>\").append(job.getId()).append(\"</a>\\\",\\\"\").append(StringEscapeUtils.escapeEcmaScript(StringEscapeUtils.escapeHtml4(job.getName()))).append(\"\\\",\\\"\").append(StringEscapeUtils.escapeEcmaScript(StringEscapeUtils.escapeHtml4(job.getUserName()))).append(\"\\\",\\\"\").append(StringEscapeUtils.escapeEcmaScript(StringEscapeUtils.escapeHtml4(job.getQueueName()))).append(\"\\\",\\\"\").append(job.getState()).append(\"\\\",\\\"\").append(String.valueOf(job.getMapsTotal())).append(\"\\\",\\\"\").append(String.valueOf(job.getMapsCompleted())).append(\"\\\",\\\"\").append(String.valueOf(job.getReducesTotal())).append(\"\\\",\\\"\").append(String.valueOf(job.getReducesCompleted())).append(\"\\\",\\\"\").append(StringUtils.formatTimeSortable(Times.elapsed(job.getStartTime(), job.getFinishTime(), false))).append(\"\\\"],\\n\");\r\n    }\r\n    if (jobsTableData.charAt(jobsTableData.length() - 2) == ',') {\r\n        jobsTableData.delete(jobsTableData.length() - 2, jobsTableData.length() - 1);\r\n    }\r\n    jobsTableData.append(\"]\");\r\n    html.script().$type(\"text/javascript\").__(\"var jobsTableData=\" + jobsTableData).__();\r\n    tbody.__().tfoot().tr().th().input(\"search_init\").$type(InputType.text).$name(\"submit_time\").$value(\"Submit Time\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"start_time\").$value(\"Start Time\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"finish_time\").$value(\"Finish Time\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"job_id\").$value(\"Job ID\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"name\").$value(\"Name\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"user\").$value(\"User\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"queue\").$value(\"Queue\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"state\").$value(\"State\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"maps_total\").$value(\"Maps Total\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"maps_completed\").$value(\"Maps Completed\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"reduces_total\").$value(\"Reduces Total\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"reduces_completed\").$value(\"Reduces Completed\").__().__().th().input(\"search_init\").$type(InputType.text).$name(\"elapsed_time\").$value(\"Elapsed Time\").__().__().__().__().__();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    String capScheduler = CapacityScheduler.class.getCanonicalName();\r\n    String fairScheduler = FairScheduler.class.getCanonicalName();\r\n    String synthTraceFile = \"src/test/resources/syn_stream.json\";\r\n    String nodeFile = \"src/test/resources/nodes.json\";\r\n    return Arrays.asList(new Object[][] { { capScheduler, \"SYNTH\", synthTraceFile, null }, { capScheduler, \"SYNTH\", synthTraceFile, nodeFile }, { fairScheduler, \"SYNTH\", synthTraceFile, nodeFile } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n    ongoingInvariantFile = \"src/test/resources/ongoing-invariants.txt\";\r\n    exitInvariantFile = \"src/test/resources/exit-invariants.txt\";\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testSimulatorRunning",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimulatorRunning() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    long timeTillShutdownInsec = 20L;\r\n    runSLS(conf, timeTillShutdownInsec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    String capScheduler = CapacityScheduler.class.getCanonicalName();\r\n    String fairScheduler = FairScheduler.class.getCanonicalName();\r\n    String synthTraceFile = \"src/test/resources/sls_dag.json\";\r\n    String nodeFile = \"src/test/resources/nodes.json\";\r\n    return Arrays.asList(new Object[][] { { capScheduler, \"SLS\", synthTraceFile, null }, { capScheduler, \"SLS\", synthTraceFile, nodeFile }, { fairScheduler, \"SLS\", synthTraceFile, nodeFile } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n    ongoingInvariantFile = \"src/test/resources/ongoing-invariants.txt\";\r\n    exitInvariantFile = \"src/test/resources/exit-invariants.txt\";\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testSimulatorRunning",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimulatorRunning() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    long timeTillShutdownInsec = 20L;\r\n    runSLS(conf, timeTillShutdownInsec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testGetToBeScheduledContainers",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testGetToBeScheduledContainers() throws Exception\n{\r\n    DAGAMSimulator dagamSimulator = new DAGAMSimulator();\r\n    List<ContainerSimulator> containerSimulators = new ArrayList<>();\r\n    containerSimulators.add(createContainerSim(1, 0));\r\n    containerSimulators.add(createContainerSim(2, 1000));\r\n    containerSimulators.add(createContainerSim(3, 1500));\r\n    containerSimulators.add(createContainerSim(4, 4000));\r\n    long startTime = System.currentTimeMillis();\r\n    List<ContainerSimulator> res = dagamSimulator.getToBeScheduledContainers(containerSimulators, startTime);\r\n    assertEquals(1, res.size());\r\n    assertEquals(1, res.get(0).getAllocationId());\r\n    startTime -= 1000;\r\n    res = dagamSimulator.getToBeScheduledContainers(containerSimulators, startTime);\r\n    assertEquals(2, res.size());\r\n    assertEquals(1, res.get(0).getAllocationId());\r\n    assertEquals(2, res.get(1).getAllocationId());\r\n    startTime -= 2000;\r\n    res = dagamSimulator.getToBeScheduledContainers(containerSimulators, startTime);\r\n    assertEquals(3, res.size());\r\n    assertEquals(1, res.get(0).getAllocationId());\r\n    assertEquals(2, res.get(1).getAllocationId());\r\n    assertEquals(3, res.get(2).getAllocationId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "createContainerSim",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "ContainerSimulator createContainerSim(long allocationId, long requestDelay)\n{\r\n    TaskContainerDefinition taskContainerDef = mock(TaskContainerDefinition.class);\r\n    when(taskContainerDef.getResource()).thenReturn(null);\r\n    when(taskContainerDef.getDuration()).thenReturn(1000L);\r\n    when(taskContainerDef.getHostname()).thenReturn(\"*\");\r\n    when(taskContainerDef.getPriority()).thenReturn(1);\r\n    when(taskContainerDef.getType()).thenReturn(\"Map\");\r\n    when(taskContainerDef.getExecutionType()).thenReturn(null);\r\n    when(taskContainerDef.getAllocationId()).thenReturn(allocationId);\r\n    when(taskContainerDef.getRequestDelay()).thenReturn(requestDelay);\r\n    return ContainerSimulator.createFromTaskContainerDefinition(taskContainerDef);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testWorkloadGenerateTime",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testWorkloadGenerateTime() throws IllegalArgumentException, IOException\n{\r\n    String workloadJson = \"{\\\"job_classes\\\": [], \\\"time_distribution\\\":[\" + \"{\\\"time\\\": 0, \\\"weight\\\": 1}, \" + \"{\\\"time\\\": 30, \\\"weight\\\": 0},\" + \"{\\\"time\\\": 60, \\\"weight\\\": 2},\" + \"{\\\"time\\\": 90, \\\"weight\\\": 1}\" + \"]}\";\r\n    JsonFactoryBuilder jsonFactoryBuilder = new JsonFactoryBuilder();\r\n    jsonFactoryBuilder.configure(JsonFactory.Feature.INTERN_FIELD_NAMES, true);\r\n    ObjectMapper mapper = new ObjectMapper(jsonFactoryBuilder.build());\r\n    mapper.configure(FAIL_ON_UNKNOWN_PROPERTIES, false);\r\n    SynthTraceJobProducer.Workload wl = mapper.readValue(workloadJson, SynthTraceJobProducer.Workload.class);\r\n    JDKRandomGenerator rand = new JDKRandomGenerator();\r\n    rand.setSeed(0);\r\n    wl.init(rand);\r\n    int bucket0 = 0;\r\n    int bucket1 = 0;\r\n    int bucket2 = 0;\r\n    int bucket3 = 0;\r\n    for (int i = 0; i < 1000; ++i) {\r\n        long time = wl.generateSubmissionTime();\r\n        LOG.info(\"Generated time \" + time);\r\n        if (time < 30) {\r\n            bucket0++;\r\n        } else if (time < 60) {\r\n            bucket1++;\r\n        } else if (time < 90) {\r\n            bucket2++;\r\n        } else {\r\n            bucket3++;\r\n        }\r\n    }\r\n    Assert.assertTrue(bucket0 > 0);\r\n    assertEquals(0, bucket1);\r\n    Assert.assertTrue(bucket2 > 0);\r\n    Assert.assertTrue(bucket3 > 0);\r\n    Assert.assertTrue(bucket2 > bucket0);\r\n    Assert.assertTrue(bucket2 > bucket3);\r\n    LOG.info(\"bucket0 {}, bucket1 {}, bucket2 {}, bucket3 {}\", bucket0, bucket1, bucket2, bucket3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testMapReduce",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMapReduce() throws IllegalArgumentException, IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE, \"src/test/resources/syn.json\");\r\n    SynthTraceJobProducer stjp = new SynthTraceJobProducer(conf);\r\n    LOG.info(stjp.toString());\r\n    SynthJob js = (SynthJob) stjp.getNextJob();\r\n    int jobCount = 0;\r\n    while (js != null) {\r\n        LOG.info(js.toString());\r\n        validateJob(js);\r\n        js = (SynthJob) stjp.getNextJob();\r\n        jobCount++;\r\n    }\r\n    Assert.assertEquals(stjp.getNumJobs(), jobCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testGeneric",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGeneric() throws IllegalArgumentException, IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE, \"src/test/resources/syn_generic.json\");\r\n    SynthTraceJobProducer stjp = new SynthTraceJobProducer(conf);\r\n    LOG.info(stjp.toString());\r\n    SynthJob js = (SynthJob) stjp.getNextJob();\r\n    int jobCount = 0;\r\n    while (js != null) {\r\n        LOG.info(js.toString());\r\n        validateJob(js);\r\n        js = (SynthJob) stjp.getNextJob();\r\n        jobCount++;\r\n    }\r\n    Assert.assertEquals(stjp.getNumJobs(), jobCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testStream",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testStream() throws IllegalArgumentException, IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE, \"src/test/resources/syn_stream.json\");\r\n    SynthTraceJobProducer stjp = new SynthTraceJobProducer(conf);\r\n    LOG.info(stjp.toString());\r\n    SynthJob js = (SynthJob) stjp.getNextJob();\r\n    int jobCount = 0;\r\n    while (js != null) {\r\n        LOG.info(js.toString());\r\n        validateJob(js);\r\n        js = (SynthJob) stjp.getNextJob();\r\n        jobCount++;\r\n    }\r\n    Assert.assertEquals(stjp.getNumJobs(), jobCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testSample",
  "errType" : [ "JsonMappingException", "JsonMappingException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testSample() throws IOException\n{\r\n    JsonFactoryBuilder jsonFactoryBuilder = new JsonFactoryBuilder();\r\n    jsonFactoryBuilder.configure(JsonFactory.Feature.INTERN_FIELD_NAMES, true);\r\n    ObjectMapper mapper = new ObjectMapper(jsonFactoryBuilder.build());\r\n    mapper.configure(FAIL_ON_UNKNOWN_PROPERTIES, false);\r\n    JDKRandomGenerator rand = new JDKRandomGenerator();\r\n    rand.setSeed(0);\r\n    String valJson = \"{\\\"val\\\" : 5 }\";\r\n    SynthTraceJobProducer.Sample valSample = mapper.readValue(valJson, SynthTraceJobProducer.Sample.class);\r\n    valSample.init(rand);\r\n    int val = valSample.getInt();\r\n    Assert.assertEquals(5, val);\r\n    String distJson = \"{\\\"val\\\" : 5, \\\"std\\\" : 1 }\";\r\n    SynthTraceJobProducer.Sample distSample = mapper.readValue(distJson, SynthTraceJobProducer.Sample.class);\r\n    distSample.init(rand);\r\n    double dist = distSample.getDouble();\r\n    Assert.assertTrue(dist > 2 && dist < 8);\r\n    String normdistJson = \"{\\\"val\\\" : 5, \\\"std\\\" : 1, \\\"dist\\\": \\\"NORM\\\" }\";\r\n    SynthTraceJobProducer.Sample normdistSample = mapper.readValue(normdistJson, SynthTraceJobProducer.Sample.class);\r\n    normdistSample.init(rand);\r\n    double normdist = normdistSample.getDouble();\r\n    Assert.assertTrue(normdist > 2 && normdist < 8);\r\n    String discreteJson = \"{\\\"discrete\\\" : [2, 4, 6, 8]}\";\r\n    SynthTraceJobProducer.Sample discreteSample = mapper.readValue(discreteJson, SynthTraceJobProducer.Sample.class);\r\n    discreteSample.init(rand);\r\n    int discrete = discreteSample.getInt();\r\n    Assert.assertTrue(Arrays.asList(new Integer[] { 2, 4, 6, 8 }).contains(discrete));\r\n    String discreteWeightsJson = \"{\\\"discrete\\\" : [2, 4, 6, 8], \" + \"\\\"weights\\\": [0, 0, 0, 1]}\";\r\n    SynthTraceJobProducer.Sample discreteWeightsSample = mapper.readValue(discreteWeightsJson, SynthTraceJobProducer.Sample.class);\r\n    discreteWeightsSample.init(rand);\r\n    int discreteWeights = discreteWeightsSample.getInt();\r\n    Assert.assertEquals(8, discreteWeights);\r\n    String invalidJson = \"{\\\"val\\\" : 5, \\\"discrete\\\" : [2, 4, 6, 8], \" + \"\\\"weights\\\": [0, 0, 0, 1]}\";\r\n    try {\r\n        mapper.readValue(invalidJson, SynthTraceJobProducer.Sample.class);\r\n        Assert.fail();\r\n    } catch (JsonMappingException e) {\r\n        Assert.assertTrue(e.getMessage().startsWith(\"Instantiation of\"));\r\n    }\r\n    String invalidDistJson = \"{\\\"val\\\" : 5, \\\"std\\\" : 1, \" + \"\\\"dist\\\": \\\"INVALID\\\" }\";\r\n    try {\r\n        mapper.readValue(invalidDistJson, SynthTraceJobProducer.Sample.class);\r\n        Assert.fail();\r\n    } catch (JsonMappingException e) {\r\n        Assert.assertTrue(e.getMessage().startsWith(\"Cannot construct instance of\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "validateJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void validateJob(SynthJob js)\n{\r\n    assertTrue(js.getSubmissionTime() > 0);\r\n    assertTrue(js.getDuration() > 0);\r\n    assertTrue(js.getTotalSlotTime() >= 0);\r\n    if (js.hasDeadline()) {\r\n        assertTrue(js.getDeadline() > js.getSubmissionTime() + js.getDuration());\r\n    }\r\n    assertTrue(js.getTasks().size() > 0);\r\n    for (SynthJob.SynthTask t : js.getTasks()) {\r\n        assertNotNull(t.getType());\r\n        assertTrue(t.getTime() > 0);\r\n        assertTrue(t.getMemory() > 0);\r\n        assertTrue(t.getVcores() > 0);\r\n        assertEquals(ExecutionType.GUARANTEED, t.getExecutionType());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp()\n{\r\n    runner = new TaskRunner();\r\n    runner.setQueueSize(5);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanUp() throws InterruptedException\n{\r\n    runner.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "testSingleTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSingleTask() throws Exception\n{\r\n    runner.start();\r\n    runner.schedule(new SingleTask(0));\r\n    SingleTask.latch.await(5000, TimeUnit.MILLISECONDS);\r\n    Assert.assertTrue(SingleTask.first);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "testDualTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDualTask() throws Exception\n{\r\n    runner.start();\r\n    runner.schedule(new DualTask(0, 10, 10));\r\n    DualTask.latch.await(5000, TimeUnit.MILLISECONDS);\r\n    Assert.assertTrue(DualTask.first);\r\n    Assert.assertTrue(DualTask.last);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "testTriTask",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testTriTask() throws Exception\n{\r\n    runner.start();\r\n    runner.schedule(new TriTask(0, 10, 5));\r\n    TriTask.latch.await(5000, TimeUnit.MILLISECONDS);\r\n    Assert.assertTrue(TriTask.first);\r\n    Assert.assertTrue(TriTask.middle);\r\n    Assert.assertTrue(TriTask.last);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "testMultiTask",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultiTask() throws Exception\n{\r\n    runner.start();\r\n    runner.schedule(new MultiTask(0, 20, 5));\r\n    MultiTask.latch.await(5000, TimeUnit.MILLISECONDS);\r\n    Assert.assertTrue(MultiTask.first);\r\n    Assert.assertEquals((20 - 0) / 5 - 2 + 1, MultiTask.middle);\r\n    Assert.assertTrue(MultiTask.last);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\scheduler",
  "methodName" : "testPreStartQueueing",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPreStartQueueing() throws Exception\n{\r\n    runner.schedule(new PreStartTask(210));\r\n    Thread.sleep(210);\r\n    runner.start();\r\n    long startedAt = System.currentTimeMillis();\r\n    PreStartTask.latch.await(5000, TimeUnit.MILLISECONDS);\r\n    long runAt = System.currentTimeMillis();\r\n    Assert.assertTrue(PreStartTask.first);\r\n    Assert.assertTrue(runAt - startedAt >= 200);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { SLSFairScheduler.class, FairScheduler.class }, { SLSCapacityScheduler.class, CapacityScheduler.class } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup()\n{\r\n    createMetricOutputDir();\r\n    conf = new YarnConfiguration();\r\n    conf.set(SLSConfiguration.METRICS_OUTPUT_DIR, metricOutputDir.toString());\r\n    conf.set(YarnConfiguration.RM_SCHEDULER, slsScheduler.getName());\r\n    conf.set(SLSConfiguration.RM_SCHEDULER, scheduler.getName());\r\n    conf.set(YarnConfiguration.NODE_LABELS_ENABLED, \"true\");\r\n    conf.setBoolean(SLSConfiguration.METRICS_SWITCH, true);\r\n    rm = new ResourceManager();\r\n    rm.init(conf);\r\n    rm.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "verifySchedulerMetrics",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifySchedulerMetrics(String appId)\n{\r\n    if (scheduler.equals(FairScheduler.class)) {\r\n        SchedulerMetrics schedulerMetrics = ((SchedulerWrapper) rm.getResourceScheduler()).getSchedulerMetrics();\r\n        MetricRegistry metricRegistry = schedulerMetrics.getMetrics();\r\n        for (FairSchedulerMetrics.Metric metric : FairSchedulerMetrics.Metric.values()) {\r\n            String key = \"variable.app.\" + appId + \".\" + metric.getValue() + \".memory\";\r\n            Assert.assertTrue(metricRegistry.getGauges().containsKey(key));\r\n            Assert.assertNotNull(metricRegistry.getGauges().get(key).getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "createMetricOutputDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createMetricOutputDir()\n{\r\n    Path testDir = Paths.get(System.getProperty(\"test.build.data\", \"target/test-dir\"));\r\n    try {\r\n        metricOutputDir = Files.createTempDirectory(testDir, \"output\");\r\n    } catch (IOException e) {\r\n        Assert.fail(e.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "deleteMetricOutputDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteMetricOutputDir()\n{\r\n    try {\r\n        FileUtils.deleteDirectory(metricOutputDir.toFile());\r\n    } catch (IOException e) {\r\n        Assert.fail(e.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "testAMSimulator",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAMSimulator() throws Exception\n{\r\n    MockAMSimulator app = new MockAMSimulator();\r\n    String appId = \"app1\";\r\n    String queue = \"default\";\r\n    List<ContainerSimulator> containers = new ArrayList<>();\r\n    HashMap<ApplicationId, AMSimulator> map = new HashMap<>();\r\n    UserName mockUser = mock(UserName.class);\r\n    when(mockUser.getValue()).thenReturn(\"user1\");\r\n    AMDefinitionRumen amDef = AMDefinitionRumen.Builder.create().withUser(mockUser).withQueue(queue).withJobId(appId).withJobStartTime(0).withJobFinishTime(1000000L).withAmResource(SLSConfiguration.getAMContainerResource(conf)).withTaskContainers(containers).build();\r\n    app.init(amDef, rm, null, true, 0, 1000, map);\r\n    app.firstStep();\r\n    verifySchedulerMetrics(appId);\r\n    Assert.assertEquals(1, rm.getRMContext().getRMApps().size());\r\n    Assert.assertNotNull(rm.getRMContext().getRMApps().get(app.appId));\r\n    app.lastStep();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "testAMSimulatorWithNodeLabels",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testAMSimulatorWithNodeLabels() throws Exception\n{\r\n    if (scheduler.equals(CapacityScheduler.class)) {\r\n        RMAdminCLI rmAdminCLI = new RMAdminCLI(conf);\r\n        String[] args = { \"-addToClusterNodeLabels\", \"label1\" };\r\n        rmAdminCLI.run(args);\r\n        MockAMSimulator app = new MockAMSimulator();\r\n        String appId = \"app1\";\r\n        String queue = \"default\";\r\n        List<ContainerSimulator> containers = new ArrayList<>();\r\n        HashMap<ApplicationId, AMSimulator> map = new HashMap<>();\r\n        UserName mockUser = mock(UserName.class);\r\n        when(mockUser.getValue()).thenReturn(\"user1\");\r\n        AMDefinitionRumen amDef = AMDefinitionRumen.Builder.create().withUser(mockUser).withQueue(queue).withJobId(appId).withJobStartTime(0).withJobFinishTime(1000000L).withAmResource(SLSConfiguration.getAMContainerResource(conf)).withTaskContainers(containers).withLabelExpression(\"label1\").build();\r\n        app.init(amDef, rm, null, true, 0, 1000, map);\r\n        app.firstStep();\r\n        verifySchedulerMetrics(appId);\r\n        ConcurrentMap<ApplicationId, RMApp> rmApps = rm.getRMContext().getRMApps();\r\n        Assert.assertEquals(1, rmApps.size());\r\n        RMApp rmApp = rmApps.get(app.appId);\r\n        Assert.assertNotNull(rmApp);\r\n        Assert.assertEquals(\"label1\", rmApp.getAmNodeLabelExpression());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "testPackageRequests",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testPackageRequests() throws YarnException\n{\r\n    MockAMSimulator app = new MockAMSimulator();\r\n    List<ContainerSimulator> containerSimulators = new ArrayList<>();\r\n    Resource resource = Resources.createResource(1024);\r\n    int priority = 1;\r\n    ExecutionType execType = ExecutionType.GUARANTEED;\r\n    String type = \"map\";\r\n    TaskContainerDefinition.Builder builder = TaskContainerDefinition.Builder.create().withResource(resource).withDuration(100).withPriority(1).withType(type).withExecutionType(execType).withAllocationId(-1).withRequestDelay(0);\r\n    ContainerSimulator s1 = ContainerSimulator.createFromTaskContainerDefinition(builder.withHostname(\"/default-rack/h1\").build());\r\n    ContainerSimulator s2 = ContainerSimulator.createFromTaskContainerDefinition(builder.withHostname(\"/default-rack/h1\").build());\r\n    ContainerSimulator s3 = ContainerSimulator.createFromTaskContainerDefinition(builder.withHostname(\"/default-rack/h2\").build());\r\n    containerSimulators.add(s1);\r\n    containerSimulators.add(s2);\r\n    containerSimulators.add(s3);\r\n    List<ResourceRequest> res = app.packageRequests(containerSimulators, priority);\r\n    Assert.assertEquals(4, res.size());\r\n    int anyRequestCount = 0;\r\n    int rackRequestCount = 0;\r\n    int nodeRequestCount = 0;\r\n    for (ResourceRequest request : res) {\r\n        String resourceName = request.getResourceName();\r\n        if (resourceName.equals(\"*\")) {\r\n            anyRequestCount++;\r\n        } else if (resourceName.equals(\"/default-rack\")) {\r\n            rackRequestCount++;\r\n        } else {\r\n            nodeRequestCount++;\r\n        }\r\n    }\r\n    Assert.assertEquals(1, anyRequestCount);\r\n    Assert.assertEquals(1, rackRequestCount);\r\n    Assert.assertEquals(2, nodeRequestCount);\r\n    containerSimulators.clear();\r\n    s1 = ContainerSimulator.createFromTaskContainerDefinition(createDefaultTaskContainerDefMock(resource, priority, execType, type, \"/default-rack/h1\", 1));\r\n    s2 = ContainerSimulator.createFromTaskContainerDefinition(createDefaultTaskContainerDefMock(resource, priority, execType, type, \"/default-rack/h1\", 2));\r\n    s3 = ContainerSimulator.createFromTaskContainerDefinition(createDefaultTaskContainerDefMock(resource, priority, execType, type, \"/default-rack/h2\", 1));\r\n    containerSimulators.add(s1);\r\n    containerSimulators.add(s2);\r\n    containerSimulators.add(s3);\r\n    res = app.packageRequests(containerSimulators, priority);\r\n    Assert.assertEquals(7, res.size());\r\n    anyRequestCount = 0;\r\n    rackRequestCount = 0;\r\n    nodeRequestCount = 0;\r\n    for (ResourceRequest request : res) {\r\n        String resourceName = request.getResourceName();\r\n        long allocationId = request.getAllocationRequestId();\r\n        Assert.assertTrue(allocationId == 1 || allocationId == 2);\r\n        if (resourceName.equals(\"*\")) {\r\n            anyRequestCount++;\r\n        } else if (resourceName.equals(\"/default-rack\")) {\r\n            rackRequestCount++;\r\n        } else {\r\n            nodeRequestCount++;\r\n        }\r\n    }\r\n    Assert.assertEquals(2, anyRequestCount);\r\n    Assert.assertEquals(2, rackRequestCount);\r\n    Assert.assertEquals(3, nodeRequestCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "testAMSimulatorRanNodesCleared",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testAMSimulatorRanNodesCleared() throws Exception\n{\r\n    NMSimulator nm = new NMSimulator();\r\n    nm.init(\"/rack1/testNode1\", Resources.createResource(1024 * 10, 10), 0, 1000, rm, -1f);\r\n    Map<NodeId, NMSimulator> nmMap = new HashMap<>();\r\n    nmMap.put(nm.getNode().getNodeID(), nm);\r\n    MockAMSimulator app = new MockAMSimulator();\r\n    app.appId = ApplicationId.newInstance(0l, 1);\r\n    SLSRunner slsRunner = Mockito.mock(SLSRunner.class);\r\n    app.se = slsRunner;\r\n    when(slsRunner.getNmMap()).thenReturn(nmMap);\r\n    app.getRanNodes().add(nm.getNode().getNodeID());\r\n    nm.getNode().getRunningApps().add(app.appId);\r\n    Assert.assertTrue(nm.getNode().getRunningApps().contains(app.appId));\r\n    app.lastStep();\r\n    Assert.assertFalse(nm.getNode().getRunningApps().contains(app.appId));\r\n    Assert.assertTrue(nm.getNode().getRunningApps().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "createDefaultTaskContainerDefMock",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskContainerDefinition createDefaultTaskContainerDefMock(Resource resource, int priority, ExecutionType execType, String type, String hostname, long allocationId)\n{\r\n    TaskContainerDefinition taskContainerDef = mock(TaskContainerDefinition.class);\r\n    when(taskContainerDef.getResource()).thenReturn(resource);\r\n    when(taskContainerDef.getDuration()).thenReturn(100L);\r\n    when(taskContainerDef.getPriority()).thenReturn(priority);\r\n    when(taskContainerDef.getType()).thenReturn(type);\r\n    when(taskContainerDef.getExecutionType()).thenReturn(execType);\r\n    when(taskContainerDef.getHostname()).thenReturn(hostname);\r\n    when(taskContainerDef.getAllocationId()).thenReturn(allocationId);\r\n    return taskContainerDef;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\appmaster",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (rm != null) {\r\n        rm.stop();\r\n    }\r\n    deleteMetricOutputDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "testSimulateInfoPageHtmlTemplate",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testSimulateInfoPageHtmlTemplate() throws Exception\n{\r\n    String simulateInfoTemplate = FileUtils.readFileToString(new File(\"src/main/html/simulate.info.html.template\"), StandardCharsets.UTF_8);\r\n    Map<String, Object> simulateInfoMap = new HashMap<>();\r\n    simulateInfoMap.put(\"Number of racks\", 10);\r\n    simulateInfoMap.put(\"Number of nodes\", 100);\r\n    simulateInfoMap.put(\"Node memory (MB)\", 1024);\r\n    simulateInfoMap.put(\"Node VCores\", 1);\r\n    simulateInfoMap.put(\"Number of applications\", 100);\r\n    simulateInfoMap.put(\"Number of tasks\", 1000);\r\n    simulateInfoMap.put(\"Average tasks per applicaion\", 10);\r\n    simulateInfoMap.put(\"Number of queues\", 4);\r\n    simulateInfoMap.put(\"Average applications per queue\", 25);\r\n    simulateInfoMap.put(\"Estimated simulate time (s)\", 10000);\r\n    StringBuilder info = new StringBuilder();\r\n    for (Map.Entry<String, Object> entry : simulateInfoMap.entrySet()) {\r\n        info.append(\"<tr>\");\r\n        info.append(\"<td class='td1'>\" + entry.getKey() + \"</td>\");\r\n        info.append(\"<td class='td2'>\" + entry.getValue() + \"</td>\");\r\n        info.append(\"</tr>\");\r\n    }\r\n    String simulateInfo = MessageFormat.format(simulateInfoTemplate, info.toString());\r\n    Assert.assertTrue(\"The simulate info html page should not be empty\", simulateInfo.length() > 0);\r\n    for (Map.Entry<String, Object> entry : simulateInfoMap.entrySet()) {\r\n        Assert.assertTrue(\"The simulate info html page should have information \" + \"of \" + entry.getKey(), simulateInfo.contains(\"<td class='td1'>\" + entry.getKey() + \"</td><td class='td2'>\" + entry.getValue() + \"</td>\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "testSimulatePageHtmlTemplate",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSimulatePageHtmlTemplate() throws Exception\n{\r\n    String simulateTemplate = FileUtils.readFileToString(new File(\"src/main/html/simulate.html.template\"), StandardCharsets.UTF_8);\r\n    Set<String> queues = new HashSet<String>();\r\n    queues.add(\"sls_queue_1\");\r\n    queues.add(\"sls_queue_2\");\r\n    queues.add(\"sls_queue_3\");\r\n    String queueInfo = \"\";\r\n    int i = 0;\r\n    for (String queue : queues) {\r\n        queueInfo += \"legends[4][\" + i + \"] = 'queue\" + queue + \".allocated.memory'\";\r\n        queueInfo += \"legends[5][\" + i + \"] = 'queue\" + queue + \".allocated.vcores'\";\r\n        i++;\r\n    }\r\n    String simulateInfo = MessageFormat.format(simulateTemplate, queueInfo, \"s\", 1000, 1000);\r\n    Assert.assertTrue(\"The simulate page html page should not be empty\", simulateInfo.length() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\web",
  "methodName" : "testTrackPageHtmlTemplate",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTrackPageHtmlTemplate() throws Exception\n{\r\n    String trackTemplate = FileUtils.readFileToString(new File(\"src/main/html/track.html.template\"), StandardCharsets.UTF_8);\r\n    String trackedQueueInfo = \"\";\r\n    Set<String> trackedQueues = new HashSet<String>();\r\n    trackedQueues.add(\"sls_queue_1\");\r\n    trackedQueues.add(\"sls_queue_2\");\r\n    trackedQueues.add(\"sls_queue_3\");\r\n    for (String queue : trackedQueues) {\r\n        trackedQueueInfo += \"<option value='Queue \" + queue + \"'>\" + queue + \"</option>\";\r\n    }\r\n    String trackedAppInfo = \"\";\r\n    Set<String> trackedApps = new HashSet<String>();\r\n    trackedApps.add(\"app_1\");\r\n    trackedApps.add(\"app_2\");\r\n    for (String job : trackedApps) {\r\n        trackedAppInfo += \"<option value='Job \" + job + \"'>\" + job + \"</option>\";\r\n    }\r\n    String trackInfo = MessageFormat.format(trackTemplate, trackedQueueInfo, trackedAppInfo, \"s\", 1000, 1000);\r\n    Assert.assertTrue(\"The queue/app tracking html page should not be empty\", trackInfo.length() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws InterruptedException\n{\r\n    if (sls != null) {\r\n        sls.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "runSLS",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void runSLS(Configuration conf, long timeout) throws Exception\n{\r\n    File tempDir = new File(\"target\", UUID.randomUUID().toString());\r\n    final List<Throwable> exceptionList = Collections.synchronizedList(new ArrayList<Throwable>());\r\n    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {\r\n\r\n        @Override\r\n        public void uncaughtException(Thread t, Throwable e) {\r\n            e.printStackTrace();\r\n            exceptionList.add(e);\r\n        }\r\n    });\r\n    File slsOutputDir = new File(tempDir.getAbsolutePath() + \"/slsoutput/\");\r\n    String[] args;\r\n    switch(traceType) {\r\n        case \"OLD_SLS\":\r\n            args = new String[] { \"-inputsls\", traceLocation, \"-output\", slsOutputDir.getAbsolutePath() };\r\n            break;\r\n        case \"OLD_RUMEN\":\r\n            args = new String[] { \"-inputrumen\", traceLocation, \"-output\", slsOutputDir.getAbsolutePath() };\r\n            break;\r\n        default:\r\n            args = new String[] { \"-tracetype\", traceType, \"-tracelocation\", traceLocation, \"-output\", slsOutputDir.getAbsolutePath() };\r\n    }\r\n    if (nodeFile != null) {\r\n        args = ArrayUtils.addAll(args, \"-nodes\", nodeFile);\r\n    }\r\n    conf.set(YarnConfiguration.RM_SCHEDULER, schedulerType);\r\n    if (ongoingInvariantFile != null) {\r\n        conf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES, MetricsInvariantChecker.class.getCanonicalName());\r\n        conf.set(MetricsInvariantChecker.INVARIANTS_FILE, ongoingInvariantFile);\r\n        conf.setBoolean(MetricsInvariantChecker.THROW_ON_VIOLATION, true);\r\n    }\r\n    sls = new SLSRunner(conf);\r\n    sls.run(args);\r\n    while (timeout >= 0) {\r\n        Thread.sleep(1000);\r\n        if (!exceptionList.isEmpty()) {\r\n            sls.stop();\r\n            Assert.fail(\"TestSLSRunner caught exception from child thread \" + \"(TaskRunner.TaskDefinition): \" + exceptionList);\r\n            break;\r\n        }\r\n        timeout--;\r\n    }\r\n    shutdownHookInvariantCheck();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "shutdownHookInvariantCheck",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void shutdownHookInvariantCheck()\n{\r\n    if (exitInvariantFile != null) {\r\n        MetricsInvariantChecker ic = new MetricsInvariantChecker();\r\n        Configuration conf = new Configuration();\r\n        conf.set(MetricsInvariantChecker.INVARIANTS_FILE, exitInvariantFile);\r\n        conf.setBoolean(MetricsInvariantChecker.THROW_ON_VIOLATION, true);\r\n        ic.init(conf, null, null);\r\n        ic.editSchedule();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    String capScheduler = CapacityScheduler.class.getCanonicalName();\r\n    String fairScheduler = FairScheduler.class.getCanonicalName();\r\n    String synthTraceFile = \"src/test/resources/syn_generic.json\";\r\n    String nodeFile = \"src/test/resources/nodes.json\";\r\n    return Arrays.asList(new Object[][] { { capScheduler, \"SYNTH\", synthTraceFile, null }, { capScheduler, \"SYNTH\", synthTraceFile, nodeFile }, { fairScheduler, \"SYNTH\", synthTraceFile, nodeFile } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n    ongoingInvariantFile = \"src/test/resources/ongoing-invariants.txt\";\r\n    exitInvariantFile = \"src/test/resources/exit-invariants.txt\";\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testSimulatorRunning",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimulatorRunning() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    long timeTillShutdownInsec = 20L;\r\n    runSLS(conf, timeTillShutdownInsec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "testGetRackHostname",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetRackHostname()\n{\r\n    String str = \"/rack1/node1\";\r\n    String[] rackHostname = SLSUtils.getRackHostName(str);\r\n    Assert.assertEquals(\"rack1\", rackHostname[0]);\r\n    Assert.assertEquals(\"node1\", rackHostname[1]);\r\n    str = \"/rackA/rackB/node1\";\r\n    rackHostname = SLSUtils.getRackHostName(str);\r\n    Assert.assertEquals(\"rackA/rackB\", rackHostname[0]);\r\n    Assert.assertEquals(\"node1\", rackHostname[1]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "testParseNodesFromNodeFile",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testParseNodesFromNodeFile() throws Exception\n{\r\n    String nodeFile = \"src/test/resources/nodes.json\";\r\n    Set<NodeDetails> nodeDetails = SLSUtils.parseNodesFromNodeFile(nodeFile, Resources.createResource(1024, 2));\r\n    Assert.assertEquals(20, nodeDetails.size());\r\n    nodeFile = \"src/test/resources/nodes-with-resources.json\";\r\n    nodeDetails = SLSUtils.parseNodesFromNodeFile(nodeFile, Resources.createResource(1024, 2));\r\n    Assert.assertEquals(4, nodeDetails.size());\r\n    for (NodeDetails nodeDetail : nodeDetails) {\r\n        if (nodeDetail.getHostname().equals(\"/rack1/node1\")) {\r\n            Assert.assertEquals(2048, nodeDetail.getNodeResource().getMemorySize());\r\n            Assert.assertEquals(6, nodeDetail.getNodeResource().getVirtualCores());\r\n        } else if (nodeDetail.getHostname().equals(\"/rack1/node2\")) {\r\n            Assert.assertEquals(1024, nodeDetail.getNodeResource().getMemorySize());\r\n            Assert.assertEquals(2, nodeDetail.getNodeResource().getVirtualCores());\r\n            Assert.assertNull(nodeDetail.getLabels());\r\n        } else if (nodeDetail.getHostname().equals(\"/rack1/node3\")) {\r\n            Assert.assertEquals(1024, nodeDetail.getNodeResource().getMemorySize());\r\n            Assert.assertEquals(2, nodeDetail.getNodeResource().getVirtualCores());\r\n            Assert.assertEquals(2, nodeDetail.getLabels().size());\r\n            for (NodeLabel nodeLabel : nodeDetail.getLabels()) {\r\n                if (nodeLabel.getName().equals(\"label1\")) {\r\n                    Assert.assertTrue(nodeLabel.isExclusive());\r\n                } else if (nodeLabel.getName().equals(\"label2\")) {\r\n                    Assert.assertFalse(nodeLabel.isExclusive());\r\n                } else {\r\n                    Assert.fail(\"Unexpected label\");\r\n                }\r\n            }\r\n        } else if (nodeDetail.getHostname().equals(\"/rack1/node4\")) {\r\n            Assert.assertEquals(6144, nodeDetail.getNodeResource().getMemorySize());\r\n            Assert.assertEquals(12, nodeDetail.getNodeResource().getVirtualCores());\r\n            Assert.assertEquals(2, nodeDetail.getLabels().size());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "testGenerateNodes",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testGenerateNodes()\n{\r\n    Set<NodeDetails> nodes = SLSUtils.generateNodes(3, 3);\r\n    Assert.assertEquals(\"Number of nodes is wrong.\", 3, nodes.size());\r\n    Assert.assertEquals(\"Number of racks is wrong.\", 3, getNumRack(nodes));\r\n    nodes = SLSUtils.generateNodes(3, 1);\r\n    Assert.assertEquals(\"Number of nodes is wrong.\", 3, nodes.size());\r\n    Assert.assertEquals(\"Number of racks is wrong.\", 1, getNumRack(nodes));\r\n    nodes = SLSUtils.generateNodes(3, 4);\r\n    Assert.assertEquals(\"Number of nodes is wrong.\", 3, nodes.size());\r\n    Assert.assertEquals(\"Number of racks is wrong.\", 3, getNumRack(nodes));\r\n    nodes = SLSUtils.generateNodes(3, 0);\r\n    Assert.assertEquals(\"Number of nodes is wrong.\", 3, nodes.size());\r\n    Assert.assertEquals(\"Number of racks is wrong.\", 1, getNumRack(nodes));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "testGenerateNodeTableMapping",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGenerateNodeTableMapping() throws Exception\n{\r\n    Set<NodeDetails> nodes = SLSUtils.generateNodes(3, 3);\r\n    File tempFile = File.createTempFile(\"testslsutils\", \".tmp\");\r\n    tempFile.deleteOnExit();\r\n    String fileName = tempFile.getAbsolutePath();\r\n    SLSUtils.generateNodeTableMapping(nodes, fileName);\r\n    List<String> lines = Files.readAllLines(Paths.get(fileName));\r\n    Assert.assertEquals(3, lines.size());\r\n    for (String line : lines) {\r\n        Assert.assertTrue(line.contains(\"node\"));\r\n        Assert.assertTrue(line.contains(\"/rack\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\utils",
  "methodName" : "getNumRack",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getNumRack(Set<NodeDetails> nodes)\n{\r\n    Set<String> racks = new HashSet<>();\r\n    for (NodeDetails node : nodes) {\r\n        String[] rackHostname = SLSUtils.getRackHostName(node.getHostname());\r\n        racks.add(rackHostname[0]);\r\n    }\r\n    return racks.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "params",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> params()\n{\r\n    return Arrays.asList(new Object[][] { { SLSFairScheduler.class, FairScheduler.class }, { SLSCapacityScheduler.class, CapacityScheduler.class } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup()\n{\r\n    conf = new YarnConfiguration();\r\n    conf.set(YarnConfiguration.RM_SCHEDULER, slsScheduler.getName());\r\n    conf.set(SLSConfiguration.RM_SCHEDULER, scheduler.getName());\r\n    conf.setBoolean(SLSConfiguration.METRICS_SWITCH, false);\r\n    rm = new ResourceManager();\r\n    rm.init(conf);\r\n    rm.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "testNMSimulator",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testNMSimulator() throws Exception\n{\r\n    NMSimulator node1 = new NMSimulator();\r\n    node1.init(\"/rack1/node1\", Resources.createResource(GB * 10, 10), 0, 1000, rm, -1f);\r\n    node1.middleStep();\r\n    int numClusterNodes = rm.getResourceScheduler().getNumClusterNodes();\r\n    int cumulativeSleepTime = 0;\r\n    int sleepInterval = 100;\r\n    while (numClusterNodes != 1 && cumulativeSleepTime < 5000) {\r\n        Thread.sleep(sleepInterval);\r\n        cumulativeSleepTime = cumulativeSleepTime + sleepInterval;\r\n        numClusterNodes = rm.getResourceScheduler().getNumClusterNodes();\r\n    }\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            return rm.getResourceScheduler().getRootQueueMetrics().getAvailableMB() > 0;\r\n        }\r\n    }, 500, 10000);\r\n    Assert.assertEquals(1, rm.getResourceScheduler().getNumClusterNodes());\r\n    Assert.assertEquals(GB * 10, rm.getResourceScheduler().getRootQueueMetrics().getAvailableMB());\r\n    Assert.assertEquals(10, rm.getResourceScheduler().getRootQueueMetrics().getAvailableVirtualCores());\r\n    ContainerId cId1 = newContainerId(1, 1, 1);\r\n    Container container1 = Container.newInstance(cId1, null, null, Resources.createResource(GB, 1), null, null);\r\n    node1.addNewContainer(container1, 100000l, null);\r\n    Assert.assertTrue(\"Node1 should have one running container.\", node1.getRunningContainers().containsKey(cId1));\r\n    ContainerId cId2 = newContainerId(2, 1, 1);\r\n    Container container2 = Container.newInstance(cId2, null, null, Resources.createResource(GB, 1), null, null);\r\n    node1.addNewContainer(container2, -1l, null);\r\n    Assert.assertTrue(\"Node1 should have one running AM container\", node1.getAMContainers().contains(cId2));\r\n    node1.cleanupContainer(cId1);\r\n    Assert.assertTrue(\"Container1 should be removed from Node1.\", node1.getCompletedContainers().contains(cId1));\r\n    node1.cleanupContainer(cId2);\r\n    Assert.assertFalse(\"Container2 should be removed from Node1.\", node1.getAMContainers().contains(cId2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "newContainerId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerId newContainerId(int appId, int appAttemptId, int cId)\n{\r\n    return BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(System.currentTimeMillis(), appId), appAttemptId), cId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "testNMSimAppAddedAndRemoved",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testNMSimAppAddedAndRemoved() throws Exception\n{\r\n    NMSimulator node = new NMSimulator();\r\n    node.init(\"/rack1/node1\", Resources.createResource(GB * 10, 10), 0, 1000, rm, -1f);\r\n    node.middleStep();\r\n    int numClusterNodes = rm.getResourceScheduler().getNumClusterNodes();\r\n    int cumulativeSleepTime = 0;\r\n    int sleepInterval = 100;\r\n    while (numClusterNodes != 1 && cumulativeSleepTime < 5000) {\r\n        Thread.sleep(sleepInterval);\r\n        cumulativeSleepTime = cumulativeSleepTime + sleepInterval;\r\n        numClusterNodes = rm.getResourceScheduler().getNumClusterNodes();\r\n    }\r\n    GenericTestUtils.waitFor(() -> rm.getResourceScheduler().getRootQueueMetrics().getAvailableMB() > 0, 500, 10000);\r\n    Assert.assertEquals(\"Node should have no runningApps.\", node.getNode().getRunningApps().size(), 0);\r\n    ApplicationId appId = BuilderUtils.newApplicationId(1, 1);\r\n    ApplicationAttemptId appAttemptId = BuilderUtils.newApplicationAttemptId(appId, 1);\r\n    ContainerId cId = BuilderUtils.newContainerId(appAttemptId, 1);\r\n    Container container = Container.newInstance(cId, null, null, Resources.createResource(GB, 1), null, null);\r\n    node.addNewContainer(container, 100000l, appId);\r\n    Assert.assertTrue(\"Node should have app: \" + appId + \" in runningApps list.\", node.getNode().getRunningApps().contains(appId));\r\n    node.finishApplication(appId);\r\n    Assert.assertFalse(\"Node should not have app: \" + appId + \" in runningApps list.\", node.getNode().getRunningApps().contains(appId));\r\n    Assert.assertEquals(\"Node should have no runningApps.\", node.getNode().getRunningApps().size(), 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "testNMSimNullAppAddedAndRemoved",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testNMSimNullAppAddedAndRemoved() throws Exception\n{\r\n    NMSimulator node = new NMSimulator();\r\n    node.init(\"/rack1/node1\", Resources.createResource(GB * 10, 10), 0, 1000, rm, -1f);\r\n    node.middleStep();\r\n    int numClusterNodes = rm.getResourceScheduler().getNumClusterNodes();\r\n    int cumulativeSleepTime = 0;\r\n    int sleepInterval = 100;\r\n    while (numClusterNodes != 1 && cumulativeSleepTime < 5000) {\r\n        Thread.sleep(sleepInterval);\r\n        cumulativeSleepTime = cumulativeSleepTime + sleepInterval;\r\n        numClusterNodes = rm.getResourceScheduler().getNumClusterNodes();\r\n    }\r\n    GenericTestUtils.waitFor(() -> rm.getResourceScheduler().getRootQueueMetrics().getAvailableMB() > 0, 500, 10000);\r\n    Assert.assertEquals(\"Node should have no runningApps.\", node.getNode().getRunningApps().size(), 0);\r\n    ContainerId cId = newContainerId(1, 1, 1);\r\n    Container container = Container.newInstance(cId, null, null, Resources.createResource(GB, 1), null, null);\r\n    node.addNewContainer(container, 100000l, null);\r\n    Assert.assertEquals(\"Node should have no runningApps if appId is null.\", node.getNode().getRunningApps().size(), 0);\r\n    ApplicationId appId = BuilderUtils.newApplicationId(1, 1);\r\n    node.finishApplication(appId);\r\n    Assert.assertEquals(\"Node should have no runningApps.\", node.getNode().getRunningApps().size(), 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls\\nodemanager",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    rm.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    return Arrays.asList(new Object[][] { { CapacityScheduler.class.getCanonicalName(), \"SYNTH\", \"src/test/resources/syn.json\", null }, { FairScheduler.class.getCanonicalName(), \"SYNTH\", \"src/test/resources/syn.json\", null } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testSimulatorRunning",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSimulatorRunning() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(YarnConfiguration.RM_SCHEDULER, schedulerType);\r\n    conf.setBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, true);\r\n    conf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES, ReservationInvariantsChecker.class.getCanonicalName());\r\n    conf.setBoolean(InvariantsChecker.THROW_ON_VIOLATION, true);\r\n    long timeTillShutDownInSec = 90;\r\n    runSLS(conf, timeTillShutDownInSec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    String capScheduler = CapacityScheduler.class.getCanonicalName();\r\n    String fairScheduler = FairScheduler.class.getCanonicalName();\r\n    String slsTraceFile = \"src/test/resources/inputsls.json\";\r\n    String rumenTraceFile = \"src/main/data/2jobs2min-rumen-jh.json\";\r\n    String synthTraceFile = \"src/test/resources/syn.json\";\r\n    String nodeFile = \"src/test/resources/nodes.json\";\r\n    return Arrays.asList(new Object[][] { { capScheduler, \"OLD_RUMEN\", rumenTraceFile, nodeFile }, { capScheduler, \"OLD_SLS\", slsTraceFile, nodeFile }, { capScheduler, \"SYNTH\", synthTraceFile, null }, { capScheduler, \"RUMEN\", rumenTraceFile, null }, { capScheduler, \"SLS\", slsTraceFile, null }, { capScheduler, \"SYNTH\", synthTraceFile, nodeFile }, { capScheduler, \"RUMEN\", rumenTraceFile, nodeFile }, { capScheduler, \"SLS\", slsTraceFile, nodeFile }, { fairScheduler, \"SYNTH\", synthTraceFile, nodeFile }, { fairScheduler, \"RUMEN\", rumenTraceFile, nodeFile }, { fairScheduler, \"SLS\", slsTraceFile, nodeFile } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n    ongoingInvariantFile = \"src/test/resources/ongoing-invariants.txt\";\r\n    exitInvariantFile = \"src/test/resources/exit-invariants.txt\";\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testSimulatorRunning",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimulatorRunning() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    long timeTillShutdownInsec = 20L;\r\n    runSLS(conf, timeTillShutdownInsec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-sls\\src\\test\\java\\org\\apache\\hadoop\\yarn\\sls",
  "methodName" : "testEnableCaching",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testEnableCaching()\n{\r\n    String networkCacheDefault = Security.getProperty(SLSRunner.NETWORK_CACHE_TTL);\r\n    String networkNegativeCacheDefault = Security.getProperty(SLSRunner.NETWORK_NEGATIVE_CACHE_TTL);\r\n    try {\r\n        Configuration conf = new Configuration(false);\r\n        conf.setBoolean(SLSConfiguration.DNS_CACHING_ENABLED, false);\r\n        SLSRunner.enableDNSCaching(conf);\r\n        assertEquals(networkCacheDefault, Security.getProperty(SLSRunner.NETWORK_CACHE_TTL));\r\n        assertEquals(networkNegativeCacheDefault, Security.getProperty(SLSRunner.NETWORK_NEGATIVE_CACHE_TTL));\r\n        conf.setBoolean(SLSConfiguration.DNS_CACHING_ENABLED, true);\r\n        SLSRunner.enableDNSCaching(conf);\r\n        assertEquals(\"-1\", Security.getProperty(SLSRunner.NETWORK_CACHE_TTL));\r\n        assertEquals(\"-1\", Security.getProperty(SLSRunner.NETWORK_NEGATIVE_CACHE_TTL));\r\n    } finally {\r\n        Security.setProperty(SLSRunner.NETWORK_CACHE_TTL, String.valueOf(networkCacheDefault));\r\n        Security.setProperty(SLSRunner.NETWORK_NEGATIVE_CACHE_TTL, String.valueOf(networkNegativeCacheDefault));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
} ]
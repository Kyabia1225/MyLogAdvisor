[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    NfsConfiguration config = new NfsConfiguration();\r\n    Mountd mountd = new Mountd(config, null, true);\r\n    mountd.start(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void start() throws IOException\n{\r\n    final InetSocketAddress httpAddr = getHttpAddress(conf);\r\n    final String httpsAddrString = conf.get(NfsConfigKeys.NFS_HTTPS_ADDRESS_KEY, NfsConfigKeys.NFS_HTTPS_ADDRESS_DEFAULT);\r\n    InetSocketAddress httpsAddr = NetUtils.createSocketAddr(httpsAddrString);\r\n    HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf, httpAddr, httpsAddr, \"nfs3\", NfsConfigKeys.DFS_NFS_KERBEROS_PRINCIPAL_KEY, NfsConfigKeys.DFS_NFS_KEYTAB_FILE_KEY);\r\n    this.httpServer = builder.build();\r\n    this.httpServer.start();\r\n    HttpConfig.Policy policy = DFSUtil.getHttpPolicy(conf);\r\n    int connIdx = 0;\r\n    if (policy.isHttpEnabled()) {\r\n        infoPort = httpServer.getConnectorAddress(connIdx++).getPort();\r\n    }\r\n    if (policy.isHttpsEnabled()) {\r\n        infoSecurePort = httpServer.getConnectorAddress(connIdx).getPort();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "stop",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop() throws IOException\n{\r\n    if (httpServer != null) {\r\n        try {\r\n            httpServer.stop();\r\n        } catch (Exception e) {\r\n            throw new IOException(e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPort()\n{\r\n    return this.infoPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getSecurePort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSecurePort()\n{\r\n    return this.infoSecurePort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getServerURI",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "URI getServerURI()\n{\r\n    InetSocketAddress addr = httpServer.getConnectorAddress(0);\r\n    return URI.create(DFSUtil.getHttpClientScheme(conf) + \"://\" + NetUtils.getHostPortString(addr));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getHttpAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InetSocketAddress getHttpAddress(Configuration conf)\n{\r\n    String addr = conf.get(NfsConfigKeys.NFS_HTTP_ADDRESS_KEY, NfsConfigKeys.NFS_HTTP_ADDRESS_DEFAULT);\r\n    return NetUtils.createSocketAddr(addr, NfsConfigKeys.NFS_HTTP_PORT_DEFAULT, NfsConfigKeys.NFS_HTTP_ADDRESS_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getMountd",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Mountd getMountd()\n{\r\n    return mountd;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "startServiceInternal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startServiceInternal(boolean register) throws IOException\n{\r\n    mountd.start(register);\r\n    start(register);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "startService",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Nfs3 startService(String[] args, DatagramSocket registrationSocket) throws IOException\n{\r\n    StringUtils.startupShutdownMessage(Nfs3.class, args, LOG);\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    boolean allowInsecurePorts = conf.getBoolean(NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_KEY, NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_DEFAULT);\r\n    final Nfs3 nfsServer = new Nfs3(conf, registrationSocket, allowInsecurePorts);\r\n    nfsServer.startServiceInternal(true);\r\n    return nfsServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop()\n{\r\n    super.stop();\r\n    mountd.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    startService(args, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void execute(Runnable task)\n{\r\n    if (executor == null) {\r\n        throw new RuntimeException(\"AsyncDataService is already shutdown\");\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Current active thread number: \" + executor.getActiveCount() + \" queue size: \" + executor.getQueue().size() + \" scheduled task number: \" + executor.getTaskCount());\r\n    }\r\n    executor.execute(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void shutdown()\n{\r\n    if (executor == null) {\r\n        LOG.warn(\"AsyncDataService has already shut down.\");\r\n    } else {\r\n        LOG.info(\"Shutting down all async data service threads...\");\r\n        executor.shutdown();\r\n        executor = null;\r\n        LOG.info(\"All async data service threads have been shut down\");\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "writeAsync",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeAsync(OpenFileCtx openFileCtx)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Scheduling write back task for fileId: \" + openFileCtx.getLatestAttr().getFileId());\r\n    }\r\n    WriteBackTask wbTask = new WriteBackTask(openFileCtx);\r\n    execute(wbTask);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "prepareAddressMap",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void prepareAddressMap() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    String[] exportsPath = config.getStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, NfsConfigKeys.DFS_NFS_EXPORT_POINT_DEFAULT);\r\n    for (String exportPath : exportsPath) {\r\n        URI exportURI = Nfs3Utils.getResolvedURI(fs, exportPath);\r\n        int namenodeId = Nfs3Utils.getNamenodeId(config, exportURI);\r\n        URI value = namenodeUriMap.get(namenodeId);\r\n        if (value == null) {\r\n            LOG.info(\"Added export: {} FileSystem URI: {} with namenodeId: {}\", exportPath, exportPath, namenodeId);\r\n            namenodeUriMap.put(namenodeId, exportURI);\r\n        } else {\r\n            String msg = String.format(\"FS:%s, Namenode ID collision for path:%s \" + \"nnid:%s uri being added:%s existing uri:%s\", fs.getScheme(), exportPath, namenodeId, exportURI, value);\r\n            LOG.error(msg);\r\n            throw new FileSystemException(msg);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getClientCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LoadingCache<DfsClientKey, DFSClient> getClientCache()\n{\r\n    return clientCache;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "closeAll",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void closeAll(boolean onlyAutomatic) throws IOException\n{\r\n    List<IOException> exceptions = new ArrayList<IOException>();\r\n    ConcurrentMap<DfsClientKey, DFSClient> map = clientCache.asMap();\r\n    for (Entry<DfsClientKey, DFSClient> item : map.entrySet()) {\r\n        final DFSClient client = item.getValue();\r\n        if (client != null) {\r\n            try {\r\n                client.close();\r\n            } catch (IOException ioe) {\r\n                exceptions.add(ioe);\r\n            }\r\n        }\r\n    }\r\n    if (!exceptions.isEmpty()) {\r\n        throw MultipleIOException.createIOException(exceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "clientLoader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CacheLoader<DfsClientKey, DFSClient> clientLoader()\n{\r\n    return new CacheLoader<DfsClientKey, DFSClient>() {\r\n\r\n        @Override\r\n        public DFSClient load(final DfsClientKey key) throws Exception {\r\n            UserGroupInformation ugi = getUserGroupInformation(key.userName, UserGroupInformation.getCurrentUser());\r\n            return ugi.doAs(new PrivilegedExceptionAction<DFSClient>() {\r\n\r\n                @Override\r\n                public DFSClient run() throws IOException {\r\n                    URI namenodeURI = namenodeUriMap.get(key.namenodeId);\r\n                    if (namenodeURI == null) {\r\n                        throw new IOException(\"No namenode URI found for user:\" + key.userName + \" namenodeId:\" + key.namenodeId);\r\n                    }\r\n                    return new DFSClient(namenodeURI, config);\r\n                }\r\n            });\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getUserGroupInformation",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "UserGroupInformation getUserGroupInformation(String effectiveUser, UserGroupInformation realUser) throws IOException\n{\r\n    Preconditions.checkNotNull(effectiveUser);\r\n    Preconditions.checkNotNull(realUser);\r\n    realUser.checkTGTAndReloginFromKeytab();\r\n    UserGroupInformation ugi = UserGroupInformation.createProxyUser(effectiveUser, realUser);\r\n    LOG.debug(\"Created ugi: {} for username: {}\", ugi, effectiveUser);\r\n    return ugi;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "clientRemovalListener",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemovalListener<DfsClientKey, DFSClient> clientRemovalListener()\n{\r\n    return new RemovalListener<DfsClientKey, DFSClient>() {\r\n\r\n        @Override\r\n        public void onRemoval(RemovalNotification<DfsClientKey, DFSClient> notification) {\r\n            DFSClient client = notification.getValue();\r\n            try {\r\n                client.close();\r\n            } catch (IOException e) {\r\n                LOG.warn(String.format(\"IOException when closing the DFSClient(%s), cause: %s\", client, e));\r\n            }\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "inputStreamRemovalListener",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemovalListener<DFSInputStreamCacheKey, FSDataInputStream> inputStreamRemovalListener()\n{\r\n    return new RemovalListener<DFSClientCache.DFSInputStreamCacheKey, FSDataInputStream>() {\r\n\r\n        @Override\r\n        public void onRemoval(RemovalNotification<DFSInputStreamCacheKey, FSDataInputStream> notification) {\r\n            try {\r\n                notification.getValue().close();\r\n            } catch (IOException ignored) {\r\n            }\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "inputStreamLoader",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CacheLoader<DFSInputStreamCacheKey, FSDataInputStream> inputStreamLoader()\n{\r\n    return new CacheLoader<DFSInputStreamCacheKey, FSDataInputStream>() {\r\n\r\n        @Override\r\n        public FSDataInputStream load(DFSInputStreamCacheKey key) throws Exception {\r\n            DFSClient client = getDfsClient(key.userId, key.namenodeId);\r\n            DFSInputStream dis = client.open(key.inodePath);\r\n            return client.createWrappedInputStream(dis);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getDfsClient",
  "errType" : [ "ExecutionException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DFSClient getDfsClient(String userName, int namenodeId)\n{\r\n    DFSClient client = null;\r\n    try {\r\n        client = clientCache.get(new DfsClientKey(userName, namenodeId));\r\n    } catch (ExecutionException e) {\r\n        LOG.error(\"Failed to create DFSClient for user: {}\", userName, e);\r\n    }\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getDfsInputStream",
  "errType" : [ "ExecutionException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream getDfsInputStream(String userName, String inodePath, int namenodeId)\n{\r\n    DFSInputStreamCacheKey k = new DFSInputStreamCacheKey(userName, inodePath, namenodeId);\r\n    FSDataInputStream s = null;\r\n    try {\r\n        s = inputstreamCache.get(k);\r\n    } catch (ExecutionException e) {\r\n        LOG.warn(\"Failed to create DFSInputStream for user: {}\", userName, e);\r\n    }\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "invalidateDfsInputStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void invalidateDfsInputStream(String userName, String inodePath, int namenodeId)\n{\r\n    DFSInputStreamCacheKey k = new DFSInputStreamCacheKey(userName, inodePath, namenodeId);\r\n    inputstreamCache.invalidate(k);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileIdPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileIdPath(FileHandle handle)\n{\r\n    return getFileIdPath(handle.getFileId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileIdPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFileIdPath(long fileId)\n{\r\n    return INODEID_PATH_PREFIX + fileId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatus getFileStatus(DFSClient client, String fileIdPath) throws IOException\n{\r\n    return client.getFileLinkInfo(fileIdPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getNfs3FileAttrFromFileStatus",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Nfs3FileAttributes getNfs3FileAttrFromFileStatus(HdfsFileStatus fs, IdMappingServiceProvider iug)\n{\r\n    NfsFileType fileType = fs.isDirectory() ? NfsFileType.NFSDIR : NfsFileType.NFSREG;\r\n    fileType = fs.isSymlink() ? NfsFileType.NFSLNK : fileType;\r\n    int nlink = (fileType == NfsFileType.NFSDIR) ? fs.getChildrenNum() + 2 : 1;\r\n    long size = (fileType == NfsFileType.NFSDIR) ? getDirSize(fs.getChildrenNum()) : fs.getLen();\r\n    return new Nfs3FileAttributes(fileType, nlink, fs.getPermission().toShort(), iug.getUidAllowingUnknown(fs.getOwner()), iug.getGidAllowingUnknown(fs.getGroup()), size, 0, fs.getFileId(), fs.getModificationTime(), fs.getAccessTime(), new Nfs3FileAttributes.Specdata3());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileAttr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Nfs3FileAttributes getFileAttr(DFSClient client, String fileIdPath, IdMappingServiceProvider iug) throws IOException\n{\r\n    HdfsFileStatus fs = getFileStatus(client, fileIdPath);\r\n    return fs == null ? null : getNfs3FileAttrFromFileStatus(fs, iug);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getDirSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDirSize(int childNum)\n{\r\n    return (childNum + 2) * 32;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getWccAttr",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "WccAttr getWccAttr(DFSClient client, String fileIdPath) throws IOException\n{\r\n    HdfsFileStatus fstat = getFileStatus(client, fileIdPath);\r\n    if (fstat == null) {\r\n        return null;\r\n    }\r\n    long size = fstat.isDirectory() ? getDirSize(fstat.getChildrenNum()) : fstat.getLen();\r\n    return new WccAttr(size, new NfsTime(fstat.getModificationTime()), new NfsTime(fstat.getModificationTime()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getWccAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "WccAttr getWccAttr(Nfs3FileAttributes attr)\n{\r\n    return attr == null ? new WccAttr() : new WccAttr(attr.getSize(), attr.getMtime(), attr.getCtime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "createWccData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "WccData createWccData(final WccAttr preOpAttr, DFSClient dfsClient, final String fileIdPath, final IdMappingServiceProvider iug) throws IOException\n{\r\n    Nfs3FileAttributes postOpDirAttr = getFileAttr(dfsClient, fileIdPath, iug);\r\n    return new WccData(preOpAttr, postOpDirAttr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "writeChannel",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeChannel(Channel channel, XDR out, int xid)\n{\r\n    if (channel == null) {\r\n        RpcProgramNfs3.LOG.info(\"Null channel should only happen in tests. Do nothing.\");\r\n        return;\r\n    }\r\n    if (RpcProgramNfs3.LOG.isDebugEnabled()) {\r\n        RpcProgramNfs3.LOG.debug(WRITE_RPC_END + xid);\r\n    }\r\n    ByteBuf outBuf = XDR.writeMessageTcp(out, true);\r\n    channel.writeAndFlush(outBuf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "writeChannelCommit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeChannelCommit(Channel channel, XDR out, int xid)\n{\r\n    if (RpcProgramNfs3.LOG.isDebugEnabled()) {\r\n        RpcProgramNfs3.LOG.debug(\"Commit done:\" + xid);\r\n    }\r\n    ByteBuf outBuf = XDR.writeMessageTcp(out, true);\r\n    channel.writeAndFlush(outBuf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "isSet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSet(int access, int bits)\n{\r\n    return (access & bits) == bits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getAccessRights",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int getAccessRights(int mode, int type)\n{\r\n    int rtn = 0;\r\n    if (isSet(mode, Nfs3Constant.ACCESS_MODE_READ)) {\r\n        rtn |= Nfs3Constant.ACCESS3_READ;\r\n        if (type == NfsFileType.NFSDIR.toValue()) {\r\n            rtn |= Nfs3Constant.ACCESS3_LOOKUP;\r\n        }\r\n    }\r\n    if (isSet(mode, Nfs3Constant.ACCESS_MODE_WRITE)) {\r\n        rtn |= Nfs3Constant.ACCESS3_MODIFY;\r\n        rtn |= Nfs3Constant.ACCESS3_EXTEND;\r\n        rtn |= Nfs3Constant.ACCESS3_DELETE;\r\n    }\r\n    if (isSet(mode, Nfs3Constant.ACCESS_MODE_EXECUTE)) {\r\n        if (type == NfsFileType.NFSREG.toValue()) {\r\n            rtn |= Nfs3Constant.ACCESS3_EXECUTE;\r\n        } else {\r\n            rtn |= Nfs3Constant.ACCESS3_LOOKUP;\r\n        }\r\n    }\r\n    return rtn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getAccessRightsForUserGroup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int getAccessRightsForUserGroup(int uid, int gid, int[] auxGids, Nfs3FileAttributes attr)\n{\r\n    int mode = attr.getMode();\r\n    if (uid == attr.getUid()) {\r\n        return getAccessRights(mode >> 6, attr.getType());\r\n    }\r\n    if (gid == attr.getGid()) {\r\n        return getAccessRights(mode >> 3, attr.getType());\r\n    }\r\n    if (auxGids != null) {\r\n        for (int auxGid : auxGids) {\r\n            if (attr.getGid() == auxGid) {\r\n                return getAccessRights(mode >> 3, attr.getType());\r\n            }\r\n        }\r\n    }\r\n    return getAccessRights(mode, attr.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "bytesToLong",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long bytesToLong(byte[] data)\n{\r\n    long n = 0xffL & data[0];\r\n    for (int i = 1; i < 8; i++) {\r\n        n = (n << 8) | (0xffL & data[i]);\r\n    }\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "longToByte",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] longToByte(long v)\n{\r\n    byte[] data = new byte[8];\r\n    data[0] = (byte) (v >>> 56);\r\n    data[1] = (byte) (v >>> 48);\r\n    data[2] = (byte) (v >>> 40);\r\n    data[3] = (byte) (v >>> 32);\r\n    data[4] = (byte) (v >>> 24);\r\n    data[5] = (byte) (v >>> 16);\r\n    data[6] = (byte) (v >>> 8);\r\n    data[7] = (byte) (v >>> 0);\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getElapsedTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getElapsedTime(long startTimeNano)\n{\r\n    return System.nanoTime() - startTimeNano;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNamenodeId(Configuration conf)\n{\r\n    URI filesystemURI = FileSystem.getDefaultUri(conf);\r\n    return getNamenodeId(conf, filesystemURI);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNamenodeId(Configuration conf, URI namenodeURI)\n{\r\n    InetSocketAddress address = DFSUtilClient.getNNAddressCheckLogical(conf, namenodeURI);\r\n    return address.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getResolvedURI",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "URI getResolvedURI(FileSystem fs, String exportPath) throws IOException\n{\r\n    URI fsURI = fs.getUri();\r\n    String scheme = fs.getScheme();\r\n    if (scheme.equalsIgnoreCase(FsConstants.VIEWFS_SCHEME)) {\r\n        ViewFileSystem viewFs = (ViewFileSystem) fs;\r\n        ViewFileSystem.MountPoint[] mountPoints = viewFs.getMountPoints();\r\n        for (ViewFileSystem.MountPoint mount : mountPoints) {\r\n            String mountedPath = mount.getMountedOnPath().toString();\r\n            if (exportPath.startsWith(mountedPath)) {\r\n                String subpath = exportPath.substring(mountedPath.length());\r\n                fsURI = mount.getTargetFileSystemURIs()[0].resolve(subpath);\r\n                break;\r\n            }\r\n        }\r\n    } else if (scheme.equalsIgnoreCase(HdfsConstants.HDFS_URI_SCHEME)) {\r\n        fsURI = fsURI.resolve(exportPath);\r\n    }\r\n    if (!fsURI.getScheme().equalsIgnoreCase(HdfsConstants.HDFS_URI_SCHEME)) {\r\n        throw new FileSystemException(\"Only HDFS is supported as underlying\" + \"FileSystem, fs scheme:\" + scheme + \" uri to be added\" + fsURI);\r\n    }\r\n    return fsURI;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "createRpcProgramNfs3",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RpcProgramNfs3 createRpcProgramNfs3(NfsConfiguration config, DatagramSocket registrationSocket, boolean allowInsecurePorts) throws IOException\n{\r\n    DefaultMetricsSystem.initialize(\"Nfs3\");\r\n    String displayName = DNS.getDefaultHost(\"default\", \"default\") + config.getInt(NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY, NfsConfigKeys.DFS_NFS_SERVER_PORT_DEFAULT);\r\n    metrics = Nfs3Metrics.create(config, displayName);\r\n    return new RpcProgramNfs3(config, registrationSocket, allowInsecurePorts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "clearDirectory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void clearDirectory(String writeDumpDir) throws IOException\n{\r\n    File dumpDir = new File(writeDumpDir);\r\n    if (dumpDir.exists()) {\r\n        LOG.info(\"Delete current dump directory {}\", writeDumpDir);\r\n        if (!(FileUtil.fullyDelete(dumpDir))) {\r\n            throw new IOException(\"Cannot remove current dump directory: \" + dumpDir);\r\n        }\r\n    }\r\n    LOG.info(\"Create new dump directory {}\", writeDumpDir);\r\n    if (!dumpDir.mkdirs()) {\r\n        throw new IOException(\"Cannot create dump directory \" + dumpDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "startDaemons",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void startDaemons()\n{\r\n    if (pauseMonitor == null) {\r\n        pauseMonitor = new JvmPauseMonitor();\r\n        pauseMonitor.init(config);\r\n        pauseMonitor.start();\r\n        metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\r\n    }\r\n    writeManager.startAsyncDataService();\r\n    try {\r\n        infoServer.start();\r\n    } catch (IOException e) {\r\n        LOG.error(\"failed to start web server\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "stopDaemons",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void stopDaemons()\n{\r\n    if (writeManager != null) {\r\n        writeManager.shutdownAsyncDataService();\r\n    }\r\n    if (pauseMonitor != null) {\r\n        pauseMonitor.stop();\r\n    }\r\n    if (infoServer != null) {\r\n        try {\r\n            infoServer.stop();\r\n        } catch (Exception e) {\r\n            LOG.warn(\"Exception shutting down web server\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getInfoServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Nfs3HttpServer getInfoServer()\n{\r\n    return this.infoServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "mapErrorStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int mapErrorStatus(IOException e)\n{\r\n    if (e instanceof FileNotFoundException) {\r\n        return Nfs3Status.NFS3ERR_STALE;\r\n    } else if (e instanceof AccessControlException) {\r\n        return Nfs3Status.NFS3ERR_ACCES;\r\n    } else {\r\n        return Nfs3Status.NFS3ERR_IO;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "nullProcedure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NFS3Response nullProcedure()\n{\r\n    LOG.debug(\"NFS NULL\");\r\n    return new NFS3Response(Nfs3Status.NFS3_OK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getattr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GETATTR3Response getattr(XDR xdr, RpcInfo info)\n{\r\n    return getattr(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getattr",
  "errType" : [ "IOException", "RemoteException", "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "GETATTR3Response getattr(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    GETATTR3Response response = new GETATTR3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    GETATTR3Request request;\r\n    try {\r\n        request = GETATTR3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid GETATTR request\");\r\n        response.setStatus(Nfs3Status.NFS3ERR_INVAL);\r\n        return response;\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"GETATTR for fileHandle: {} client: {}\", handle.dumpFileHandle(), remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    Nfs3FileAttributes attrs = null;\r\n    try {\r\n        attrs = writeManager.getFileAttr(dfsClient, handle, iug);\r\n    } catch (RemoteException r) {\r\n        LOG.warn(\"Exception\", r);\r\n        IOException io = r.unwrapRemoteException();\r\n        if (io instanceof AuthorizationException) {\r\n            return new GETATTR3Response(Nfs3Status.NFS3ERR_ACCES);\r\n        } else {\r\n            return new GETATTR3Response(Nfs3Status.NFS3ERR_IO);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.info(\"Can't get file attribute, fileId={}\", handle.getFileId(), e);\r\n        int status = mapErrorStatus(e);\r\n        response.setStatus(status);\r\n        return response;\r\n    }\r\n    if (attrs == null) {\r\n        LOG.error(\"Can't get path for fileId: {}\", handle.getFileId());\r\n        response.setStatus(Nfs3Status.NFS3ERR_STALE);\r\n        return response;\r\n    }\r\n    response.setPostOpAttr(attrs);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setattrInternal",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void setattrInternal(DFSClient dfsClient, String fileIdPath, SetAttr3 newAttr, boolean setMode) throws IOException\n{\r\n    EnumSet<SetAttrField> updateFields = newAttr.getUpdateFields();\r\n    if (setMode && updateFields.contains(SetAttrField.MODE)) {\r\n        LOG.debug(\"set new mode: {}\", newAttr.getMode());\r\n        dfsClient.setPermission(fileIdPath, new FsPermission((short) (newAttr.getMode())));\r\n    }\r\n    if (updateFields.contains(SetAttrField.UID) || updateFields.contains(SetAttrField.GID)) {\r\n        String uname = updateFields.contains(SetAttrField.UID) ? iug.getUserName(newAttr.getUid(), IdMappingConstant.UNKNOWN_USER) : null;\r\n        String gname = updateFields.contains(SetAttrField.GID) ? iug.getGroupName(newAttr.getGid(), IdMappingConstant.UNKNOWN_GROUP) : null;\r\n        dfsClient.setOwner(fileIdPath, uname, gname);\r\n    }\r\n    long atime = updateFields.contains(SetAttrField.ATIME) ? newAttr.getAtime().getMilliSeconds() : -1;\r\n    long mtime = updateFields.contains(SetAttrField.MTIME) ? newAttr.getMtime().getMilliSeconds() : -1;\r\n    if (atime != -1 || mtime != -1) {\r\n        LOG.debug(\"set atime: {} mtime: {}\", atime, mtime);\r\n        dfsClient.setTimes(fileIdPath, mtime, atime);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setattr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SETATTR3Response setattr(XDR xdr, RpcInfo info)\n{\r\n    return setattr(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setattr",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "SETATTR3Response setattr(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    SETATTR3Response response = new SETATTR3Response(Nfs3Status.NFS3_OK);\r\n    SETATTR3Request request;\r\n    try {\r\n        request = SETATTR3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid SETATTR request\");\r\n        response.setStatus(Nfs3Status.NFS3ERR_INVAL);\r\n        return response;\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS SETATTR fileHandle: {} client: {}\", handle.dumpFileHandle(), remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    if (request.getAttr().getUpdateFields().contains(SetAttrField.SIZE)) {\r\n        LOG.error(\"Setting file size is not supported when setattr, fileId: {}\", handle.getFileId());\r\n        response.setStatus(Nfs3Status.NFS3ERR_INVAL);\r\n        return response;\r\n    }\r\n    String fileIdPath = Nfs3Utils.getFileIdPath(handle);\r\n    Nfs3FileAttributes preOpAttr = null;\r\n    try {\r\n        preOpAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\r\n        if (preOpAttr == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            response.setStatus(Nfs3Status.NFS3ERR_STALE);\r\n            return response;\r\n        }\r\n        WccAttr preOpWcc = Nfs3Utils.getWccAttr(preOpAttr);\r\n        if (request.isCheck()) {\r\n            if (!preOpAttr.getCtime().equals(request.getCtime())) {\r\n                WccData wccData = new WccData(preOpWcc, preOpAttr);\r\n                return new SETATTR3Response(Nfs3Status.NFS3ERR_NOT_SYNC, wccData);\r\n            }\r\n        }\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            return new SETATTR3Response(Nfs3Status.NFS3ERR_ACCES, new WccData(preOpWcc, preOpAttr));\r\n        }\r\n        setattrInternal(dfsClient, fileIdPath, request.getAttr(), true);\r\n        Nfs3FileAttributes postOpAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\r\n        WccData wccData = new WccData(preOpWcc, postOpAttr);\r\n        return new SETATTR3Response(Nfs3Status.NFS3_OK, wccData);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        WccData wccData = null;\r\n        try {\r\n            wccData = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpAttr), dfsClient, fileIdPath, iug);\r\n        } catch (IOException e1) {\r\n            LOG.info(\"Can't get postOpAttr for fileIdPath: {}\", fileIdPath, e1);\r\n        }\r\n        int status = mapErrorStatus(e);\r\n        return new SETATTR3Response(status, wccData);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "lookup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LOOKUP3Response lookup(XDR xdr, RpcInfo info)\n{\r\n    return lookup(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "lookup",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "LOOKUP3Response lookup(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    LOOKUP3Response response = new LOOKUP3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    LOOKUP3Request request;\r\n    try {\r\n        request = LOOKUP3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid LOOKUP request\");\r\n        return new LOOKUP3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle dirHandle = request.getHandle();\r\n    String fileName = request.getName();\r\n    int namenodeId = dirHandle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS LOOKUP dir fileHandle: {} name: {} client: {}\", dirHandle.dumpFileHandle(), fileName, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    try {\r\n        String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\r\n        Nfs3FileAttributes postOpObjAttr = writeManager.getFileAttr(dfsClient, dirHandle, fileName, namenodeId);\r\n        if (postOpObjAttr == null) {\r\n            LOG.debug(\"NFS LOOKUP fileId: {} name: {} does not exist\", dirHandle.getFileId(), fileName);\r\n            Nfs3FileAttributes postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n            return new LOOKUP3Response(Nfs3Status.NFS3ERR_NOENT, null, null, postOpDirAttr);\r\n        }\r\n        Nfs3FileAttributes postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n        if (postOpDirAttr == null) {\r\n            LOG.info(\"Can't get path for dir fileId: {}\", dirHandle.getFileId());\r\n            return new LOOKUP3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        FileHandle fileHandle = new FileHandle(postOpObjAttr.getFileId(), namenodeId);\r\n        return new LOOKUP3Response(Nfs3Status.NFS3_OK, fileHandle, postOpObjAttr, postOpDirAttr);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new LOOKUP3Response(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ACCESS3Response access(XDR xdr, RpcInfo info)\n{\r\n    return access(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "access",
  "errType" : [ "IOException", "RemoteException", "IOException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "ACCESS3Response access(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    ACCESS3Response response = new ACCESS3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    ACCESS3Request request;\r\n    try {\r\n        request = ACCESS3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid ACCESS request\");\r\n        return new ACCESS3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS ACCESS fileHandle: {} client: {}\", handle.dumpFileHandle(), remoteAddress);\r\n    }\r\n    Nfs3FileAttributes attrs;\r\n    try {\r\n        attrs = writeManager.getFileAttr(dfsClient, handle, iug);\r\n        if (attrs == null) {\r\n            LOG.error(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new ACCESS3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (iug.getUserName(securityHandler.getUid(), \"unknown\").equals(superuser)) {\r\n            int access = Nfs3Constant.ACCESS3_LOOKUP | Nfs3Constant.ACCESS3_DELETE | Nfs3Constant.ACCESS3_EXECUTE | Nfs3Constant.ACCESS3_EXTEND | Nfs3Constant.ACCESS3_MODIFY | Nfs3Constant.ACCESS3_READ;\r\n            return new ACCESS3Response(Nfs3Status.NFS3_OK, attrs, access);\r\n        }\r\n        int access = Nfs3Utils.getAccessRightsForUserGroup(securityHandler.getUid(), securityHandler.getGid(), securityHandler.getAuxGids(), attrs);\r\n        return new ACCESS3Response(Nfs3Status.NFS3_OK, attrs, access);\r\n    } catch (RemoteException r) {\r\n        LOG.warn(\"Exception\", r);\r\n        IOException io = r.unwrapRemoteException();\r\n        if (io instanceof AuthorizationException) {\r\n            return new ACCESS3Response(Nfs3Status.NFS3ERR_ACCES);\r\n        } else {\r\n            return new ACCESS3Response(Nfs3Status.NFS3ERR_IO);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new ACCESS3Response(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "readlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "READLINK3Response readlink(XDR xdr, RpcInfo info)\n{\r\n    return readlink(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "readlink",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "READLINK3Response readlink(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    READLINK3Response response = new READLINK3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    READLINK3Request request;\r\n    try {\r\n        request = READLINK3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid READLINK request\");\r\n        return new READLINK3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS READLINK fileHandle: {} client: {}\", handle.dumpFileHandle(), remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    String fileIdPath = Nfs3Utils.getFileIdPath(handle);\r\n    try {\r\n        String target = dfsClient.getLinkTarget(fileIdPath);\r\n        Nfs3FileAttributes postOpAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\r\n        if (postOpAttr == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new READLINK3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (postOpAttr.getType() != NfsFileType.NFSLNK.toValue()) {\r\n            LOG.error(\"Not a symlink, fileId: {}\", handle.getFileId());\r\n            return new READLINK3Response(Nfs3Status.NFS3ERR_INVAL);\r\n        }\r\n        if (target == null) {\r\n            LOG.error(\"Symlink target should not be null, fileId: {}\", handle.getFileId());\r\n            return new READLINK3Response(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        }\r\n        int rtmax = config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY, NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\r\n        if (rtmax < target.getBytes(Charset.forName(\"UTF-8\")).length) {\r\n            LOG.error(\"Link size: {} is larger than max transfer size: {}\", target.getBytes(Charset.forName(\"UTF-8\")).length, rtmax);\r\n            return new READLINK3Response(Nfs3Status.NFS3ERR_IO, postOpAttr, new byte[0]);\r\n        }\r\n        return new READLINK3Response(Nfs3Status.NFS3_OK, postOpAttr, target.getBytes(Charset.forName(\"UTF-8\")));\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Readlink error\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new READLINK3Response(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "READ3Response read(XDR xdr, RpcInfo info)\n{\r\n    return read(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "read",
  "errType" : [ "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 34,
  "sourceCodeText" : "READ3Response read(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    READ3Response response = new READ3Response(Nfs3Status.NFS3_OK);\r\n    final String userName = securityHandler.getUser();\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    READ3Request request;\r\n    try {\r\n        request = READ3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid READ request\");\r\n        return new READ3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    long offset = request.getOffset();\r\n    int count = request.getCount();\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS READ fileHandle: {} offset: {} count: {} client: {}\", handle.dumpFileHandle(), offset, count, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(userName, namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    Nfs3FileAttributes attrs;\r\n    boolean eof;\r\n    if (count == 0) {\r\n        try {\r\n            attrs = Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle), iug);\r\n        } catch (IOException e) {\r\n            LOG.debug(\"Get error accessing file, fileId: {}\", handle.getFileId(), e);\r\n            return new READ3Response(Nfs3Status.NFS3ERR_IO);\r\n        }\r\n        if (attrs == null) {\r\n            LOG.debug(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new READ3Response(Nfs3Status.NFS3ERR_NOENT);\r\n        }\r\n        int access = Nfs3Utils.getAccessRightsForUserGroup(securityHandler.getUid(), securityHandler.getGid(), securityHandler.getAuxGids(), attrs);\r\n        if ((access & Nfs3Constant.ACCESS3_READ) != 0) {\r\n            eof = offset >= attrs.getSize();\r\n            return new READ3Response(Nfs3Status.NFS3_OK, attrs, 0, eof, ByteBuffer.wrap(new byte[0]));\r\n        } else {\r\n            return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\r\n        }\r\n    }\r\n    int ret = writeManager.commitBeforeRead(dfsClient, handle, offset + count);\r\n    if (ret != Nfs3Status.NFS3_OK) {\r\n        LOG.warn(\"commitBeforeRead didn't succeed with ret={}. \" + \"Read may not get most recent data.\", ret);\r\n    }\r\n    try {\r\n        int rtmax = config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY, NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\r\n        int buffSize = Math.min(rtmax, count);\r\n        byte[] readbuffer = new byte[buffSize];\r\n        int readCount = 0;\r\n        for (int i = 0; i < 1; ++i) {\r\n            FSDataInputStream fis = clientCache.getDfsInputStream(userName, Nfs3Utils.getFileIdPath(handle), namenodeId);\r\n            if (fis == null) {\r\n                return new READ3Response(Nfs3Status.NFS3ERR_ACCES);\r\n            }\r\n            try {\r\n                readCount = fis.read(offset, readbuffer, 0, count);\r\n                metrics.incrBytesRead(readCount);\r\n            } catch (IOException e) {\r\n                if (e.getMessage().equals(\"Stream closed\")) {\r\n                    clientCache.invalidateDfsInputStream(userName, Nfs3Utils.getFileIdPath(handle), namenodeId);\r\n                    continue;\r\n                } else {\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n        attrs = Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle), iug);\r\n        if (readCount < count) {\r\n            LOG.info(\"Partial read. Asked offset: {} count: {} and read back: {} \" + \"file size: {}\", offset, count, readCount, attrs.getSize());\r\n        }\r\n        if (readCount < 0) {\r\n            readCount = 0;\r\n        }\r\n        eof = (offset + readCount) >= attrs.getSize();\r\n        return new READ3Response(Nfs3Status.NFS3_OK, attrs, readCount, eof, ByteBuffer.wrap(readbuffer));\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Read error. Offset: {} count: {}\", offset, count, e);\r\n        int status = mapErrorStatus(e);\r\n        return new READ3Response(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "WRITE3Response write(XDR xdr, RpcInfo info)\n{\r\n    SecurityHandler securityHandler = getSecurityHandler(info);\r\n    RpcCall rpcCall = (RpcCall) info.header();\r\n    int xid = rpcCall.getXid();\r\n    SocketAddress remoteAddress = info.remoteAddress();\r\n    return write(xdr, info.channel(), xid, securityHandler, remoteAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "write",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "WRITE3Response write(XDR xdr, Channel channel, int xid, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK);\r\n    WRITE3Request request;\r\n    try {\r\n        request = WRITE3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid WRITE request\");\r\n        return new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    long offset = request.getOffset();\r\n    int count = request.getCount();\r\n    WriteStableHow stableHow = request.getStableHow();\r\n    byte[] data = request.getData().array();\r\n    if (data.length < count) {\r\n        LOG.error(\"Invalid argument, data size is less than count in request\");\r\n        return new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS WRITE fileHandle: {} offset: {} length: {} \" + \"stableHow: {} xid: {} client: {}\", handle.dumpFileHandle(), offset, count, stableHow.getValue(), xid, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    Nfs3FileAttributes preOpAttr = null;\r\n    try {\r\n        preOpAttr = writeManager.getFileAttr(dfsClient, handle, iug);\r\n        if (preOpAttr == null) {\r\n            LOG.error(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new WRITE3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            return new WRITE3Response(Nfs3Status.NFS3ERR_ACCES, new WccData(Nfs3Utils.getWccAttr(preOpAttr), preOpAttr), 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n        }\r\n        LOG.debug(\"requested offset={} and current filesize={}\", offset, preOpAttr.getSize());\r\n        writeManager.handleWrite(dfsClient, request, channel, xid, preOpAttr);\r\n    } catch (IOException e) {\r\n        LOG.info(\"Error writing to fileId {} at offset {} and length {}\", handle.getFileId(), offset, data.length, e);\r\n        Nfs3FileAttributes postOpAttr = null;\r\n        try {\r\n            postOpAttr = writeManager.getFileAttr(dfsClient, handle, iug);\r\n        } catch (IOException e1) {\r\n            LOG.info(\"Can't get postOpAttr for fileId: {}\", e1);\r\n        }\r\n        WccAttr attr = preOpAttr == null ? null : Nfs3Utils.getWccAttr(preOpAttr);\r\n        WccData fileWcc = new WccData(attr, postOpAttr);\r\n        int status = mapErrorStatus(e);\r\n        return new WRITE3Response(status, fileWcc, 0, request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CREATE3Response create(XDR xdr, RpcInfo info)\n{\r\n    return create(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "create",
  "errType" : [ "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 44,
  "sourceCodeText" : "CREATE3Response create(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    CREATE3Response response = new CREATE3Response(Nfs3Status.NFS3_OK);\r\n    CREATE3Request request;\r\n    try {\r\n        request = CREATE3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid CREATE request\");\r\n        return new CREATE3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle dirHandle = request.getHandle();\r\n    String fileName = request.getName();\r\n    int namenodeId = dirHandle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS CREATE dir fileHandle: {} filename: {} client: {}\", dirHandle.dumpFileHandle(), fileName, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    int createMode = request.getMode();\r\n    if ((createMode != Nfs3Constant.CREATE_EXCLUSIVE) && request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE) && request.getObjAttr().getSize() != 0) {\r\n        LOG.error(\"Setting file size is not supported when creating file: {} \" + \"dir fileId: {}\", fileName, dirHandle.getFileId());\r\n        return new CREATE3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    HdfsDataOutputStream fos = null;\r\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\r\n    Nfs3FileAttributes preOpDirAttr = null;\r\n    Nfs3FileAttributes postOpObjAttr = null;\r\n    FileHandle fileHandle = null;\r\n    WccData dirWcc = null;\r\n    try {\r\n        preOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n        if (preOpDirAttr == null) {\r\n            LOG.error(\"Can't get path for dirHandle: {}\", dirHandle);\r\n            return new CREATE3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            return new CREATE3Response(Nfs3Status.NFS3ERR_ACCES, null, preOpDirAttr, new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), preOpDirAttr));\r\n        }\r\n        String fileIdPath = Nfs3Utils.getFileIdPath(dirHandle) + \"/\" + fileName;\r\n        SetAttr3 setAttr3 = request.getObjAttr();\r\n        assert (setAttr3 != null);\r\n        FsPermission permission = setAttr3.getUpdateFields().contains(SetAttrField.MODE) ? new FsPermission((short) setAttr3.getMode()) : FsPermission.getDefault().applyUMask(umask);\r\n        EnumSet<CreateFlag> flag = (createMode != Nfs3Constant.CREATE_EXCLUSIVE) ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE) : EnumSet.of(CreateFlag.CREATE);\r\n        fos = dfsClient.createWrappedOutputStream(dfsClient.create(fileIdPath, permission, flag, false, replication, blockSize, null, bufferSize, null), null);\r\n        if ((createMode == Nfs3Constant.CREATE_UNCHECKED) || (createMode == Nfs3Constant.CREATE_GUARDED)) {\r\n            if (!setAttr3.getUpdateFields().contains(SetAttrField.GID)) {\r\n                setAttr3.getUpdateFields().add(SetAttrField.GID);\r\n                setAttr3.setGid(securityHandler.getGid());\r\n            }\r\n            setattrInternal(dfsClient, fileIdPath, setAttr3, false);\r\n        }\r\n        postOpObjAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\r\n        dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\r\n        OpenFileCtx openFileCtx = new OpenFileCtx(fos, postOpObjAttr, writeDumpDir + \"/\" + postOpObjAttr.getFileId(), dfsClient, iug, aixCompatMode, config);\r\n        fileHandle = new FileHandle(postOpObjAttr.getFileId(), namenodeId);\r\n        if (!writeManager.addOpenFileStream(fileHandle, openFileCtx)) {\r\n            LOG.warn(\"Can't add more stream, close it.\" + \" Future write will become append\");\r\n            fos.close();\r\n            fos = null;\r\n        } else {\r\n            LOG.debug(\"Opened stream for file: {}, fileId: {}\", fileName, fileHandle.getFileId());\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception\", e);\r\n        if (fos != null) {\r\n            try {\r\n                fos.close();\r\n            } catch (IOException e1) {\r\n                LOG.error(\"Can't close stream for dirFileId: {} filename: {}\", dirHandle.getFileId(), fileName, e1);\r\n            }\r\n        }\r\n        if (dirWcc == null) {\r\n            try {\r\n                dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\r\n            } catch (IOException e1) {\r\n                LOG.info(\"Can't get postOpDirAttr for dirFileId: {}\", dirHandle.getFileId(), e1);\r\n            }\r\n        }\r\n        int status = mapErrorStatus(e);\r\n        return new CREATE3Response(status, fileHandle, postOpObjAttr, dirWcc);\r\n    }\r\n    return new CREATE3Response(Nfs3Status.NFS3_OK, fileHandle, postOpObjAttr, dirWcc);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "mkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MKDIR3Response mkdir(XDR xdr, RpcInfo info)\n{\r\n    return mkdir(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "mkdir",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 35,
  "sourceCodeText" : "MKDIR3Response mkdir(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    MKDIR3Response response = new MKDIR3Response(Nfs3Status.NFS3_OK);\r\n    MKDIR3Request request;\r\n    try {\r\n        request = MKDIR3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid MKDIR request\");\r\n        return new MKDIR3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle dirHandle = request.getHandle();\r\n    String fileName = request.getName();\r\n    int namenodeId = dirHandle.getNamenodeId();\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS MKDIR dirHandle: {} filename: {} client: {}\", dirHandle.dumpFileHandle(), fileName, remoteAddress);\r\n    }\r\n    if (request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE)) {\r\n        LOG.error(\"Setting file size is not supported when mkdir: \" + \"{} in dirHandle {}\", fileName, dirHandle);\r\n        return new MKDIR3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\r\n    Nfs3FileAttributes preOpDirAttr = null;\r\n    Nfs3FileAttributes postOpDirAttr = null;\r\n    Nfs3FileAttributes postOpObjAttr = null;\r\n    FileHandle objFileHandle = null;\r\n    try {\r\n        preOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n        if (preOpDirAttr == null) {\r\n            LOG.info(\"Can't get path for dir fileId: {}\", dirHandle.getFileId());\r\n            return new MKDIR3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            return new MKDIR3Response(Nfs3Status.NFS3ERR_ACCES, null, preOpDirAttr, new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), preOpDirAttr));\r\n        }\r\n        final String fileIdPath = dirFileIdPath + \"/\" + fileName;\r\n        SetAttr3 setAttr3 = request.getObjAttr();\r\n        FsPermission permission = setAttr3.getUpdateFields().contains(SetAttrField.MODE) ? new FsPermission((short) setAttr3.getMode()) : FsPermission.getDefault().applyUMask(umask);\r\n        if (!dfsClient.mkdirs(fileIdPath, permission, false)) {\r\n            WccData dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\r\n            return new MKDIR3Response(Nfs3Status.NFS3ERR_IO, null, null, dirWcc);\r\n        }\r\n        if (!setAttr3.getUpdateFields().contains(SetAttrField.GID)) {\r\n            setAttr3.getUpdateFields().add(SetAttrField.GID);\r\n            setAttr3.setGid(securityHandler.getGid());\r\n        }\r\n        setattrInternal(dfsClient, fileIdPath, setAttr3, false);\r\n        postOpObjAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\r\n        objFileHandle = new FileHandle(postOpObjAttr.getFileId(), namenodeId);\r\n        WccData dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\r\n        return new MKDIR3Response(Nfs3Status.NFS3_OK, new FileHandle(postOpObjAttr.getFileId(), namenodeId), postOpObjAttr, dirWcc);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        if (postOpDirAttr == null) {\r\n            try {\r\n                postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n            } catch (IOException e1) {\r\n                LOG.info(\"Can't get postOpDirAttr for {}\", dirFileIdPath, e);\r\n            }\r\n        }\r\n        WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), postOpDirAttr);\r\n        int status = mapErrorStatus(e);\r\n        return new MKDIR3Response(status, objFileHandle, postOpObjAttr, dirWcc);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "mknod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "READDIR3Response mknod(XDR xdr, RpcInfo info)\n{\r\n    return new READDIR3Response(Nfs3Status.NFS3ERR_NOTSUPP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "REMOVE3Response remove(XDR xdr, RpcInfo info)\n{\r\n    return remove(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "remove",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "REMOVE3Response remove(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    REMOVE3Response response = new REMOVE3Response(Nfs3Status.NFS3_OK);\r\n    REMOVE3Request request;\r\n    try {\r\n        request = REMOVE3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid REMOVE request\");\r\n        return new REMOVE3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle dirHandle = request.getHandle();\r\n    int namenodeId = dirHandle.getNamenodeId();\r\n    String fileName = request.getName();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS REMOVE dir fileHandle: {} fileName: {} client: {}\", dirHandle.dumpFileHandle(), fileName, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\r\n    Nfs3FileAttributes preOpDirAttr = null;\r\n    Nfs3FileAttributes postOpDirAttr = null;\r\n    try {\r\n        preOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n        if (preOpDirAttr == null) {\r\n            LOG.info(\"Can't get path for dir fileId: {}\", dirHandle.getFileId());\r\n            return new REMOVE3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        WccData errWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), preOpDirAttr);\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            return new REMOVE3Response(Nfs3Status.NFS3ERR_ACCES, errWcc);\r\n        }\r\n        String fileIdPath = dirFileIdPath + \"/\" + fileName;\r\n        HdfsFileStatus fstat = Nfs3Utils.getFileStatus(dfsClient, fileIdPath);\r\n        if (fstat == null) {\r\n            return new REMOVE3Response(Nfs3Status.NFS3ERR_NOENT, errWcc);\r\n        }\r\n        if (fstat.isDirectory()) {\r\n            return new REMOVE3Response(Nfs3Status.NFS3ERR_ISDIR, errWcc);\r\n        }\r\n        boolean result = dfsClient.delete(fileIdPath, false);\r\n        WccData dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\r\n        if (!result) {\r\n            return new REMOVE3Response(Nfs3Status.NFS3ERR_ACCES, dirWcc);\r\n        }\r\n        return new REMOVE3Response(Nfs3Status.NFS3_OK, dirWcc);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        if (postOpDirAttr == null) {\r\n            try {\r\n                postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n            } catch (IOException e1) {\r\n                LOG.info(\"Can't get postOpDirAttr for {}\", dirFileIdPath, e1);\r\n            }\r\n        }\r\n        WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), postOpDirAttr);\r\n        int status = mapErrorStatus(e);\r\n        return new REMOVE3Response(status, dirWcc);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "rmdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RMDIR3Response rmdir(XDR xdr, RpcInfo info)\n{\r\n    return rmdir(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "rmdir",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "RMDIR3Response rmdir(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    RMDIR3Response response = new RMDIR3Response(Nfs3Status.NFS3_OK);\r\n    RMDIR3Request request;\r\n    try {\r\n        request = RMDIR3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid RMDIR request\");\r\n        return new RMDIR3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle dirHandle = request.getHandle();\r\n    String fileName = request.getName();\r\n    int namenodeId = dirHandle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS RMDIR dir fileHandle: {} fileName: {} client: {}\", dirHandle.dumpFileHandle(), fileName, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    String dirFileIdPath = Nfs3Utils.getFileIdPath(dirHandle);\r\n    Nfs3FileAttributes preOpDirAttr = null;\r\n    Nfs3FileAttributes postOpDirAttr = null;\r\n    try {\r\n        preOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n        if (preOpDirAttr == null) {\r\n            LOG.info(\"Can't get path for dir fileId: {}\", dirHandle.getFileId());\r\n            return new RMDIR3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        WccData errWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), preOpDirAttr);\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            return new RMDIR3Response(Nfs3Status.NFS3ERR_ACCES, errWcc);\r\n        }\r\n        String fileIdPath = dirFileIdPath + \"/\" + fileName;\r\n        HdfsFileStatus fstat = Nfs3Utils.getFileStatus(dfsClient, fileIdPath);\r\n        if (fstat == null) {\r\n            return new RMDIR3Response(Nfs3Status.NFS3ERR_NOENT, errWcc);\r\n        }\r\n        if (!fstat.isDirectory()) {\r\n            return new RMDIR3Response(Nfs3Status.NFS3ERR_NOTDIR, errWcc);\r\n        }\r\n        if (fstat.getChildrenNum() > 0) {\r\n            return new RMDIR3Response(Nfs3Status.NFS3ERR_NOTEMPTY, errWcc);\r\n        }\r\n        boolean result = dfsClient.delete(fileIdPath, false);\r\n        WccData dirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr), dfsClient, dirFileIdPath, iug);\r\n        if (!result) {\r\n            return new RMDIR3Response(Nfs3Status.NFS3ERR_ACCES, dirWcc);\r\n        }\r\n        return new RMDIR3Response(Nfs3Status.NFS3_OK, dirWcc);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        if (postOpDirAttr == null) {\r\n            try {\r\n                postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n            } catch (IOException e1) {\r\n                LOG.info(\"Can't get postOpDirAttr for {}\", dirFileIdPath, e1);\r\n            }\r\n        }\r\n        WccData dirWcc = new WccData(Nfs3Utils.getWccAttr(preOpDirAttr), postOpDirAttr);\r\n        int status = mapErrorStatus(e);\r\n        return new RMDIR3Response(status, dirWcc);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RENAME3Response rename(XDR xdr, RpcInfo info)\n{\r\n    return rename(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "rename",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "RENAME3Response rename(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    RENAME3Response response = new RENAME3Response(Nfs3Status.NFS3_OK);\r\n    RENAME3Request request = null;\r\n    try {\r\n        request = RENAME3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid RENAME request\");\r\n        return new RENAME3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle fromHandle = request.getFromDirHandle();\r\n    int fromNamenodeId = fromHandle.getNamenodeId();\r\n    String fromName = request.getFromName();\r\n    FileHandle toHandle = request.getToDirHandle();\r\n    int toNamenodeId = toHandle.getNamenodeId();\r\n    String toName = request.getToName();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS RENAME from: {}/{} to: {}/{} client: {}\", fromHandle.dumpFileHandle(), fromName, toHandle.dumpFileHandle(), toName, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), fromNamenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    if (fromNamenodeId != toNamenodeId) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_INVAL);\r\n        return response;\r\n    }\r\n    String fromDirFileIdPath = Nfs3Utils.getFileIdPath(fromHandle);\r\n    String toDirFileIdPath = Nfs3Utils.getFileIdPath(toHandle);\r\n    Nfs3FileAttributes fromPreOpAttr = null;\r\n    Nfs3FileAttributes toPreOpAttr = null;\r\n    WccData fromDirWcc = null;\r\n    WccData toDirWcc = null;\r\n    try {\r\n        fromPreOpAttr = Nfs3Utils.getFileAttr(dfsClient, fromDirFileIdPath, iug);\r\n        if (fromPreOpAttr == null) {\r\n            LOG.info(\"Can't get path for fromHandle fileId: {}\", fromHandle.getFileId());\r\n            return new RENAME3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        toPreOpAttr = Nfs3Utils.getFileAttr(dfsClient, toDirFileIdPath, iug);\r\n        if (toPreOpAttr == null) {\r\n            LOG.info(\"Can't get path for toHandle fileId: {}\", toHandle.getFileId());\r\n            return new RENAME3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            WccData fromWcc = new WccData(Nfs3Utils.getWccAttr(fromPreOpAttr), fromPreOpAttr);\r\n            WccData toWcc = new WccData(Nfs3Utils.getWccAttr(toPreOpAttr), toPreOpAttr);\r\n            return new RENAME3Response(Nfs3Status.NFS3ERR_ACCES, fromWcc, toWcc);\r\n        }\r\n        String src = fromDirFileIdPath + \"/\" + fromName;\r\n        String dst = toDirFileIdPath + \"/\" + toName;\r\n        dfsClient.rename(src, dst, Options.Rename.NONE);\r\n        fromDirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(fromPreOpAttr), dfsClient, fromDirFileIdPath, iug);\r\n        toDirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(toPreOpAttr), dfsClient, toDirFileIdPath, iug);\r\n        return new RENAME3Response(Nfs3Status.NFS3_OK, fromDirWcc, toDirWcc);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        try {\r\n            fromDirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(fromPreOpAttr), dfsClient, fromDirFileIdPath, iug);\r\n            toDirWcc = Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(toPreOpAttr), dfsClient, toDirFileIdPath, iug);\r\n        } catch (IOException e1) {\r\n            LOG.info(\"Can't get postOpDirAttr for {} or {}\", fromDirFileIdPath, toDirFileIdPath, e1);\r\n        }\r\n        int status = mapErrorStatus(e);\r\n        return new RENAME3Response(status, fromDirWcc, toDirWcc);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "symlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SYMLINK3Response symlink(XDR xdr, RpcInfo info)\n{\r\n    return symlink(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "symlink",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "SYMLINK3Response symlink(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    SYMLINK3Response response = new SYMLINK3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    SYMLINK3Request request;\r\n    try {\r\n        request = SYMLINK3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid SYMLINK request\");\r\n        response.setStatus(Nfs3Status.NFS3ERR_INVAL);\r\n        return response;\r\n    }\r\n    FileHandle dirHandle = request.getHandle();\r\n    String name = request.getName();\r\n    String symData = request.getSymData();\r\n    String linkDirIdPath = Nfs3Utils.getFileIdPath(dirHandle);\r\n    int namenodeId = dirHandle.getNamenodeId();\r\n    String linkIdPath = linkDirIdPath + \"/\" + name;\r\n    LOG.debug(\"NFS SYMLINK, target: {} link: {} namenodeId: {} client: {}\", symData, linkIdPath, namenodeId, remoteAddress);\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    try {\r\n        WccData dirWcc = response.getDirWcc();\r\n        WccAttr preOpAttr = Nfs3Utils.getWccAttr(dfsClient, linkDirIdPath);\r\n        dirWcc.setPreOpAttr(preOpAttr);\r\n        dfsClient.createSymlink(symData, linkIdPath, false);\r\n        HdfsFileStatus linkstat = dfsClient.getFileLinkInfo(linkIdPath);\r\n        Nfs3FileAttributes objAttr = Nfs3Utils.getNfs3FileAttrFromFileStatus(linkstat, iug);\r\n        dirWcc.setPostOpAttr(Nfs3Utils.getFileAttr(dfsClient, linkDirIdPath, iug));\r\n        return new SYMLINK3Response(Nfs3Status.NFS3_OK, new FileHandle(objAttr.getFileId(), namenodeId), objAttr, dirWcc);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        response.setStatus(status);\r\n        return response;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "link",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "READDIR3Response link(XDR xdr, RpcInfo info)\n{\r\n    return new READDIR3Response(Nfs3Status.NFS3ERR_NOTSUPP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "listPaths",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DirectoryListing listPaths(DFSClient dfsClient, String dirFileIdPath, byte[] startAfter) throws IOException\n{\r\n    DirectoryListing dlisting;\r\n    try {\r\n        dlisting = dfsClient.listPaths(dirFileIdPath, startAfter);\r\n    } catch (RemoteException e) {\r\n        IOException io = e.unwrapRemoteException();\r\n        if (!(io instanceof DirectoryListingStartAfterNotFoundException)) {\r\n            throw io;\r\n        }\r\n        LOG.info(\"Cookie couldn't be found: {}, do listing from beginning\", new String(startAfter, Charset.forName(\"UTF-8\")));\r\n        dlisting = dfsClient.listPaths(dirFileIdPath, HdfsFileStatus.EMPTY_NAME);\r\n    }\r\n    return dlisting;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "readdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "READDIR3Response readdir(XDR xdr, RpcInfo info)\n{\r\n    return readdir(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "readdir",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 44,
  "sourceCodeText" : "READDIR3Response readdir(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    READDIR3Response response = new READDIR3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    READDIR3Request request;\r\n    try {\r\n        request = READDIR3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid READDIR request\");\r\n        return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    long cookie = request.getCookie();\r\n    if (cookie < 0) {\r\n        LOG.error(\"Invalid READDIR request, with negative cookie: {}\", cookie);\r\n        return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    long count = request.getCount();\r\n    if (count <= 0) {\r\n        LOG.info(\"Nonpositive count in invalid READDIR request: {}\", count);\r\n        return new READDIR3Response(Nfs3Status.NFS3_OK);\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS READDIR fileHandle: {} cookie: {} count: {} client: {}\", handle.dumpFileHandle(), cookie, count, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    HdfsFileStatus dirStatus;\r\n    DirectoryListing dlisting;\r\n    Nfs3FileAttributes postOpAttr;\r\n    long dotdotFileId = 0;\r\n    try {\r\n        String dirFileIdPath = Nfs3Utils.getFileIdPath(handle);\r\n        dirStatus = dfsClient.getFileInfo(dirFileIdPath);\r\n        if (dirStatus == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (!dirStatus.isDirectory()) {\r\n            LOG.error(\"Can't readdir for regular file, fileId: {}\", handle.getFileId());\r\n            return new READDIR3Response(Nfs3Status.NFS3ERR_NOTDIR);\r\n        }\r\n        long cookieVerf = request.getCookieVerf();\r\n        if ((cookieVerf != 0) && (cookieVerf != dirStatus.getModificationTime())) {\r\n            if (aixCompatMode) {\r\n                LOG.warn(\"AIX compatibility mode enabled, ignoring cookieverf \" + \"mismatches.\");\r\n            } else {\r\n                LOG.error(\"CookieVerf mismatch. request cookieVerf: {} \" + \"dir cookieVerf: {}\", cookieVerf, dirStatus.getModificationTime());\r\n                return new READDIR3Response(Nfs3Status.NFS3ERR_BAD_COOKIE, Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug));\r\n            }\r\n        }\r\n        if (cookie == 0) {\r\n            String dotdotFileIdPath = dirFileIdPath + \"/..\";\r\n            HdfsFileStatus dotdotStatus = dfsClient.getFileInfo(dotdotFileIdPath);\r\n            if (dotdotStatus == null) {\r\n                throw new IOException(\"Can't get path for handle path: \" + dotdotFileIdPath);\r\n            }\r\n            dotdotFileId = dotdotStatus.getFileId();\r\n        }\r\n        byte[] startAfter;\r\n        if (cookie == 0) {\r\n            startAfter = HdfsFileStatus.EMPTY_NAME;\r\n        } else {\r\n            String inodeIdPath = Nfs3Utils.getFileIdPath(cookie);\r\n            startAfter = inodeIdPath.getBytes(Charset.forName(\"UTF-8\"));\r\n        }\r\n        dlisting = listPaths(dfsClient, dirFileIdPath, startAfter);\r\n        postOpAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n        if (postOpAttr == null) {\r\n            LOG.error(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new READDIR3Response(status);\r\n    }\r\n    HdfsFileStatus[] fstatus = dlisting.getPartialListing();\r\n    int n = (int) Math.min(fstatus.length, count - 2);\r\n    boolean eof = (n >= fstatus.length) && !dlisting.hasMore();\r\n    Entry3[] entries;\r\n    if (cookie == 0) {\r\n        entries = new Entry3[n + 2];\r\n        entries[0] = new READDIR3Response.Entry3(postOpAttr.getFileId(), \".\", 0);\r\n        entries[1] = new READDIR3Response.Entry3(dotdotFileId, \"..\", dotdotFileId);\r\n        for (int i = 2; i < n + 2; i++) {\r\n            entries[i] = new READDIR3Response.Entry3(fstatus[i - 2].getFileId(), fstatus[i - 2].getLocalName(), fstatus[i - 2].getFileId());\r\n        }\r\n    } else {\r\n        entries = new Entry3[n];\r\n        for (int i = 0; i < n; i++) {\r\n            entries[i] = new READDIR3Response.Entry3(fstatus[i].getFileId(), fstatus[i].getLocalName(), fstatus[i].getFileId());\r\n        }\r\n    }\r\n    DirList3 dirList = new READDIR3Response.DirList3(entries, eof);\r\n    return new READDIR3Response(Nfs3Status.NFS3_OK, postOpAttr, dirStatus.getModificationTime(), dirList);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "readdirplus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "READDIRPLUS3Response readdirplus(XDR xdr, RpcInfo info)\n{\r\n    return readdirplus(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "readdirplus",
  "errType" : [ "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 48,
  "sourceCodeText" : "READDIRPLUS3Response readdirplus(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_ACCES);\r\n    }\r\n    READDIRPLUS3Request request = null;\r\n    try {\r\n        request = READDIRPLUS3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid READDIRPLUS request\");\r\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    long cookie = request.getCookie();\r\n    if (cookie < 0) {\r\n        LOG.error(\"Invalid READDIRPLUS request, with negative cookie: {}\", cookie);\r\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    long dirCount = request.getDirCount();\r\n    if (dirCount <= 0) {\r\n        LOG.info(\"Nonpositive dircount in invalid READDIRPLUS request: {}\", dirCount);\r\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    int maxCount = request.getMaxCount();\r\n    if (maxCount <= 0) {\r\n        LOG.info(\"Nonpositive maxcount in invalid READDIRPLUS request: {}\", maxCount);\r\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS READDIRPLUS fileHandle: {} cookie: {} dirCount: {} \" + \"maxCount: {} client: {}\", handle.dumpFileHandle(), cookie, dirCount, maxCount, remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n    }\r\n    HdfsFileStatus dirStatus;\r\n    DirectoryListing dlisting;\r\n    Nfs3FileAttributes postOpDirAttr;\r\n    long dotdotFileId = 0;\r\n    HdfsFileStatus dotdotStatus = null;\r\n    try {\r\n        String dirFileIdPath = Nfs3Utils.getFileIdPath(handle);\r\n        dirStatus = dfsClient.getFileInfo(dirFileIdPath);\r\n        if (dirStatus == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (!dirStatus.isDirectory()) {\r\n            LOG.error(\"Can't readdirplus for regular file, fileId: {}\", handle.getFileId());\r\n            return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_NOTDIR);\r\n        }\r\n        long cookieVerf = request.getCookieVerf();\r\n        if ((cookieVerf != 0) && (cookieVerf != dirStatus.getModificationTime())) {\r\n            if (aixCompatMode) {\r\n                LOG.warn(\"AIX compatibility mode enabled, ignoring cookieverf \" + \"mismatches.\");\r\n            } else {\r\n                LOG.error(\"cookieverf mismatch. request cookieverf: {} \" + \"dir cookieverf: {}\", cookieVerf, dirStatus.getModificationTime());\r\n                return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_BAD_COOKIE, Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug), 0, null);\r\n            }\r\n        }\r\n        if (cookie == 0) {\r\n            String dotdotFileIdPath = dirFileIdPath + \"/..\";\r\n            dotdotStatus = dfsClient.getFileInfo(dotdotFileIdPath);\r\n            if (dotdotStatus == null) {\r\n                throw new IOException(\"Can't get path for handle path: \" + dotdotFileIdPath);\r\n            }\r\n            dotdotFileId = dotdotStatus.getFileId();\r\n        }\r\n        byte[] startAfter;\r\n        if (cookie == 0) {\r\n            startAfter = HdfsFileStatus.EMPTY_NAME;\r\n        } else {\r\n            String inodeIdPath = Nfs3Utils.getFileIdPath(cookie);\r\n            startAfter = inodeIdPath.getBytes(Charset.forName(\"UTF-8\"));\r\n        }\r\n        dlisting = listPaths(dfsClient, dirFileIdPath, startAfter);\r\n        postOpDirAttr = Nfs3Utils.getFileAttr(dfsClient, dirFileIdPath, iug);\r\n        if (postOpDirAttr == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new READDIRPLUS3Response(status);\r\n    }\r\n    HdfsFileStatus[] fstatus = dlisting.getPartialListing();\r\n    int n = (int) Math.min(fstatus.length, dirCount - 2);\r\n    boolean eof = (n >= fstatus.length) && !dlisting.hasMore();\r\n    READDIRPLUS3Response.EntryPlus3[] entries;\r\n    if (cookie == 0) {\r\n        entries = new READDIRPLUS3Response.EntryPlus3[n + 2];\r\n        entries[0] = new READDIRPLUS3Response.EntryPlus3(postOpDirAttr.getFileId(), \".\", 0, postOpDirAttr, new FileHandle(postOpDirAttr.getFileId(), namenodeId));\r\n        entries[1] = new READDIRPLUS3Response.EntryPlus3(dotdotFileId, \"..\", dotdotFileId, Nfs3Utils.getNfs3FileAttrFromFileStatus(dotdotStatus, iug), new FileHandle(dotdotFileId, namenodeId));\r\n        for (int i = 2; i < n + 2; i++) {\r\n            long fileId = fstatus[i - 2].getFileId();\r\n            FileHandle childHandle = new FileHandle(fileId, namenodeId);\r\n            Nfs3FileAttributes attr;\r\n            try {\r\n                attr = writeManager.getFileAttr(dfsClient, childHandle, iug);\r\n            } catch (IOException e) {\r\n                LOG.info(\"Can't get file attributes for fileId: {}\", fileId, e);\r\n                continue;\r\n            }\r\n            entries[i] = new READDIRPLUS3Response.EntryPlus3(fileId, fstatus[i - 2].getLocalName(), fileId, attr, childHandle);\r\n        }\r\n    } else {\r\n        entries = new READDIRPLUS3Response.EntryPlus3[n];\r\n        for (int i = 0; i < n; i++) {\r\n            long fileId = fstatus[i].getFileId();\r\n            FileHandle childHandle = new FileHandle(fileId, namenodeId);\r\n            Nfs3FileAttributes attr;\r\n            try {\r\n                attr = writeManager.getFileAttr(dfsClient, childHandle, iug);\r\n            } catch (IOException e) {\r\n                LOG.info(\"Can't get file attributes for fileId: {}\", fileId, e);\r\n                continue;\r\n            }\r\n            entries[i] = new READDIRPLUS3Response.EntryPlus3(fileId, fstatus[i].getLocalName(), fileId, attr, childHandle);\r\n        }\r\n    }\r\n    DirListPlus3 dirListPlus = new READDIRPLUS3Response.DirListPlus3(entries, eof);\r\n    return new READDIRPLUS3Response(Nfs3Status.NFS3_OK, postOpDirAttr, dirStatus.getModificationTime(), dirListPlus);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "fsstat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSSTAT3Response fsstat(XDR xdr, RpcInfo info)\n{\r\n    return fsstat(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "fsstat",
  "errType" : [ "IOException", "RemoteException", "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "FSSTAT3Response fsstat(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    FSSTAT3Response response = new FSSTAT3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    FSSTAT3Request request;\r\n    try {\r\n        request = FSSTAT3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid FSSTAT request\");\r\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS FSSTAT fileHandle: {} client: {}\", handle.dumpFileHandle(), remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    try {\r\n        FsStatus fsStatus = dfsClient.getDiskStatus();\r\n        long totalBytes = fsStatus.getCapacity();\r\n        long freeBytes = fsStatus.getRemaining();\r\n        Nfs3FileAttributes attrs = writeManager.getFileAttr(dfsClient, handle, iug);\r\n        if (attrs == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new FSSTAT3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        long maxFsObjects = config.getLong(\"dfs.max.objects\", 0);\r\n        if (maxFsObjects == 0) {\r\n            maxFsObjects = Integer.MAX_VALUE;\r\n        }\r\n        return new FSSTAT3Response(Nfs3Status.NFS3_OK, attrs, totalBytes, freeBytes, freeBytes, maxFsObjects, maxFsObjects, maxFsObjects, 0);\r\n    } catch (RemoteException r) {\r\n        LOG.warn(\"Exception\", r);\r\n        IOException io = r.unwrapRemoteException();\r\n        if (io instanceof AuthorizationException) {\r\n            return new FSSTAT3Response(Nfs3Status.NFS3ERR_ACCES);\r\n        } else {\r\n            return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new FSSTAT3Response(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "fsinfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSINFO3Response fsinfo(XDR xdr, RpcInfo info)\n{\r\n    return fsinfo(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "fsinfo",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "FSINFO3Response fsinfo(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    FSINFO3Response response = new FSINFO3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    FSINFO3Request request;\r\n    try {\r\n        request = FSINFO3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid FSINFO request\");\r\n        return new FSINFO3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS FSINFO fileHandle: {} client: {}\", remoteAddress, handle.dumpFileHandle(), remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    try {\r\n        int rtmax = config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY, NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);\r\n        int wtmax = config.getInt(NfsConfigKeys.DFS_NFS_MAX_WRITE_TRANSFER_SIZE_KEY, NfsConfigKeys.DFS_NFS_MAX_WRITE_TRANSFER_SIZE_DEFAULT);\r\n        int dtperf = config.getInt(NfsConfigKeys.DFS_NFS_MAX_READDIR_TRANSFER_SIZE_KEY, NfsConfigKeys.DFS_NFS_MAX_READDIR_TRANSFER_SIZE_DEFAULT);\r\n        Nfs3FileAttributes attrs = Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle), iug);\r\n        if (attrs == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new FSINFO3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        int fsProperty = Nfs3Constant.FSF3_CANSETTIME | Nfs3Constant.FSF3_HOMOGENEOUS;\r\n        return new FSINFO3Response(Nfs3Status.NFS3_OK, attrs, rtmax, rtmax, 1, wtmax, wtmax, 1, dtperf, Long.MAX_VALUE, new NfsTime(1), fsProperty);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new FSINFO3Response(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "pathconf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PATHCONF3Response pathconf(XDR xdr, RpcInfo info)\n{\r\n    return pathconf(xdr, getSecurityHandler(info), info.remoteAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "pathconf",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "PATHCONF3Response pathconf(XDR xdr, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    PATHCONF3Response response = new PATHCONF3Response(Nfs3Status.NFS3_OK);\r\n    if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_ONLY)) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_ACCES);\r\n        return response;\r\n    }\r\n    PATHCONF3Request request;\r\n    try {\r\n        request = PATHCONF3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid PATHCONF request\");\r\n        return new PATHCONF3Response(Nfs3Status.NFS3ERR_INVAL);\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    Nfs3FileAttributes attrs;\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS PATHCONF fileHandle: {} client: {}\", handle.dumpFileHandle(), remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    try {\r\n        attrs = Nfs3Utils.getFileAttr(dfsClient, Nfs3Utils.getFileIdPath(handle), iug);\r\n        if (attrs == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new PATHCONF3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        return new PATHCONF3Response(Nfs3Status.NFS3_OK, attrs, 0, HdfsServerConstants.MAX_PATH_LENGTH, true, false, false, true);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        int status = mapErrorStatus(e);\r\n        return new PATHCONF3Response(status);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "COMMIT3Response commit(XDR xdr, RpcInfo info)\n{\r\n    SecurityHandler securityHandler = getSecurityHandler(info);\r\n    RpcCall rpcCall = (RpcCall) info.header();\r\n    int xid = rpcCall.getXid();\r\n    SocketAddress remoteAddress = info.remoteAddress();\r\n    return commit(xdr, info.channel(), xid, securityHandler, remoteAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "commit",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "COMMIT3Response commit(XDR xdr, Channel channel, int xid, SecurityHandler securityHandler, SocketAddress remoteAddress)\n{\r\n    COMMIT3Response response = new COMMIT3Response(Nfs3Status.NFS3_OK);\r\n    COMMIT3Request request;\r\n    try {\r\n        request = COMMIT3Request.deserialize(xdr);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Invalid COMMIT request\");\r\n        response.setStatus(Nfs3Status.NFS3ERR_INVAL);\r\n        return response;\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    int namenodeId = handle.getNamenodeId();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"NFS COMMIT fileHandle: {} offset={} count={} client: {}\", handle.dumpFileHandle(), request.getOffset(), request.getCount(), remoteAddress);\r\n    }\r\n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser(), namenodeId);\r\n    if (dfsClient == null) {\r\n        response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\r\n        return response;\r\n    }\r\n    String fileIdPath = Nfs3Utils.getFileIdPath(handle);\r\n    Nfs3FileAttributes preOpAttr = null;\r\n    try {\r\n        preOpAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\r\n        if (preOpAttr == null) {\r\n            LOG.info(\"Can't get path for fileId: {}\", handle.getFileId());\r\n            return new COMMIT3Response(Nfs3Status.NFS3ERR_STALE);\r\n        }\r\n        if (!checkAccessPrivilege(remoteAddress, AccessPrivilege.READ_WRITE)) {\r\n            return new COMMIT3Response(Nfs3Status.NFS3ERR_ACCES, new WccData(Nfs3Utils.getWccAttr(preOpAttr), preOpAttr), Nfs3Constant.WRITE_COMMIT_VERF);\r\n        }\r\n        long commitOffset = (request.getCount() == 0) ? 0 : (request.getOffset() + request.getCount());\r\n        writeManager.handleCommit(dfsClient, handle, commitOffset, channel, xid, preOpAttr, namenodeId);\r\n        return null;\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception\", e);\r\n        Nfs3FileAttributes postOpAttr = null;\r\n        try {\r\n            postOpAttr = writeManager.getFileAttr(dfsClient, handle, iug);\r\n        } catch (IOException e1) {\r\n            LOG.info(\"Can't get postOpAttr for fileId: {}\", handle.getFileId(), e1);\r\n        }\r\n        WccData fileWcc = new WccData(Nfs3Utils.getWccAttr(preOpAttr), postOpAttr);\r\n        int status = mapErrorStatus(e);\r\n        return new COMMIT3Response(status, fileWcc, Nfs3Constant.WRITE_COMMIT_VERF);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getSecurityHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SecurityHandler getSecurityHandler(Credentials credentials, Verifier verifier)\n{\r\n    if (credentials instanceof CredentialsSys) {\r\n        return new SysSecurityHandler((CredentialsSys) credentials, iug);\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getSecurityHandler",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SecurityHandler getSecurityHandler(RpcInfo info)\n{\r\n    RpcCall rpcCall = (RpcCall) info.header();\r\n    return getSecurityHandler(rpcCall.getCredential(), rpcCall.getVerifier());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "handleInternal",
  "errType" : null,
  "containingMethodsNum" : 73,
  "sourceCodeText" : "void handleInternal(ChannelHandlerContext ctx, RpcInfo info)\n{\r\n    RpcCall rpcCall = (RpcCall) info.header();\r\n    final NFSPROC3 nfsproc3 = NFSPROC3.fromValue(rpcCall.getProcedure());\r\n    int xid = rpcCall.getXid();\r\n    byte[] data = new byte[info.data().readableBytes()];\r\n    info.data().readBytes(data);\r\n    XDR xdr = new XDR(data);\r\n    XDR out = new XDR();\r\n    InetAddress client = ((InetSocketAddress) info.remoteAddress()).getAddress();\r\n    Credentials credentials = rpcCall.getCredential();\r\n    if (nfsproc3 != NFSPROC3.NULL) {\r\n        if (credentials.getFlavor() != AuthFlavor.AUTH_SYS && credentials.getFlavor() != AuthFlavor.RPCSEC_GSS) {\r\n            LOG.info(\"Wrong RPC AUTH flavor, {} is not AUTH_SYS or RPCSEC_GSS.\", credentials.getFlavor());\r\n            XDR reply = new XDR();\r\n            RpcDeniedReply rdr = new RpcDeniedReply(xid, RpcReply.ReplyState.MSG_ACCEPTED, RpcDeniedReply.RejectState.AUTH_ERROR, new VerifierNone());\r\n            rdr.write(reply);\r\n            ByteBuf buf = Unpooled.wrappedBuffer(reply.asReadOnlyWrap().buffer());\r\n            RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());\r\n            RpcUtil.sendRpcResponse(ctx, rsp);\r\n            return;\r\n        }\r\n    }\r\n    if (!isIdempotent(rpcCall)) {\r\n        RpcCallCache.CacheEntry entry = rpcCallCache.checkOrAddToCache(client, xid);\r\n        if (entry != null) {\r\n            if (entry.isCompleted()) {\r\n                LOG.info(\"Sending the cached reply to retransmitted request {}\", xid);\r\n                RpcUtil.sendRpcResponse(ctx, entry.getResponse());\r\n                return;\r\n            } else {\r\n                LOG.info(\"Retransmitted request, transaction still in progress {}\", xid);\r\n                return;\r\n            }\r\n        }\r\n    }\r\n    final long startTime = System.nanoTime();\r\n    NFS3Response response = null;\r\n    if (nfsproc3 == NFSPROC3.NULL) {\r\n        response = nullProcedure();\r\n    } else if (nfsproc3 == NFSPROC3.GETATTR) {\r\n        response = getattr(xdr, info);\r\n        metrics.addGetattr(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.SETATTR) {\r\n        response = setattr(xdr, info);\r\n        metrics.addSetattr(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.LOOKUP) {\r\n        response = lookup(xdr, info);\r\n        metrics.addLookup(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.ACCESS) {\r\n        response = access(xdr, info);\r\n        metrics.addAccess(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.READLINK) {\r\n        response = readlink(xdr, info);\r\n        metrics.addReadlink(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.READ) {\r\n        LOG.debug(\"{}{}\", Nfs3Utils.READ_RPC_START, xid);\r\n        response = read(xdr, info);\r\n        LOG.debug(\"{}{}\", Nfs3Utils.READ_RPC_END, xid);\r\n        metrics.addRead(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.WRITE) {\r\n        LOG.debug(\"{}{}\", Nfs3Utils.WRITE_RPC_START, xid);\r\n        response = write(xdr, info);\r\n    } else if (nfsproc3 == NFSPROC3.CREATE) {\r\n        response = create(xdr, info);\r\n        metrics.addCreate(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.MKDIR) {\r\n        response = mkdir(xdr, info);\r\n        metrics.addMkdir(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.SYMLINK) {\r\n        response = symlink(xdr, info);\r\n        metrics.addSymlink(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.MKNOD) {\r\n        response = mknod(xdr, info);\r\n        metrics.addMknod(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.REMOVE) {\r\n        response = remove(xdr, info);\r\n        metrics.addRemove(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.RMDIR) {\r\n        response = rmdir(xdr, info);\r\n        metrics.addRmdir(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.RENAME) {\r\n        response = rename(xdr, info);\r\n        metrics.addRename(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.LINK) {\r\n        response = link(xdr, info);\r\n        metrics.addLink(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.READDIR) {\r\n        response = readdir(xdr, info);\r\n        metrics.addReaddir(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.READDIRPLUS) {\r\n        response = readdirplus(xdr, info);\r\n        metrics.addReaddirplus(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.FSSTAT) {\r\n        response = fsstat(xdr, info);\r\n        metrics.addFsstat(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.FSINFO) {\r\n        response = fsinfo(xdr, info);\r\n        metrics.addFsinfo(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.PATHCONF) {\r\n        response = pathconf(xdr, info);\r\n        metrics.addPathconf(Nfs3Utils.getElapsedTime(startTime));\r\n    } else if (nfsproc3 == NFSPROC3.COMMIT) {\r\n        response = commit(xdr, info);\r\n    } else {\r\n        RpcAcceptedReply.getInstance(xid, RpcAcceptedReply.AcceptState.PROC_UNAVAIL, new VerifierNone()).write(out);\r\n    }\r\n    if (response == null) {\r\n        LOG.debug(\"No sync response, expect an async response for request XID={}\", rpcCall.getXid());\r\n        return;\r\n    }\r\n    out = response.serialize(out, xid, new VerifierNone());\r\n    ByteBuf buf = Unpooled.wrappedBuffer(out.asReadOnlyWrap().buffer());\r\n    RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());\r\n    if (!isIdempotent(rpcCall)) {\r\n        rpcCallCache.callCompleted(client, xid, rsp);\r\n    }\r\n    RpcUtil.sendRpcResponse(ctx, rsp);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "isIdempotent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isIdempotent(RpcCall call)\n{\r\n    final NFSPROC3 nfsproc3 = NFSPROC3.fromValue(call.getProcedure());\r\n    return nfsproc3 == null || nfsproc3.isIdempotent();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "checkAccessPrivilege",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean checkAccessPrivilege(SocketAddress remoteAddress, final AccessPrivilege expected)\n{\r\n    if (!doPortMonitoring(remoteAddress)) {\r\n        return false;\r\n    }\r\n    if (exports == null) {\r\n        return false;\r\n    }\r\n    InetAddress client = ((InetSocketAddress) remoteAddress).getAddress();\r\n    AccessPrivilege access = exports.getAccessPrivilege(client);\r\n    if (access == AccessPrivilege.NONE) {\r\n        return false;\r\n    }\r\n    if (access == AccessPrivilege.READ_ONLY && expected == AccessPrivilege.READ_WRITE) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getWriteManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WriteManager getWriteManager()\n{\r\n    return this.writeManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Nfs3Metrics create(Configuration conf, String gatewayName)\n{\r\n    String sessionId = conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY);\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    JvmMetrics jm = JvmMetrics.create(gatewayName, sessionId, ms);\r\n    int[] intervals = conf.getInts(NfsConfigKeys.NFS_METRICS_PERCENTILES_INTERVALS_KEY);\r\n    return ms.register(new Nfs3Metrics(gatewayName, sessionId, intervals, jm));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "name",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String name()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getJvmMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JvmMetrics getJvmMetrics()\n{\r\n    return jvmMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "incrBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrBytesWritten(long bytes)\n{\r\n    bytesWritten.incr(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "incrBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrBytesRead(long bytes)\n{\r\n    bytesRead.incr(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addGetattr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addGetattr(long latencyNanos)\n{\r\n    getattr.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addSetattr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addSetattr(long latencyNanos)\n{\r\n    setattr.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addLookup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addLookup(long latencyNanos)\n{\r\n    lookup.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addAccess(long latencyNanos)\n{\r\n    access.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addReadlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addReadlink(long latencyNanos)\n{\r\n    readlink.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addRead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addRead(long latencyNanos)\n{\r\n    read.add(latencyNanos);\r\n    for (MutableQuantiles q : readNanosQuantiles) {\r\n        q.add(latencyNanos);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addWrite",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addWrite(long latencyNanos)\n{\r\n    write.add(latencyNanos);\r\n    for (MutableQuantiles q : writeNanosQuantiles) {\r\n        q.add(latencyNanos);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addCreate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCreate(long latencyNanos)\n{\r\n    create.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addMkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addMkdir(long latencyNanos)\n{\r\n    mkdir.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addSymlink(long latencyNanos)\n{\r\n    symlink.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addMknod",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addMknod(long latencyNanos)\n{\r\n    mknod.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addRemove",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRemove(long latencyNanos)\n{\r\n    remove.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addRmdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRmdir(long latencyNanos)\n{\r\n    rmdir.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addRename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRename(long latencyNanos)\n{\r\n    rename.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addLink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addLink(long latencyNanos)\n{\r\n    link.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addReaddir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addReaddir(long latencyNanos)\n{\r\n    readdir.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addReaddirplus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addReaddirplus(long latencyNanos)\n{\r\n    readdirplus.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addFsstat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFsstat(long latencyNanos)\n{\r\n    fsstat.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addFsinfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFsinfo(long latencyNanos)\n{\r\n    fsinfo.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addPathconf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addPathconf(long latencyNanos)\n{\r\n    pathconf.add(latencyNanos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addCommit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addCommit(long latencyNanos)\n{\r\n    commit.add(latencyNanos);\r\n    for (MutableQuantiles q : commitNanosQuantiles) {\r\n        q.add(latencyNanos);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\conf",
  "methodName" : "addDeprecatedKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDeprecatedKeys()\n{\r\n    Configuration.addDeprecations(new DeprecationDelta[] { new DeprecationDelta(\"nfs3.server.port\", NfsConfigKeys.DFS_NFS_SERVER_PORT_KEY), new DeprecationDelta(\"nfs3.mountd.port\", NfsConfigKeys.DFS_NFS_MOUNTD_PORT_KEY), new DeprecationDelta(\"dfs.nfs.exports.cache.size\", Nfs3Constant.NFS_EXPORTS_CACHE_SIZE_KEY), new DeprecationDelta(\"dfs.nfs.exports.cache.expirytime.millis\", Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY), new DeprecationDelta(\"hadoop.nfs.userupdate.milly\", IdMappingConstant.USERGROUPID_UPDATE_MILLIS_KEY), new DeprecationDelta(\"nfs.usergroup.update.millis\", IdMappingConstant.USERGROUPID_UPDATE_MILLIS_KEY), new DeprecationDelta(\"nfs.static.mapping.file\", IdMappingConstant.STATIC_ID_MAPPING_FILE_KEY), new DeprecationDelta(\"dfs.nfs3.enableDump\", NfsConfigKeys.DFS_NFS_FILE_DUMP_KEY), new DeprecationDelta(\"dfs.nfs3.dump.dir\", NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_KEY), new DeprecationDelta(\"dfs.nfs3.max.open.files\", NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY), new DeprecationDelta(\"dfs.nfs3.stream.timeout\", NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_KEY), new DeprecationDelta(\"dfs.nfs3.export.point\", NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY), new DeprecationDelta(\"nfs.allow.insecure.ports\", NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_KEY), new DeprecationDelta(\"dfs.nfs.keytab.file\", NfsConfigKeys.DFS_NFS_KEYTAB_FILE_KEY), new DeprecationDelta(\"dfs.nfs.kerberos.principal\", NfsConfigKeys.DFS_NFS_KERBEROS_PRINCIPAL_KEY), new DeprecationDelta(\"dfs.nfs.rtmax\", NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY), new DeprecationDelta(\"dfs.nfs.wtmax\", NfsConfigKeys.DFS_NFS_MAX_WRITE_TRANSFER_SIZE_KEY), new DeprecationDelta(\"dfs.nfs.dtmax\", NfsConfigKeys.DFS_NFS_MAX_READDIR_TRANSFER_SIZE_KEY) });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "updateLastAccessTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateLastAccessTime()\n{\r\n    lastAccessTime = Time.monotonicNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "checkStreamTimeout",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean checkStreamTimeout(long streamTimeout)\n{\r\n    return Time.monotonicNow() - lastAccessTime > streamTimeout;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getLastAccessTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastAccessTime()\n{\r\n    return lastAccessTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getNextOffset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNextOffset()\n{\r\n    return nextOffset.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getActiveState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getActiveState()\n{\r\n    return this.activeState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "hasPendingWork",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasPendingWork()\n{\r\n    return (pendingWrites.size() != 0 || pendingCommits.size() != 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "updateNonSequentialWriteInMemory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long updateNonSequentialWriteInMemory(long count)\n{\r\n    long newValue = nonSequentialWriteInMemory.addAndGet(count);\r\n    LOG.debug(\"Update nonSequentialWriteInMemory by {} new value: {}\", count, newValue);\r\n    Preconditions.checkState(newValue >= 0, \"nonSequentialWriteInMemory is negative \" + newValue + \" after update with count \" + count);\r\n    return newValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getLatestAttr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Nfs3FileAttributes getLatestAttr()\n{\r\n    return latestAttr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFlushedOffset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFlushedOffset()\n{\r\n    return fos.getPos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "waitForDump",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void waitForDump()\n{\r\n    if (!enabledDump) {\r\n        LOG.debug(\"Do nothing, dump is disabled.\");\r\n        return;\r\n    }\r\n    if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {\r\n        return;\r\n    }\r\n    synchronized (this) {\r\n        if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {\r\n            LOG.debug(\"Asking dumper to dump...\");\r\n            if (dumpThread == null) {\r\n                dumpThread = new Daemon(new Dumper());\r\n                dumpThread.start();\r\n            } else {\r\n                this.notifyAll();\r\n            }\r\n        }\r\n        while (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {\r\n            try {\r\n                this.wait();\r\n            } catch (InterruptedException ignored) {\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "checkRepeatedWriteRequest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "WriteCtx checkRepeatedWriteRequest(WRITE3Request request, Channel channel, int xid)\n{\r\n    OffsetRange range = new OffsetRange(request.getOffset(), request.getOffset() + request.getCount());\r\n    WriteCtx writeCtx = pendingWrites.get(range);\r\n    if (writeCtx == null) {\r\n        return null;\r\n    } else {\r\n        if (xid != writeCtx.getXid()) {\r\n            LOG.warn(\"Got a repeated request, same range, with a different xid: \" + \"{} xid in old request: {}\", xid, writeCtx.getXid());\r\n        }\r\n        return writeCtx;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "receivedNewWrite",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void receivedNewWrite(DFSClient dfsClient, WRITE3Request request, Channel channel, int xid, AsyncDataService asyncDataService, IdMappingServiceProvider iug)\n{\r\n    if (!activeState) {\r\n        LOG.info(\"OpenFileCtx is inactive, fileId: {}\", request.getHandle().dumpFileHandle());\r\n        WccData fileWcc = new WccData(latestAttr.getWccAttr(), latestAttr);\r\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO, fileWcc, 0, request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\r\n        Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n    } else {\r\n        updateLastAccessTime();\r\n        WriteCtx existantWriteCtx = checkRepeatedWriteRequest(request, channel, xid);\r\n        if (existantWriteCtx != null) {\r\n            if (!existantWriteCtx.getReplied()) {\r\n                LOG.debug(\"Repeated write request which hasn't been served: \" + \"xid={}, drop it.\", xid);\r\n            } else {\r\n                LOG.debug(\"Repeated write request which is already served: xid={}\" + \", resend response.\", xid);\r\n                WccData fileWcc = new WccData(latestAttr.getWccAttr(), latestAttr);\r\n                WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK, fileWcc, request.getCount(), request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\r\n                Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n            }\r\n        } else {\r\n            receivedNewWriteInternal(dfsClient, request, channel, xid, asyncDataService, iug);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "alterWriteRequest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void alterWriteRequest(WRITE3Request request, long cachedOffset)\n{\r\n    long offset = request.getOffset();\r\n    int count = request.getCount();\r\n    long smallerCount = offset + count - cachedOffset;\r\n    LOG.debug(\"Got overwrite with appended data [{}-{}),\" + \" current offset {},\" + \" drop the overlapped section [{}-{})\" + \" and append new data [{}-{}).\", offset, (offset + count), cachedOffset, offset, cachedOffset, cachedOffset, (offset + count));\r\n    ByteBuffer data = request.getData();\r\n    Preconditions.checkState(data.position() == 0, \"The write request data has non-zero position\");\r\n    data.position((int) (cachedOffset - offset));\r\n    Preconditions.checkState(data.limit() - data.position() == smallerCount, \"The write request buffer has wrong limit/position regarding count\");\r\n    request.setOffset(cachedOffset);\r\n    request.setCount((int) smallerCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "trimWriteRequest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void trimWriteRequest(WriteCtx writeCtx, long currentOffset)\n{\r\n    long offset = writeCtx.getOffset();\r\n    if (LOG.isDebugEnabled()) {\r\n        int count = writeCtx.getCount();\r\n        LOG.debug(String.format(\"Trim request [%d-%d),\" + \" current offset %d,\" + \" drop the overlapped section [%d-%d)\" + \" and write new data [%d-%d)\", offset, (offset + count), currentOffset, offset, (currentOffset), currentOffset, (offset + count)));\r\n    }\r\n    writeCtx.trimWrite((int) (currentOffset - offset));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addWritesToCache",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "WriteCtx addWritesToCache(WRITE3Request request, Channel channel, int xid)\n{\r\n    long offset = request.getOffset();\r\n    int count = request.getCount();\r\n    long cachedOffset = nextOffset.get();\r\n    int originalCount = WriteCtx.INVALID_ORIGINAL_COUNT;\r\n    LOG.debug(\"requested offset={} and current offset={}\", offset, cachedOffset);\r\n    if (offset + count <= cachedOffset) {\r\n        LOG.warn(String.format(\"Got overwrite [%d-%d) smaller than\" + \" current offset %d,\" + \" drop the request.\", offset, (offset + count), cachedOffset));\r\n        return null;\r\n    }\r\n    if ((offset < cachedOffset) && (offset + count > cachedOffset)) {\r\n        LOG.warn(String.format(\"Got overwrite with appended data [%d-%d),\" + \" current offset %d,\" + \" drop the overlapped section [%d-%d)\" + \" and append new data [%d-%d).\", offset, (offset + count), cachedOffset, offset, cachedOffset, cachedOffset, (offset + count)));\r\n        LOG.warn(\"Modify this write to write only the appended data\");\r\n        alterWriteRequest(request, cachedOffset);\r\n        originalCount = count;\r\n        offset = request.getOffset();\r\n        count = request.getCount();\r\n    }\r\n    if (offset < cachedOffset) {\r\n        LOG.warn(\"(offset,count,nextOffset): ({},{},{})\", offset, count, nextOffset);\r\n        return null;\r\n    } else {\r\n        DataState dataState = offset == cachedOffset ? WriteCtx.DataState.NO_DUMP : WriteCtx.DataState.ALLOW_DUMP;\r\n        WriteCtx writeCtx = new WriteCtx(request.getHandle(), request.getOffset(), request.getCount(), originalCount, request.getStableHow(), request.getData(), channel, xid, false, dataState);\r\n        LOG.debug(\"Add new write to the list with nextOffset {}\" + \" and requested offset={}\", cachedOffset, offset);\r\n        if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\r\n            updateNonSequentialWriteInMemory(count);\r\n        }\r\n        WriteCtx oldWriteCtx = checkRepeatedWriteRequest(request, channel, xid);\r\n        if (oldWriteCtx == null) {\r\n            pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\r\n            LOG.debug(\"New write buffered with xid {} nextOffset {}\" + \"req offset={} mapsize={}\", xid, cachedOffset, offset, pendingWrites.size());\r\n        } else {\r\n            LOG.warn(\"Got a repeated request, same range, with xid: \" + \"{} nextOffset {} req offset={}\", xid, cachedOffset, offset);\r\n        }\r\n        return writeCtx;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "processOverWrite",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void processOverWrite(DFSClient dfsClient, WRITE3Request request, Channel channel, int xid, IdMappingServiceProvider iug)\n{\r\n    WccData wccData = new WccData(latestAttr.getWccAttr(), null);\r\n    long offset = request.getOffset();\r\n    int count = request.getCount();\r\n    WriteStableHow stableHow = request.getStableHow();\r\n    WRITE3Response response;\r\n    long cachedOffset = nextOffset.get();\r\n    if (offset + count > cachedOffset) {\r\n        LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\r\n        response = new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0, WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\r\n    } else {\r\n        LOG.debug(\"Process perfectOverWrite\");\r\n        response = processPerfectOverWrite(dfsClient, offset, count, stableHow, request.getData().array(), Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\r\n    }\r\n    updateLastAccessTime();\r\n    Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "checkAndStartWrite",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean checkAndStartWrite(AsyncDataService asyncDataService, WriteCtx writeCtx)\n{\r\n    if (writeCtx.getOffset() == nextOffset.get()) {\r\n        if (!asyncStatus) {\r\n            LOG.debug(\"Trigger the write back task. Current nextOffset: {}\", nextOffset.get());\r\n            asyncStatus = true;\r\n            asyncWriteBackStartOffset = writeCtx.getOffset();\r\n            asyncDataService.execute(new AsyncDataService.WriteBackTask(this));\r\n        } else {\r\n            LOG.debug(\"The write back thread is working.\");\r\n        }\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "receivedNewWriteInternal",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void receivedNewWriteInternal(DFSClient dfsClient, WRITE3Request request, Channel channel, int xid, AsyncDataService asyncDataService, IdMappingServiceProvider iug)\n{\r\n    WriteStableHow stableHow = request.getStableHow();\r\n    WccAttr preOpAttr = latestAttr.getWccAttr();\r\n    int count = request.getCount();\r\n    WriteCtx writeCtx = addWritesToCache(request, channel, xid);\r\n    if (writeCtx == null) {\r\n        processOverWrite(dfsClient, request, channel, xid, iug);\r\n    } else {\r\n        boolean startWriting = checkAndStartWrite(asyncDataService, writeCtx);\r\n        if (!startWriting) {\r\n            waitForDump();\r\n            if (stableHow != WriteStableHow.UNSTABLE) {\r\n                LOG.info(\"Have to change stable write to unstable write: {}\", request.getStableHow());\r\n                stableHow = WriteStableHow.UNSTABLE;\r\n            }\r\n            LOG.debug(\"UNSTABLE write request, send response for offset: {}\", writeCtx.getOffset());\r\n            WccData fileWcc = new WccData(preOpAttr, latestAttr);\r\n            WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK, fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n            RpcProgramNfs3.metrics.addWrite(Nfs3Utils.getElapsedTime(writeCtx.startTime));\r\n            Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n            writeCtx.setReplied(true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "processPerfectOverWrite",
  "errType" : [ "ClosedChannelException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "WRITE3Response processPerfectOverWrite(DFSClient dfsClient, long offset, int count, WriteStableHow stableHow, byte[] data, String path, WccData wccData, IdMappingServiceProvider iug)\n{\r\n    WRITE3Response response;\r\n    byte[] readbuffer = new byte[count];\r\n    int readCount = 0;\r\n    FSDataInputStream fis = null;\r\n    try {\r\n        fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\r\n    } catch (ClosedChannelException closedException) {\r\n        LOG.info(\"The FSDataOutputStream has been closed. \" + \"Continue processing the perfect overwrite.\");\r\n    } catch (IOException e) {\r\n        LOG.info(\"hsync failed when processing possible perfect overwrite, \" + \"path={} error: {}\", path, e.toString());\r\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n    }\r\n    try {\r\n        fis = dfsClient.createWrappedInputStream(dfsClient.open(path));\r\n        readCount = fis.read(offset, readbuffer, 0, count);\r\n        if (readCount < count) {\r\n            LOG.error(\"Can't read back {} bytes, partial read size: {}\", count, readCount);\r\n            return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.info(\"Read failed when processing possible perfect overwrite, \" + \"path={}\", path, e);\r\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, fis);\r\n    }\r\n    Comparator comparator = new Comparator();\r\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) != 0) {\r\n        LOG.info(\"Perfect overwrite has different content\");\r\n        response = new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n    } else {\r\n        LOG.info(\"Perfect overwrite has same content,\" + \" updating the mtime, then return success\");\r\n        Nfs3FileAttributes postOpAttr = null;\r\n        try {\r\n            dfsClient.setTimes(path, Time.monotonicNow(), -1);\r\n            postOpAttr = Nfs3Utils.getFileAttr(dfsClient, path, iug);\r\n        } catch (IOException e) {\r\n            LOG.info(\"Got error when processing perfect overwrite, path={} \" + \"error: {}\", path, e.toString());\r\n            return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n        }\r\n        wccData.setPostOpAttr(postOpAttr);\r\n        response = new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "checkCommit",
  "errType" : [ "ClosedChannelException", "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "COMMIT_STATUS checkCommit(DFSClient dfsClient, long commitOffset, Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead)\n{\r\n    if (!fromRead) {\r\n        Preconditions.checkState(channel != null && preOpAttr != null);\r\n        updateLastAccessTime();\r\n    }\r\n    Preconditions.checkState(commitOffset >= 0);\r\n    COMMIT_STATUS ret = checkCommitInternal(commitOffset, channel, xid, preOpAttr, fromRead);\r\n    LOG.debug(\"Got commit status: {}\", ret.name());\r\n    if (ret == COMMIT_STATUS.COMMIT_DO_SYNC || ret == COMMIT_STATUS.COMMIT_FINISHED) {\r\n        try {\r\n            fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\r\n            ret = COMMIT_STATUS.COMMIT_FINISHED;\r\n        } catch (ClosedChannelException cce) {\r\n            if (pendingWrites.isEmpty()) {\r\n                ret = COMMIT_STATUS.COMMIT_FINISHED;\r\n            } else {\r\n                ret = COMMIT_STATUS.COMMIT_ERROR;\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Got stream error during data sync\", e);\r\n            ret = COMMIT_STATUS.COMMIT_ERROR;\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "checkSequential",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean checkSequential(final long commitOffset, final long nextOffset)\n{\r\n    Preconditions.checkState(commitOffset >= nextOffset, \"commitOffset \" + commitOffset + \" less than nextOffset \" + nextOffset);\r\n    long offset = nextOffset;\r\n    Iterator<OffsetRange> it = pendingWrites.descendingKeySet().iterator();\r\n    while (it.hasNext()) {\r\n        OffsetRange range = it.next();\r\n        if (range.getMin() != offset) {\r\n            return false;\r\n        }\r\n        offset = range.getMax();\r\n        if (offset > commitOffset) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "handleSpecialWait",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "COMMIT_STATUS handleSpecialWait(boolean fromRead, long commitOffset, Channel channel, int xid, Nfs3FileAttributes preOpAttr)\n{\r\n    if (!fromRead) {\r\n        CommitCtx commitCtx = new CommitCtx(commitOffset, channel, xid, preOpAttr);\r\n        pendingCommits.put(commitOffset, commitCtx);\r\n    }\r\n    LOG.debug(\"return COMMIT_SPECIAL_WAIT\");\r\n    return COMMIT_STATUS.COMMIT_SPECIAL_WAIT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "checkCommitInternal",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "COMMIT_STATUS checkCommitInternal(long commitOffset, Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead)\n{\r\n    if (!activeState) {\r\n        if (pendingWrites.isEmpty()) {\r\n            return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\r\n        } else {\r\n            return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\r\n        }\r\n    }\r\n    long flushed = getFlushedOffset();\r\n    LOG.debug(\"getFlushedOffset={} commitOffset={} nextOffset={}\", flushed, commitOffset, nextOffset.get());\r\n    if (pendingWrites.isEmpty()) {\r\n        if (aixCompatMode) {\r\n            return COMMIT_STATUS.COMMIT_FINISHED;\r\n        } else {\r\n            if (flushed < nextOffset.get()) {\r\n                LOG.debug(\"get commit while still writing to the requested offset,\" + \" with empty queue\");\r\n                return handleSpecialWait(fromRead, nextOffset.get(), channel, xid, preOpAttr);\r\n            } else {\r\n                return COMMIT_STATUS.COMMIT_FINISHED;\r\n            }\r\n        }\r\n    }\r\n    Preconditions.checkState(flushed <= nextOffset.get(), \"flushed \" + flushed + \" is larger than nextOffset \" + nextOffset.get());\r\n    if (uploadLargeFile && !aixCompatMode) {\r\n        long co = (commitOffset > 0) ? commitOffset : pendingWrites.firstEntry().getKey().getMax() - 1;\r\n        if (co <= flushed) {\r\n            return COMMIT_STATUS.COMMIT_DO_SYNC;\r\n        } else if (co < nextOffset.get()) {\r\n            LOG.debug(\"get commit while still writing to the requested offset\");\r\n            return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\r\n        } else {\r\n            if (checkSequential(co, nextOffset.get())) {\r\n                return handleSpecialWait(fromRead, co, channel, xid, preOpAttr);\r\n            } else {\r\n                LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\r\n                return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\r\n            }\r\n        }\r\n    }\r\n    if (commitOffset > 0) {\r\n        if (aixCompatMode) {\r\n            if (commitOffset <= flushed) {\r\n                return COMMIT_STATUS.COMMIT_DO_SYNC;\r\n            }\r\n        } else {\r\n            if (commitOffset > flushed) {\r\n                if (!fromRead) {\r\n                    CommitCtx commitCtx = new CommitCtx(commitOffset, channel, xid, preOpAttr);\r\n                    pendingCommits.put(commitOffset, commitCtx);\r\n                }\r\n                return COMMIT_STATUS.COMMIT_WAIT;\r\n            } else {\r\n                return COMMIT_STATUS.COMMIT_DO_SYNC;\r\n            }\r\n        }\r\n    }\r\n    Entry<OffsetRange, WriteCtx> key = pendingWrites.firstEntry();\r\n    if (!fromRead) {\r\n        long maxOffset = key.getKey().getMax() - 1;\r\n        Preconditions.checkState(maxOffset > 0);\r\n        CommitCtx commitCtx = new CommitCtx(maxOffset, channel, xid, preOpAttr);\r\n        pendingCommits.put(maxOffset, commitCtx);\r\n    }\r\n    return COMMIT_STATUS.COMMIT_WAIT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "streamCleanup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean streamCleanup(FileHandle handle, long streamTimeout)\n{\r\n    Preconditions.checkState(streamTimeout >= NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);\r\n    if (!activeState) {\r\n        return true;\r\n    }\r\n    boolean flag = false;\r\n    if (checkStreamTimeout(streamTimeout)) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"stream can be closed for fileId: {}\", handle.dumpFileHandle());\r\n        }\r\n        flag = true;\r\n    }\r\n    return flag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "offerNextToWrite",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "WriteCtx offerNextToWrite()\n{\r\n    if (pendingWrites.isEmpty()) {\r\n        LOG.debug(\"The async write task has no pending writes, fileId: {}\", latestAttr.getFileId());\r\n        processCommits(nextOffset.get());\r\n        this.asyncStatus = false;\r\n        return null;\r\n    }\r\n    Entry<OffsetRange, WriteCtx> lastEntry = pendingWrites.lastEntry();\r\n    OffsetRange range = lastEntry.getKey();\r\n    WriteCtx toWrite = lastEntry.getValue();\r\n    LOG.trace(\"range.getMin()={} nextOffset={}\", range.getMin(), nextOffset);\r\n    long offset = nextOffset.get();\r\n    if (range.getMin() > offset) {\r\n        LOG.debug(\"The next sequential write has not arrived yet\");\r\n        processCommits(nextOffset.get());\r\n        this.asyncStatus = false;\r\n    } else if (range.getMax() <= offset) {\r\n        LOG.debug(\"Remove write {} which is already written from the list\", range);\r\n        pendingWrites.remove(range);\r\n    } else if (range.getMin() < offset && range.getMax() > offset) {\r\n        LOG.warn(\"Got an overlapping write {}, nextOffset={}. \" + \"Remove and trim it\", range, offset);\r\n        pendingWrites.remove(range);\r\n        trimWriteRequest(toWrite, offset);\r\n        nextOffset.addAndGet(toWrite.getCount());\r\n        LOG.debug(\"Change nextOffset (after trim) to {}\", nextOffset.get());\r\n        return toWrite;\r\n    } else {\r\n        LOG.debug(\"Remove write {} from the list\", range);\r\n        pendingWrites.remove(range);\r\n        nextOffset.addAndGet(toWrite.getCount());\r\n        LOG.debug(\"Change nextOffset to {}\", nextOffset.get());\r\n        return toWrite;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "executeWriteBack",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void executeWriteBack()\n{\r\n    Preconditions.checkState(asyncStatus, \"openFileCtx has false asyncStatus, fileId: \" + latestAttr.getFileId());\r\n    final long startOffset = asyncWriteBackStartOffset;\r\n    try {\r\n        while (activeState) {\r\n            WriteCtx toWrite = offerNextToWrite();\r\n            if (toWrite != null) {\r\n                doSingleWrite(toWrite);\r\n                updateLastAccessTime();\r\n            } else {\r\n                break;\r\n            }\r\n        }\r\n        if (!activeState) {\r\n            LOG.debug(\"The openFileCtx is not active anymore, fileId: {}\", latestAttr.getFileId());\r\n        }\r\n    } finally {\r\n        synchronized (this) {\r\n            if (startOffset == asyncWriteBackStartOffset) {\r\n                asyncStatus = false;\r\n            } else {\r\n                LOG.info(\"Another async task is already started before this one \" + \"is finalized. fileId: {} asyncStatus: {} \" + \"original startOffset: {} \" + \"new startOffset: {}. Won't change asyncStatus here.\", latestAttr.getFileId(), asyncStatus, startOffset, asyncWriteBackStartOffset);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "processCommits",
  "errType" : [ "ClosedChannelException", "IOException", "IOException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void processCommits(long offset)\n{\r\n    Preconditions.checkState(offset > 0);\r\n    long flushedOffset = getFlushedOffset();\r\n    Entry<Long, CommitCtx> entry = pendingCommits.firstEntry();\r\n    if (entry == null || entry.getValue().offset > flushedOffset) {\r\n        return;\r\n    }\r\n    int status = Nfs3Status.NFS3ERR_IO;\r\n    try {\r\n        fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\r\n        status = Nfs3Status.NFS3_OK;\r\n    } catch (ClosedChannelException cce) {\r\n        if (!pendingWrites.isEmpty()) {\r\n            LOG.error(\"Can't sync for fileId: {}. \" + \"Channel closed with writes pending\", latestAttr.getFileId(), cce);\r\n        }\r\n        status = Nfs3Status.NFS3ERR_IO;\r\n    } catch (IOException e) {\r\n        LOG.error(\"Got stream error during data sync: \", e);\r\n        status = Nfs3Status.NFS3ERR_IO;\r\n    }\r\n    try {\r\n        latestAttr = Nfs3Utils.getFileAttr(client, Nfs3Utils.getFileIdPath(latestAttr.getFileId()), iug);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Can't get new file attr, fileId: \" + latestAttr.getFileId(), e);\r\n        status = Nfs3Status.NFS3ERR_IO;\r\n    }\r\n    if (latestAttr.getSize() != offset) {\r\n        LOG.error(\"After sync, the expect file size: {}, \" + \"however actual file size is: {}\", offset, latestAttr.getSize());\r\n        status = Nfs3Status.NFS3ERR_IO;\r\n    }\r\n    WccData wccData = new WccData(Nfs3Utils.getWccAttr(latestAttr), latestAttr);\r\n    while (entry != null && entry.getValue().offset <= flushedOffset) {\r\n        pendingCommits.remove(entry.getKey());\r\n        CommitCtx commit = entry.getValue();\r\n        COMMIT3Response response = new COMMIT3Response(status, wccData, Nfs3Constant.WRITE_COMMIT_VERF);\r\n        RpcProgramNfs3.metrics.addCommit(Nfs3Utils.getElapsedTime(commit.startTime));\r\n        Nfs3Utils.writeChannelCommit(commit.getChannel(), response.serialize(new XDR(), commit.getXid(), new VerifierNone()), commit.getXid());\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"FileId: {} Service time: {}ns. \" + \"Sent response for commit: {}\", latestAttr.getFileId(), Nfs3Utils.getElapsedTime(commit.startTime), commit);\r\n        }\r\n        entry = pendingCommits.firstEntry();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "doSingleWrite",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void doSingleWrite(final WriteCtx writeCtx)\n{\r\n    Channel channel = writeCtx.getChannel();\r\n    int xid = writeCtx.getXid();\r\n    long offset = writeCtx.getOffset();\r\n    int count = writeCtx.getCount();\r\n    WriteStableHow stableHow = writeCtx.getStableHow();\r\n    FileHandle handle = writeCtx.getHandle();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"do write, fileHandle {} offset: {} length: {} stableHow: {}\", handle.dumpFileHandle(), offset, count, stableHow.name());\r\n    }\r\n    try {\r\n        writeCtx.writeData(fos);\r\n        RpcProgramNfs3.metrics.incrBytesWritten(writeCtx.getCount());\r\n        long flushedOffset = getFlushedOffset();\r\n        if (flushedOffset != (offset + count)) {\r\n            throw new IOException(\"output stream is out of sync, pos=\" + flushedOffset + \" and nextOffset should be\" + (offset + count));\r\n        }\r\n        if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\r\n            synchronized (writeCtx) {\r\n                if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\r\n                    writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);\r\n                    updateNonSequentialWriteInMemory(-count);\r\n                    if (LOG.isDebugEnabled()) {\r\n                        LOG.debug(\"After writing {} at offset {}, \" + \"updated the memory count, new value: {}\", handle.dumpFileHandle(), offset, nonSequentialWriteInMemory.get());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (!writeCtx.getReplied()) {\r\n            if (stableHow != WriteStableHow.UNSTABLE) {\r\n                LOG.info(\"Do sync for stable write: {}\", writeCtx);\r\n                try {\r\n                    if (stableHow == WriteStableHow.DATA_SYNC) {\r\n                        fos.hsync();\r\n                    } else {\r\n                        Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC, \"Unknown WriteStableHow: \" + stableHow);\r\n                        fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\r\n                    }\r\n                } catch (IOException e) {\r\n                    LOG.error(\"hsync failed with writeCtx: {}\", writeCtx, e);\r\n                    throw e;\r\n                }\r\n            }\r\n            WccAttr preOpAttr = latestAttr.getWccAttr();\r\n            WccData fileWcc = new WccData(preOpAttr, latestAttr);\r\n            if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {\r\n                LOG.warn(\"Return original count: {} instead of real data count: {}\", writeCtx.getOriginalCount(), count);\r\n                count = writeCtx.getOriginalCount();\r\n            }\r\n            WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK, fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\r\n            RpcProgramNfs3.metrics.addWrite(Nfs3Utils.getElapsedTime(writeCtx.startTime));\r\n            Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n        }\r\n        processCommits(writeCtx.getOffset() + writeCtx.getCount());\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error writing to fileHandle {} at offset {} and length {}\", handle.dumpFileHandle(), offset, count, e);\r\n        if (!writeCtx.getReplied()) {\r\n            WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO);\r\n            Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n        }\r\n        LOG.info(\"Clean up open file context for fileId: {}\", latestAttr.getFileId());\r\n        cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "cleanup",
  "errType" : [ "InterruptedException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void cleanup()\n{\r\n    if (!activeState) {\r\n        LOG.info(\"Current OpenFileCtx is already inactive, no need to cleanup.\");\r\n        return;\r\n    }\r\n    activeState = false;\r\n    if (dumpThread != null && dumpThread.isAlive()) {\r\n        dumpThread.interrupt();\r\n        try {\r\n            dumpThread.join(3000);\r\n        } catch (InterruptedException ignored) {\r\n        }\r\n    }\r\n    try {\r\n        if (fos != null) {\r\n            fos.close();\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.info(\"Can't close stream for fileId: {}, error: {}\", latestAttr.getFileId(), e.toString());\r\n    }\r\n    LOG.info(\"There are {} pending writes.\", pendingWrites.size());\r\n    WccAttr preOpAttr = latestAttr.getWccAttr();\r\n    while (!pendingWrites.isEmpty()) {\r\n        OffsetRange key = pendingWrites.firstKey();\r\n        LOG.info(\"Fail pending write: {}, nextOffset={}\", key, nextOffset.get());\r\n        WriteCtx writeCtx = pendingWrites.remove(key);\r\n        if (!writeCtx.getReplied()) {\r\n            WccData fileWcc = new WccData(preOpAttr, latestAttr);\r\n            WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO, fileWcc, 0, writeCtx.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\r\n            Nfs3Utils.writeChannel(writeCtx.getChannel(), response.serialize(new XDR(), writeCtx.getXid(), new VerifierNone()), writeCtx.getXid());\r\n        }\r\n    }\r\n    if (dumpOut != null) {\r\n        try {\r\n            dumpOut.close();\r\n        } catch (IOException e) {\r\n            LOG.error(\"Failed to close outputstream of dump file {}\", dumpFilePath, e);\r\n        }\r\n        File dumpFile = new File(dumpFilePath);\r\n        if (dumpFile.exists() && !dumpFile.delete()) {\r\n            LOG.error(\"Failed to delete dumpfile: {}\", dumpFile);\r\n        }\r\n    }\r\n    if (raf != null) {\r\n        try {\r\n            raf.close();\r\n        } catch (IOException e) {\r\n            LOG.error(\"Got exception when closing input stream of dump file.\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getPendingWritesForTest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConcurrentNavigableMap<OffsetRange, WriteCtx> getPendingWritesForTest()\n{\r\n    return pendingWrites;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getPendingCommitsForTest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConcurrentNavigableMap<Long, CommitCtx> getPendingCommitsForTest()\n{\r\n    return pendingCommits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getNextOffsetForTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNextOffsetForTest()\n{\r\n    return nextOffset.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setNextOffsetForTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNextOffsetForTest(long newValue)\n{\r\n    nextOffset.set(newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setActiveStatusForTest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setActiveStatusForTest(boolean activeState)\n{\r\n    this.activeState = activeState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"activeState: %b asyncStatus: %b nextOffset: %d\", activeState, asyncStatus, nextOffset.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "addOpenFileStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean addOpenFileStream(FileHandle h, OpenFileCtx ctx)\n{\r\n    return fileContextCache.put(h, ctx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "startAsyncDataService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startAsyncDataService()\n{\r\n    if (asyncDataServiceStarted) {\r\n        return;\r\n    }\r\n    fileContextCache.start();\r\n    this.asyncDataService = new AsyncDataService();\r\n    asyncDataServiceStarted = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "shutdownAsyncDataService",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdownAsyncDataService()\n{\r\n    if (!asyncDataServiceStarted) {\r\n        return;\r\n    }\r\n    asyncDataServiceStarted = false;\r\n    asyncDataService.shutdown();\r\n    fileContextCache.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "handleWrite",
  "errType" : [ "RemoteException", "IOException", "IOException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void handleWrite(DFSClient dfsClient, WRITE3Request request, Channel channel, int xid, Nfs3FileAttributes preOpAttr) throws IOException\n{\r\n    int count = request.getCount();\r\n    byte[] data = request.getData().array();\r\n    if (data.length < count) {\r\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);\r\n        Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n        return;\r\n    }\r\n    FileHandle handle = request.getHandle();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"handleWrite \" + request);\r\n    }\r\n    FileHandle fileHandle = request.getHandle();\r\n    OpenFileCtx openFileCtx = fileContextCache.get(fileHandle);\r\n    if (openFileCtx == null) {\r\n        LOG.info(\"No opened stream for fileHandle: \" + fileHandle.dumpFileHandle());\r\n        String fileIdPath = Nfs3Utils.getFileIdPath(fileHandle.getFileId());\r\n        HdfsDataOutputStream fos = null;\r\n        Nfs3FileAttributes latestAttr = null;\r\n        try {\r\n            int bufferSize = config.getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY, CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);\r\n            fos = dfsClient.append(fileIdPath, bufferSize, EnumSet.of(CreateFlag.APPEND), null, null);\r\n            latestAttr = Nfs3Utils.getFileAttr(dfsClient, fileIdPath, iug);\r\n        } catch (RemoteException e) {\r\n            IOException io = e.unwrapRemoteException();\r\n            if (io instanceof AlreadyBeingCreatedException) {\r\n                LOG.warn(\"Can't append file: \" + fileIdPath + \". Possibly the file is being closed. Drop the request: \" + request + \", wait for the client to retry...\");\r\n                return;\r\n            }\r\n            throw e;\r\n        } catch (IOException e) {\r\n            LOG.error(\"Can't append to file: \" + fileIdPath, e);\r\n            if (fos != null) {\r\n                fos.close();\r\n            }\r\n            WccData fileWcc = new WccData(Nfs3Utils.getWccAttr(preOpAttr), preOpAttr);\r\n            WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO, fileWcc, count, request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\r\n            Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n            return;\r\n        }\r\n        String writeDumpDir = config.get(NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_KEY, NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_DEFAULT);\r\n        openFileCtx = new OpenFileCtx(fos, latestAttr, writeDumpDir + \"/\" + fileHandle.getFileId(), dfsClient, iug, aixCompatMode, config);\r\n        if (!addOpenFileStream(fileHandle, openFileCtx)) {\r\n            LOG.info(\"Can't add new stream. Close it. Tell client to retry.\");\r\n            try {\r\n                fos.close();\r\n            } catch (IOException e) {\r\n                LOG.error(\"Can't close stream for fileHandle: \" + handle.dumpFileHandle(), e);\r\n            }\r\n            WccData fileWcc = new WccData(latestAttr.getWccAttr(), latestAttr);\r\n            WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_JUKEBOX, fileWcc, 0, request.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\r\n            Nfs3Utils.writeChannel(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n            return;\r\n        }\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Opened stream for appending file: \" + fileHandle.dumpFileHandle());\r\n        }\r\n    }\r\n    openFileCtx.receivedNewWrite(dfsClient, request, channel, xid, asyncDataService, iug);\r\n    return;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "commitBeforeRead",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int commitBeforeRead(DFSClient dfsClient, FileHandle fileHandle, long commitOffset)\n{\r\n    int status;\r\n    OpenFileCtx openFileCtx = fileContextCache.get(fileHandle);\r\n    if (openFileCtx == null) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"No opened stream for fileId: \" + fileHandle.dumpFileHandle() + \" commitOffset=\" + commitOffset + \". Return success in this case.\");\r\n        }\r\n        status = Nfs3Status.NFS3_OK;\r\n    } else {\r\n        COMMIT_STATUS ret = openFileCtx.checkCommit(dfsClient, commitOffset, null, 0, null, true);\r\n        switch(ret) {\r\n            case COMMIT_FINISHED:\r\n            case COMMIT_INACTIVE_CTX:\r\n                status = Nfs3Status.NFS3_OK;\r\n                break;\r\n            case COMMIT_INACTIVE_WITH_PENDING_WRITE:\r\n            case COMMIT_ERROR:\r\n                status = Nfs3Status.NFS3ERR_IO;\r\n                break;\r\n            case COMMIT_WAIT:\r\n            case COMMIT_SPECIAL_WAIT:\r\n                status = Nfs3Status.NFS3ERR_JUKEBOX;\r\n                break;\r\n            case COMMIT_SPECIAL_SUCCESS:\r\n                status = Nfs3Status.NFS3_OK;\r\n                break;\r\n            default:\r\n                LOG.error(\"Should not get commit return code: \" + ret.name());\r\n                throw new RuntimeException(\"Should not get commit return code: \" + ret.name());\r\n        }\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "handleCommit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void handleCommit(DFSClient dfsClient, FileHandle fileHandle, long commitOffset, Channel channel, int xid, Nfs3FileAttributes preOpAttr, int namenodeId)\n{\r\n    long startTime = System.nanoTime();\r\n    int status;\r\n    OpenFileCtx openFileCtx = fileContextCache.get(fileHandle);\r\n    if (openFileCtx == null) {\r\n        LOG.info(\"No opened stream for fileId: \" + fileHandle.dumpFileHandle() + \" commitOffset=\" + commitOffset + \". Return success in this case.\");\r\n        status = Nfs3Status.NFS3_OK;\r\n    } else {\r\n        COMMIT_STATUS ret = openFileCtx.checkCommit(dfsClient, commitOffset, channel, xid, preOpAttr, false);\r\n        switch(ret) {\r\n            case COMMIT_FINISHED:\r\n            case COMMIT_INACTIVE_CTX:\r\n                status = Nfs3Status.NFS3_OK;\r\n                break;\r\n            case COMMIT_INACTIVE_WITH_PENDING_WRITE:\r\n            case COMMIT_ERROR:\r\n                status = Nfs3Status.NFS3ERR_IO;\r\n                break;\r\n            case COMMIT_WAIT:\r\n                return;\r\n            case COMMIT_SPECIAL_WAIT:\r\n                status = Nfs3Status.NFS3ERR_JUKEBOX;\r\n                break;\r\n            case COMMIT_SPECIAL_SUCCESS:\r\n                status = Nfs3Status.NFS3_OK;\r\n                break;\r\n            default:\r\n                LOG.error(\"Should not get commit return code: \" + ret.name());\r\n                throw new RuntimeException(\"Should not get commit return code: \" + ret.name());\r\n        }\r\n    }\r\n    Nfs3FileAttributes postOpAttr = null;\r\n    try {\r\n        postOpAttr = getFileAttr(dfsClient, new FileHandle(preOpAttr.getFileId(), namenodeId), iug);\r\n    } catch (IOException e1) {\r\n        LOG.info(\"Can't get postOpAttr for fileId: \" + preOpAttr.getFileId(), e1);\r\n    }\r\n    WccData fileWcc = new WccData(Nfs3Utils.getWccAttr(preOpAttr), postOpAttr);\r\n    COMMIT3Response response = new COMMIT3Response(status, fileWcc, Nfs3Constant.WRITE_COMMIT_VERF);\r\n    RpcProgramNfs3.metrics.addCommit(Nfs3Utils.getElapsedTime(startTime));\r\n    Nfs3Utils.writeChannelCommit(channel, response.serialize(new XDR(), xid, new VerifierNone()), xid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileAttr",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Nfs3FileAttributes getFileAttr(DFSClient client, FileHandle fileHandle, IdMappingServiceProvider iug) throws IOException\n{\r\n    String fileIdPath = Nfs3Utils.getFileIdPath(fileHandle);\r\n    Nfs3FileAttributes attr = Nfs3Utils.getFileAttr(client, fileIdPath, iug);\r\n    if (attr != null) {\r\n        OpenFileCtx openFileCtx = fileContextCache.get(fileHandle);\r\n        if (openFileCtx != null) {\r\n            attr.setSize(openFileCtx.getNextOffset());\r\n            attr.setUsed(openFileCtx.getNextOffset());\r\n        }\r\n    }\r\n    return attr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getFileAttr",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Nfs3FileAttributes getFileAttr(DFSClient client, FileHandle dirHandle, String fileName, int namenodeId) throws IOException\n{\r\n    String fileIdPath = Nfs3Utils.getFileIdPath(dirHandle) + \"/\" + fileName;\r\n    Nfs3FileAttributes attr = Nfs3Utils.getFileAttr(client, fileIdPath, iug);\r\n    if ((attr != null) && (attr.getType() == NfsFileType.NFSREG.toValue())) {\r\n        OpenFileCtx openFileCtx = fileContextCache.get(new FileHandle(attr.getFileId(), namenodeId));\r\n        if (openFileCtx != null) {\r\n            attr.setSize(openFileCtx.getNextOffset());\r\n            attr.setUsed(openFileCtx.getNextOffset());\r\n        }\r\n    }\r\n    return attr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getOpenFileCtxCache",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OpenFileCtxCache getOpenFileCtxCache()\n{\r\n    return this.fileContextCache;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getMin",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMin()\n{\r\n    return min;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getMax",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMax()\n{\r\n    return max;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    return (int) (min ^ max);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o instanceof OffsetRange) {\r\n        OffsetRange range = (OffsetRange) o;\r\n        return (min == range.getMin()) && (max == range.getMax());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return \"[\" + getMin() + \", \" + getMax() + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "addExports",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addExports() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    String[] exportsPath = config.getStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY, NfsConfigKeys.DFS_NFS_EXPORT_POINT_DEFAULT);\r\n    for (String exportPath : exportsPath) {\r\n        URI exportURI = Nfs3Utils.getResolvedURI(fs, exportPath);\r\n        LOG.info(\"FS:\" + fs.getScheme() + \" adding export Path:\" + exportPath + \" with URI: \" + exportURI.toString());\r\n        exports.put(exportPath, exportURI);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "nullOp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "XDR nullOp(XDR out, int xid, InetAddress client)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"MOUNT NULLOP : \" + \" client: \" + client);\r\n    }\r\n    return RpcAcceptedReply.getAcceptInstance(xid, new VerifierNone()).write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "mnt",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "XDR mnt(XDR xdr, XDR out, int xid, InetAddress client)\n{\r\n    if (hostsMatcher == null) {\r\n        return MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out, xid, null);\r\n    }\r\n    AccessPrivilege accessPrivilege = hostsMatcher.getAccessPrivilege(client);\r\n    if (accessPrivilege == AccessPrivilege.NONE) {\r\n        return MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out, xid, null);\r\n    }\r\n    String path = xdr.readString();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"MOUNT MNT path: \" + path + \" client: \" + client);\r\n    }\r\n    String host = client.getHostName();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Got host: \" + host + \" path: \" + path);\r\n    }\r\n    URI exportURI = exports.get(path);\r\n    if (exportURI == null) {\r\n        LOG.info(\"Path \" + path + \" is not shared.\");\r\n        MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT, out, xid, null);\r\n        return out;\r\n    }\r\n    DFSClient dfsClient = null;\r\n    try {\r\n        dfsClient = new DFSClient(exportURI, config);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Can't get handle for export:\" + path, e);\r\n        MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT, out, xid, null);\r\n        return out;\r\n    }\r\n    FileHandle handle = null;\r\n    try {\r\n        HdfsFileStatus exFileStatus = dfsClient.getFileInfo(exportURI.getPath());\r\n        handle = new FileHandle(exFileStatus.getFileId(), Nfs3Utils.getNamenodeId(config, exportURI));\r\n    } catch (IOException e) {\r\n        LOG.error(\"Can't get handle for export:\" + path, e);\r\n        MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT, out, xid, null);\r\n        return out;\r\n    }\r\n    assert (handle != null);\r\n    LOG.info(\"Giving handle (fileHandle:\" + handle.dumpFileHandle() + \" file URI: \" + exportURI + \") to client for export \" + path);\r\n    mounts.add(new MountEntry(host, path));\r\n    MountResponse.writeMNTResponse(Nfs3Status.NFS3_OK, out, xid, handle.getContent());\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "dump",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "XDR dump(XDR out, int xid, InetAddress client)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"MOUNT NULLOP : \" + \" client: \" + client);\r\n    }\r\n    List<MountEntry> copy = new ArrayList<MountEntry>(mounts);\r\n    MountResponse.writeMountList(out, xid, copy);\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "umnt",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "XDR umnt(XDR xdr, XDR out, int xid, InetAddress client)\n{\r\n    String path = xdr.readString();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"MOUNT UMNT path: \" + path + \" client: \" + client);\r\n    }\r\n    String host = client.getHostName();\r\n    mounts.remove(new MountEntry(host, path));\r\n    RpcAcceptedReply.getAcceptInstance(xid, new VerifierNone()).write(out);\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "umntall",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "XDR umntall(XDR out, int xid, InetAddress client)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"MOUNT UMNTALL : \" + \" client: \" + client);\r\n    }\r\n    mounts.clear();\r\n    return RpcAcceptedReply.getAcceptInstance(xid, new VerifierNone()).write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "handleInternal",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void handleInternal(ChannelHandlerContext ctx, RpcInfo info)\n{\r\n    RpcCall rpcCall = (RpcCall) info.header();\r\n    final MNTPROC mntproc = MNTPROC.fromValue(rpcCall.getProcedure());\r\n    int xid = rpcCall.getXid();\r\n    byte[] data = new byte[info.data().readableBytes()];\r\n    info.data().readBytes(data);\r\n    XDR xdr = new XDR(data);\r\n    XDR out = new XDR();\r\n    InetAddress client = ((InetSocketAddress) info.remoteAddress()).getAddress();\r\n    if (mntproc == MNTPROC.NULL) {\r\n        out = nullOp(out, xid, client);\r\n    } else if (mntproc == MNTPROC.MNT) {\r\n        if (!doPortMonitoring(info.remoteAddress())) {\r\n            out = MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out, xid, null);\r\n        } else {\r\n            out = mnt(xdr, out, xid, client);\r\n        }\r\n    } else if (mntproc == MNTPROC.DUMP) {\r\n        out = dump(out, xid, client);\r\n    } else if (mntproc == MNTPROC.UMNT) {\r\n        out = umnt(xdr, out, xid, client);\r\n    } else if (mntproc == MNTPROC.UMNTALL) {\r\n        umntall(out, xid, client);\r\n    } else if (mntproc == MNTPROC.EXPORT) {\r\n        List<NfsExports> hostsMatchers = new ArrayList<NfsExports>();\r\n        if (hostsMatcher != null) {\r\n            List exportsList = getExports();\r\n            for (int i = 0; i < exportsList.size(); i++) {\r\n                hostsMatchers.add(hostsMatcher);\r\n            }\r\n            out = MountResponse.writeExportList(out, xid, exportsList, hostsMatchers);\r\n        } else {\r\n            RpcAcceptedReply.getInstance(xid, RpcAcceptedReply.AcceptState.PROC_UNAVAIL, new VerifierNone()).write(out);\r\n        }\r\n    } else {\r\n        RpcAcceptedReply.getInstance(xid, RpcAcceptedReply.AcceptState.PROC_UNAVAIL, new VerifierNone()).write(out);\r\n    }\r\n    ByteBuf buf = Unpooled.wrappedBuffer(out.asReadOnlyWrap().buffer());\r\n    RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());\r\n    RpcUtil.sendRpcResponse(ctx, rsp);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "isIdempotent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isIdempotent(RpcCall call)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\mount",
  "methodName" : "getExports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> getExports()\n{\r\n    return new ArrayList<>(this.exports.keySet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "init",
  "errType" : [ "SocketException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void init(DaemonContext context) throws Exception\n{\r\n    System.err.println(\"Initializing privileged NFS client socket...\");\r\n    NfsConfiguration conf = new NfsConfiguration();\r\n    int clientPort = conf.getInt(NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_KEY, NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_DEFAULT);\r\n    if (clientPort < 1 || clientPort > 1023) {\r\n        throw new RuntimeException(\"Must start privileged NFS server with '\" + NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_KEY + \"' configured to a \" + \"privileged port.\");\r\n    }\r\n    try {\r\n        InetSocketAddress socketAddress = new InetSocketAddress(\"localhost\", clientPort);\r\n        registrationSocket = new DatagramSocket(null);\r\n        registrationSocket.setReuseAddress(true);\r\n        registrationSocket.bind(socketAddress);\r\n    } catch (SocketException e) {\r\n        LOG.error(\"Init failed for port=\" + clientPort, e);\r\n        throw e;\r\n    }\r\n    args = context.getArguments();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void start() throws Exception\n{\r\n    nfs3Server = Nfs3.startService(args, registrationSocket);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop() throws Exception\n{\r\n    if (nfs3Server != null) {\r\n        nfs3Server.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroy()\n{\r\n    if (registrationSocket != null && !registrationSocket.isClosed()) {\r\n        registrationSocket.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getOriginalCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getOriginalCount()\n{\r\n    return originalCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "trimWrite",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void trimWrite(int delta)\n{\r\n    Preconditions.checkState(delta < count);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Trim write request by delta:\" + delta + \" \" + toString());\r\n    }\r\n    synchronized (this) {\r\n        trimDelta = delta;\r\n        if (originalCount == INVALID_ORIGINAL_COUNT) {\r\n            originalCount = count;\r\n        }\r\n        trimData();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getDataState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataState getDataState()\n{\r\n    return dataState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setDataState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDataState(DataState dataState)\n{\r\n    this.dataState = dataState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "dumpData",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "long dumpData(FileOutputStream dumpOut, RandomAccessFile raf) throws IOException\n{\r\n    if (dataState != DataState.ALLOW_DUMP) {\r\n        if (LOG.isTraceEnabled()) {\r\n            LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\" + replied + \",\" + dataState + \")\");\r\n        }\r\n        return 0;\r\n    }\r\n    Preconditions.checkState(getOriginalCount() == INVALID_ORIGINAL_COUNT);\r\n    this.raf = raf;\r\n    dumpFileOffset = dumpOut.getChannel().position();\r\n    dumpOut.write(data.array(), 0, count);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\r\n    }\r\n    if (dataState == DataState.ALLOW_DUMP) {\r\n        synchronized (this) {\r\n            if (dataState == DataState.ALLOW_DUMP) {\r\n                data = null;\r\n                dataState = DataState.DUMPED;\r\n                return count;\r\n            }\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getHandle",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileHandle getHandle()\n{\r\n    return handle;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOffset()\n{\r\n    synchronized (this) {\r\n        return offset + trimDelta;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getPlainOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPlainOffset()\n{\r\n    return offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCount()\n{\r\n    synchronized (this) {\r\n        return count - trimDelta;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getStableHow",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WriteStableHow getStableHow()\n{\r\n    return stableHow;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ByteBuffer getData() throws IOException\n{\r\n    if (dataState != DataState.DUMPED) {\r\n        synchronized (this) {\r\n            if (dataState != DataState.DUMPED) {\r\n                Preconditions.checkState(data != null);\r\n                return data;\r\n            }\r\n        }\r\n    }\r\n    this.loadData();\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "loadData",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void loadData() throws IOException\n{\r\n    Preconditions.checkState(data == null);\r\n    byte[] rawData = new byte[count];\r\n    raf.seek(dumpFileOffset);\r\n    int size = raf.read(rawData, 0, count);\r\n    if (size != count) {\r\n        throw new IOException(\"Data count is \" + count + \", but read back \" + size + \"bytes\");\r\n    }\r\n    synchronized (this) {\r\n        data = ByteBuffer.wrap(rawData);\r\n        trimData();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "trimData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void trimData()\n{\r\n    if (data != null && trimDelta > 0) {\r\n        dataState = DataState.NO_DUMP;\r\n        data.position(data.position() + trimDelta);\r\n        offset += trimDelta;\r\n        count -= trimDelta;\r\n        trimDelta = 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "writeData",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeData(HdfsDataOutputStream fos) throws IOException\n{\r\n    Preconditions.checkState(fos != null);\r\n    ByteBuffer dataBuffer;\r\n    try {\r\n        dataBuffer = getData();\r\n    } catch (Exception e1) {\r\n        LOG.error(\"Failed to get request data offset:\" + getPlainOffset() + \" \" + \"count:\" + count + \" error:\" + e1);\r\n        throw new IOException(\"Can't get WriteCtx.data\");\r\n    }\r\n    byte[] data = dataBuffer.array();\r\n    int position = dataBuffer.position();\r\n    int limit = dataBuffer.limit();\r\n    Preconditions.checkState(limit - position == count);\r\n    if (position != 0) {\r\n        if (limit != getOriginalCount()) {\r\n            throw new IOException(\"Modified write has differnt original size.\" + \"buff position:\" + position + \" buff limit:\" + limit + \". \" + toString());\r\n        }\r\n    }\r\n    fos.write(data, position, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getChannel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Channel getChannel()\n{\r\n    return channel;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getXid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getXid()\n{\r\n    return xid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getReplied",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getReplied()\n{\r\n    return replied;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "setReplied",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReplied(boolean replied)\n{\r\n    this.replied = replied;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    return \"FileHandle:\" + handle.dumpFileHandle() + \" offset:\" + getPlainOffset() + \" \" + \"count:\" + count + \" originalCount:\" + getOriginalCount() + \" stableHow:\" + stableHow + \" replied:\" + replied + \" dataState:\" + dataState + \" xid:\" + xid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "getEntryToEvict",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "Entry<FileHandle, OpenFileCtx> getEntryToEvict()\n{\r\n    Iterator<Entry<FileHandle, OpenFileCtx>> it = openFileMap.entrySet().iterator();\r\n    if (LOG.isTraceEnabled()) {\r\n        LOG.trace(\"openFileMap size:\" + size());\r\n    }\r\n    Entry<FileHandle, OpenFileCtx> idlest = null;\r\n    while (it.hasNext()) {\r\n        Entry<FileHandle, OpenFileCtx> pairs = it.next();\r\n        OpenFileCtx ctx = pairs.getValue();\r\n        if (!ctx.getActiveState()) {\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Got one inactive stream: \" + ctx);\r\n            }\r\n            return pairs;\r\n        }\r\n        if (ctx.hasPendingWork()) {\r\n            continue;\r\n        }\r\n        if (idlest == null) {\r\n            idlest = pairs;\r\n        } else {\r\n            if (ctx.getLastAccessTime() < idlest.getValue().getLastAccessTime()) {\r\n                idlest = pairs;\r\n            }\r\n        }\r\n    }\r\n    if (idlest == null) {\r\n        LOG.warn(\"No eviction candidate. All streams have pending work.\");\r\n        return null;\r\n    } else {\r\n        long idleTime = Time.monotonicNow() - idlest.getValue().getLastAccessTime();\r\n        if (idleTime < NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT) {\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"idlest stream's idle time:\" + idleTime);\r\n            }\r\n            LOG.warn(\"All opened streams are busy, can't remove any from cache.\");\r\n            return null;\r\n        } else {\r\n            return idlest;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean put(FileHandle h, OpenFileCtx context)\n{\r\n    OpenFileCtx toEvict = null;\r\n    synchronized (this) {\r\n        Preconditions.checkState(size() <= this.maxStreams, \"stream cache size \" + size() + \"  is larger than maximum\" + this.maxStreams);\r\n        if (size() == this.maxStreams) {\r\n            Entry<FileHandle, OpenFileCtx> pairs = getEntryToEvict();\r\n            if (pairs == null) {\r\n                return false;\r\n            } else {\r\n                if (LOG.isDebugEnabled()) {\r\n                    LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\r\n                }\r\n                toEvict = openFileMap.remove(pairs.getKey());\r\n                Preconditions.checkState(toEvict == pairs.getValue(), \"The deleted entry is not the same as odlest found.\");\r\n            }\r\n        }\r\n        openFileMap.put(h, context);\r\n    }\r\n    if (toEvict != null) {\r\n        toEvict.cleanup();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "scan",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void scan(long streamTimeout)\n{\r\n    ArrayList<OpenFileCtx> ctxToRemove = new ArrayList<OpenFileCtx>();\r\n    Iterator<Entry<FileHandle, OpenFileCtx>> it = openFileMap.entrySet().iterator();\r\n    if (LOG.isTraceEnabled()) {\r\n        LOG.trace(\"openFileMap size:\" + size());\r\n    }\r\n    while (it.hasNext()) {\r\n        Entry<FileHandle, OpenFileCtx> pairs = it.next();\r\n        FileHandle handle = pairs.getKey();\r\n        OpenFileCtx ctx = pairs.getValue();\r\n        if (!ctx.streamCleanup(handle, streamTimeout)) {\r\n            continue;\r\n        }\r\n        synchronized (this) {\r\n            OpenFileCtx ctx2 = openFileMap.get(handle);\r\n            if (ctx2 != null) {\r\n                if (ctx2.streamCleanup(handle, streamTimeout)) {\r\n                    openFileMap.remove(handle);\r\n                    if (LOG.isDebugEnabled()) {\r\n                        LOG.debug(\"After remove stream \" + handle.dumpFileHandle() + \", the stream number:\" + size());\r\n                    }\r\n                    ctxToRemove.add(ctx2);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    for (OpenFileCtx ofc : ctxToRemove) {\r\n        ofc.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OpenFileCtx get(FileHandle key)\n{\r\n    return openFileMap.get(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int size()\n{\r\n    return openFileMap.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void start()\n{\r\n    streamMonitor.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "cleanAll",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void cleanAll()\n{\r\n    ArrayList<OpenFileCtx> cleanedContext = new ArrayList<OpenFileCtx>();\r\n    synchronized (this) {\r\n        Iterator<Entry<FileHandle, OpenFileCtx>> it = openFileMap.entrySet().iterator();\r\n        if (LOG.isTraceEnabled()) {\r\n            LOG.trace(\"openFileMap size:\" + size());\r\n        }\r\n        while (it.hasNext()) {\r\n            Entry<FileHandle, OpenFileCtx> pairs = it.next();\r\n            OpenFileCtx ctx = pairs.getValue();\r\n            it.remove();\r\n            cleanedContext.add(ctx);\r\n        }\r\n    }\r\n    for (OpenFileCtx ofc : cleanedContext) {\r\n        ofc.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-nfs\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\nfs\\nfs3",
  "methodName" : "shutdown",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void shutdown()\n{\r\n    if (streamMonitor.isAlive()) {\r\n        streamMonitor.shouldRun(false);\r\n        streamMonitor.interrupt();\r\n        try {\r\n            streamMonitor.join(3000);\r\n        } catch (InterruptedException ignored) {\r\n        }\r\n    }\r\n    cleanAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
} ]
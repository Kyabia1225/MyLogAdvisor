[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCause",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonClientException getCause()\n{\r\n    return (AmazonClientException) super.getCause();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMessage()\n{\r\n    return operation + \": \" + getCause().getMessage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "splitPathToElements",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<String> splitPathToElements(Path path)\n{\r\n    checkArgument(path.isAbsolute(), \"path is relative\");\r\n    String uriPath = path.toUri().getPath();\r\n    checkArgument(!uriPath.isEmpty(), \"empty path\");\r\n    if (\"/\".equals(uriPath)) {\r\n        return new ArrayList<>(0);\r\n    }\r\n    List<String> elements = new ArrayList<>();\r\n    int len = uriPath.length();\r\n    int firstElementChar = 1;\r\n    int endOfElement = uriPath.indexOf('/', firstElementChar);\r\n    while (endOfElement > 0) {\r\n        elements.add(uriPath.substring(firstElementChar, endOfElement));\r\n        firstElementChar = endOfElement + 1;\r\n        endOfElement = firstElementChar == len ? -1 : uriPath.indexOf('/', firstElementChar);\r\n    }\r\n    if (firstElementChar != len) {\r\n        elements.add(uriPath.substring(firstElementChar));\r\n    }\r\n    return elements;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isMagicPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMagicPath(List<String> elements)\n{\r\n    return elements.contains(MAGIC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "containsBasePath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean containsBasePath(List<String> elements)\n{\r\n    return elements.contains(BASE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "magicElementIndex",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int magicElementIndex(List<String> elements)\n{\r\n    int index = elements.indexOf(MAGIC);\r\n    checkArgument(index >= 0, E_NO_MAGIC_PATH_ELEMENT);\r\n    return index;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "magicPathParents",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> magicPathParents(List<String> elements)\n{\r\n    return elements.subList(0, magicElementIndex(elements));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "magicPathChildren",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<String> magicPathChildren(List<String> elements)\n{\r\n    int index = magicElementIndex(elements);\r\n    int len = elements.size();\r\n    if (index == len - 1) {\r\n        return Collections.emptyList();\r\n    } else {\r\n        return elements.subList(index + 1, len);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "basePathChildren",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<String> basePathChildren(List<String> elements)\n{\r\n    int index = elements.indexOf(BASE);\r\n    if (index < 0) {\r\n        return Collections.emptyList();\r\n    }\r\n    int len = elements.size();\r\n    if (index == len - 1) {\r\n        return Collections.emptyList();\r\n    } else {\r\n        return elements.subList(index + 1, len);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "elementsToKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String elementsToKey(List<String> elements)\n{\r\n    return StringUtils.join(\"/\", elements);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "filename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String filename(List<String> elements)\n{\r\n    return lastElement(elements);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "lastElement",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String lastElement(List<String> strings)\n{\r\n    checkArgument(!strings.isEmpty(), \"empty list\");\r\n    return strings.get(strings.size() - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "magicSubdir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path magicSubdir(Path destDir)\n{\r\n    return new Path(destDir, MAGIC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "finalDestination",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<String> finalDestination(List<String> elements)\n{\r\n    if (isMagicPath(elements)) {\r\n        List<String> destDir = magicPathParents(elements);\r\n        List<String> children = magicPathChildren(elements);\r\n        checkArgument(!children.isEmpty(), \"No path found under \" + MAGIC);\r\n        ArrayList<String> dest = new ArrayList<>(destDir);\r\n        if (containsBasePath(children)) {\r\n            List<String> baseChildren = basePathChildren(children);\r\n            checkArgument(!baseChildren.isEmpty(), \"No path found under \" + BASE);\r\n            dest.addAll(baseChildren);\r\n        } else {\r\n            dest.add(filename(children));\r\n        }\r\n        return dest;\r\n    } else {\r\n        return elements;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getInvoker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Invoker getInvoker()\n{\r\n    return invoker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem.Statistics getStats()\n{\r\n    return stats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDstFileStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileStatus getDstFileStatus()\n{\r\n    return dstFileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AWSSecurityTokenServiceClientBuilder builder(final Configuration conf, final String bucket, final AWSCredentialsProvider credentials) throws IOException\n{\r\n    final ClientConfiguration awsConf = S3AUtils.createAwsConf(conf, bucket, Constants.AWS_SERVICE_IDENTIFIER_STS);\r\n    String endpoint = conf.getTrimmed(DELEGATION_TOKEN_ENDPOINT, DEFAULT_DELEGATION_TOKEN_ENDPOINT);\r\n    String region = conf.getTrimmed(DELEGATION_TOKEN_REGION, DEFAULT_DELEGATION_TOKEN_REGION);\r\n    return builder(credentials, awsConf, endpoint, region);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSSecurityTokenServiceClientBuilder builder(final Configuration conf, final String bucket, final AWSCredentialsProvider credentials, final String stsEndpoint, final String stsRegion) throws IOException\n{\r\n    final ClientConfiguration awsConf = S3AUtils.createAwsConf(conf, bucket, Constants.AWS_SERVICE_IDENTIFIER_STS);\r\n    return builder(credentials, awsConf, stsEndpoint, stsRegion);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "AWSSecurityTokenServiceClientBuilder builder(final AWSCredentialsProvider credentials, final ClientConfiguration awsConf, final String stsEndpoint, final String stsRegion)\n{\r\n    final AWSSecurityTokenServiceClientBuilder builder = AWSSecurityTokenServiceClientBuilder.standard();\r\n    Preconditions.checkArgument(credentials != null, \"No credentials\");\r\n    builder.withClientConfiguration(awsConf);\r\n    builder.withCredentials(credentials);\r\n    boolean destIsStandardEndpoint = STS_STANDARD.equals(stsEndpoint);\r\n    if (isNotEmpty(stsEndpoint) && !destIsStandardEndpoint) {\r\n        Preconditions.checkArgument(isNotEmpty(stsRegion), \"STS endpoint is set to %s but no signing region was provided\", stsEndpoint);\r\n        LOG.debug(\"STS Endpoint={}; region='{}'\", stsEndpoint, stsRegion);\r\n        builder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(stsEndpoint, stsRegion));\r\n    } else {\r\n        Preconditions.checkArgument(isEmpty(stsRegion), \"STS signing region set set to %s but no STS endpoint specified\", stsRegion);\r\n    }\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createClientConnection",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "STSClient createClientConnection(final AWSSecurityTokenService tokenService, final Invoker invoker) throws IOException\n{\r\n    return new STSClient(tokenService, invoker);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getCredentials",
  "errType" : [ "IOException", "AWSSecurityTokenServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSCredentials getCredentials()\n{\r\n    try {\r\n        return invoker.retryUntranslated(\"getCredentials\", true, stsProvider::getCredentials);\r\n    } catch (IOException e) {\r\n        throw new CredentialInitializationException(\"getCredentials failed: \" + e, e);\r\n    } catch (AWSSecurityTokenServiceException e) {\r\n        LOG.error(\"Failed to get credentials for role {}\", arn, e);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refresh()\n{\r\n    stsProvider.refresh();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    S3AUtils.closeAutocloseables(LOG, stsProvider, credentialsToSTS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AssumedRoleCredentialProvider{\");\r\n    sb.append(\"role='\").append(arn).append('\\'');\r\n    sb.append(\", session'\").append(sessionName).append('\\'');\r\n    sb.append(\", duration=\").append(duration);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "buildSessionName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String buildSessionName() throws IOException\n{\r\n    return sanitize(UserGroupInformation.getCurrentUser().getShortUserName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "sanitize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String sanitize(String session)\n{\r\n    StringBuilder r = new StringBuilder(session.length());\r\n    for (char c : session.toCharArray()) {\r\n        if (\"abcdefghijklmnopqrstuvwxyz0123456789,.@-\".contains(Character.toString(c).toLowerCase(Locale.ENGLISH))) {\r\n            r.append(c);\r\n        } else {\r\n            r.append('-');\r\n        }\r\n    }\r\n    return r.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "operationRetried",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void operationRetried(String text, Exception ex, int retries, boolean idempotent)\n{\r\n    if (retries == 0) {\r\n        LOG.info(\"Retried {}\", text);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createAndStartAuditManager",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AuditManagerS3A createAndStartAuditManager(Configuration conf, IOStatisticsStore iostatistics)\n{\r\n    AuditManagerS3A auditManager;\r\n    if (conf.getBoolean(AUDIT_ENABLED, AUDIT_ENABLED_DEFAULT)) {\r\n        auditManager = new ActiveAuditManagerS3A(requireNonNull(iostatistics));\r\n    } else {\r\n        LOG.debug(\"auditing is disabled\");\r\n        auditManager = stubAuditManager();\r\n    }\r\n    auditManager.init(conf);\r\n    auditManager.start();\r\n    LOG.debug(\"Started Audit Manager {}\", auditManager);\r\n    return auditManager;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "stubAuditManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditManagerS3A stubAuditManager()\n{\r\n    return new NoopAuditManagerS3A();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "createAndInitAuditor",
  "errType" : [ "NoSuchMethodException|InstantiationException|RuntimeException|IllegalAccessException|InvocationTargetException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "OperationAuditor createAndInitAuditor(Configuration conf, String key, OperationAuditorOptions options) throws IOException\n{\r\n    final Class<? extends OperationAuditor> auditClassname = conf.getClass(key, LoggingAuditor.class, OperationAuditor.class);\r\n    try {\r\n        LOG.debug(\"Auditor class is {}\", auditClassname);\r\n        final Constructor<? extends OperationAuditor> constructor = auditClassname.getConstructor();\r\n        final OperationAuditor instance = constructor.newInstance();\r\n        instance.init(options);\r\n        return instance;\r\n    } catch (NoSuchMethodException | InstantiationException | RuntimeException | IllegalAccessException | InvocationTargetException e) {\r\n        throw new IOException(\"Failed to instantiate class \" + auditClassname + \" defined in \" + key + \": \" + e, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "retrieveAttachedSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AWSAuditEventCallbacks retrieveAttachedSpan(final T request)\n{\r\n    return request.getHandlerContext(AUDIT_SPAN_HANDLER_CONTEXT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "attachSpanToRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void attachSpanToRequest(final T request, final AWSAuditEventCallbacks span)\n{\r\n    request.addHandlerContext(AUDIT_SPAN_HANDLER_CONTEXT, span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "translateException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException translateException(String operation, Path path, AmazonClientException exception)\n{\r\n    return translateException(operation, path.toString(), exception);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "translateException",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "IOException translateException(@Nullable String operation, String path, SdkBaseException exception)\n{\r\n    String message = String.format(\"%s%s: %s\", operation, StringUtils.isNotEmpty(path) ? (\" on \" + path) : \"\", exception);\r\n    if (!(exception instanceof AmazonServiceException)) {\r\n        Exception innerCause = containsInterruptedException(exception);\r\n        if (innerCause != null) {\r\n            return translateInterruptedException(exception, innerCause, message);\r\n        }\r\n        if (isMessageTranslatableToEOF(exception)) {\r\n            return (EOFException) new EOFException(message).initCause(exception);\r\n        }\r\n        if (exception instanceof CredentialInitializationException) {\r\n            return (AccessDeniedException) new AccessDeniedException(path, null, exception.toString()).initCause(exception);\r\n        }\r\n        return new AWSClientIOException(message, exception);\r\n    } else {\r\n        IOException ioe;\r\n        AmazonServiceException ase = (AmazonServiceException) exception;\r\n        AmazonS3Exception s3Exception = ase instanceof AmazonS3Exception ? (AmazonS3Exception) ase : null;\r\n        int status = ase.getStatusCode();\r\n        message = message + \":\" + ase.getErrorCode();\r\n        switch(status) {\r\n            case 301:\r\n            case 307:\r\n                if (s3Exception != null) {\r\n                    if (s3Exception.getAdditionalDetails() != null && s3Exception.getAdditionalDetails().containsKey(ENDPOINT_KEY)) {\r\n                        message = String.format(\"Received permanent redirect response to \" + \"endpoint %s.  This likely indicates that the S3 endpoint \" + \"configured in %s does not match the AWS region containing \" + \"the bucket.\", s3Exception.getAdditionalDetails().get(ENDPOINT_KEY), ENDPOINT);\r\n                    }\r\n                    ioe = new AWSRedirectException(message, s3Exception);\r\n                } else {\r\n                    ioe = new AWSRedirectException(message, ase);\r\n                }\r\n                break;\r\n            case 400:\r\n                ioe = new AWSBadRequestException(message, ase);\r\n                break;\r\n            case 401:\r\n            case 403:\r\n                ioe = new AccessDeniedException(path, null, message);\r\n                ioe.initCause(ase);\r\n                break;\r\n            case 404:\r\n                if (isUnknownBucket(ase)) {\r\n                    ioe = new UnknownStoreException(path, message, ase);\r\n                } else {\r\n                    ioe = new FileNotFoundException(message);\r\n                    ioe.initCause(ase);\r\n                }\r\n                break;\r\n            case 410:\r\n                ioe = new FileNotFoundException(message);\r\n                ioe.initCause(ase);\r\n                break;\r\n            case 405:\r\n                ioe = new AWSBadRequestException(message, s3Exception);\r\n                break;\r\n            case 416:\r\n                ioe = new EOFException(message);\r\n                ioe.initCause(ase);\r\n                break;\r\n            case 443:\r\n            case 444:\r\n                ioe = new AWSNoResponseException(message, ase);\r\n                break;\r\n            case 503:\r\n                ioe = new AWSServiceThrottledException(message, ase);\r\n                break;\r\n            case 500:\r\n                ioe = new AWSStatus500Exception(message, ase);\r\n                break;\r\n            case 200:\r\n                if (exception instanceof MultiObjectDeleteException) {\r\n                    return translateDeleteException(message, (MultiObjectDeleteException) exception);\r\n                }\r\n            default:\r\n                ioe = s3Exception != null ? new AWSS3IOException(message, s3Exception) : new AWSServiceIOException(message, ase);\r\n                break;\r\n        }\r\n        return ioe;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "extractException",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "IOException extractException(String operation, String path, ExecutionException ee)\n{\r\n    IOException ioe;\r\n    Throwable cause = ee.getCause();\r\n    if (cause instanceof AmazonClientException) {\r\n        ioe = translateException(operation, path, (AmazonClientException) cause);\r\n    } else if (cause instanceof IOException) {\r\n        ioe = (IOException) cause;\r\n    } else {\r\n        ioe = new IOException(operation + \" failed: \" + cause, cause);\r\n    }\r\n    return ioe;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "containsInterruptedException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Exception containsInterruptedException(Throwable thrown)\n{\r\n    if (thrown == null) {\r\n        return null;\r\n    }\r\n    if (thrown instanceof InterruptedException || thrown instanceof InterruptedIOException || thrown instanceof AbortedException) {\r\n        return (Exception) thrown;\r\n    }\r\n    return containsInterruptedException(thrown.getCause());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "translateInterruptedException",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "InterruptedIOException translateInterruptedException(SdkBaseException exception, final Exception innerCause, String message)\n{\r\n    InterruptedIOException ioe;\r\n    if (innerCause instanceof SocketTimeoutException) {\r\n        ioe = new SocketTimeoutException(message);\r\n    } else {\r\n        String name = innerCause.getClass().getName();\r\n        if (name.endsWith(\".ConnectTimeoutException\") || name.endsWith(\"$ConnectTimeoutException\")) {\r\n            ioe = new ConnectTimeoutException(message);\r\n        } else {\r\n            ioe = new InterruptedIOException(message);\r\n        }\r\n    }\r\n    ioe.initCause(exception);\r\n    return ioe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isThrottleException",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isThrottleException(Exception ex)\n{\r\n    return ex instanceof AWSServiceThrottledException || (ex instanceof AmazonServiceException && 503 == ((AmazonServiceException) ex).getStatusCode()) || (ex instanceof SdkBaseException && RetryUtils.isThrottlingException((SdkBaseException) ex));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isMessageTranslatableToEOF",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isMessageTranslatableToEOF(SdkBaseException ex)\n{\r\n    return ex.toString().contains(EOF_MESSAGE_IN_XML_PARSER) || ex.toString().contains(EOF_READ_DIFFERENT_LENGTH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "stringify",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String stringify(AmazonServiceException e)\n{\r\n    StringBuilder builder = new StringBuilder(String.format(\"%s: %s error %d: %s; %s%s%n\", e.getErrorType(), e.getServiceName(), e.getStatusCode(), e.getErrorCode(), e.getErrorMessage(), (e.isRetryable() ? \" (retryable)\" : \"\")));\r\n    String rawResponseContent = e.getRawResponseContent();\r\n    if (rawResponseContent != null) {\r\n        builder.append(rawResponseContent);\r\n    }\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "stringify",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String stringify(AmazonS3Exception e)\n{\r\n    StringBuilder builder = new StringBuilder(stringify((AmazonServiceException) e));\r\n    Map<String, String> details = e.getAdditionalDetails();\r\n    if (details != null) {\r\n        builder.append('\\n');\r\n        for (Map.Entry<String, String> d : details.entrySet()) {\r\n            builder.append(d.getKey()).append('=').append(d.getValue()).append('\\n');\r\n        }\r\n    }\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3AFileStatus createFileStatus(Path keyPath, S3ObjectSummary summary, long blockSize, String owner, String eTag, String versionId, boolean isCSEEnabled)\n{\r\n    long size = summary.getSize();\r\n    if (isCSEEnabled && size >= CSE_PADDING_LENGTH) {\r\n        size -= CSE_PADDING_LENGTH;\r\n    }\r\n    return createFileStatus(keyPath, objectRepresentsDirectory(summary.getKey()), size, summary.getLastModified(), blockSize, owner, eTag, versionId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createUploadFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus createUploadFileStatus(Path keyPath, boolean isDir, long size, long blockSize, String owner, String eTag, String versionId)\n{\r\n    Date date = isDir ? null : new Date();\r\n    return createFileStatus(keyPath, isDir, size, date, blockSize, owner, eTag, versionId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus createFileStatus(Path keyPath, boolean isDir, long size, Date modified, long blockSize, String owner, String eTag, String versionId)\n{\r\n    if (isDir) {\r\n        return new S3AFileStatus(Tristate.UNKNOWN, keyPath, owner);\r\n    } else {\r\n        return new S3AFileStatus(size, dateToLong(modified), keyPath, blockSize, owner, eTag, versionId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "objectRepresentsDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean objectRepresentsDirectory(final String name)\n{\r\n    return !name.isEmpty() && name.charAt(name.length() - 1) == '/';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "dateToLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long dateToLong(final Date date)\n{\r\n    if (date == null) {\r\n        return 0L;\r\n    }\r\n    return date.getTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createAWSCredentialProviderSet",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AWSCredentialProviderList createAWSCredentialProviderSet(@Nullable URI binding, Configuration conf) throws IOException\n{\r\n    S3xLoginHelper.rejectSecretsInURIs(binding);\r\n    AWSCredentialProviderList credentials = buildAWSProviderList(binding, conf, AWS_CREDENTIALS_PROVIDER, STANDARD_AWS_PROVIDERS, new HashSet<>());\r\n    LOG.debug(\"For URI {}, using credentials {}\", binding, credentials);\r\n    return credentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "loadAWSProviderClasses",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<Class<?>> loadAWSProviderClasses(Configuration conf, String key, Class<?>... defaultValue) throws IOException\n{\r\n    try {\r\n        return Arrays.asList(conf.getClasses(key, defaultValue));\r\n    } catch (RuntimeException e) {\r\n        Throwable c = e.getCause() != null ? e.getCause() : e;\r\n        throw new IOException(\"From option \" + key + ' ' + c, c);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildAWSProviderList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AWSCredentialProviderList buildAWSProviderList(@Nullable final URI binding, final Configuration conf, final String key, final List<Class<?>> defaultValues, final Set<Class<?>> forbidden) throws IOException\n{\r\n    List<Class<?>> awsClasses = loadAWSProviderClasses(conf, key, defaultValues.toArray(new Class[defaultValues.size()]));\r\n    if (awsClasses.isEmpty()) {\r\n        awsClasses = defaultValues;\r\n    }\r\n    AWSCredentialProviderList providers = new AWSCredentialProviderList();\r\n    for (Class<?> aClass : awsClasses) {\r\n        if (forbidden.contains(aClass)) {\r\n            throw new IOException(E_FORBIDDEN_AWS_PROVIDER + \" in option \" + key + \": \" + aClass);\r\n        }\r\n        providers.add(createAWSCredentialProvider(conf, aClass, binding));\r\n    }\r\n    return providers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createAWSCredentialProvider",
  "errType" : [ "InvocationTargetException", "ReflectiveOperationException|IllegalArgumentException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "AWSCredentialsProvider createAWSCredentialProvider(Configuration conf, Class<?> credClass, @Nullable URI uri) throws IOException\n{\r\n    AWSCredentialsProvider credentials = null;\r\n    String className = credClass.getName();\r\n    if (!AWSCredentialsProvider.class.isAssignableFrom(credClass)) {\r\n        throw new IOException(\"Class \" + credClass + \" \" + NOT_AWS_PROVIDER);\r\n    }\r\n    if (Modifier.isAbstract(credClass.getModifiers())) {\r\n        throw new IOException(\"Class \" + credClass + \" \" + ABSTRACT_PROVIDER);\r\n    }\r\n    LOG.debug(\"Credential provider class is {}\", className);\r\n    try {\r\n        Constructor cons = getConstructor(credClass, URI.class, Configuration.class);\r\n        if (cons != null) {\r\n            credentials = (AWSCredentialsProvider) cons.newInstance(uri, conf);\r\n            return credentials;\r\n        }\r\n        cons = getConstructor(credClass, Configuration.class);\r\n        if (cons != null) {\r\n            credentials = (AWSCredentialsProvider) cons.newInstance(conf);\r\n            return credentials;\r\n        }\r\n        Method factory = getFactoryMethod(credClass, AWSCredentialsProvider.class, \"getInstance\");\r\n        if (factory != null) {\r\n            credentials = (AWSCredentialsProvider) factory.invoke(null);\r\n            return credentials;\r\n        }\r\n        cons = getConstructor(credClass);\r\n        if (cons != null) {\r\n            credentials = (AWSCredentialsProvider) cons.newInstance();\r\n            return credentials;\r\n        }\r\n        throw new IOException(String.format(\"%s \" + CONSTRUCTOR_EXCEPTION + \".  A class specified in %s must provide a public constructor \" + \"of a supported signature, or a public factory method named \" + \"getInstance that accepts no arguments.\", className, AWS_CREDENTIALS_PROVIDER));\r\n    } catch (InvocationTargetException e) {\r\n        Throwable targetException = e.getTargetException();\r\n        if (targetException == null) {\r\n            targetException = e;\r\n        }\r\n        if (targetException instanceof IOException) {\r\n            throw (IOException) targetException;\r\n        } else if (targetException instanceof SdkBaseException) {\r\n            throw translateException(\"Instantiate \" + className, \"\", (SdkBaseException) targetException);\r\n        } else {\r\n            throw new IOException(className + \" \" + INSTANTIATION_EXCEPTION + \": \" + targetException, targetException);\r\n        }\r\n    } catch (ReflectiveOperationException | IllegalArgumentException e) {\r\n        throw new IOException(className + \" \" + INSTANTIATION_EXCEPTION + \": \" + e, e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setIfDefined",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean setIfDefined(Configuration config, String key, String val, String origin)\n{\r\n    if (StringUtils.isNotEmpty(val)) {\r\n        config.set(key, val, origin);\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAWSAccessKeys",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "S3xLoginHelper.Login getAWSAccessKeys(URI name, Configuration conf) throws IOException\n{\r\n    S3xLoginHelper.rejectSecretsInURIs(name);\r\n    Configuration c = ProviderUtils.excludeIncompatibleCredentialProviders(conf, S3AFileSystem.class);\r\n    String bucket = name != null ? name.getHost() : \"\";\r\n    String accessKey = lookupPassword(bucket, c, ACCESS_KEY);\r\n    String secretKey = lookupPassword(bucket, c, SECRET_KEY);\r\n    return new S3xLoginHelper.Login(accessKey, secretKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupPassword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String lookupPassword(String bucket, Configuration conf, String baseKey, String overrideVal) throws IOException\n{\r\n    return lookupPassword(bucket, conf, baseKey, overrideVal, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupPassword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String lookupPassword(String bucket, Configuration conf, String baseKey) throws IOException\n{\r\n    return lookupPassword(bucket, conf, baseKey, null, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupPassword",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String lookupPassword(String bucket, Configuration conf, String baseKey, String overrideVal, String defVal) throws IOException\n{\r\n    String initialVal;\r\n    Preconditions.checkArgument(baseKey.startsWith(FS_S3A_PREFIX), \"%s does not start with $%s\", baseKey, FS_S3A_PREFIX);\r\n    if (StringUtils.isNotEmpty(bucket)) {\r\n        String subkey = baseKey.substring(FS_S3A_PREFIX.length());\r\n        String shortBucketKey = String.format(BUCKET_PATTERN, bucket, subkey);\r\n        String longBucketKey = String.format(BUCKET_PATTERN, bucket, baseKey);\r\n        initialVal = getPassword(conf, longBucketKey, overrideVal);\r\n        initialVal = getPassword(conf, shortBucketKey, initialVal);\r\n    } else {\r\n        initialVal = overrideVal;\r\n    }\r\n    return getPassword(conf, baseKey, initialVal, defVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPassword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPassword(Configuration conf, String key, String val) throws IOException\n{\r\n    return getPassword(conf, key, val, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPassword",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getPassword(Configuration conf, String key, String val, String defVal) throws IOException\n{\r\n    return isEmpty(val) ? lookupPassword(conf, key, defVal) : val;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupPassword",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String lookupPassword(Configuration conf, String key, String defVal) throws IOException\n{\r\n    try {\r\n        final char[] pass = conf.getPassword(key);\r\n        return pass != null ? new String(pass).trim() : defVal;\r\n    } catch (IOException ioe) {\r\n        throw new IOException(\"Cannot find password option \" + key, ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "stringify",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String stringify(S3ObjectSummary summary)\n{\r\n    StringBuilder builder = new StringBuilder(summary.getKey().length() + 100);\r\n    builder.append(summary.getKey()).append(' ');\r\n    builder.append(\"size=\").append(summary.getSize());\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "intOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int intOption(Configuration conf, String key, int defVal, int min)\n{\r\n    int v = conf.getInt(key, defVal);\r\n    Preconditions.checkArgument(v >= min, String.format(\"Value of %s: %d is below the minimum value %d\", key, v, min));\r\n    LOG.debug(\"Value of {} is {}\", key, v);\r\n    return v;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "longOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long longOption(Configuration conf, String key, long defVal, long min)\n{\r\n    long v = conf.getLong(key, defVal);\r\n    Preconditions.checkArgument(v >= min, String.format(\"Value of %s: %d is below the minimum value %d\", key, v, min));\r\n    LOG.debug(\"Value of {} is {}\", key, v);\r\n    return v;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "longBytesOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long longBytesOption(Configuration conf, String key, long defVal, long min)\n{\r\n    long v = conf.getLongBytes(key, defVal);\r\n    Preconditions.checkArgument(v >= min, String.format(\"Value of %s: %d is below the minimum value %d\", key, v, min));\r\n    LOG.debug(\"Value of {} is {}\", key, v);\r\n    return v;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMultipartSizeProperty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getMultipartSizeProperty(Configuration conf, String property, long defVal)\n{\r\n    long partSize = conf.getLongBytes(property, defVal);\r\n    if (partSize < MULTIPART_MIN_SIZE) {\r\n        LOG.warn(\"{} must be at least 5 MB; configured value is {}\", property, partSize);\r\n        partSize = MULTIPART_MIN_SIZE;\r\n    }\r\n    return partSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "ensureOutputParameterInRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int ensureOutputParameterInRange(String name, long size)\n{\r\n    if (size > Integer.MAX_VALUE) {\r\n        LOG.warn(\"s3a: {} capped to ~2.14GB\" + \" (maximum allowed size with current output mechanism)\", name);\r\n        return Integer.MAX_VALUE;\r\n    } else {\r\n        return (int) size;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getConstructor",
  "errType" : [ "NoSuchMethodException|SecurityException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Constructor<?> getConstructor(Class<?> cl, Class<?>... args)\n{\r\n    try {\r\n        Constructor cons = cl.getDeclaredConstructor(args);\r\n        return Modifier.isPublic(cons.getModifiers()) ? cons : null;\r\n    } catch (NoSuchMethodException | SecurityException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFactoryMethod",
  "errType" : [ "NoSuchMethodException|SecurityException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Method getFactoryMethod(Class<?> cl, Class<?> returnType, String methodName)\n{\r\n    try {\r\n        Method m = cl.getDeclaredMethod(methodName);\r\n        if (Modifier.isPublic(m.getModifiers()) && Modifier.isStatic(m.getModifiers()) && returnType.isAssignableFrom(m.getReturnType())) {\r\n            return m;\r\n        } else {\r\n            return null;\r\n        }\r\n    } catch (NoSuchMethodException | SecurityException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "propagateBucketOptions",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Configuration propagateBucketOptions(Configuration source, String bucket)\n{\r\n    Preconditions.checkArgument(StringUtils.isNotEmpty(bucket), \"bucket is null/empty\");\r\n    final String bucketPrefix = FS_S3A_BUCKET_PREFIX + bucket + '.';\r\n    LOG.debug(\"Propagating entries under {}\", bucketPrefix);\r\n    final Configuration dest = new Configuration(source);\r\n    for (Map.Entry<String, String> entry : source) {\r\n        final String key = entry.getKey();\r\n        final String value = entry.getValue();\r\n        if (!key.startsWith(bucketPrefix) || bucketPrefix.equals(key)) {\r\n            continue;\r\n        }\r\n        final String stripped = key.substring(bucketPrefix.length());\r\n        if (stripped.startsWith(\"bucket.\") || \"impl\".equals(stripped)) {\r\n            LOG.debug(\"Ignoring bucket option {}\", key);\r\n        } else {\r\n            String origin = \"[\" + StringUtils.join(source.getPropertySources(key), \", \") + \"]\";\r\n            final String generic = FS_S3A_PREFIX + stripped;\r\n            LOG.debug(\"Updating {} from {}\", generic, origin);\r\n            dest.set(generic, value, key + \" via \" + origin);\r\n        }\r\n    }\r\n    return dest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteQuietly",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteQuietly(FileSystem fs, Path path, boolean recursive)\n{\r\n    try {\r\n        fs.delete(path, recursive);\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to delete {}\", path, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteWithWarning",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteWithWarning(FileSystem fs, Path path, boolean recursive)\n{\r\n    try {\r\n        fs.delete(path, recursive);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Failed to delete {}\", path, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createAwsConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientConfiguration createAwsConf(Configuration conf, String bucket) throws IOException\n{\r\n    return createAwsConf(conf, bucket, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createAwsConf",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ClientConfiguration createAwsConf(Configuration conf, String bucket, String awsServiceIdentifier) throws IOException\n{\r\n    final ClientConfiguration awsConf = new ClientConfiguration();\r\n    initConnectionSettings(conf, awsConf);\r\n    initProxySupport(conf, bucket, awsConf);\r\n    initUserAgent(conf, awsConf);\r\n    if (StringUtils.isNotEmpty(awsServiceIdentifier)) {\r\n        String configKey = null;\r\n        switch(awsServiceIdentifier) {\r\n            case AWS_SERVICE_IDENTIFIER_S3:\r\n                configKey = SIGNING_ALGORITHM_S3;\r\n                break;\r\n            case AWS_SERVICE_IDENTIFIER_STS:\r\n                configKey = SIGNING_ALGORITHM_STS;\r\n                break;\r\n            default:\r\n        }\r\n        if (configKey != null) {\r\n            String signerOverride = conf.getTrimmed(configKey, \"\");\r\n            if (!signerOverride.isEmpty()) {\r\n                LOG.debug(\"Signer override for {}} = {}\", awsServiceIdentifier, signerOverride);\r\n                awsConf.setSignerOverride(signerOverride);\r\n            }\r\n        }\r\n    }\r\n    return awsConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initConnectionSettings",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void initConnectionSettings(Configuration conf, ClientConfiguration awsConf) throws IOException\n{\r\n    awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS, DEFAULT_MAXIMUM_CONNECTIONS, 1));\r\n    initProtocolSettings(conf, awsConf);\r\n    awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES, DEFAULT_MAX_ERROR_RETRIES, 0));\r\n    awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT, DEFAULT_ESTABLISH_TIMEOUT, 0));\r\n    awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT, DEFAULT_SOCKET_TIMEOUT, 0));\r\n    int sockSendBuffer = intOption(conf, SOCKET_SEND_BUFFER, DEFAULT_SOCKET_SEND_BUFFER, 2048);\r\n    int sockRecvBuffer = intOption(conf, SOCKET_RECV_BUFFER, DEFAULT_SOCKET_RECV_BUFFER, 2048);\r\n    long requestTimeoutMillis = conf.getTimeDuration(REQUEST_TIMEOUT, DEFAULT_REQUEST_TIMEOUT, TimeUnit.SECONDS, TimeUnit.MILLISECONDS);\r\n    if (requestTimeoutMillis > Integer.MAX_VALUE) {\r\n        LOG.debug(\"Request timeout is too high({} ms). Setting to {} ms instead\", requestTimeoutMillis, Integer.MAX_VALUE);\r\n        requestTimeoutMillis = Integer.MAX_VALUE;\r\n    }\r\n    awsConf.setRequestTimeout((int) requestTimeoutMillis);\r\n    awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\r\n    String signerOverride = conf.getTrimmed(SIGNING_ALGORITHM, \"\");\r\n    if (!signerOverride.isEmpty()) {\r\n        LOG.debug(\"Signer override = {}\", signerOverride);\r\n        awsConf.setSignerOverride(signerOverride);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initProtocolSettings",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initProtocolSettings(Configuration conf, ClientConfiguration awsConf) throws IOException\n{\r\n    boolean secureConnections = conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS);\r\n    awsConf.setProtocol(secureConnections ? Protocol.HTTPS : Protocol.HTTP);\r\n    if (secureConnections) {\r\n        NetworkBinding.bindSSLChannelMode(conf, awsConf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initProxySupport",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void initProxySupport(Configuration conf, String bucket, ClientConfiguration awsConf) throws IllegalArgumentException, IOException\n{\r\n    String proxyHost = conf.getTrimmed(PROXY_HOST, \"\");\r\n    int proxyPort = conf.getInt(PROXY_PORT, -1);\r\n    if (!proxyHost.isEmpty()) {\r\n        awsConf.setProxyHost(proxyHost);\r\n        if (proxyPort >= 0) {\r\n            awsConf.setProxyPort(proxyPort);\r\n        } else {\r\n            if (conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS)) {\r\n                LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\r\n                awsConf.setProxyPort(443);\r\n            } else {\r\n                LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\r\n                awsConf.setProxyPort(80);\r\n            }\r\n        }\r\n        final String proxyUsername = lookupPassword(bucket, conf, PROXY_USERNAME, null, null);\r\n        final String proxyPassword = lookupPassword(bucket, conf, PROXY_PASSWORD, null, null);\r\n        if ((proxyUsername == null) != (proxyPassword == null)) {\r\n            String msg = \"Proxy error: \" + PROXY_USERNAME + \" or \" + PROXY_PASSWORD + \" set without the other.\";\r\n            LOG.error(msg);\r\n            throw new IllegalArgumentException(msg);\r\n        }\r\n        awsConf.setProxyUsername(proxyUsername);\r\n        awsConf.setProxyPassword(proxyPassword);\r\n        awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\r\n        awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" + \"domain {} as workstation {}\", awsConf.getProxyHost(), awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()), awsConf.getProxyPassword(), awsConf.getProxyDomain(), awsConf.getProxyWorkstation());\r\n        }\r\n    } else if (proxyPort >= 0) {\r\n        String msg = \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\r\n        LOG.error(msg);\r\n        throw new IllegalArgumentException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initUserAgent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void initUserAgent(Configuration conf, ClientConfiguration awsConf)\n{\r\n    String userAgent = \"Hadoop \" + VersionInfo.getVersion();\r\n    String userAgentPrefix = conf.getTrimmed(USER_AGENT_PREFIX, \"\");\r\n    if (!userAgentPrefix.isEmpty()) {\r\n        userAgent = userAgentPrefix + \", \" + userAgent;\r\n    }\r\n    LOG.debug(\"Using User-Agent: {}\", userAgent);\r\n    awsConf.setUserAgentPrefix(userAgent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "iteratorToStatuses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus[] iteratorToStatuses(RemoteIterator<S3AFileStatus> iterator) throws IOException\n{\r\n    S3AFileStatus[] statuses = RemoteIterators.toArray(iterator, new S3AFileStatus[0]);\r\n    return statuses;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "applyLocatedFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long applyLocatedFiles(RemoteIterator<? extends LocatedFileStatus> iterator, CallOnLocatedFileStatus eval) throws IOException\n{\r\n    return RemoteIterators.foreach(iterator, eval::call);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mapLocatedFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<T> mapLocatedFiles(RemoteIterator<? extends LocatedFileStatus> iterator, LocatedFileStatusMap<T> eval) throws IOException\n{\r\n    final List<T> results = new ArrayList<>();\r\n    applyLocatedFiles(iterator, (s) -> results.add(eval.call(s)));\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "flatmapLocatedFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<T> flatmapLocatedFiles(RemoteIterator<LocatedFileStatus> iterator, LocatedFileStatusMap<Optional<T>> eval) throws IOException\n{\r\n    final List<T> results = new ArrayList<>();\r\n    applyLocatedFiles(iterator, (s) -> eval.call(s).map(r -> results.add(r)));\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listAndFilter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<LocatedFileStatus> listAndFilter(FileSystem fileSystem, Path path, boolean recursive, PathFilter filter) throws IOException\n{\r\n    return flatmapLocatedFiles(fileSystem.listFiles(path, recursive), status -> maybe(filter.accept(status.getPath()), status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybe",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Optional<T> maybe(boolean include, T value)\n{\r\n    return include ? Optional.of(value) : Optional.empty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "patchSecurityCredentialProviders",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void patchSecurityCredentialProviders(Configuration conf)\n{\r\n    Collection<String> customCredentials = conf.getStringCollection(S3A_SECURITY_CREDENTIAL_PROVIDER_PATH);\r\n    Collection<String> hadoopCredentials = conf.getStringCollection(CREDENTIAL_PROVIDER_PATH);\r\n    if (!customCredentials.isEmpty()) {\r\n        List<String> all = Lists.newArrayList(customCredentials);\r\n        all.addAll(hadoopCredentials);\r\n        String joined = StringUtils.join(all, ',');\r\n        LOG.debug(\"Setting {} to {}\", CREDENTIAL_PROVIDER_PATH, joined);\r\n        conf.set(CREDENTIAL_PROVIDER_PATH, joined, \"patch of \" + S3A_SECURITY_CREDENTIAL_PROVIDER_PATH);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupBucketSecret",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String lookupBucketSecret(String bucket, Configuration conf, String baseKey) throws IOException\n{\r\n    Preconditions.checkArgument(!isEmpty(bucket), \"null/empty bucket argument\");\r\n    Preconditions.checkArgument(baseKey.startsWith(FS_S3A_PREFIX), \"%s does not start with $%s\", baseKey, FS_S3A_PREFIX);\r\n    String subkey = baseKey.substring(FS_S3A_PREFIX.length());\r\n    String longBucketKey = String.format(BUCKET_PATTERN, bucket, baseKey);\r\n    String initialVal = getPassword(conf, longBucketKey, null, null);\r\n    String shortBucketKey = String.format(BUCKET_PATTERN, bucket, subkey);\r\n    return getPassword(conf, shortBucketKey, initialVal, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getS3EncryptionKey",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getS3EncryptionKey(String bucket, Configuration conf)\n{\r\n    try {\r\n        return getS3EncryptionKey(bucket, conf, false);\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getS3EncryptionKey",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getS3EncryptionKey(String bucket, Configuration conf, boolean propagateExceptions) throws IOException\n{\r\n    try {\r\n        String key = lookupBucketSecret(bucket, conf, S3_ENCRYPTION_KEY);\r\n        if (key == null) {\r\n            key = lookupBucketSecret(bucket, conf, SERVER_SIDE_ENCRYPTION_KEY);\r\n        }\r\n        if (key == null) {\r\n            key = lookupPassword(null, conf, S3_ENCRYPTION_KEY);\r\n        }\r\n        if (key == null) {\r\n            key = lookupPassword(null, conf, SERVER_SIDE_ENCRYPTION_KEY);\r\n        }\r\n        if (key == null) {\r\n            key = \"\";\r\n        }\r\n        return key;\r\n    } catch (IOException e) {\r\n        if (propagateExceptions) {\r\n            throw e;\r\n        }\r\n        LOG.warn(\"Cannot retrieve {} for bucket {}\", S3_ENCRYPTION_KEY, bucket, e);\r\n        return \"\";\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getEncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AEncryptionMethods getEncryptionAlgorithm(String bucket, Configuration conf) throws IOException\n{\r\n    return buildEncryptionSecrets(bucket, conf).getEncryptionMethod();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildEncryptionSecrets",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "EncryptionSecrets buildEncryptionSecrets(String bucket, Configuration conf) throws IOException\n{\r\n    String algorithm = lookupBucketSecret(bucket, conf, S3_ENCRYPTION_ALGORITHM);\r\n    if (algorithm == null) {\r\n        algorithm = lookupBucketSecret(bucket, conf, SERVER_SIDE_ENCRYPTION_ALGORITHM);\r\n    }\r\n    if (algorithm == null) {\r\n        algorithm = lookupPassword(null, conf, S3_ENCRYPTION_ALGORITHM);\r\n    }\r\n    if (algorithm == null) {\r\n        algorithm = lookupPassword(null, conf, SERVER_SIDE_ENCRYPTION_ALGORITHM);\r\n    }\r\n    final S3AEncryptionMethods encryptionMethod = S3AEncryptionMethods.getMethod(algorithm);\r\n    String encryptionKey = getS3EncryptionKey(bucket, conf, encryptionMethod.requiresSecret());\r\n    int encryptionKeyLen = StringUtils.isBlank(encryptionKey) ? 0 : encryptionKey.length();\r\n    String diagnostics = passwordDiagnostics(encryptionKey, \"key\");\r\n    switch(encryptionMethod) {\r\n        case SSE_C:\r\n            LOG.debug(\"Using SSE-C with {}\", diagnostics);\r\n            if (encryptionKeyLen == 0) {\r\n                throw new IOException(SSE_C_NO_KEY_ERROR);\r\n            }\r\n            break;\r\n        case SSE_S3:\r\n            if (encryptionKeyLen != 0) {\r\n                throw new IOException(SSE_S3_WITH_KEY_ERROR + \" (\" + diagnostics + \")\");\r\n            }\r\n            break;\r\n        case SSE_KMS:\r\n            LOG.debug(\"Using SSE-KMS with {}\", diagnostics);\r\n            break;\r\n        case CSE_KMS:\r\n            LOG.debug(\"Using CSE-KMS with {}\", diagnostics);\r\n            break;\r\n        case NONE:\r\n        default:\r\n            LOG.debug(\"Data is unencrypted\");\r\n            break;\r\n    }\r\n    return new EncryptionSecrets(encryptionMethod, encryptionKey);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "passwordDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String passwordDiagnostics(String pass, String description)\n{\r\n    if (pass == null) {\r\n        return \"null \" + description;\r\n    }\r\n    int len = pass.length();\r\n    switch(len) {\r\n        case 0:\r\n            return \"empty \" + description;\r\n        case 1:\r\n            return description + \" of length 1\";\r\n        default:\r\n            return description + \" of length \" + len + \" ending with \" + pass.charAt(len - 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "closeAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeAll(Logger log, Closeable... closeables)\n{\r\n    cleanupWithLogger(log, closeables);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "closeAutocloseables",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void closeAutocloseables(Logger log, AutoCloseable... closeables)\n{\r\n    if (log == null) {\r\n        log = LOG;\r\n    }\r\n    for (AutoCloseable c : closeables) {\r\n        if (c != null) {\r\n            try {\r\n                log.debug(\"Closing {}\", c);\r\n                c.close();\r\n            } catch (Exception e) {\r\n                log.debug(\"Exception in closing {}\", c, e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setBucketOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setBucketOption(Configuration conf, String bucket, String genericKey, String value)\n{\r\n    final String baseKey = genericKey.startsWith(FS_S3A_PREFIX) ? genericKey.substring(FS_S3A_PREFIX.length()) : genericKey;\r\n    conf.set(FS_S3A_BUCKET_PREFIX + bucket + '.' + baseKey, value, \"S3AUtils\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "clearBucketOption",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void clearBucketOption(Configuration conf, String bucket, String genericKey)\n{\r\n    final String baseKey = genericKey.startsWith(FS_S3A_PREFIX) ? genericKey.substring(FS_S3A_PREFIX.length()) : genericKey;\r\n    String k = FS_S3A_BUCKET_PREFIX + bucket + '.' + baseKey;\r\n    LOG.debug(\"Unset {}\", k);\r\n    conf.unset(k);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBucketOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getBucketOption(Configuration conf, String bucket, String genericKey)\n{\r\n    final String baseKey = genericKey.startsWith(FS_S3A_PREFIX) ? genericKey.substring(FS_S3A_PREFIX.length()) : genericKey;\r\n    return conf.get(FS_S3A_BUCKET_PREFIX + bucket + '.' + baseKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeAddTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String maybeAddTrailingSlash(String key)\n{\r\n    if (!key.isEmpty() && !key.endsWith(\"/\")) {\r\n        return key + '/';\r\n    } else {\r\n        return key;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getExitCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getExitCode()\n{\r\n    return exitCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "withExitCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RenameFailedException withExitCode(boolean code)\n{\r\n    this.exitCode = code;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getSpanId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSpanId()\n{\r\n    return spanId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getOperationName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOperationName()\n{\r\n    return operationName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimestamp()\n{\r\n    return timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "activate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanS3A activate()\n{\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    deactivate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getOwnerAccountId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOwnerAccountId()\n{\r\n    return ownerAccountId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRegion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRegion()\n{\r\n    return region;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFullArn",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFullArn()\n{\r\n    return fullArn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getEndpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getEndpoint()\n{\r\n    return String.format(ACCESSPOINT_ENDPOINT_FORMAT, region);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "accessPointFromArn",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ArnResource accessPointFromArn(String arn) throws IllegalArgumentException\n{\r\n    Arn parsed = Arn.fromString(arn);\r\n    if (parsed.getRegion().isEmpty() || parsed.getAccountId().isEmpty() || parsed.getResourceAsString().isEmpty()) {\r\n        throw new IllegalArgumentException(String.format(\"Access Point Arn %s has an invalid format or missing properties\", arn));\r\n    }\r\n    String resourceName = parsed.getResource().getResource();\r\n    return new ArnResource(resourceName, parsed.getAccountId(), parsed.getRegion(), parsed.getPartition(), arn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "fromSTSCredentials",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "MarshalledCredentials fromSTSCredentials(final Credentials credentials)\n{\r\n    MarshalledCredentials marshalled = new MarshalledCredentials(credentials.getAccessKeyId(), credentials.getSecretAccessKey(), credentials.getSessionToken());\r\n    Date date = credentials.getExpiration();\r\n    marshalled.setExpiration(date != null ? date.getTime() : 0);\r\n    return marshalled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "fromAWSCredentials",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MarshalledCredentials fromAWSCredentials(final AWSSessionCredentials credentials)\n{\r\n    return new MarshalledCredentials(credentials.getAWSAccessKeyId(), credentials.getAWSSecretKey(), credentials.getSessionToken());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "fromEnvironment",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MarshalledCredentials fromEnvironment(final Map<String, String> env)\n{\r\n    return new MarshalledCredentials(nullToEmptyString(env.get(\"AWS_ACCESS_KEY\")), nullToEmptyString(env.get(\"AWS_SECRET_KEY\")), nullToEmptyString(env.get(\"AWS_SESSION_TOKEN\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "nullToEmptyString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String nullToEmptyString(final String src)\n{\r\n    return src == null ? \"\" : src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "fromFileSystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "MarshalledCredentials fromFileSystem(final URI uri, final Configuration conf) throws IOException\n{\r\n    final String bucket = uri != null ? uri.getHost() : \"\";\r\n    final Configuration leanConf = ProviderUtils.excludeIncompatibleCredentialProviders(conf, S3AFileSystem.class);\r\n    return new MarshalledCredentials(lookupPassword(bucket, leanConf, ACCESS_KEY), lookupPassword(bucket, leanConf, SECRET_KEY), lookupPassword(bucket, leanConf, SESSION_TOKEN));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "toAWSCredentials",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AWSCredentials toAWSCredentials(final MarshalledCredentials marshalled, final MarshalledCredentials.CredentialTypeRequired typeRequired, final String component) throws NoAuthWithAWSException, NoAwsCredentialsException\n{\r\n    if (marshalled.isEmpty()) {\r\n        throw new NoAwsCredentialsException(component, NO_AWS_CREDENTIALS);\r\n    }\r\n    if (!marshalled.isValid(typeRequired)) {\r\n        throw new NoAuthWithAWSException(component + \":\" + marshalled.buildInvalidCredentialsError(typeRequired));\r\n    }\r\n    final String accessKey = marshalled.getAccessKey();\r\n    final String secretKey = marshalled.getSecretKey();\r\n    if (marshalled.hasSessionToken()) {\r\n        return new BasicSessionCredentials(accessKey, secretKey, marshalled.getSessionToken());\r\n    } else {\r\n        return new BasicAWSCredentials(accessKey, secretKey);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "requestSessionCredentials",
  "errType" : [ "SdkClientException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MarshalledCredentials requestSessionCredentials(final AWSCredentialsProvider parentCredentials, final ClientConfiguration awsConf, final String stsEndpoint, final String stsRegion, final int duration, final Invoker invoker) throws IOException\n{\r\n    try {\r\n        final AWSSecurityTokenService tokenService = STSClientFactory.builder(parentCredentials, awsConf, stsEndpoint.isEmpty() ? null : stsEndpoint, stsRegion).build();\r\n        return fromSTSCredentials(STSClientFactory.createClientConnection(tokenService, invoker).requestSessionCredentials(duration, TimeUnit.SECONDS));\r\n    } catch (SdkClientException e) {\r\n        if (stsRegion.isEmpty()) {\r\n            LOG.error(\"Region must be provided when requesting session credentials.\", e);\r\n        }\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUsage()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    IOUtils.cleanupWithLogger(LOG, baseFS);\r\n    baseFS = null;\r\n    filesystem = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getDeltaComponent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getDeltaComponent(TimeUnit unit, String arg)\n{\r\n    String raw = getCommandFormat().getOptValue(arg);\r\n    if (raw == null || raw.isEmpty()) {\r\n        return 0;\r\n    }\r\n    Long parsed = Long.parseLong(raw);\r\n    return unit.toMillis(parsed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "ageOptionsToMsec",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long ageOptionsToMsec()\n{\r\n    long cliDelta = 0;\r\n    cliDelta += getDeltaComponent(TimeUnit.DAYS, DAYS_FLAG);\r\n    cliDelta += getDeltaComponent(TimeUnit.HOURS, HOURS_FLAG);\r\n    cliDelta += getDeltaComponent(TimeUnit.MINUTES, MINUTES_FLAG);\r\n    cliDelta += getDeltaComponent(TimeUnit.SECONDS, SECONDS_FLAG);\r\n    return cliDelta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "addAgeOptions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addAgeOptions()\n{\r\n    CommandFormat format = getCommandFormat();\r\n    format.addOptionWithValue(DAYS_FLAG);\r\n    format.addOptionWithValue(HOURS_FLAG);\r\n    format.addOptionWithValue(MINUTES_FLAG);\r\n    format.addOptionWithValue(SECONDS_FLAG);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "initS3AFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initS3AFileSystem(String path) throws IOException\n{\r\n    LOG.debug(\"Initializing S3A FS to {}\", path);\r\n    URI uri = toUri(path);\r\n    bindFilesystem(FileSystem.newInstance(uri, getConf()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "parseArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> parseArgs(String[] args)\n{\r\n    return getCommandFormat().parse(args, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getFilesystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AFileSystem getFilesystem()\n{\r\n    return filesystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "bindFilesystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "S3AFileSystem bindFilesystem(FileSystem bindingFS)\n{\r\n    FileSystem fs = bindingFS;\r\n    baseFS = bindingFS;\r\n    while (fs instanceof FilterFileSystem) {\r\n        fs = ((FilterFileSystem) fs).getRawFileSystem();\r\n    }\r\n    if (!(fs instanceof S3AFileSystem)) {\r\n        throw new ExitUtil.ExitException(EXIT_SERVICE_UNAVAILABLE, WRONG_FILESYSTEM + \"URI \" + fs.getUri() + \" : \" + fs.getClass().getName());\r\n    }\r\n    filesystem = (S3AFileSystem) fs;\r\n    return filesystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "resetBindings",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetBindings()\n{\r\n    filesystem = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getCommandFormat",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommandFormat getCommandFormat()\n{\r\n    return commandFormat;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    return run(args, System.out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int run(String[] args, PrintStream out) throws Exception, ExitUtil.ExitException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "dumpFileSystemStatistics",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void dumpFileSystemStatistics(PrintStream stream)\n{\r\n    FileSystem fs = getFilesystem();\r\n    if (fs == null) {\r\n        return;\r\n    }\r\n    println(stream, \"%nIO Statistics for %s%n\", fs.getUri());\r\n    final IOStatistics iostats = retrieveIOStatistics(fs);\r\n    if (iostats != null) {\r\n        println(stream, ioStatisticsToPrettyString(iostats));\r\n    } else {\r\n        println(stream, \"FileSystem does not provide IOStatistics\");\r\n    }\r\n    println(stream, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "toUri",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI toUri(String s3Path)\n{\r\n    URI uri;\r\n    try {\r\n        uri = new URI(s3Path);\r\n    } catch (URISyntaxException e) {\r\n        throw invalidArgs(\"Not a valid fileystem path: %s\", s3Path);\r\n    }\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "printHelp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void printHelp()\n{\r\n    if (command == null) {\r\n        errorln(\"Usage: hadoop \" + USAGE);\r\n        errorln(\"\\tperform S3A connector administrative commands.\");\r\n    } else {\r\n        errorln(\"Usage: hadoop \" + ENTRY_POINT + command.getUsage());\r\n    }\r\n    errorln();\r\n    errorln(COMMON_USAGE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "errorln",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void errorln()\n{\r\n    System.err.println();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "errorln",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void errorln(String x)\n{\r\n    System.err.println(x);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "println",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void println(PrintStream out, String format, Object... args)\n{\r\n    out.println(String.format(format, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "notFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExitUtil.ExitException notFound(FileNotFoundException e)\n{\r\n    return new ExitUtil.ExitException(E_NOT_FOUND, e.toString(), e);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "invalidArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExitUtil.ExitException invalidArgs(String format, Object... args)\n{\r\n    return exitException(INVALID_ARGUMENT, format, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "badState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExitUtil.ExitException badState(String format, Object... args)\n{\r\n    return exitException(E_BAD_STATE, format, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "s3guardUnsupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExitUtil.ExitException s3guardUnsupported()\n{\r\n    throw exitException(E_S3GUARD_UNSUPPORTED, E_UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "userAborted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExitUtil.ExitException userAborted(String format, Object... args)\n{\r\n    return exitException(ERROR, format, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "exitException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExitUtil.ExitException exitException(final int exitCode, final String format, final Object... args)\n{\r\n    return new ExitUtil.ExitException(exitCode, String.format(format, args));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int run(Configuration conf, String... args) throws Exception\n{\r\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r\n    if (otherArgs.length == 0) {\r\n        printHelp();\r\n        throw new ExitUtil.ExitException(E_USAGE, \"No arguments provided\");\r\n    }\r\n    final String subCommand = otherArgs[0];\r\n    LOG.debug(\"Executing command {}\", subCommand);\r\n    if (UNSUPPORTED_COMMANDS.contains(subCommand)) {\r\n        throw s3guardUnsupported();\r\n    }\r\n    switch(subCommand) {\r\n        case BucketInfo.NAME:\r\n            command = new BucketInfo(conf);\r\n            break;\r\n        case MarkerTool.MARKERS:\r\n            command = new MarkerTool(conf);\r\n            break;\r\n        case Uploads.NAME:\r\n            command = new Uploads(conf);\r\n            break;\r\n        case SelectTool.NAME:\r\n            command = new SelectTool(conf);\r\n            break;\r\n        default:\r\n            printHelp();\r\n            throw new ExitUtil.ExitException(E_USAGE, \"Unknown command \" + subCommand);\r\n    }\r\n    try {\r\n        return ToolRunner.run(conf, command, otherArgs);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, command);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "main",
  "errType" : [ "CommandFormat.UnknownOptionException", "ExitUtil.ExitException", "FileNotFoundException", "Throwable" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    try {\r\n        int ret = run(new Configuration(), args);\r\n        exit(ret, \"\");\r\n    } catch (CommandFormat.UnknownOptionException e) {\r\n        errorln(e.getMessage());\r\n        printHelp();\r\n        exit(E_USAGE, e.getMessage());\r\n    } catch (ExitUtil.ExitException e) {\r\n        LOG.debug(\"Exception raised\", e);\r\n        exit(e.getExitCode(), e.toString());\r\n    } catch (FileNotFoundException e) {\r\n        errorln(e.toString());\r\n        LOG.debug(\"Not found:\", e);\r\n        exit(EXIT_NOT_FOUND, e.toString());\r\n    } catch (Throwable e) {\r\n        if (e instanceof ExitCodeProvider) {\r\n            final ExitCodeProvider ec = (ExitCodeProvider) e;\r\n            LOG.debug(\"Exception raised\", e);\r\n            exit(ec.getExitCode(), e.toString());\r\n        } else {\r\n            e.printStackTrace(System.err);\r\n            exit(ERROR, e.toString());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "exit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void exit(int status, String text)\n{\r\n    ExitUtil.terminate(status, text);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "serializer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JsonSerialization<PendingSet> serializer()\n{\r\n    return new JsonSerialization<>(PendingSet.class, false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "PendingSet load(FileSystem fs, Path path) throws IOException\n{\r\n    LOG.debug(\"Reading pending commits in file {}\", path);\r\n    PendingSet instance = serializer().load(fs, path);\r\n    instance.validate();\r\n    return instance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PendingSet load(FileSystem fs, FileStatus status) throws IOException\n{\r\n    return load(fs, status.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void add(SinglePendingCommit commit)\n{\r\n    commits.add(commit);\r\n    IOStatisticsSnapshot st = commit.getIOStatistics();\r\n    if (st != null) {\r\n        iostats.aggregate(st);\r\n        st.clear();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "readObject",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readObject(ObjectInputStream inStream) throws IOException, ClassNotFoundException\n{\r\n    inStream.defaultReadObject();\r\n    validate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void validate() throws ValidationFailure\n{\r\n    verify(version == VERSION, \"Wrong version: %s\", version);\r\n    validateCollectionClass(extraData.keySet(), String.class);\r\n    validateCollectionClass(extraData.values(), String.class);\r\n    Set<String> destinations = new HashSet<>(commits.size());\r\n    validateCollectionClass(commits, SinglePendingCommit.class);\r\n    for (SinglePendingCommit c : commits) {\r\n        c.validate();\r\n        verify(!destinations.contains(c.getDestinationKey()), \"Destination %s is written to by more than one pending commit\", c.getDestinationKey());\r\n        destinations.add(c.getDestinationKey());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] toBytes() throws IOException\n{\r\n    return serializer().toBytes(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int size()\n{\r\n    return commits != null ? commits.size() : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void save(FileSystem fs, Path path, boolean overwrite) throws IOException\n{\r\n    serializer().save(fs, path, this, overwrite);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getVersion()\n{\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVersion(int version)\n{\r\n    this.version = version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getCommits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<SinglePendingCommit> getCommits()\n{\r\n    return commits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setCommits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCommits(List<SinglePendingCommit> commits)\n{\r\n    this.commits = commits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "putExtraData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void putExtraData(String key, String value)\n{\r\n    extraData.put(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobId(String jobId)\n{\r\n    this.jobId = jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsSnapshot getIOStatistics()\n{\r\n    return iostats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIOStatistics(final IOStatisticsSnapshot ioStatistics)\n{\r\n    this.iostats = ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "versionMismatchError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void versionMismatchError()\n{\r\n    counter.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getVersionMismatches",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getVersionMismatches()\n{\r\n    return counter.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"PartitionedStagingCommitter{\");\r\n    sb.append(super.toString());\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "commitTaskInternal",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int commitTaskInternal(TaskAttemptContext context, List<? extends FileStatus> taskOutput) throws IOException\n{\r\n    Path attemptPath = getTaskAttemptPath(context);\r\n    Set<String> partitions = Paths.getPartitions(attemptPath, taskOutput);\r\n    FileSystem fs = getDestFS();\r\n    if (getConflictResolutionMode(context, fs.getConf()) == ConflictResolution.FAIL) {\r\n        for (String partition : partitions) {\r\n            Path partitionPath = getFinalPath(partition + \"/file\", context).getParent();\r\n            if (fs.exists(partitionPath)) {\r\n                throw failDestinationExists(partitionPath, \"Committing task \" + context.getTaskAttemptID());\r\n            }\r\n        }\r\n    }\r\n    return super.commitTaskInternal(context, taskOutput);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "preCommitJob",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void preCommitJob(final JobContext context, final ActiveCommit pending) throws IOException\n{\r\n    FileSystem fs = getDestFS();\r\n    Configuration fsConf = fs.getConf();\r\n    boolean shouldPrecheckPendingFiles = true;\r\n    switch(getConflictResolutionMode(context, fsConf)) {\r\n        case FAIL:\r\n            break;\r\n        case APPEND:\r\n            break;\r\n        case REPLACE:\r\n            replacePartitions(context, pending);\r\n            shouldPrecheckPendingFiles = false;\r\n            break;\r\n        default:\r\n            throw new PathCommitException(\"\", getRole() + \": unknown conflict resolution mode: \" + getConflictResolutionMode(context, fsConf));\r\n    }\r\n    if (shouldPrecheckPendingFiles) {\r\n        precommitCheckPendingFiles(context, pending);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "replacePartitions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void replacePartitions(final JobContext context, final ActiveCommit pending) throws IOException\n{\r\n    Map<Path, String> partitions = new ConcurrentHashMap<>();\r\n    FileSystem sourceFS = pending.getSourceFS();\r\n    Tasks.Submitter submitter = buildSubmitter(context);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Replacing partitions\")) {\r\n        Tasks.foreach(pending.getSourceFiles()).stopOnFailure().suppressExceptions(false).executeWith(submitter).run(status -> {\r\n            PendingSet pendingSet = PendingSet.load(sourceFS, status);\r\n            Path lastParent = null;\r\n            for (SinglePendingCommit commit : pendingSet.getCommits()) {\r\n                Path parent = commit.destinationPath().getParent();\r\n                if (parent != null && !parent.equals(lastParent)) {\r\n                    partitions.put(parent, \"\");\r\n                    lastParent = parent;\r\n                }\r\n            }\r\n        });\r\n    }\r\n    FileSystem fs = getDestFS();\r\n    Tasks.foreach(partitions.keySet()).stopOnFailure().suppressExceptions(false).executeWith(submitter).run(partitionPath -> {\r\n        LOG.debug(\"{}: removing partition path to be replaced: \" + getRole(), partitionPath);\r\n        fs.delete(partitionPath, true);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "isUnknownBucket",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isUnknownBucket(AmazonServiceException e)\n{\r\n    return e.getStatusCode() == SC_404 && AwsErrorCodes.E_NO_SUCH_BUCKET.equals(e.getErrorCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "isObjectNotFound",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isObjectNotFound(AmazonServiceException e)\n{\r\n    return e.getStatusCode() == SC_404 && !isUnknownBucket(e);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getCopyResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CopyResult getCopyResult()\n{\r\n    return copyResult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getInterruptedException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InterruptedException getInterruptedException()\n{\r\n    return interruptedException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getAwsException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SdkBaseException getAwsException()\n{\r\n    return awsException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "waitForCopy",
  "errType" : [ "SdkBaseException", "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CopyOutcome waitForCopy(Copy copy)\n{\r\n    try {\r\n        CopyResult result = copy.waitForCopyResult();\r\n        return new CopyOutcome(result, null, null);\r\n    } catch (SdkBaseException e) {\r\n        return new CopyOutcome(null, null, e);\r\n    } catch (InterruptedException e) {\r\n        return new CopyOutcome(null, e, null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return statistics.getIOStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3AMultipartUploader{\");\r\n    sb.append(\"base=\").append(getBasePath());\r\n    sb.append(\"; statistics=\").append(ioStatisticsToString(statistics.getIOStatistics()));\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "startUpload",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CompletableFuture<UploadHandle> startUpload(final Path filePath) throws IOException\n{\r\n    Path dest = context.makeQualified(filePath);\r\n    checkPath(dest);\r\n    String key = context.pathToKey(dest);\r\n    return context.submit(new CompletableFuture<>(), () -> {\r\n        String uploadId = writeOperations.initiateMultiPartUpload(key);\r\n        statistics.uploadStarted();\r\n        return BBUploadHandle.from(ByteBuffer.wrap(uploadId.getBytes(Charsets.UTF_8)));\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "putPart",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "CompletableFuture<PartHandle> putPart(final UploadHandle uploadId, final int partNumber, final Path filePath, final InputStream inputStream, final long lengthInBytes) throws IOException\n{\r\n    Path dest = context.makeQualified(filePath);\r\n    checkPutArguments(dest, inputStream, partNumber, uploadId, lengthInBytes);\r\n    byte[] uploadIdBytes = uploadId.toByteArray();\r\n    checkUploadId(uploadIdBytes);\r\n    String key = context.pathToKey(dest);\r\n    String uploadIdString = new String(uploadIdBytes, 0, uploadIdBytes.length, Charsets.UTF_8);\r\n    return context.submit(new CompletableFuture<>(), () -> {\r\n        UploadPartRequest request = writeOperations.newUploadPartRequest(key, uploadIdString, partNumber, (int) lengthInBytes, inputStream, null, 0L);\r\n        UploadPartResult result = writeOperations.uploadPart(request);\r\n        statistics.partPut(lengthInBytes);\r\n        String eTag = result.getETag();\r\n        return BBPartHandle.from(ByteBuffer.wrap(buildPartHandlePayload(filePath.toUri().toString(), uploadIdString, result.getPartNumber(), eTag, lengthInBytes)));\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "complete",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "CompletableFuture<PathHandle> complete(final UploadHandle uploadHandle, final Path filePath, final Map<Integer, PartHandle> handleMap) throws IOException\n{\r\n    Path dest = context.makeQualified(filePath);\r\n    checkPath(dest);\r\n    byte[] uploadIdBytes = uploadHandle.toByteArray();\r\n    checkUploadId(uploadIdBytes);\r\n    checkPartHandles(handleMap);\r\n    List<Map.Entry<Integer, PartHandle>> handles = new ArrayList<>(handleMap.entrySet());\r\n    handles.sort(Comparator.comparingInt(Map.Entry::getKey));\r\n    int count = handles.size();\r\n    String key = context.pathToKey(dest);\r\n    String uploadIdStr = new String(uploadIdBytes, 0, uploadIdBytes.length, Charsets.UTF_8);\r\n    ArrayList<PartETag> eTags = new ArrayList<>();\r\n    eTags.ensureCapacity(handles.size());\r\n    long totalLength = 0;\r\n    Set<Integer> ids = new HashSet<>(count);\r\n    for (Map.Entry<Integer, PartHandle> handle : handles) {\r\n        PartHandlePayload payload = parsePartHandlePayload(handle.getValue().toByteArray());\r\n        payload.validate(uploadIdStr, filePath);\r\n        ids.add(payload.getPartNumber());\r\n        totalLength += payload.getLen();\r\n        eTags.add(new PartETag(handle.getKey(), payload.getEtag()));\r\n    }\r\n    Preconditions.checkArgument(ids.size() == count, \"Duplicate PartHandles\");\r\n    long finalLen = totalLength;\r\n    return context.submit(new CompletableFuture<>(), () -> {\r\n        CompleteMultipartUploadResult result = writeOperations.commitUpload(key, uploadIdStr, eTags, finalLen);\r\n        byte[] eTag = result.getETag().getBytes(Charsets.UTF_8);\r\n        statistics.uploadCompleted();\r\n        return (PathHandle) () -> ByteBuffer.wrap(eTag);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "abort",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "CompletableFuture<Void> abort(final UploadHandle uploadId, final Path filePath) throws IOException\n{\r\n    Path dest = context.makeQualified(filePath);\r\n    checkPath(dest);\r\n    final byte[] uploadIdBytes = uploadId.toByteArray();\r\n    checkUploadId(uploadIdBytes);\r\n    String uploadIdString = new String(uploadIdBytes, 0, uploadIdBytes.length, Charsets.UTF_8);\r\n    return context.submit(new CompletableFuture<>(), () -> {\r\n        writeOperations.abortMultipartCommit(context.pathToKey(dest), uploadIdString);\r\n        statistics.uploadAborted();\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "abortUploadsUnderPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CompletableFuture<Integer> abortUploadsUnderPath(final Path path) throws IOException\n{\r\n    statistics.abortUploadsUnderPathInvoked();\r\n    return context.submit(new CompletableFuture<>(), () -> writeOperations.abortMultipartUploadsUnderPath(context.pathToKey(path)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "buildPartHandlePayload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] buildPartHandlePayload(final String path, final String uploadId, final int partNumber, final String etag, final long len) throws IOException\n{\r\n    return new PartHandlePayload(path, uploadId, partNumber, len, etag).toBytes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "parsePartHandlePayload",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "PartHandlePayload parsePartHandlePayload(final byte[] data) throws IOException\n{\r\n    try (DataInputStream input = new DataInputStream(new ByteArrayInputStream(data))) {\r\n        final String header = input.readUTF();\r\n        if (!HEADER.equals(header)) {\r\n            throw new IOException(\"Wrong header string: \\\"\" + header + \"\\\"\");\r\n        }\r\n        final String path = input.readUTF();\r\n        final String uploadId = input.readUTF();\r\n        final int partNumber = input.readInt();\r\n        final long len = input.readLong();\r\n        final String etag = input.readUTF();\r\n        if (len < 0) {\r\n            throw new IOException(\"Negative length\");\r\n        }\r\n        return new PartHandlePayload(path, uploadId, partNumber, len, etag);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createThrottleRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RetryPolicy createThrottleRetryPolicy(final Configuration conf)\n{\r\n    return exponentialBackoffRetry(conf.getInt(RETRY_THROTTLE_LIMIT, RETRY_THROTTLE_LIMIT_DEFAULT), conf.getTimeDuration(RETRY_THROTTLE_INTERVAL, RETRY_THROTTLE_INTERVAL_DEFAULT, TimeUnit.MILLISECONDS), TimeUnit.MILLISECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createExceptionMap",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "Map<Class<? extends Exception>, RetryPolicy> createExceptionMap()\n{\r\n    Map<Class<? extends Exception>, RetryPolicy> policyMap = new HashMap<>();\r\n    policyMap.put(UnknownHostException.class, fail);\r\n    policyMap.put(NoRouteToHostException.class, fail);\r\n    policyMap.put(InterruptedException.class, fail);\r\n    policyMap.put(InterruptedIOException.class, fail);\r\n    policyMap.put(AccessDeniedException.class, fail);\r\n    policyMap.put(NoAuthWithAWSException.class, fail);\r\n    policyMap.put(FileNotFoundException.class, fail);\r\n    policyMap.put(UnknownStoreException.class, fail);\r\n    policyMap.put(InvalidRequestException.class, fail);\r\n    policyMap.put(RemoteFileChangedException.class, fail);\r\n    policyMap.put(NoVersionAttributeException.class, fail);\r\n    policyMap.put(AWSRedirectException.class, fail);\r\n    policyMap.put(AWSServiceThrottledException.class, throttlePolicy);\r\n    policyMap.put(ConnectTimeoutException.class, connectivityFailure);\r\n    policyMap.put(EOFException.class, retryIdempotentCalls);\r\n    policyMap.put(AWSBadRequestException.class, fail);\r\n    policyMap.put(AWSStatus500Exception.class, connectivityFailure);\r\n    policyMap.put(AWSNoResponseException.class, retryIdempotentCalls);\r\n    policyMap.put(AWSClientIOException.class, retryIdempotentCalls);\r\n    policyMap.put(AWSServiceIOException.class, retryIdempotentCalls);\r\n    policyMap.put(AWSS3IOException.class, retryIdempotentCalls);\r\n    policyMap.put(SocketTimeoutException.class, retryIdempotentCalls);\r\n    return policyMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "shouldRetry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RetryAction shouldRetry(Exception exception, int retries, int failovers, boolean idempotent) throws Exception\n{\r\n    Preconditions.checkArgument(exception != null, \"Null exception\");\r\n    Exception ex = exception;\r\n    if (exception instanceof AmazonClientException) {\r\n        ex = S3AUtils.translateException(\"\", \"\", (AmazonClientException) exception);\r\n    }\r\n    return retryPolicy.shouldRetry(ex, retries, failovers, idempotent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getAccessKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAccessKey()\n{\r\n    return accessKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getSecretKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSecretKey()\n{\r\n    return secretKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getSessionToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSessionToken()\n{\r\n    return sessionToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getExpiration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpiration()\n{\r\n    return expiration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setExpiration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setExpiration(final long expiration)\n{\r\n    this.expiration = expiration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getExpirationDateTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Optional<OffsetDateTime> getExpirationDateTime()\n{\r\n    return expiration == 0 ? Optional.empty() : Optional.of(OffsetDateTime.ofInstant(new Date(expiration).toInstant(), ZoneOffset.UTC));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getRoleARN",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRoleARN()\n{\r\n    return roleARN;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setRoleARN",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRoleARN(String roleARN)\n{\r\n    this.roleARN = requireNonNull(roleARN);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setAccessKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAccessKey(final String accessKey)\n{\r\n    this.accessKey = requireNonNull(accessKey, \"access key\");\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setSecretKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSecretKey(final String secretKey)\n{\r\n    this.secretKey = requireNonNull(secretKey, \"secret key\");\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setSessionToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSessionToken(final String sessionToken)\n{\r\n    this.sessionToken = requireNonNull(sessionToken, \"session token\");\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    MarshalledCredentials that = (MarshalledCredentials) o;\r\n    return expiration == that.expiration && Objects.equals(accessKey, that.accessKey) && Objects.equals(secretKey, that.secretKey) && Objects.equals(sessionToken, that.sessionToken) && Objects.equals(roleARN, that.roleARN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hash(accessKey, secretKey, sessionToken, roleARN, expiration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    if (isEmpty()) {\r\n        return \"Empty credentials\";\r\n    }\r\n    String validity = isValid(CredentialTypeRequired.AnyNonEmpty) ? \"valid\" : \"invalid\";\r\n    if (!hasSessionToken()) {\r\n        return \"full credentials (\" + validity + \")\";\r\n    } else {\r\n        return String.format(\"session credentials, expiry %s; %s(%s)\", getExpirationDateTime().map(x -> x.format(DateTimeFormatter.ISO_DATE_TIME)).orElse(\"unknown\"), (isNotEmpty(roleARN) ? (\"role \\\"\" + roleARN + \"\\\" \") : \"\"), validity);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "isEmpty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isEmpty()\n{\r\n    return !(isNotEmpty(accessKey) && isNotEmpty(secretKey));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "isValid",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean isValid(final CredentialTypeRequired required)\n{\r\n    if (accessKey == null || secretKey == null || sessionToken == null) {\r\n        return false;\r\n    }\r\n    boolean hasAccessAndSecretKeys = isNotEmpty(accessKey) && isNotEmpty(secretKey);\r\n    boolean hasSessionToken = hasSessionToken();\r\n    switch(required) {\r\n        case AnyIncludingEmpty:\r\n            return true;\r\n        case Empty:\r\n            return !hasAccessAndSecretKeys;\r\n        case AnyNonEmpty:\r\n            return hasAccessAndSecretKeys;\r\n        case FullOnly:\r\n            return hasAccessAndSecretKeys && !hasSessionToken;\r\n        case SessionOnly:\r\n            return hasAccessAndSecretKeys && hasSessionToken();\r\n        default:\r\n            return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "hasSessionToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasSessionToken()\n{\r\n    return isNotEmpty(sessionToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    validate(\"Writing \" + this + \": \", CredentialTypeRequired.AnyIncludingEmpty);\r\n    Text.writeString(out, accessKey);\r\n    Text.writeString(out, secretKey);\r\n    Text.writeString(out, sessionToken);\r\n    Text.writeString(out, roleARN);\r\n    out.writeLong(expiration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    accessKey = Text.readString(in, MAX_SECRET_LENGTH);\r\n    secretKey = Text.readString(in, MAX_SECRET_LENGTH);\r\n    sessionToken = Text.readString(in, MAX_SECRET_LENGTH);\r\n    roleARN = Text.readString(in, MAX_SECRET_LENGTH);\r\n    expiration = in.readLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validate(final String message, final CredentialTypeRequired typeRequired) throws IOException\n{\r\n    if (!isValid(typeRequired)) {\r\n        throw new DelegationTokenIOException(message + buildInvalidCredentialsError(typeRequired));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "buildInvalidCredentialsError",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String buildInvalidCredentialsError(final CredentialTypeRequired typeRequired)\n{\r\n    if (isEmpty()) {\r\n        return \" \" + MarshalledCredentialBinding.NO_AWS_CREDENTIALS;\r\n    } else {\r\n        return \" \" + INVALID_CREDENTIALS + \" in \" + toString() + \" required: \" + typeRequired;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "setSecretsInConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setSecretsInConfiguration(Configuration config)\n{\r\n    config.set(ACCESS_KEY, accessKey);\r\n    config.set(SECRET_KEY, secretKey);\r\n    S3AUtils.setIfDefined(config, SESSION_TOKEN, sessionToken, \"session credentials\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "empty",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MarshalledCredentials empty()\n{\r\n    return new MarshalledCredentials(\"\", \"\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "addDeprecatedKeys",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addDeprecatedKeys()\n{\r\n    Configuration.DeprecationDelta[] deltas = { new Configuration.DeprecationDelta(FS_S3A_COMMITTER_STAGING_ABORT_PENDING_UPLOADS, FS_S3A_COMMITTER_ABORT_PENDING_UPLOADS), new Configuration.DeprecationDelta(SERVER_SIDE_ENCRYPTION_ALGORITHM, S3_ENCRYPTION_ALGORITHM), new Configuration.DeprecationDelta(SERVER_SIDE_ENCRYPTION_KEY, S3_ENCRYPTION_KEY) };\r\n    if (deltas.length > 0) {\r\n        Configuration.addDeprecations(deltas);\r\n        Configuration.reloadExistingConfigurations();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initialize",
  "errType" : [ "AmazonClientException", "IOException|RuntimeException" ],
  "containingMethodsNum" : 71,
  "sourceCodeText" : "void initialize(URI name, Configuration originalConf) throws IOException\n{\r\n    bucket = name.getHost();\r\n    AuditSpan span = null;\r\n    try {\r\n        LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\r\n        Configuration conf = propagateBucketOptions(originalConf, bucket);\r\n        conf = ProviderUtils.excludeIncompatibleCredentialProviders(conf, S3AFileSystem.class);\r\n        String arn = String.format(ARN_BUCKET_OPTION, bucket);\r\n        String configuredArn = conf.getTrimmed(arn, \"\");\r\n        if (!configuredArn.isEmpty()) {\r\n            accessPoint = ArnResource.accessPointFromArn(configuredArn);\r\n            LOG.info(\"Using AccessPoint ARN \\\"{}\\\" for bucket {}\", configuredArn, bucket);\r\n            bucket = accessPoint.getFullArn();\r\n        } else if (conf.getBoolean(AWS_S3_ACCESSPOINT_REQUIRED, false)) {\r\n            LOG.warn(\"Access Point usage is required because \\\"{}\\\" is enabled,\" + \" but not configured for the bucket: {}\", AWS_S3_ACCESSPOINT_REQUIRED, bucket);\r\n            throw new PathIOException(bucket, AP_REQUIRED_EXCEPTION);\r\n        }\r\n        conf.setClassLoader(this.getClass().getClassLoader());\r\n        patchSecurityCredentialProviders(conf);\r\n        boolean delegationTokensEnabled = hasDelegationTokenBinding(conf);\r\n        if (delegationTokensEnabled) {\r\n            LOG.debug(\"Using delegation tokens\");\r\n        }\r\n        setUri(name, delegationTokensEnabled);\r\n        super.initialize(uri, conf);\r\n        setConf(conf);\r\n        setEncryptionSecrets(buildEncryptionSecrets(bucket, conf));\r\n        invoker = new Invoker(new S3ARetryPolicy(getConf()), onRetry);\r\n        instrumentation = new S3AInstrumentation(uri);\r\n        initializeStatisticsBinding();\r\n        isCSEEnabled = S3AEncryptionMethods.CSE_KMS.getMethod().equals(getS3EncryptionAlgorithm().getMethod());\r\n        LOG.debug(\"Client Side Encryption enabled: {}\", isCSEEnabled);\r\n        setCSEGauge();\r\n        owner = UserGroupInformation.getCurrentUser();\r\n        username = owner.getShortUserName();\r\n        workingDir = new Path(\"/user\", username).makeQualified(this.uri, this.getWorkingDirectory());\r\n        maxKeys = intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\r\n        partSize = getMultipartSizeProperty(conf, MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\r\n        multiPartThreshold = getMultipartSizeProperty(conf, MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\r\n        longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\r\n        enableMultiObjectsDelete = conf.getBoolean(ENABLE_MULTI_DELETE, true);\r\n        readAhead = longBytesOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\r\n        initThreadPools(conf);\r\n        int listVersion = conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\r\n        if (listVersion < 1 || listVersion > 2) {\r\n            LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" + \"version 2\", listVersion);\r\n        }\r\n        useListV1 = (listVersion == 1);\r\n        if (accessPoint != null && useListV1) {\r\n            LOG.warn(\"V1 list configured in fs.s3a.list.version. This is not supported in by\" + \" access points. Upgrading to V2\");\r\n            useListV1 = false;\r\n        }\r\n        signerManager = new SignerManager(bucket, this, conf, owner);\r\n        signerManager.initCustomSigners();\r\n        initializeAuditService();\r\n        requestFactory = createRequestFactory();\r\n        span = createSpan(INITIALIZE_SPAN, bucket, null);\r\n        bindAWSClient(name, delegationTokensEnabled);\r\n        initTransferManager();\r\n        doBucketProbing();\r\n        inputPolicy = S3AInputPolicy.getPolicy(conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\r\n        LOG.debug(\"Input fadvise policy = {}\", inputPolicy);\r\n        changeDetectionPolicy = ChangeDetectionPolicy.getPolicy(conf);\r\n        LOG.debug(\"Change detection policy = {}\", changeDetectionPolicy);\r\n        boolean magicCommitterEnabled = conf.getBoolean(CommitConstants.MAGIC_COMMITTER_ENABLED, CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\r\n        LOG.debug(\"Filesystem support for magic committers {} enabled\", magicCommitterEnabled ? \"is\" : \"is not\");\r\n        committerIntegration = new MagicCommitIntegration(this, magicCommitterEnabled);\r\n        boolean blockUploadEnabled = conf.getBoolean(FAST_UPLOAD, true);\r\n        if (!blockUploadEnabled) {\r\n            LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\r\n        }\r\n        blockOutputBuffer = conf.getTrimmed(FAST_UPLOAD_BUFFER, DEFAULT_FAST_UPLOAD_BUFFER);\r\n        partSize = ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\r\n        blockFactory = S3ADataBlocks.createFactory(this, blockOutputBuffer);\r\n        blockOutputActiveBlocks = intOption(conf, FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\r\n        if (isCSEEnabled) {\r\n            blockOutputActiveBlocks = 1;\r\n        }\r\n        LOG.debug(\"Using S3ABlockOutputStream with buffer = {}; block={};\" + \" queue limit={}\", blockOutputBuffer, partSize, blockOutputActiveBlocks);\r\n        checkNoS3Guard(this.getUri(), getConf());\r\n        allowAuthoritativePaths = S3Guard.getAuthoritativePaths(this);\r\n        directoryPolicy = DirectoryPolicyImpl.getDirectoryPolicy(conf, this::allowAuthoritative);\r\n        LOG.debug(\"Directory marker retention policy is {}\", directoryPolicy);\r\n        initMultipartUploads(conf);\r\n        pageSize = intOption(getConf(), BULK_DELETE_PAGE_SIZE, BULK_DELETE_PAGE_SIZE_DEFAULT, 0);\r\n        checkArgument(pageSize <= InternalConstants.MAX_ENTRIES_TO_DELETE, \"page size out of range: %s\", pageSize);\r\n        listing = new Listing(listingOperationCallbacks, createStoreContext());\r\n    } catch (AmazonClientException e) {\r\n        cleanupWithLogger(LOG, span);\r\n        stopAllServices();\r\n        throw translateException(\"initializing \", new Path(name), e);\r\n    } catch (IOException | RuntimeException e) {\r\n        cleanupWithLogger(LOG, span);\r\n        stopAllServices();\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setCSEGauge",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setCSEGauge()\n{\r\n    IOStatisticsStore ioStatisticsStore = (IOStatisticsStore) getIOStatistics();\r\n    if (isCSEEnabled) {\r\n        ioStatisticsStore.setGauge(CLIENT_SIDE_ENCRYPTION_ENABLED.getSymbol(), 1L);\r\n    } else {\r\n        ioStatisticsStore.setGauge(CLIENT_SIDE_ENCRYPTION_ENABLED.getSymbol(), 0L);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "doBucketProbing",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void doBucketProbing() throws IOException\n{\r\n    int bucketProbe = getConf().getInt(S3A_BUCKET_PROBE, S3A_BUCKET_PROBE_DEFAULT);\r\n    Preconditions.checkArgument(bucketProbe >= 0, \"Value of \" + S3A_BUCKET_PROBE + \" should be >= 0\");\r\n    switch(bucketProbe) {\r\n        case 0:\r\n            LOG.debug(\"skipping check for bucket existence\");\r\n            break;\r\n        case 1:\r\n            logDnsLookup(getConf());\r\n            verifyBucketExists();\r\n            break;\r\n        case 2:\r\n            logDnsLookup(getConf());\r\n            verifyBucketExistsV2();\r\n            break;\r\n        default:\r\n            LOG.warn(\"Unknown bucket probe option {}: {}; falling back to check #2\", S3A_BUCKET_PROBE, bucketProbe);\r\n            verifyBucketExistsV2();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initializeStatisticsBinding",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initializeStatisticsBinding()\n{\r\n    storageStatistics = createStorageStatistics(requireNonNull(getIOStatistics()));\r\n    statisticsContext = new BondedS3AStatisticsContext(new BondedS3AStatisticsContext.S3AFSStatisticsSource() {\r\n\r\n        @Override\r\n        public S3AInstrumentation getInstrumentation() {\r\n            return S3AFileSystem.this.getInstrumentation();\r\n        }\r\n\r\n        @Override\r\n        public Statistics getInstanceStatistics() {\r\n            return S3AFileSystem.this.statistics;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initThreadPools",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void initThreadPools(Configuration conf)\n{\r\n    final String name = \"s3a-transfer-\" + getBucket();\r\n    int maxThreads = conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\r\n    if (maxThreads < 2) {\r\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\r\n        maxThreads = 2;\r\n    }\r\n    int totalTasks = intOption(conf, MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\r\n    long keepAliveTime = longOption(conf, KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME, 0);\r\n    boundedThreadPool = BlockingThreadPoolExecutorService.newInstance(maxThreads, maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS, name + \"-bounded\");\r\n    unboundedThreadPool = new ThreadPoolExecutor(maxThreads, Integer.MAX_VALUE, keepAliveTime, TimeUnit.SECONDS, new LinkedBlockingQueue<>(), BlockingThreadPoolExecutorService.newDaemonThreadFactory(name + \"-unbounded\"));\r\n    unboundedThreadPool.allowCoreThreadTimeOut(true);\r\n    executorCapacity = intOption(conf, EXECUTOR_CAPACITY, DEFAULT_EXECUTOR_CAPACITY, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createStorageStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AStorageStatistics createStorageStatistics(final IOStatistics ioStatistics)\n{\r\n    return (S3AStorageStatistics) GlobalStorageStatistics.INSTANCE.put(S3AStorageStatistics.NAME, () -> new S3AStorageStatistics(ioStatistics));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyBucketExists",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyBucketExists() throws UnknownStoreException, IOException\n{\r\n    if (!invoker.retry(\"doesBucketExist\", bucket, true, trackDurationOfOperation(getDurationTrackerFactory(), STORE_EXISTS_PROBE.getSymbol(), () -> s3.doesBucketExist(bucket)))) {\r\n        throw new UnknownStoreException(\"s3a://\" + bucket + \"/\", \" Bucket does \" + \"not exist\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "verifyBucketExistsV2",
  "errType" : [ "AmazonServiceException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyBucketExistsV2() throws UnknownStoreException, IOException\n{\r\n    if (!invoker.retry(\"doesBucketExistV2\", bucket, true, trackDurationOfOperation(getDurationTrackerFactory(), STORE_EXISTS_PROBE.getSymbol(), () -> {\r\n        try {\r\n            s3.getBucketAcl(bucket);\r\n        } catch (AmazonServiceException ex) {\r\n            int statusCode = ex.getStatusCode();\r\n            if (statusCode == SC_404 || (statusCode == SC_403 && ex.getMessage().contains(AP_INACCESSIBLE))) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }))) {\r\n        throw new UnknownStoreException(\"s3a://\" + bucket + \"/\", \" Bucket does \" + \"not exist\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getInstrumentation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInstrumentation getInstrumentation()\n{\r\n    return instrumentation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFsStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem.Statistics getFsStatistics()\n{\r\n    return statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Listing getListing()\n{\r\n    return listing;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "bindAWSClient",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void bindAWSClient(URI name, boolean dtEnabled) throws IOException\n{\r\n    Configuration conf = getConf();\r\n    credentials = null;\r\n    String uaSuffix = \"\";\r\n    if (dtEnabled) {\r\n        LOG.debug(\"Using delegation tokens\");\r\n        S3ADelegationTokens tokens = new S3ADelegationTokens();\r\n        this.delegationTokens = Optional.of(tokens);\r\n        tokens.bindToFileSystem(getCanonicalUri(), createStoreContext(), createDelegationOperations());\r\n        tokens.init(conf);\r\n        tokens.start();\r\n        if (tokens.isBoundToDT()) {\r\n            LOG.debug(\"Using existing delegation token\");\r\n        } else {\r\n            LOG.debug(\"No delegation token for this instance\");\r\n        }\r\n        credentials = tokens.getCredentialProviders();\r\n        tokens.getEncryptionSecrets().ifPresent(this::setEncryptionSecrets);\r\n        uaSuffix = tokens.getUserAgentField();\r\n    } else {\r\n        credentials = createAWSCredentialProviderSet(name, conf);\r\n    }\r\n    LOG.debug(\"Using credential provider {}\", credentials);\r\n    Class<? extends S3ClientFactory> s3ClientFactoryClass = conf.getClass(S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL, S3ClientFactory.class);\r\n    String endpoint = accessPoint == null ? conf.getTrimmed(ENDPOINT, DEFAULT_ENDPOINT) : accessPoint.getEndpoint();\r\n    S3ClientFactory.S3ClientCreationParameters parameters = null;\r\n    parameters = new S3ClientFactory.S3ClientCreationParameters().withCredentialSet(credentials).withEndpoint(endpoint).withMetrics(statisticsContext.newStatisticsFromAwsSdk()).withPathStyleAccess(conf.getBoolean(PATH_STYLE_ACCESS, false)).withUserAgentSuffix(uaSuffix).withRequesterPays(conf.getBoolean(ALLOW_REQUESTER_PAYS, DEFAULT_ALLOW_REQUESTER_PAYS)).withRequestHandlers(auditManager.createRequestHandlers());\r\n    s3 = ReflectionUtils.newInstance(s3ClientFactoryClass, conf).createS3Client(getUri(), parameters);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initializeAuditService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initializeAuditService() throws IOException\n{\r\n    auditManager = AuditIntegration.createAndStartAuditManager(getConf(), instrumentation.createMetricsUpdatingStore());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAuditManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditManagerS3A getAuditManager()\n{\r\n    return auditManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAuditor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationAuditor getAuditor()\n{\r\n    return getAuditManager().getAuditor();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getActiveAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A getActiveAuditSpan()\n{\r\n    return getAuditManager().getActiveAuditSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAuditSpanSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanSource getAuditSpanSource()\n{\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A createSpan(String operation, @Nullable String path1, @Nullable String path2) throws IOException\n{\r\n    return getAuditManager().createSpan(operation, path1, path2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createRequestFactory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RequestFactory createRequestFactory()\n{\r\n    long partCountLimit = longOption(getConf(), UPLOAD_PART_COUNT_LIMIT, DEFAULT_UPLOAD_PART_COUNT_LIMIT, 1);\r\n    if (partCountLimit != DEFAULT_UPLOAD_PART_COUNT_LIMIT) {\r\n        LOG.warn(\"Configuration property {} shouldn't be overridden by client\", UPLOAD_PART_COUNT_LIMIT);\r\n    }\r\n    initCannedAcls(getConf());\r\n    String contentEncoding = getConf().getTrimmed(CONTENT_ENCODING, null);\r\n    return RequestFactoryImpl.builder().withBucket(requireNonNull(bucket)).withCannedACL(getCannedACL()).withEncryptionSecrets(requireNonNull(encryptionSecrets)).withMultipartPartCountLimit(partCountLimit).withRequestPreparer(getAuditManager()::requestCreated).withContentEncoding(contentEncoding).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRequestFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RequestFactory getRequestFactory()\n{\r\n    return requestFactory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createDelegationOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationOperations createDelegationOperations()\n{\r\n    return new DelegationOperationsImpl();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setEncryptionSecrets",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setEncryptionSecrets(final EncryptionSecrets secrets)\n{\r\n    this.encryptionSecrets = secrets;\r\n    if (requestFactory != null) {\r\n        requestFactory.setEncryptionSecrets(secrets);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getEncryptionSecrets",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EncryptionSecrets getEncryptionSecrets()\n{\r\n    return encryptionSecrets;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initTransferManager",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void initTransferManager()\n{\r\n    TransferManagerConfiguration transferConfiguration = new TransferManagerConfiguration();\r\n    transferConfiguration.setMinimumUploadPartSize(partSize);\r\n    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\r\n    transferConfiguration.setMultipartCopyPartSize(partSize);\r\n    transferConfiguration.setMultipartCopyThreshold(multiPartThreshold);\r\n    transfers = new TransferManager(s3, unboundedThreadPool);\r\n    transfers.setConfiguration(transferConfiguration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initCannedAcls",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initCannedAcls(Configuration conf)\n{\r\n    String cannedACLName = conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\r\n    if (!cannedACLName.isEmpty()) {\r\n        cannedACL = CannedAccessControlList.valueOf(cannedACLName);\r\n    } else {\r\n        cannedACL = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initMultipartUploads",
  "errType" : [ "AccessDeniedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void initMultipartUploads(Configuration conf) throws IOException\n{\r\n    boolean purgeExistingMultipart = conf.getBoolean(PURGE_EXISTING_MULTIPART, DEFAULT_PURGE_EXISTING_MULTIPART);\r\n    long purgeExistingMultipartAge = longOption(conf, PURGE_EXISTING_MULTIPART_AGE, DEFAULT_PURGE_EXISTING_MULTIPART_AGE, 0);\r\n    if (purgeExistingMultipart) {\r\n        try {\r\n            abortOutstandingMultipartUploads(purgeExistingMultipartAge);\r\n        } catch (AccessDeniedException e) {\r\n            instrumentation.errorIgnored();\r\n            LOG.debug(\"Failed to purge multipart uploads against {},\" + \" FS may be read only\", bucket);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abortOutstandingMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void abortOutstandingMultipartUploads(long seconds) throws IOException\n{\r\n    Preconditions.checkArgument(seconds >= 0);\r\n    Date purgeBefore = new Date(new Date().getTime() - seconds * 1000);\r\n    LOG.debug(\"Purging outstanding multipart uploads older than {}\", purgeBefore);\r\n    invoker.retry(\"Purging multipart uploads\", bucket, true, () -> transfers.abortMultipartUploads(bucket, purgeBefore));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return \"s3a\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUri(URI fsUri, boolean canonicalize)\n{\r\n    URI u = S3xLoginHelper.buildFSURI(fsUri);\r\n    this.uri = canonicalize ? u : canonicalizeUri(u);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCanonicalUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getCanonicalUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDefaultPort()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAmazonS3Client",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AmazonS3 getAmazonS3Client()\n{\r\n    return s3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAmazonS3ClientForTesting",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonS3 getAmazonS3ClientForTesting(String reason)\n{\r\n    LOG.warn(\"Access to S3A client requested, reason {}\", reason);\r\n    return s3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setAmazonS3Client",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setAmazonS3Client(AmazonS3 client)\n{\r\n    Preconditions.checkNotNull(client, \"client\");\r\n    LOG.debug(\"Setting S3 client to {}\", client);\r\n    s3 = client;\r\n    initThreadPools(getConf());\r\n    initTransferManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBucketLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBucketLocation() throws IOException\n{\r\n    return getBucketLocation(bucket);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBucketLocation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getBucketLocation(String bucketName) throws IOException\n{\r\n    final String region = trackDurationAndSpan(STORE_EXISTS_PROBE, bucketName, null, () -> invoker.retry(\"getBucketLocation()\", bucketName, true, () -> accessPoint != null ? accessPoint.getRegion() : s3.getBucketLocation(bucketName)));\r\n    return fixBucketRegion(region);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getReadAheadRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadAheadRange()\n{\r\n    return readAhead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getInputPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputPolicy getInputPolicy()\n{\r\n    return inputPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getChangeDetectionPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ChangeDetectionPolicy getChangeDetectionPolicy()\n{\r\n    return changeDetectionPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getS3EncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AEncryptionMethods getS3EncryptionAlgorithm()\n{\r\n    return encryptionSecrets.getEncryptionMethod();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createTmpFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "File createTmpFileForWrite(String pathStr, long size, Configuration conf) throws IOException\n{\r\n    if (directoryAllocator == null) {\r\n        synchronized (this) {\r\n            String bufferDir = conf.get(BUFFER_DIR) != null ? BUFFER_DIR : HADOOP_TMP_DIR;\r\n            directoryAllocator = new LocalDirAllocator(bufferDir);\r\n        }\r\n    }\r\n    Path path = directoryAllocator.getLocalPathForWrite(pathStr, size, conf);\r\n    File dir = new File(path.getParent().toUri().getPath());\r\n    String prefix = path.getName();\r\n    return File.createTempFile(prefix, null, dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBucket()\n{\r\n    return bucket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBucket(String bucket)\n{\r\n    this.bucket = bucket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCannedACL",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CannedAccessControlList getCannedACL()\n{\r\n    return cannedACL;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setInputPolicy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputPolicy(S3AInputPolicy inputPolicy)\n{\r\n    Objects.requireNonNull(inputPolicy, \"Null inputStrategy\");\r\n    LOG.debug(\"Setting input strategy: {}\", inputPolicy);\r\n    this.inputPolicy = inputPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "pathToKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String pathToKey(Path path)\n{\r\n    if (!path.isAbsolute()) {\r\n        path = new Path(workingDir, path);\r\n    }\r\n    if (path.toUri().getScheme() != null && path.toUri().getPath().isEmpty()) {\r\n        return \"\";\r\n    }\r\n    return path.toUri().getPath().substring(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeAddTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String maybeAddTrailingSlash(String key)\n{\r\n    return S3AUtils.maybeAddTrailingSlash(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "keyToPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path keyToPath(String key)\n{\r\n    return new Path(\"/\" + key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "keyToQualifiedPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path keyToQualifiedPath(String key)\n{\r\n    return qualify(keyToPath(key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "makeQualified",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Path makeQualified(final Path path)\n{\r\n    Path q = super.makeQualified(path);\r\n    if (!q.isRoot()) {\r\n        String urlString = q.toUri().toString();\r\n        if (urlString.endsWith(Path.SEPARATOR)) {\r\n            LOG.debug(\"Stripping trailing '/' from {}\", q);\r\n            q = new Path(urlString.substring(0, urlString.length() - 1));\r\n        }\r\n    }\r\n    if (!q.isRoot() && q.getName().isEmpty()) {\r\n        q = q.getParent();\r\n    }\r\n    return q;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "qualify",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path qualify(Path path)\n{\r\n    return makeQualified(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkPath(Path path)\n{\r\n    S3xLoginHelper.checkPath(getConf(), getUri(), path, getDefaultPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "canonicalizeUri",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI canonicalizeUri(URI rawUri)\n{\r\n    return S3xLoginHelper.canonicalizeUri(rawUri, getDefaultPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataInputStream open(Path f, int bufferSize) throws IOException\n{\r\n    return open(f, Optional.empty(), Optional.empty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "FSDataInputStream open(final Path file, final Optional<Configuration> options, final Optional<S3AFileStatus> providedStatus) throws IOException\n{\r\n    final Path path = qualify(file);\r\n    final AuditSpan auditSpan = entryPoint(INVOCATION_OPEN, path);\r\n    S3AFileStatus fileStatus = extractOrFetchSimpleFileStatus(path, providedStatus);\r\n    S3AReadOpContext readContext;\r\n    if (options.isPresent()) {\r\n        Configuration o = options.get();\r\n        S3AInputPolicy policy = S3AInputPolicy.getPolicy(o.get(INPUT_FADVISE, inputPolicy.toString()));\r\n        long readAheadRange2 = o.getLong(READAHEAD_RANGE, readAhead);\r\n        readContext = createReadContext(fileStatus, policy, changeDetectionPolicy, readAheadRange2, auditSpan);\r\n    } else {\r\n        readContext = createReadContext(fileStatus, inputPolicy, changeDetectionPolicy, readAhead, auditSpan);\r\n    }\r\n    LOG.debug(\"Opening '{}'\", readContext);\r\n    return new FSDataInputStream(new S3AInputStream(readContext, createObjectAttributes(fileStatus), createInputStreamCallbacks(auditSpan)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createInputStreamCallbacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputStream.InputStreamCallbacks createInputStreamCallbacks(final AuditSpan auditSpan)\n{\r\n    return new InputStreamCallbacksImpl(auditSpan);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createReadContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AReadOpContext createReadContext(final FileStatus fileStatus, final S3AInputPolicy seekPolicy, final ChangeDetectionPolicy changePolicy, final long readAheadRange, final AuditSpan auditSpan)\n{\r\n    return new S3AReadOpContext(fileStatus.getPath(), invoker, statistics, statisticsContext, fileStatus, seekPolicy, changePolicy, readAheadRange, auditSpan);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createObjectAttributes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "S3ObjectAttributes createObjectAttributes(final Path f, final String eTag, final String versionId, final long len)\n{\r\n    return new S3ObjectAttributes(bucket, f, pathToKey(f), getS3EncryptionAlgorithm(), encryptionSecrets.getEncryptionKey(), eTag, versionId, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createObjectAttributes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3ObjectAttributes createObjectAttributes(final S3AFileStatus fileStatus)\n{\r\n    return createObjectAttributes(fileStatus.getPath(), fileStatus.getEtag(), fileStatus.getVersionId(), fileStatus.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    final Path path = qualify(f);\r\n    return trackDurationAndSpan(INVOCATION_CREATE, path, () -> innerCreateFile(path, permission, overwrite, bufferSize, replication, blockSize, progress));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "innerCreateFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "FSDataOutputStream innerCreateFile(Path path, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    String key = pathToKey(path);\r\n    FileStatus status = null;\r\n    try {\r\n        status = innerGetFileStatus(path, false, overwrite ? StatusProbeEnum.DIRECTORIES : StatusProbeEnum.ALL);\r\n        if (status.isDirectory()) {\r\n            throw new FileAlreadyExistsException(path + \" is a directory\");\r\n        }\r\n        if (!overwrite) {\r\n            throw new FileAlreadyExistsException(path + \" already exists\");\r\n        }\r\n        LOG.debug(\"Overwriting file {}\", path);\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n    instrumentation.fileCreated();\r\n    PutTracker putTracker = committerIntegration.createTracker(path, key);\r\n    String destKey = putTracker.getDestKey();\r\n    final BlockOutputStreamStatistics outputStreamStatistics = statisticsContext.newOutputStreamStatistics();\r\n    final S3ABlockOutputStream.BlockOutputStreamBuilder builder = S3ABlockOutputStream.builder().withKey(destKey).withBlockFactory(blockFactory).withBlockSize(partSize).withStatistics(outputStreamStatistics).withProgress(progress).withPutTracker(putTracker).withWriteOperations(createWriteOperationHelper(getActiveAuditSpan())).withExecutorService(new SemaphoredDelegatingExecutor(boundedThreadPool, blockOutputActiveBlocks, true, outputStreamStatistics)).withDowngradeSyncableExceptions(getConf().getBoolean(DOWNGRADE_SYNCABLE_EXCEPTIONS, DOWNGRADE_SYNCABLE_EXCEPTIONS_DEFAULT)).withCSEEnabled(isCSEEnabled);\r\n    return new FSDataOutputStream(new S3ABlockOutputStream(builder), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getWriteOperationHelper",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "WriteOperationHelper getWriteOperationHelper()\n{\r\n    return createWriteOperationHelper(getActiveAuditSpan());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createWriteOperationHelper",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "WriteOperationHelper createWriteOperationHelper(AuditSpan auditSpan)\n{\r\n    return new WriteOperationHelper(this, getConf(), statisticsContext, getAuditSpanSource(), auditSpan);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createNonRecursive",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(Path p, FsPermission permission, EnumSet<CreateFlag> flags, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    final Path path = makeQualified(p);\r\n    try (AuditSpan span = entryPoint(INVOCATION_CREATE_NON_RECURSIVE, path)) {\r\n        Path parent = path.getParent();\r\n        if (parent != null && !parent.isRoot()) {\r\n            S3AFileStatus status;\r\n            try {\r\n                status = innerGetFileStatus(parent, false, StatusProbeEnum.DIRECTORIES);\r\n            } catch (FileNotFoundException e) {\r\n                status = innerGetFileStatus(parent, false, StatusProbeEnum.HEAD_ONLY);\r\n            }\r\n            if (!status.isDirectory()) {\r\n                throw new FileAlreadyExistsException(\"Not a directory: \" + parent);\r\n            }\r\n        }\r\n        return innerCreateFile(path, permission, flags.contains(CreateFlag.OVERWRITE), bufferSize, replication, blockSize, progress);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FSDataOutputStream append(Path f, int bufferSize, Progressable progress) throws IOException\n{\r\n    throw new UnsupportedOperationException(\"Append is not supported \" + \"by S3AFileSystem\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "rename",
  "errType" : [ "AmazonClientException", "RenameFailedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean rename(Path src, Path dst) throws IOException\n{\r\n    try {\r\n        long bytesCopied = trackDurationAndSpan(INVOCATION_RENAME, src.toString(), dst.toString(), () -> innerRename(src, dst));\r\n        LOG.debug(\"Copied {} bytes\", bytesCopied);\r\n        return true;\r\n    } catch (AmazonClientException e) {\r\n        throw translateException(\"rename(\" + src + \", \" + dst + \")\", src, e);\r\n    } catch (RenameFailedException e) {\r\n        LOG.info(\"{}\", e.getMessage());\r\n        LOG.debug(\"rename failure\", e);\r\n        return e.getExitCode();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initiateRename",
  "errType" : [ "FileNotFoundException", "FileNotFoundException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "Pair<S3AFileStatus, S3AFileStatus> initiateRename(final Path src, final Path dst) throws IOException\n{\r\n    String srcKey = pathToKey(src);\r\n    String dstKey = pathToKey(dst);\r\n    if (srcKey.isEmpty()) {\r\n        throw new RenameFailedException(src, dst, \"source is root directory\");\r\n    }\r\n    if (dstKey.isEmpty()) {\r\n        throw new RenameFailedException(src, dst, \"dest is root directory\");\r\n    }\r\n    S3AFileStatus srcStatus = innerGetFileStatus(src, true, StatusProbeEnum.ALL);\r\n    if (srcKey.equals(dstKey)) {\r\n        LOG.debug(\"rename: src and dest refer to the same file or directory: {}\", dst);\r\n        throw new RenameFailedException(src, dst, \"source and dest refer to the same file or directory\").withExitCode(srcStatus.isFile());\r\n    }\r\n    S3AFileStatus dstStatus = null;\r\n    try {\r\n        dstStatus = innerGetFileStatus(dst, true, StatusProbeEnum.ALL);\r\n        if (srcStatus.isDirectory()) {\r\n            if (dstStatus.isFile()) {\r\n                throw new FileAlreadyExistsException(\"Failed to rename \" + src + \" to \" + dst + \"; source is a directory and dest is a file\");\r\n            } else if (dstStatus.isEmptyDirectory() != Tristate.TRUE) {\r\n                throw new RenameFailedException(src, dst, \"Destination is a non-empty directory\").withExitCode(false);\r\n            }\r\n        } else {\r\n            if (dstStatus.isFile()) {\r\n                throw new FileAlreadyExistsException(\"Failed to rename \" + src + \" to \" + dst + \"; destination file exists\");\r\n            }\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        LOG.debug(\"rename: destination path {} not found\", dst);\r\n        Path parent = dst.getParent();\r\n        if (!pathToKey(parent).isEmpty() && !parent.equals(src.getParent())) {\r\n            try {\r\n                S3AFileStatus dstParentStatus = innerGetFileStatus(parent, false, StatusProbeEnum.FILE);\r\n                if (!dstParentStatus.isDirectory()) {\r\n                    throw new RenameFailedException(src, dst, \"destination parent is not a directory\");\r\n                }\r\n            } catch (FileNotFoundException expected) {\r\n            }\r\n        }\r\n    }\r\n    return Pair.of(srcStatus, dstStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "innerRename",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "long innerRename(Path source, Path dest) throws RenameFailedException, FileNotFoundException, IOException, AmazonClientException\n{\r\n    Path src = qualify(source);\r\n    Path dst = qualify(dest);\r\n    LOG.debug(\"Rename path {} to {}\", src, dst);\r\n    String srcKey = pathToKey(src);\r\n    String dstKey = pathToKey(dst);\r\n    Pair<S3AFileStatus, S3AFileStatus> p = initiateRename(src, dst);\r\n    RenameOperation renameOperation = new RenameOperation(createStoreContext(), src, srcKey, p.getLeft(), dst, dstKey, p.getRight(), new OperationCallbacksImpl(), pageSize);\r\n    return renameOperation.execute();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFsDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<? extends TokenIdentifier> getFsDelegationToken() throws IOException\n{\r\n    return getDelegationToken(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ObjectMetadata getObjectMetadata(Path path) throws IOException\n{\r\n    return trackDurationAndSpan(INVOCATION_GET_FILE_STATUS, path, () -> getObjectMetadata(makeQualified(path), null, invoker, \"getObjectMetadata\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ObjectMetadata getObjectMetadata(Path path, ChangeTracker changeTracker, Invoker changeInvoker, String operation) throws IOException\n{\r\n    String key = pathToKey(path);\r\n    return once(operation, path.toString(), () -> getObjectMetadata(key, changeTracker, changeInvoker, operation));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "entryPoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpan entryPoint(Statistic operation, Path path) throws IOException\n{\r\n    return entryPoint(operation, (path != null ? pathToKey(path) : null), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "entryPoint",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AuditSpan entryPoint(Statistic operation, @Nullable String path1, @Nullable String path2) throws IOException\n{\r\n    checkNotClosed();\r\n    incrementStatistic(operation);\r\n    return createSpan(operation.getSymbol(), path1, path2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "trackDurationAndSpan",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "B trackDurationAndSpan(Statistic statistic, String path, String path2, CallableRaisingIOE<B> input) throws IOException\n{\r\n    checkNotClosed();\r\n    try (AuditSpan span = createSpan(statistic.getSymbol(), path, path2)) {\r\n        return trackDuration(getDurationTrackerFactory(), statistic.getSymbol(), input);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "trackDurationAndSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "B trackDurationAndSpan(Statistic statistic, @Nullable Path path, CallableRaisingIOE<B> input) throws IOException\n{\r\n    return trackDurationAndSpan(statistic, path != null ? pathToKey(path) : null, null, input);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic)\n{\r\n    incrementStatistic(statistic, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic, long count)\n{\r\n    statisticsContext.incrementCounter(statistic, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "decrementGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void decrementGauge(Statistic statistic, long count)\n{\r\n    statisticsContext.decrementGauge(statistic, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementGauge(Statistic statistic, long count)\n{\r\n    statisticsContext.incrementGauge(statistic, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "operationRetried",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void operationRetried(Exception ex)\n{\r\n    if (isThrottleException(ex)) {\r\n        LOG.debug(\"Request throttled\");\r\n        incrementStatistic(STORE_IO_THROTTLED);\r\n        statisticsContext.addValueToQuantiles(STORE_IO_THROTTLE_RATE, 1);\r\n    } else {\r\n        incrementStatistic(STORE_IO_RETRY);\r\n        incrementStatistic(IGNORED_ERRORS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "operationRetried",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void operationRetried(String text, Exception ex, int retries, boolean idempotent)\n{\r\n    operationRetried(ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getStorageStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AStorageStatistics getStorageStatistics()\n{\r\n    return storageStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return instrumentation != null ? instrumentation.getIOStatistics() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDurationTrackerFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DurationTrackerFactory getDurationTrackerFactory()\n{\r\n    return instrumentation != null ? instrumentation.getDurationTrackerFactory() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ObjectMetadata getObjectMetadata(String key) throws IOException\n{\r\n    return getObjectMetadata(key, null, invoker, \"getObjectMetadata\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getObjectMetadata",
  "errType" : [ "AmazonServiceException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ObjectMetadata getObjectMetadata(String key, ChangeTracker changeTracker, Invoker changeInvoker, String operation) throws IOException\n{\r\n    ObjectMetadata meta = changeInvoker.retryUntranslated(\"GET \" + key, true, () -> {\r\n        GetObjectMetadataRequest request = getRequestFactory().newGetObjectMetadataRequest(key);\r\n        incrementStatistic(OBJECT_METADATA_REQUESTS);\r\n        DurationTracker duration = getDurationTrackerFactory().trackDuration(ACTION_HTTP_HEAD_REQUEST.getSymbol());\r\n        try {\r\n            LOG.debug(\"HEAD {} with change tracker {}\", key, changeTracker);\r\n            if (changeTracker != null) {\r\n                changeTracker.maybeApplyConstraint(request);\r\n            }\r\n            ObjectMetadata objectMetadata = s3.getObjectMetadata(request);\r\n            if (changeTracker != null) {\r\n                changeTracker.processMetadata(objectMetadata, operation);\r\n            }\r\n            return objectMetadata;\r\n        } catch (AmazonServiceException ase) {\r\n            if (!isObjectNotFound(ase)) {\r\n                duration.failed();\r\n            }\r\n            throw ase;\r\n        } finally {\r\n            duration.close();\r\n        }\r\n    });\r\n    incrementReadOperations();\r\n    return meta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listObjects",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "S3ListResult listObjects(S3ListRequest request, @Nullable final DurationTrackerFactory trackerFactory) throws IOException\n{\r\n    incrementReadOperations();\r\n    LOG.debug(\"LIST {}\", request);\r\n    validateListArguments(request);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"LIST\")) {\r\n        return invoker.retryUntranslated(request.toString(), true, trackDurationOfOperation(trackerFactory, OBJECT_LIST_REQUEST, () -> {\r\n            if (useListV1) {\r\n                return S3ListResult.v1(s3.listObjects(request.getV1()));\r\n            } else {\r\n                return S3ListResult.v2(s3.listObjectsV2(request.getV2()));\r\n            }\r\n        }));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "validateListArguments",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateListArguments(S3ListRequest request)\n{\r\n    if (useListV1) {\r\n        Preconditions.checkArgument(request.isV1());\r\n    } else {\r\n        Preconditions.checkArgument(!request.isV1());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "continueListObjects",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "S3ListResult continueListObjects(S3ListRequest request, S3ListResult prevResult, final DurationTrackerFactory trackerFactory) throws IOException\n{\r\n    incrementReadOperations();\r\n    validateListArguments(request);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"LIST (continued)\")) {\r\n        return invoker.retryUntranslated(request.toString(), true, trackDurationOfOperation(trackerFactory, OBJECT_CONTINUE_LIST_REQUEST, () -> {\r\n            if (useListV1) {\r\n                return S3ListResult.v1(s3.listNextBatchOfObjects(getRequestFactory().newListNextBatchOfObjectsRequest(prevResult.getV1())));\r\n            } else {\r\n                request.getV2().setContinuationToken(prevResult.getV2().getNextContinuationToken());\r\n                return S3ListResult.v2(s3.listObjectsV2(request.getV2()));\r\n            }\r\n        }));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementReadOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementReadOperations()\n{\r\n    statistics.incrementReadOps(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementWriteOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementWriteOperations()\n{\r\n    statistics.incrementWriteOps(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteObject",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteObject(String key) throws AmazonClientException, IOException\n{\r\n    blockRootDelete(key);\r\n    incrementWriteOperations();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"deleting %s\", key)) {\r\n        invoker.retryUntranslated(String.format(\"Delete %s:/%s\", bucket, key), DELETE_CONSIDERED_IDEMPOTENT, () -> {\r\n            incrementStatistic(OBJECT_DELETE_OBJECTS);\r\n            trackDurationOfInvocation(getDurationTrackerFactory(), OBJECT_DELETE_REQUEST.getSymbol(), () -> s3.deleteObject(getRequestFactory().newDeleteObjectRequest(key)));\r\n            return null;\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteObjectAtPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteObjectAtPath(Path f, String key, boolean isFile) throws AmazonClientException, IOException\n{\r\n    if (isFile) {\r\n        instrumentation.fileDeleted(1);\r\n    } else {\r\n        instrumentation.directoryDeleted();\r\n    }\r\n    deleteObject(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "blockRootDelete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void blockRootDelete(String key) throws InvalidRequestException\n{\r\n    if (key.isEmpty() || \"/\".equals(key)) {\r\n        throw new InvalidRequestException(\"Bucket \" + bucket + \" cannot be deleted\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteObjects",
  "errType" : [ "MultiObjectDeleteException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DeleteObjectsResult deleteObjects(DeleteObjectsRequest deleteRequest) throws MultiObjectDeleteException, AmazonClientException, IOException\n{\r\n    incrementWriteOperations();\r\n    BulkDeleteRetryHandler retryHandler = new BulkDeleteRetryHandler(createStoreContext());\r\n    int keyCount = deleteRequest.getKeys().size();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"DELETE %d keys\", keyCount)) {\r\n        return invoker.retryUntranslated(\"delete\", DELETE_CONSIDERED_IDEMPOTENT, (text, e, r, i) -> {\r\n            retryHandler.bulkDeleteRetried(deleteRequest, e);\r\n        }, trackDurationOfOperation(getDurationTrackerFactory(), OBJECT_BULK_DELETE_REQUEST.getSymbol(), () -> {\r\n            incrementStatistic(OBJECT_DELETE_OBJECTS, keyCount);\r\n            return s3.deleteObjects(deleteRequest);\r\n        }));\r\n    } catch (MultiObjectDeleteException e) {\r\n        List<MultiObjectDeleteException.DeleteError> errors = e.getErrors();\r\n        LOG.debug(\"Partial failure of delete, {} errors\", errors.size(), e);\r\n        for (MultiObjectDeleteException.DeleteError error : errors) {\r\n            LOG.debug(\"{}: \\\"{}\\\" - {}\", error.getKey(), error.getCode(), error.getMessage());\r\n        }\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newPutObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PutObjectRequest newPutObjectRequest(String key, ObjectMetadata metadata, File srcfile)\n{\r\n    return requestFactory.newPutObjectRequest(key, metadata, srcfile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ObjectMetadata newObjectMetadata(long length)\n{\r\n    return requestFactory.newObjectMetadata(length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "putObject",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "UploadInfo putObject(PutObjectRequest putObjectRequest)\n{\r\n    long len = getPutRequestLength(putObjectRequest);\r\n    LOG.debug(\"PUT {} bytes to {} via transfer manager \", len, putObjectRequest.getKey());\r\n    incrementPutStartStatistics(len);\r\n    Upload upload = transfers.upload(putObjectRequest);\r\n    return new UploadInfo(upload, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "putObjectDirect",
  "errType" : [ "SdkBaseException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "PutObjectResult putObjectDirect(PutObjectRequest putObjectRequest) throws AmazonClientException\n{\r\n    long len = getPutRequestLength(putObjectRequest);\r\n    LOG.debug(\"PUT {} bytes to {}\", len, putObjectRequest.getKey());\r\n    incrementPutStartStatistics(len);\r\n    try {\r\n        PutObjectResult result = trackDurationOfSupplier(getDurationTrackerFactory(), OBJECT_PUT_REQUESTS.getSymbol(), () -> s3.putObject(putObjectRequest));\r\n        incrementPutCompletedStatistics(true, len);\r\n        finishedWrite(putObjectRequest.getKey(), len, result.getETag(), result.getVersionId());\r\n        return result;\r\n    } catch (SdkBaseException e) {\r\n        incrementPutCompletedStatistics(false, len);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPutRequestLength",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getPutRequestLength(PutObjectRequest putObjectRequest)\n{\r\n    long len;\r\n    if (putObjectRequest.getFile() != null) {\r\n        len = putObjectRequest.getFile().length();\r\n    } else {\r\n        len = putObjectRequest.getMetadata().getContentLength();\r\n    }\r\n    Preconditions.checkState(len >= 0, \"Cannot PUT object of unknown length\");\r\n    return len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "uploadPart",
  "errType" : [ "AmazonClientException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "UploadPartResult uploadPart(UploadPartRequest request) throws AmazonClientException\n{\r\n    long len = request.getPartSize();\r\n    incrementPutStartStatistics(len);\r\n    try {\r\n        UploadPartResult uploadPartResult = s3.uploadPart(request);\r\n        incrementPutCompletedStatistics(true, len);\r\n        return uploadPartResult;\r\n    } catch (AmazonClientException e) {\r\n        incrementPutCompletedStatistics(false, len);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementPutStartStatistics",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void incrementPutStartStatistics(long bytes)\n{\r\n    LOG.debug(\"PUT start {} bytes\", bytes);\r\n    incrementWriteOperations();\r\n    incrementGauge(OBJECT_PUT_REQUESTS_ACTIVE, 1);\r\n    if (bytes > 0) {\r\n        incrementGauge(OBJECT_PUT_BYTES_PENDING, bytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementPutCompletedStatistics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void incrementPutCompletedStatistics(boolean success, long bytes)\n{\r\n    LOG.debug(\"PUT completed success={}; {} bytes\", success, bytes);\r\n    if (bytes > 0) {\r\n        incrementStatistic(OBJECT_PUT_BYTES, bytes);\r\n        decrementGauge(OBJECT_PUT_BYTES_PENDING, bytes);\r\n    }\r\n    incrementStatistic(OBJECT_PUT_REQUESTS_COMPLETED);\r\n    decrementGauge(OBJECT_PUT_REQUESTS_ACTIVE, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementPutProgressStatistics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incrementPutProgressStatistics(String key, long bytes)\n{\r\n    PROGRESS.debug(\"PUT {}: {} bytes\", key, bytes);\r\n    incrementWriteOperations();\r\n    if (bytes > 0) {\r\n        statistics.incrementBytesWritten(bytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "removeKeysS3",
  "errType" : [ "MultiObjectDeleteException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void removeKeysS3(List<DeleteObjectsRequest.KeyVersion> keysToDelete, boolean deleteFakeDir) throws MultiObjectDeleteException, AmazonClientException, IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Initiating delete operation for {} objects\", keysToDelete.size());\r\n        for (DeleteObjectsRequest.KeyVersion key : keysToDelete) {\r\n            LOG.debug(\" {} {}\", key.getKey(), key.getVersion() != null ? key.getVersion() : \"\");\r\n        }\r\n    }\r\n    if (keysToDelete.isEmpty()) {\r\n        return;\r\n    }\r\n    for (DeleteObjectsRequest.KeyVersion keyVersion : keysToDelete) {\r\n        blockRootDelete(keyVersion.getKey());\r\n    }\r\n    try {\r\n        if (enableMultiObjectsDelete) {\r\n            if (keysToDelete.size() <= pageSize) {\r\n                deleteObjects(getRequestFactory().newBulkDeleteRequest(keysToDelete));\r\n            } else {\r\n                LOG.debug(\"Partitioning the keys to delete as it is more than \" + \"page size. Number of keys: {}, Page size: {}\", keysToDelete.size(), pageSize);\r\n                for (List<DeleteObjectsRequest.KeyVersion> batchOfKeysToDelete : Lists.partition(keysToDelete, pageSize)) {\r\n                    deleteObjects(getRequestFactory().newBulkDeleteRequest(batchOfKeysToDelete));\r\n                }\r\n            }\r\n        } else {\r\n            for (DeleteObjectsRequest.KeyVersion keyVersion : keysToDelete) {\r\n                deleteObject(keyVersion.getKey());\r\n            }\r\n        }\r\n    } catch (MultiObjectDeleteException ex) {\r\n        int rejected = ex.getErrors().size();\r\n        noteDeleted(keysToDelete.size() - rejected, deleteFakeDir);\r\n        incrementStatistic(FILES_DELETE_REJECTED, rejected);\r\n        throw ex;\r\n    }\r\n    noteDeleted(keysToDelete.size(), deleteFakeDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "noteDeleted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void noteDeleted(final int count, final boolean deleteFakeDir)\n{\r\n    if (!deleteFakeDir) {\r\n        instrumentation.fileDeleted(count);\r\n    } else {\r\n        instrumentation.fakeDirsDeleted(count);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "removeKeys",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void removeKeys(final List<DeleteObjectsRequest.KeyVersion> keysToDelete, final boolean deleteFakeDir) throws MultiObjectDeleteException, AmazonClientException, IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Deleting %d keys\", keysToDelete.size())) {\r\n        removeKeysS3(keysToDelete, deleteFakeDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "delete",
  "errType" : [ "FileNotFoundException", "AmazonClientException", "AccessDeniedException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive) throws IOException\n{\r\n    checkNotClosed();\r\n    final Path path = qualify(f);\r\n    try (AuditSpan span = createSpan(INVOCATION_DELETE.getSymbol(), path.toString(), null)) {\r\n        boolean outcome = trackDuration(getDurationTrackerFactory(), INVOCATION_DELETE.getSymbol(), new DeleteOperation(createStoreContext(), innerGetFileStatus(path, true, StatusProbeEnum.ALL), recursive, new OperationCallbacksImpl(), pageSize));\r\n        if (outcome) {\r\n            try {\r\n                maybeCreateFakeParentDirectory(path);\r\n            } catch (AccessDeniedException e) {\r\n                LOG.warn(\"Cannot create directory marker at {}: {}\", f.getParent(), e.toString());\r\n                LOG.debug(\"Failed to create fake dir above {}\", path, e);\r\n            }\r\n        }\r\n        return outcome;\r\n    } catch (FileNotFoundException e) {\r\n        LOG.debug(\"Couldn't delete {} - does not exist: {}\", path, e.toString());\r\n        instrumentation.errorIgnored();\r\n        return false;\r\n    } catch (AmazonClientException e) {\r\n        throw translateException(\"delete\", path, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFakeDirectoryIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createFakeDirectoryIfNecessary(Path f) throws IOException, AmazonClientException\n{\r\n    String key = pathToKey(f);\r\n    if (!key.isEmpty() && !s3Exists(f, StatusProbeEnum.DIRECTORIES)) {\r\n        LOG.debug(\"Creating new fake directory at {}\", f);\r\n        createFakeDirectory(key);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeCreateFakeParentDirectory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void maybeCreateFakeParentDirectory(Path path) throws IOException, AmazonClientException\n{\r\n    Path parent = path.getParent();\r\n    if (parent != null && !parent.isRoot()) {\r\n        createFakeDirectoryIfNecessary(parent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(Path p) throws FileNotFoundException, IOException\n{\r\n    Path path = qualify(p);\r\n    return typeCastingRemoteIterator(trackDurationAndSpan(INVOCATION_LIST_STATUS, path, () -> once(\"listStatus\", path.toString(), () -> innerListStatus(p))));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus[] listStatus(Path f) throws FileNotFoundException, IOException\n{\r\n    Path path = qualify(f);\r\n    return trackDurationAndSpan(INVOCATION_LIST_STATUS, path, () -> once(\"listStatus\", path.toString(), () -> iteratorToStatuses(innerListStatus(path))));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "innerListStatus",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "RemoteIterator<S3AFileStatus> innerListStatus(Path f) throws FileNotFoundException, IOException, AmazonClientException\n{\r\n    Path path = qualify(f);\r\n    LOG.debug(\"List status for path: {}\", path);\r\n    final RemoteIterator<S3AFileStatus> statusIt = listing.getFileStatusesAssumingNonEmptyDir(path, getActiveAuditSpan());\r\n    if (!statusIt.hasNext()) {\r\n        final S3AFileStatus fileStatus = innerGetFileStatus(path, false, StatusProbeEnum.ALL);\r\n        if (fileStatus.isFile()) {\r\n            LOG.debug(\"Adding: rd (not a dir): {}\", path);\r\n            S3AFileStatus[] stats = new S3AFileStatus[1];\r\n            stats[0] = fileStatus;\r\n            return listing.createProvidedFileStatusIterator(stats, ACCEPT_ALL, Listing.ACCEPT_ALL_BUT_S3N);\r\n        }\r\n    }\r\n    return statusIt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "allowAuthoritative",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean allowAuthoritative(final Path path)\n{\r\n    return S3Guard.allowAuthoritative(path, this, allowAuthoritativePaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createListObjectsRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3ListRequest createListObjectsRequest(String key, String delimiter)\n{\r\n    return createListObjectsRequest(key, delimiter, maxKeys);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createListObjectsRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "S3ListRequest createListObjectsRequest(String key, String delimiter, int limit)\n{\r\n    if (!useListV1) {\r\n        ListObjectsV2Request request = getRequestFactory().newListObjectsV2Request(key, delimiter, limit);\r\n        return S3ListRequest.v2(request);\r\n    } else {\r\n        ListObjectsRequest request = getRequestFactory().newListObjectsV1Request(key, delimiter, limit);\r\n        return S3ListRequest.v1(request);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWorkingDirectory(Path newDir)\n{\r\n    workingDir = makeQualified(newDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    return workingDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getUsername",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUsername()\n{\r\n    return username;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserGroupInformation getOwner()\n{\r\n    return owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(Path p, FsPermission permission) throws IOException, FileAlreadyExistsException\n{\r\n    Path path = qualify(p);\r\n    return trackDurationAndSpan(INVOCATION_MKDIRS, path, new MkdirOperation(createStoreContext(), path, createMkdirOperationCallbacks()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createMkdirOperationCallbacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MkdirOperation.MkdirCallbacks createMkdirOperationCallbacks()\n{\r\n    return new MkdirOperationCallbacksImpl();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getContentSummary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ContentSummary getContentSummary(final Path f) throws IOException\n{\r\n    final Path path = qualify(f);\r\n    return trackDurationAndSpan(INVOCATION_GET_CONTENT_SUMMARY, path, new GetContentSummaryOperation(createStoreContext(), path, createGetContentSummaryCallbacks()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createGetContentSummaryCallbacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetContentSummaryOperation.GetContentSummaryCallbacks createGetContentSummaryCallbacks()\n{\r\n    return new GetContentSummaryCallbacksImpl();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void access(final Path f, final FsAction mode) throws AccessControlException, FileNotFoundException, IOException\n{\r\n    Path path = qualify(f);\r\n    LOG.debug(\"check access mode {} for {}\", path, mode);\r\n    trackDurationAndSpan(INVOCATION_ACCESS, path, () -> {\r\n        final S3AFileStatus stat = innerGetFileStatus(path, false, StatusProbeEnum.ALL);\r\n        if (!getAuditManager().checkAccess(path, stat, mode)) {\r\n            incrementStatistic(AUDIT_ACCESS_CHECK_FAILURE);\r\n            throw new AccessControlException(String.format(\"Permission denied: user=%s, path=\\\"%s\\\":%s:%s:%s%s\", getOwner().getUserName(), stat.getPath(), stat.getOwner(), stat.getGroup(), stat.isDirectory() ? \"d\" : \"-\", mode));\r\n        }\r\n        return true;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileStatus(final Path f) throws IOException\n{\r\n    Path path = qualify(f);\r\n    return trackDurationAndSpan(INVOCATION_GET_FILE_STATUS, path, () -> innerGetFileStatus(path, false, StatusProbeEnum.ALL));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "innerGetFileStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "S3AFileStatus innerGetFileStatus(final Path f, final boolean needEmptyDirectoryFlag, final Set<StatusProbeEnum> probes) throws IOException\n{\r\n    final Path path = qualify(f);\r\n    String key = pathToKey(path);\r\n    LOG.debug(\"Getting path status for {}  ({}); needEmptyDirectory={}\", path, key, needEmptyDirectoryFlag);\r\n    return s3GetFileStatus(path, key, probes, needEmptyDirectoryFlag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "s3GetFileStatus",
  "errType" : [ "AmazonServiceException", "AmazonClientException", "AmazonServiceException", "AmazonClientException" ],
  "containingMethodsNum" : 34,
  "sourceCodeText" : "S3AFileStatus s3GetFileStatus(final Path path, final String key, final Set<StatusProbeEnum> probes, final boolean needEmptyDirectoryFlag) throws IOException\n{\r\n    LOG.debug(\"S3GetFileStatus {}\", path);\r\n    Preconditions.checkArgument(!needEmptyDirectoryFlag || probes.contains(StatusProbeEnum.List), \"s3GetFileStatus(%s) wants to know if a directory is empty but\" + \" does not request a list probe\", path);\r\n    if (key.isEmpty() && !needEmptyDirectoryFlag) {\r\n        return new S3AFileStatus(Tristate.UNKNOWN, path, username);\r\n    }\r\n    if (!key.isEmpty() && !key.endsWith(\"/\") && probes.contains(StatusProbeEnum.Head)) {\r\n        try {\r\n            ObjectMetadata meta = getObjectMetadata(key);\r\n            LOG.debug(\"Found exact file: normal file {}\", key);\r\n            long contentLength = meta.getContentLength();\r\n            if (isCSEEnabled && meta.getUserMetaDataOf(Headers.CRYPTO_CEK_ALGORITHM) != null && contentLength >= CSE_PADDING_LENGTH) {\r\n                contentLength -= CSE_PADDING_LENGTH;\r\n            }\r\n            return new S3AFileStatus(contentLength, dateToLong(meta.getLastModified()), path, getDefaultBlockSize(path), username, meta.getETag(), meta.getVersionId());\r\n        } catch (AmazonServiceException e) {\r\n            if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {\r\n                throw translateException(\"getFileStatus\", path, e);\r\n            }\r\n        } catch (AmazonClientException e) {\r\n            throw translateException(\"getFileStatus\", path, e);\r\n        }\r\n    }\r\n    if (probes.contains(StatusProbeEnum.List)) {\r\n        try {\r\n            String dirKey = maybeAddTrailingSlash(key);\r\n            final int listSize = 2;\r\n            S3ListRequest request = createListObjectsRequest(dirKey, \"/\", listSize);\r\n            S3ListResult listResult = listObjects(request, getDurationTrackerFactory());\r\n            if (listResult.hasPrefixesOrObjects()) {\r\n                if (LOG.isDebugEnabled()) {\r\n                    LOG.debug(\"Found path as directory (with /)\");\r\n                    listResult.logAtDebug(LOG);\r\n                }\r\n                if (needEmptyDirectoryFlag && listResult.representsEmptyDirectory(dirKey)) {\r\n                    return new S3AFileStatus(Tristate.TRUE, path, username);\r\n                }\r\n                return new S3AFileStatus(Tristate.FALSE, path, username);\r\n            } else if (key.isEmpty()) {\r\n                LOG.debug(\"Found root directory\");\r\n                return new S3AFileStatus(Tristate.TRUE, path, username);\r\n            }\r\n        } catch (AmazonServiceException e) {\r\n            if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {\r\n                throw translateException(\"getFileStatus\", path, e);\r\n            }\r\n        } catch (AmazonClientException e) {\r\n            throw translateException(\"getFileStatus\", path, e);\r\n        }\r\n    }\r\n    LOG.debug(\"Not Found: {}\", path);\r\n    throw new FileNotFoundException(\"No such file or directory: \" + path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "s3Exists",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean s3Exists(final Path path, final Set<StatusProbeEnum> probes) throws IOException\n{\r\n    String key = pathToKey(path);\r\n    try {\r\n        s3GetFileStatus(path, key, probes, false);\r\n        return true;\r\n    } catch (FileNotFoundException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "copyFromLocalFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst) throws IOException\n{\r\n    checkNotClosed();\r\n    LOG.debug(\"Copying local file from {} to {}\", src, dst);\r\n    trackDurationAndSpan(INVOCATION_COPY_FROM_LOCAL_FILE, dst, () -> new CopyFromLocalOperation(createStoreContext(), src, dst, delSrc, overwrite, createCopyFromLocalCallbacks()).execute());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createCopyFromLocalCallbacks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CopyFromLocalOperation.CopyFromLocalOperationCallbacks createCopyFromLocalCallbacks() throws IOException\n{\r\n    LocalFileSystem local = getLocal(getConf());\r\n    return new CopyFromLocalCallbacksImpl(local);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "executePut",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "UploadResult executePut(PutObjectRequest putObjectRequest, Progressable progress) throws InterruptedIOException\n{\r\n    String key = putObjectRequest.getKey();\r\n    UploadInfo info = putObject(putObjectRequest);\r\n    Upload upload = info.getUpload();\r\n    ProgressableProgressListener listener = new ProgressableProgressListener(this, key, upload, progress);\r\n    upload.addProgressListener(listener);\r\n    UploadResult result = waitForUploadCompletion(key, info);\r\n    listener.uploadCompleted();\r\n    finishedWrite(key, info.getLength(), result.getETag(), result.getVersionId());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "waitForUploadCompletion",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "UploadResult waitForUploadCompletion(String key, UploadInfo uploadInfo) throws InterruptedIOException\n{\r\n    Upload upload = uploadInfo.getUpload();\r\n    try {\r\n        UploadResult result = upload.waitForUploadResult();\r\n        incrementPutCompletedStatistics(true, uploadInfo.getLength());\r\n        return result;\r\n    } catch (InterruptedException e) {\r\n        LOG.info(\"Interrupted: aborting upload\");\r\n        incrementPutCompletedStatistics(false, uploadInfo.getLength());\r\n        upload.abort();\r\n        throw (InterruptedIOException) new InterruptedIOException(\"Interrupted in PUT to \" + keyToQualifiedPath(key)).initCause(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (closed.getAndSet(true)) {\r\n        return;\r\n    }\r\n    isClosed = true;\r\n    LOG.debug(\"Filesystem {} is closed\", uri);\r\n    if (getConf() != null) {\r\n        String iostatisticsLoggingLevel = getConf().getTrimmed(IOSTATISTICS_LOGGING_LEVEL, IOSTATISTICS_LOGGING_LEVEL_DEFAULT);\r\n        logIOStatisticsAtLevel(LOG, iostatisticsLoggingLevel, getIOStatistics());\r\n    }\r\n    try {\r\n        super.close();\r\n    } finally {\r\n        stopAllServices();\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Statistics for {}: {}\", uri, IOStatisticsLogging.ioStatisticsToPrettyString(getIOStatistics()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "stopAllServices",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void stopAllServices()\n{\r\n    if (transfers != null) {\r\n        try {\r\n            transfers.shutdownNow(true);\r\n        } catch (RuntimeException e) {\r\n            LOG.debug(\"When shutting down\", e);\r\n        }\r\n        transfers = null;\r\n    }\r\n    HadoopExecutors.shutdown(boundedThreadPool, LOG, THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\r\n    boundedThreadPool = null;\r\n    HadoopExecutors.shutdown(unboundedThreadPool, LOG, THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\r\n    unboundedThreadPool = null;\r\n    cleanupWithLogger(LOG, instrumentation, delegationTokens.orElse(null), signerManager, auditManager);\r\n    closeAutocloseables(LOG, credentials);\r\n    delegationTokens = Optional.empty();\r\n    signerManager = null;\r\n    credentials = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkNotClosed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkNotClosed() throws IOException\n{\r\n    if (isClosed) {\r\n        throw new IOException(uri + \": \" + E_FS_CLOSED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Optional<S3ADelegationTokens> getDelegationTokens()\n{\r\n    return delegationTokens;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    if (!delegationTokens.isPresent()) {\r\n        return null;\r\n    }\r\n    S3ADelegationTokens dt = delegationTokens.get();\r\n    return dt.getTokenIssuingPolicy() != NoTokensAvailable ? dt.getCanonicalServiceName() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> getDelegationToken(String renewer) throws IOException\n{\r\n    checkNotClosed();\r\n    LOG.debug(\"Delegation token requested\");\r\n    if (delegationTokens.isPresent()) {\r\n        return trackDurationAndSpan(INVOCATION_GET_DELEGATION_TOKEN, null, () -> delegationTokens.get().getBoundOrNewDT(encryptionSecrets, (renewer != null ? new Text(renewer) : new Text())));\r\n    } else {\r\n        LOG.debug(\"Token support is not enabled\");\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAdditionalTokenIssuers",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DelegationTokenIssuer[] getAdditionalTokenIssuers() throws IOException\n{\r\n    checkNotClosed();\r\n    if (delegationTokens.isPresent()) {\r\n        return delegationTokens.get().getAdditionalTokenIssuers();\r\n    } else {\r\n        LOG.debug(\"Token support is not enabled\");\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listAWSPolicyRules",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<RoleModel.Statement> listAWSPolicyRules(final Set<AccessLevel> access)\n{\r\n    if (access.isEmpty()) {\r\n        return Collections.emptyList();\r\n    }\r\n    List<RoleModel.Statement> statements = new ArrayList<>(allowS3Operations(bucket, access.contains(AccessLevel.WRITE) || access.contains(AccessLevel.ADMIN)));\r\n    statements.add(STATEMENT_ALLOW_SSE_KMS_RW);\r\n    return statements;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "copyFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "CopyResult copyFile(String srcKey, String dstKey, long size, S3ObjectAttributes srcAttributes, S3AReadOpContext readContext) throws IOException, InterruptedIOException\n{\r\n    LOG.debug(\"copyFile {} -> {} \", srcKey, dstKey);\r\n    ProgressListener progressListener = progressEvent -> {\r\n        switch(progressEvent.getEventType()) {\r\n            case TRANSFER_PART_COMPLETED_EVENT:\r\n                incrementWriteOperations();\r\n                break;\r\n            default:\r\n                break;\r\n        }\r\n    };\r\n    ChangeTracker changeTracker = new ChangeTracker(keyToQualifiedPath(srcKey).toString(), changeDetectionPolicy, readContext.getS3AStatisticsContext().newInputStreamStatistics().getChangeTrackerStatistics(), srcAttributes);\r\n    String action = \"copyFile(\" + srcKey + \", \" + dstKey + \")\";\r\n    Invoker readInvoker = readContext.getReadInvoker();\r\n    ObjectMetadata srcom;\r\n    try {\r\n        srcom = once(action, srcKey, () -> getObjectMetadata(srcKey, changeTracker, readInvoker, \"copy\"));\r\n    } catch (FileNotFoundException e) {\r\n        LOG.debug(\"getObjectMetadata({}) failed to find an expected file\", srcKey, e);\r\n        throw new RemoteFileChangedException(keyToQualifiedPath(srcKey).toString(), action, RemoteFileChangedException.FILE_NOT_FOUND_SINGLE_ATTEMPT, e);\r\n    }\r\n    return readInvoker.retry(action, srcKey, true, () -> {\r\n        CopyObjectRequest copyObjectRequest = getRequestFactory().newCopyObjectRequest(srcKey, dstKey, srcom);\r\n        changeTracker.maybeApplyConstraint(copyObjectRequest);\r\n        incrementStatistic(OBJECT_COPY_REQUESTS);\r\n        Copy copy = transfers.copy(copyObjectRequest, getAuditManager().createStateChangeListener());\r\n        copy.addProgressListener(progressListener);\r\n        CopyOutcome copyOutcome = CopyOutcome.waitForCopy(copy);\r\n        InterruptedException interruptedException = copyOutcome.getInterruptedException();\r\n        if (interruptedException != null) {\r\n            throw (IOException) new InterruptedIOException(\"Interrupted copying \" + srcKey + \" to \" + dstKey + \", cancelling\").initCause(interruptedException);\r\n        }\r\n        SdkBaseException awsException = copyOutcome.getAwsException();\r\n        if (awsException != null) {\r\n            changeTracker.processException(awsException, \"copy\");\r\n            throw awsException;\r\n        }\r\n        CopyResult result = copyOutcome.getCopyResult();\r\n        changeTracker.processResponse(result);\r\n        incrementWriteOperations();\r\n        instrumentation.filesCopied(1, size);\r\n        return result;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initiateMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InitiateMultipartUploadResult initiateMultipartUpload(InitiateMultipartUploadRequest request) throws IOException\n{\r\n    LOG.debug(\"Initiate multipart upload to {}\", request.getKey());\r\n    return trackDurationOfSupplier(getDurationTrackerFactory(), OBJECT_MULTIPART_UPLOAD_INITIATED.getSymbol(), () -> getAmazonS3Client().initiateMultipartUpload(request));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "finishedWrite",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void finishedWrite(String key, long length, String eTag, String versionId)\n{\r\n    LOG.debug(\"Finished write to {}, len {}. etag {}, version {}\", key, length, eTag, versionId);\r\n    Path p = keyToQualifiedPath(key);\r\n    Preconditions.checkArgument(length >= 0, \"content length is negative\");\r\n    CompletableFuture<?> deletion;\r\n    if (!keepDirectoryMarkers(p)) {\r\n        deletion = submit(unboundedThreadPool, getActiveAuditSpan(), () -> {\r\n            deleteUnnecessaryFakeDirectories(p.getParent());\r\n            return null;\r\n        });\r\n    } else {\r\n        deletion = null;\r\n    }\r\n    waitForCompletionIgnoringExceptions(deletion);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "keepDirectoryMarkers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean keepDirectoryMarkers(Path path)\n{\r\n    return directoryPolicy.keepDirectoryMarkers(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteUnnecessaryFakeDirectories",
  "errType" : [ "AmazonClientException|IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void deleteUnnecessaryFakeDirectories(Path path)\n{\r\n    List<DeleteObjectsRequest.KeyVersion> keysToRemove = new ArrayList<>();\r\n    while (!path.isRoot()) {\r\n        String key = pathToKey(path);\r\n        key = (key.endsWith(\"/\")) ? key : (key + \"/\");\r\n        LOG.trace(\"To delete unnecessary fake directory {} for {}\", key, path);\r\n        keysToRemove.add(new DeleteObjectsRequest.KeyVersion(key));\r\n        path = path.getParent();\r\n    }\r\n    try {\r\n        removeKeys(keysToRemove, true);\r\n    } catch (AmazonClientException | IOException e) {\r\n        instrumentation.errorIgnored();\r\n        if (LOG.isDebugEnabled()) {\r\n            StringBuilder sb = new StringBuilder();\r\n            for (DeleteObjectsRequest.KeyVersion kv : keysToRemove) {\r\n                sb.append(kv.getKey()).append(\",\");\r\n            }\r\n            LOG.debug(\"While deleting keys {} \", sb.toString(), e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFakeDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createFakeDirectory(final String objectName) throws IOException\n{\r\n    createEmptyObject(objectName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createEmptyObject",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createEmptyObject(final String objectName) throws IOException\n{\r\n    invoker.retry(\"PUT 0-byte object \", objectName, true, () -> putObjectDirect(getRequestFactory().newDirectoryMarkerRequest(objectName)));\r\n    incrementPutProgressStatistics(objectName, 0);\r\n    instrumentation.directoryCreated();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDefaultBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDefaultBlockSize()\n{\r\n    return getConf().getLongBytes(FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDirectoryMarkerPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DirectoryPolicy getDirectoryMarkerPolicy()\n{\r\n    return directoryPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3AFileSystem{\");\r\n    sb.append(\"uri=\").append(uri);\r\n    sb.append(\", workingDir=\").append(workingDir);\r\n    sb.append(\", inputPolicy=\").append(inputPolicy);\r\n    sb.append(\", partSize=\").append(partSize);\r\n    sb.append(\", enableMultiObjectsDelete=\").append(enableMultiObjectsDelete);\r\n    sb.append(\", maxKeys=\").append(maxKeys);\r\n    if (cannedACL != null) {\r\n        sb.append(\", cannedACL=\").append(cannedACL.toString());\r\n    }\r\n    sb.append(\", readAhead=\").append(readAhead);\r\n    if (getConf() != null) {\r\n        sb.append(\", blockSize=\").append(getDefaultBlockSize());\r\n    }\r\n    sb.append(\", multiPartThreshold=\").append(multiPartThreshold);\r\n    if (getS3EncryptionAlgorithm() != null) {\r\n        sb.append(\", s3EncryptionAlgorithm='\").append(getS3EncryptionAlgorithm()).append('\\'');\r\n    }\r\n    if (blockFactory != null) {\r\n        sb.append(\", blockFactory=\").append(blockFactory);\r\n    }\r\n    sb.append(\", auditManager=\").append(auditManager);\r\n    sb.append(\", authoritativePath=\").append(allowAuthoritativePaths);\r\n    sb.append(\", useListV1=\").append(useListV1);\r\n    if (committerIntegration != null) {\r\n        sb.append(\", magicCommitter=\").append(isMagicCommitEnabled());\r\n    }\r\n    sb.append(\", boundedExecutor=\").append(boundedThreadPool);\r\n    sb.append(\", unboundedExecutor=\").append(unboundedThreadPool);\r\n    sb.append(\", credentials=\").append(credentials);\r\n    sb.append(\", delegation tokens=\").append(delegationTokens.map(Objects::toString).orElse(\"disabled\"));\r\n    sb.append(\", \").append(directoryPolicy);\r\n    if (getInstrumentation() != null) {\r\n        sb.append(\", instrumentation {\").append(getInstrumentation().toString()).append(\"}\");\r\n    }\r\n    sb.append(\", ClientSideEncryption=\").append(isCSEEnabled);\r\n    if (accessPoint != null) {\r\n        sb.append(\", arnForBucket=\").append(accessPoint.getFullArn());\r\n    }\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPartitionSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPartitionSize()\n{\r\n    return partSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMultiPartThreshold",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMultiPartThreshold()\n{\r\n    return multiPartThreshold;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMaxKeys",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxKeys()\n{\r\n    return maxKeys;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isMagicCommitEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMagicCommitEnabled()\n{\r\n    return committerIntegration.isMagicCommitEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isMagicCommitPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMagicCommitPath(Path path)\n{\r\n    return committerIntegration.isMagicCommitPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "globStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus[] globStatus(Path pathPattern) throws IOException\n{\r\n    return globStatus(pathPattern, ACCEPT_ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "globStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus[] globStatus(final Path pathPattern, final PathFilter filter) throws IOException\n{\r\n    return trackDurationAndSpan(INVOCATION_GLOB_STATUS, pathPattern, () -> Globber.createGlobber(this).withPathPattern(pathPattern).withPathFiltern(filter).withResolveSymlinks(false).build().glob());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "exists",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean exists(Path f) throws IOException\n{\r\n    final Path path = qualify(f);\r\n    try {\r\n        trackDurationAndSpan(INVOCATION_EXISTS, path, () -> innerGetFileStatus(path, false, StatusProbeEnum.ALL));\r\n        return true;\r\n    } catch (FileNotFoundException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isDirectory",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isDirectory(Path f) throws IOException\n{\r\n    final Path path = qualify(f);\r\n    try {\r\n        return trackDurationAndSpan(INVOCATION_IS_DIRECTORY, path, () -> innerGetFileStatus(path, false, StatusProbeEnum.DIRECTORIES).isDirectory());\r\n    } catch (FileNotFoundException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isFile(Path f) throws IOException\n{\r\n    final Path path = qualify(f);\r\n    try {\r\n        return trackDurationAndSpan(INVOCATION_IS_FILE, path, () -> innerGetFileStatus(path, false, StatusProbeEnum.HEAD_ONLY).isFile());\r\n    } catch (FileNotFoundException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFileChecksum",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "EtagChecksum getFileChecksum(Path f, final long length) throws IOException\n{\r\n    Preconditions.checkArgument(length >= 0);\r\n    final Path path = qualify(f);\r\n    if (getConf().getBoolean(ETAG_CHECKSUM_ENABLED, ETAG_CHECKSUM_ENABLED_DEFAULT)) {\r\n        return trackDurationAndSpan(INVOCATION_GET_FILE_CHECKSUM, path, () -> {\r\n            LOG.debug(\"getFileChecksum({})\", path);\r\n            ObjectMetadata headers = getObjectMetadata(path, null, invoker, \"getFileChecksum are\");\r\n            String eTag = headers.getETag();\r\n            return eTag != null ? new EtagChecksum(eTag) : null;\r\n        });\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getHeaderProcessing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HeaderProcessing getHeaderProcessing()\n{\r\n    return new HeaderProcessing(createStoreContext(), createHeaderProcessingCallbacks());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] getXAttr(final Path path, final String name) throws IOException\n{\r\n    checkNotClosed();\r\n    try (AuditSpan span = createSpan(INVOCATION_XATTR_GET_NAMED.getSymbol(), path.toString(), null)) {\r\n        return getHeaderProcessing().getXAttr(path, name);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(final Path path) throws IOException\n{\r\n    checkNotClosed();\r\n    try (AuditSpan span = createSpan(INVOCATION_XATTR_GET_MAP.getSymbol(), path.toString(), null)) {\r\n        return getHeaderProcessing().getXAttrs(path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(final Path path, final List<String> names) throws IOException\n{\r\n    checkNotClosed();\r\n    try (AuditSpan span = createSpan(INVOCATION_XATTR_GET_NAMED_MAP.getSymbol(), path.toString(), null)) {\r\n        return getHeaderProcessing().getXAttrs(path, names);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> listXAttrs(final Path path) throws IOException\n{\r\n    checkNotClosed();\r\n    try (AuditSpan span = createSpan(INVOCATION_OP_XATTR_LIST.getSymbol(), path.toString(), null)) {\r\n        return getHeaderProcessing().listXAttrs(path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createHeaderProcessingCallbacks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HeaderProcessing.HeaderProcessingCallbacks createHeaderProcessingCallbacks()\n{\r\n    return new HeaderProcessingCallbacksImpl();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listFiles(Path f, boolean recursive) throws FileNotFoundException, IOException\n{\r\n    final Path path = qualify(f);\r\n    return toLocatedFileStatusIterator(trackDurationAndSpan(INVOCATION_LIST_FILES, path, () -> innerListFiles(path, recursive, new Listing.AcceptFilesOnly(path), null)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listFilesAndEmptyDirectories",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<S3ALocatedFileStatus> listFilesAndEmptyDirectories(Path f, boolean recursive) throws IOException\n{\r\n    final Path path = qualify(f);\r\n    return trackDurationAndSpan(INVOCATION_LIST_FILES, path, () -> innerListFiles(path, recursive, Listing.ACCEPT_ALL_BUT_S3N, null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "innerListFiles",
  "errType" : [ "AmazonClientException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "RemoteIterator<S3ALocatedFileStatus> innerListFiles(final Path f, final boolean recursive, final Listing.FileStatusAcceptor acceptor, final S3AFileStatus status) throws IOException\n{\r\n    Path path = qualify(f);\r\n    LOG.debug(\"listFiles({}, {})\", path, recursive);\r\n    try {\r\n        if (status != null && status.isFile()) {\r\n            LOG.debug(\"Path is a file: {}\", path);\r\n            return listing.createSingleStatusIterator(toLocatedFileStatus(status));\r\n        }\r\n        RemoteIterator<S3ALocatedFileStatus> listFilesAssumingDir = listing.getListFilesAssumingDir(path, recursive, acceptor, getActiveAuditSpan());\r\n        if (!listFilesAssumingDir.hasNext()) {\r\n            final S3AFileStatus fileStatus = status != null ? status : innerGetFileStatus(path, false, StatusProbeEnum.ALL);\r\n            if (fileStatus.isFile()) {\r\n                return listing.createSingleStatusIterator(toLocatedFileStatus(fileStatus));\r\n            }\r\n        }\r\n        return listFilesAssumingDir;\r\n    } catch (AmazonClientException e) {\r\n        throw translateException(\"listFiles\", path, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listLocatedStatus(Path f) throws FileNotFoundException, IOException\n{\r\n    return listLocatedStatus(f, ACCEPT_ALL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f, final PathFilter filter) throws FileNotFoundException, IOException\n{\r\n    Path path = qualify(f);\r\n    AuditSpan span = entryPoint(INVOCATION_LIST_LOCATED_STATUS, path);\r\n    LOG.debug(\"listLocatedStatus({}, {}\", path, filter);\r\n    RemoteIterator<? extends LocatedFileStatus> iterator = once(\"listLocatedStatus\", path.toString(), () -> {\r\n        final RemoteIterator<S3ALocatedFileStatus> locatedFileStatusIteratorForDir = listing.getLocatedFileStatusIteratorForDir(path, filter, span);\r\n        if (!locatedFileStatusIteratorForDir.hasNext()) {\r\n            final S3AFileStatus fileStatus = innerGetFileStatus(path, false, StatusProbeEnum.ALL);\r\n            if (fileStatus.isFile()) {\r\n                LOG.debug(\"Path is a file\");\r\n                return listing.createSingleStatusIterator(filter.accept(path) ? toLocatedFileStatus(fileStatus) : null);\r\n            }\r\n        }\r\n        return locatedFileStatusIteratorForDir;\r\n    });\r\n    return toLocatedFileStatusIterator(iterator);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toLocatedFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3ALocatedFileStatus toLocatedFileStatus(S3AFileStatus status) throws IOException\n{\r\n    return new S3ALocatedFileStatus(status, status.isFile() ? getFileBlockLocations(status, 0, status.getLen()) : null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listUploads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MultipartUtils.UploadIterator listUploads(@Nullable String prefix) throws IOException\n{\r\n    return trackDurationAndSpan(MULTIPART_UPLOAD_LIST, prefix, null, () -> MultipartUtils.listMultipartUploads(createStoreContext(), s3, prefix, maxKeys));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<MultipartUpload> listMultipartUploads(String prefix) throws IOException\n{\r\n    if (prefix != null && !prefix.isEmpty() && !prefix.endsWith(\"/\")) {\r\n        prefix = prefix + \"/\";\r\n    }\r\n    String p = prefix;\r\n    return invoker.retry(\"listMultipartUploads\", p, true, () -> {\r\n        ListMultipartUploadsRequest request = getRequestFactory().newListMultipartUploadsRequest(p);\r\n        return s3.listMultipartUploads(request).getMultipartUploads();\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abortMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abortMultipartUpload(String destKey, String uploadId)\n{\r\n    LOG.info(\"Aborting multipart upload {} to {}\", uploadId, destKey);\r\n    getAmazonS3Client().abortMultipartUpload(getRequestFactory().newAbortMultipartUploadRequest(destKey, uploadId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abortMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void abortMultipartUpload(MultipartUpload upload)\n{\r\n    String destKey;\r\n    String uploadId;\r\n    destKey = upload.getKey();\r\n    uploadId = upload.getUploadId();\r\n    if (LOG.isInfoEnabled()) {\r\n        DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\r\n        LOG.debug(\"Aborting multipart upload {} to {} initiated by {} on {}\", uploadId, destKey, upload.getInitiator(), df.format(upload.getInitiated()));\r\n    }\r\n    getAmazonS3Client().abortMultipartUpload(getRequestFactory().newAbortMultipartUploadRequest(destKey, uploadId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newCommitterStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CommitterStatistics newCommitterStatistics()\n{\r\n    return statisticsContext.newCommitterStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean hasPathCapability(final Path path, final String capability) throws IOException\n{\r\n    final Path p = makeQualified(path);\r\n    String cap = validatePathCapabilityArgs(p, capability);\r\n    switch(cap) {\r\n        case CommitConstants.STORE_CAPABILITY_MAGIC_COMMITTER:\r\n        case CommitConstants.STORE_CAPABILITY_MAGIC_COMMITTER_OLD:\r\n            return isMagicCommitEnabled();\r\n        case SelectConstants.S3_SELECT_CAPABILITY:\r\n            return !isCSEEnabled && SelectBinding.isSelectEnabled(getConf());\r\n        case CommonPathCapabilities.FS_CHECKSUMS:\r\n            return getConf().getBoolean(ETAG_CHECKSUM_ENABLED, ETAG_CHECKSUM_ENABLED_DEFAULT);\r\n        case CommonPathCapabilities.ABORTABLE_STREAM:\r\n            return true;\r\n        case CommonPathCapabilities.FS_MULTIPART_UPLOADER:\r\n            return !isCSEEnabled;\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_AWARE:\r\n            return true;\r\n        case CommonPathCapabilities.ETAGS_AVAILABLE:\r\n            return true;\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_KEEP:\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_DELETE:\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_AUTHORITATIVE:\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP:\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE:\r\n            return getDirectoryMarkerPolicy().hasPathCapability(path, cap);\r\n        default:\r\n            return super.hasPathCapability(p, cap);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hasCapability",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    try {\r\n        return hasPathCapability(new Path(\"/\"), capability);\r\n    } catch (IOException ex) {\r\n        LOG.debug(\"Ignoring exception on hasCapability({}})\", capability, ex);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "shareCredentials",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSCredentialProviderList shareCredentials(final String purpose)\n{\r\n    LOG.debug(\"Sharing credentials for: {}\", purpose);\r\n    return credentials.share();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "select",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "FSDataInputStream select(final Path source, final String expression, final Configuration options, final Optional<S3AFileStatus> providedStatus) throws IOException\n{\r\n    final AuditSpan auditSpan = entryPoint(OBJECT_SELECT_REQUESTS, source);\r\n    requireSelectSupport(source);\r\n    final Path path = makeQualified(source);\r\n    final S3AFileStatus fileStatus = extractOrFetchSimpleFileStatus(path, providedStatus);\r\n    long ra = options.getLong(READAHEAD_RANGE, readAhead);\r\n    S3ObjectAttributes objectAttributes = createObjectAttributes(fileStatus);\r\n    S3AReadOpContext readContext = createReadContext(fileStatus, inputPolicy, changeDetectionPolicy, ra, auditSpan);\r\n    if (changeDetectionPolicy.getSource() != ChangeDetectionPolicy.Source.None && fileStatus.getEtag() != null) {\r\n        ChangeTracker changeTracker = new ChangeTracker(uri.toString(), changeDetectionPolicy, readContext.getS3AStatisticsContext().newInputStreamStatistics().getChangeTrackerStatistics(), objectAttributes);\r\n        Invoker readInvoker = readContext.getReadInvoker();\r\n        getObjectMetadata(path, changeTracker, readInvoker, \"select\");\r\n    }\r\n    SelectBinding selectBinding = new SelectBinding(createWriteOperationHelper(auditSpan));\r\n    return selectBinding.select(readContext, expression, options, objectAttributes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "requireSelectSupport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireSelectSupport(final Path source) throws UnsupportedOperationException\n{\r\n    if (!isCSEEnabled && !SelectBinding.isSelectEnabled(getConf())) {\r\n        throw new UnsupportedOperationException(SelectConstants.SELECT_UNSUPPORTED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "extractOrFetchSimpleFileStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "S3AFileStatus extractOrFetchSimpleFileStatus(final Path path, final Optional<S3AFileStatus> optStatus) throws IOException\n{\r\n    S3AFileStatus fileStatus;\r\n    if (optStatus.isPresent()) {\r\n        fileStatus = optStatus.get();\r\n        if (fileStatus.isDirectory()) {\r\n            throw new FileNotFoundException(path.toString() + \" is a directory\");\r\n        }\r\n    } else {\r\n        fileStatus = innerGetFileStatus(path, false, StatusProbeEnum.HEAD_ONLY);\r\n    }\r\n    return fileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "openFileWithOptions",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "CompletableFuture<FSDataInputStream> openFileWithOptions(final Path rawPath, final OpenFileParameters parameters) throws IOException\n{\r\n    final Path path = qualify(rawPath);\r\n    Configuration options = parameters.getOptions();\r\n    Set<String> mandatoryKeys = parameters.getMandatoryKeys();\r\n    String sql = options.get(SelectConstants.SELECT_SQL, null);\r\n    boolean isSelect = sql != null;\r\n    if (isSelect) {\r\n        rejectUnknownMandatoryKeys(mandatoryKeys, InternalSelectConstants.SELECT_OPTIONS, \"for \" + path + \" in S3 Select operation\");\r\n    } else {\r\n        rejectUnknownMandatoryKeys(mandatoryKeys, InternalConstants.STANDARD_OPENFILE_KEYS, \"for \" + path + \" in non-select file I/O\");\r\n    }\r\n    FileStatus providedStatus = parameters.getStatus();\r\n    S3AFileStatus fileStatus;\r\n    if (providedStatus != null) {\r\n        Preconditions.checkArgument(path.equals(providedStatus.getPath()), \"FileStatus parameter is not for the path %s: %s\", path, providedStatus);\r\n        if (providedStatus instanceof S3AFileStatus) {\r\n            LOG.debug(\"File was opened with a supplied S3AFileStatus;\" + \" skipping getFileStatus call in open() operation: {}\", providedStatus);\r\n            fileStatus = (S3AFileStatus) providedStatus;\r\n        } else if (providedStatus instanceof S3ALocatedFileStatus) {\r\n            LOG.debug(\"File was opened with a supplied S3ALocatedFileStatus;\" + \" skipping getFileStatus call in open() operation: {}\", providedStatus);\r\n            fileStatus = ((S3ALocatedFileStatus) providedStatus).toS3AFileStatus();\r\n        } else {\r\n            LOG.debug(\"Ignoring file status {}\", providedStatus);\r\n            fileStatus = null;\r\n        }\r\n    } else {\r\n        fileStatus = null;\r\n    }\r\n    Optional<S3AFileStatus> ost = Optional.ofNullable(fileStatus);\r\n    CompletableFuture<FSDataInputStream> result = new CompletableFuture<>();\r\n    if (!isSelect) {\r\n        unboundedThreadPool.submit(() -> LambdaUtils.eval(result, () -> open(path, Optional.of(options), ost)));\r\n    } else {\r\n        requireSelectSupport(path);\r\n        unboundedThreadPool.submit(() -> LambdaUtils.eval(result, () -> select(path, sql, options, ost)));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createMultipartUploader",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "S3AMultipartUploaderBuilder createMultipartUploader(final Path basePath) throws IOException\n{\r\n    if (isCSEEnabled) {\r\n        throw new UnsupportedOperationException(\"Multi-part uploader not \" + \"supported for Client side encryption.\");\r\n    }\r\n    final Path path = makeQualified(basePath);\r\n    try (AuditSpan span = entryPoint(MULTIPART_UPLOAD_INSTANTIATED, path)) {\r\n        StoreContext ctx = createStoreContext();\r\n        return new S3AMultipartUploaderBuilder(this, createWriteOperationHelper(span), ctx, path, statisticsContext.createMultipartUploaderStatistics());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createStoreContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StoreContext createStoreContext()\n{\r\n    return new StoreContextBuilder().setFsURI(getUri()).setBucket(getBucket()).setConfiguration(getConf()).setUsername(getUsername()).setOwner(owner).setExecutor(boundedThreadPool).setExecutorCapacity(executorCapacity).setInvoker(invoker).setInstrumentation(statisticsContext).setStorageStatistics(getStorageStatistics()).setInputPolicy(getInputPolicy()).setChangeDetectionPolicy(changeDetectionPolicy).setMultiObjectDeleteEnabled(enableMultiObjectsDelete).setUseListV1(useListV1).setContextAccessors(new ContextAccessorsImpl()).setAuditor(getAuditor()).setEnableCSE(isCSEEnabled).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createMarkerToolOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MarkerToolOperations createMarkerToolOperations(final String target) throws IOException\n{\r\n    createSpan(\"marker-tool-scan\", target, null);\r\n    return new MarkerToolOperationsImpl(new OperationCallbacksImpl());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initializeClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initializeClass()\n{\r\n    LOG.debug(\"Initialize S3A class\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isCSEEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCSEEnabled()\n{\r\n    return isCSEEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getCredentials",
  "errType" : [ "AmazonClientException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSCredentials getCredentials()\n{\r\n    try {\r\n        return provider.getCredentials();\r\n    } catch (AmazonClientException e) {\r\n        throw new NoAwsCredentialsException(\"IAMInstanceCredentialsProvider\", e.getMessage(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refresh()\n{\r\n    provider.refresh();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    rejectOutOfSpan = conf.getBoolean(REJECT_OUT_OF_SPAN_OPERATIONS, false);\r\n    String jobID = extractJobID(conf);\r\n    if (jobID != null) {\r\n        addAttribute(AuditConstants.PARAM_JOB_ID, jobID);\r\n    }\r\n    headerEnabled = getConfig().getBoolean(REFERRER_HEADER_ENABLED, REFERRER_HEADER_ENABLED_DEFAULT);\r\n    filters = conf.getTrimmedStringCollection(REFERRER_HEADER_FILTER);\r\n    final CommonAuditContext currentContext = currentAuditContext();\r\n    warningSpan = new WarningSpan(OUTSIDE_SPAN, currentContext, createSpanID(), null, null);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"LoggingAuditor{\");\r\n    sb.append(\"ID='\").append(getAuditorId()).append('\\'');\r\n    sb.append(\", headerEnabled=\").append(headerEnabled);\r\n    sb.append(\", rejectOutOfSpan=\").append(rejectOutOfSpan);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createSpan",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AuditSpanS3A createSpan(final String operation, @Nullable final String path1, @Nullable final String path2)\n{\r\n    LoggingAuditSpan span = new LoggingAuditSpan(createSpanID(), operation, prepareActiveContext(), path1, path2);\r\n    span.start();\r\n    return span;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "prepareActiveContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CommonAuditContext prepareActiveContext()\n{\r\n    return currentAuditContext();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "addAttribute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addAttribute(String key, String value)\n{\r\n    attributes.put(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getUnbondedSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanS3A getUnbondedSpan()\n{\r\n    return warningSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getLastHeader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLastHeader()\n{\r\n    return lastHeader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "setLastHeader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastHeader(final String lastHeader)\n{\r\n    this.lastHeader = lastHeader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getUpload",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Upload getUpload()\n{\r\n    return upload;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength()\n{\r\n    return length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setInputPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputPolicy(S3AInputPolicy inputPolicy)\n{\r\n    this.inputPolicy = inputPolicy;\r\n    streamStatistics.inputPolicySet(inputPolicy.ordinal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "reopen",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void reopen(String reason, long targetPos, long length, boolean forceAbort) throws IOException\n{\r\n    if (isObjectStreamOpen()) {\r\n        closeStream(\"reopen(\" + reason + \")\", contentRangeFinish, forceAbort);\r\n    }\r\n    contentRangeFinish = calculateRequestLimit(inputPolicy, targetPos, length, contentLength, readahead);\r\n    LOG.debug(\"reopen({}) for {} range[{}-{}], length={},\" + \" streamPosition={}, nextReadPosition={}, policy={}\", uri, reason, targetPos, contentRangeFinish, length, pos, nextReadPos, inputPolicy);\r\n    long opencount = streamStatistics.streamOpened();\r\n    GetObjectRequest request = client.newGetRequest(key).withRange(targetPos, contentRangeFinish - 1);\r\n    String operation = opencount == 0 ? OPERATION_OPEN : OPERATION_REOPEN;\r\n    String text = String.format(\"%s %s at %d\", operation, uri, targetPos);\r\n    changeTracker.maybeApplyConstraint(request);\r\n    DurationTracker tracker = streamStatistics.initiateGetRequest();\r\n    try {\r\n        object = Invoker.once(text, uri, () -> client.getObject(request));\r\n    } catch (IOException e) {\r\n        tracker.failed();\r\n        throw e;\r\n    } finally {\r\n        tracker.close();\r\n    }\r\n    changeTracker.processResponse(object, operation, targetPos);\r\n    wrappedStream = object.getObjectContent();\r\n    contentRangeStart = targetPos;\r\n    if (wrappedStream == null) {\r\n        throw new PathIOException(uri, \"Null IO stream from \" + operation + \" of (\" + reason + \") \");\r\n    }\r\n    this.pos = targetPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return (nextReadPos < 0) ? 0 : nextReadPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void seek(long targetPos) throws IOException\n{\r\n    checkNotClosed();\r\n    if (targetPos < 0) {\r\n        throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK + \" \" + targetPos);\r\n    }\r\n    if (this.contentLength <= 0) {\r\n        return;\r\n    }\r\n    nextReadPos = targetPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "seekQuietly",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void seekQuietly(long positiveTargetPos)\n{\r\n    try {\r\n        seek(positiveTargetPos);\r\n    } catch (IOException ioe) {\r\n        LOG.debug(\"Ignoring IOE on seek of {} to {}\", uri, positiveTargetPos, ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "seekInStream",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void seekInStream(long targetPos, long length) throws IOException\n{\r\n    checkNotClosed();\r\n    if (wrappedStream == null) {\r\n        return;\r\n    }\r\n    long diff = targetPos - pos;\r\n    if (diff > 0) {\r\n        int available = wrappedStream.available();\r\n        long forwardSeekRange = Math.max(readahead, available);\r\n        long remainingInCurrentRequest = remainingInCurrentRequest();\r\n        long forwardSeekLimit = Math.min(remainingInCurrentRequest, forwardSeekRange);\r\n        boolean skipForward = remainingInCurrentRequest > 0 && diff < forwardSeekLimit;\r\n        if (skipForward) {\r\n            LOG.debug(\"Forward seek on {}, of {} bytes\", uri, diff);\r\n            long skipped = wrappedStream.skip(diff);\r\n            if (skipped > 0) {\r\n                pos += skipped;\r\n            }\r\n            streamStatistics.seekForwards(diff, skipped);\r\n            if (pos == targetPos) {\r\n                LOG.debug(\"Now at {}: bytes remaining in current request: {}\", pos, remainingInCurrentRequest());\r\n                return;\r\n            } else {\r\n                LOG.warn(\"Failed to seek on {} to {}. Current position {}\", uri, targetPos, pos);\r\n            }\r\n        } else {\r\n            streamStatistics.seekForwards(diff, 0);\r\n        }\r\n    } else if (diff < 0) {\r\n        streamStatistics.seekBackwards(diff);\r\n        if (inputPolicy.equals(S3AInputPolicy.Normal)) {\r\n            LOG.info(\"Switching to Random IO seek policy\");\r\n            setInputPolicy(S3AInputPolicy.Random);\r\n        }\r\n    } else {\r\n        if (remainingInCurrentRequest() > 0) {\r\n            return;\r\n        }\r\n    }\r\n    closeStream(\"seekInStream()\", this.contentRangeFinish, false);\r\n    pos = targetPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean seekToNewSource(long targetPos) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lazySeek",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void lazySeek(long targetPos, long len) throws IOException\n{\r\n    Invoker invoker = context.getReadInvoker();\r\n    invoker.maybeRetry(streamStatistics.getOpenOperations() == 0, \"lazySeek\", pathStr, true, () -> {\r\n        seekInStream(targetPos, len);\r\n        if (wrappedStream == null) {\r\n            reopen(\"read from new offset\", targetPos, len, false);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementBytesRead",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void incrementBytesRead(long bytesRead)\n{\r\n    streamStatistics.bytesRead(bytesRead);\r\n    if (context.stats != null && bytesRead > 0) {\r\n        context.stats.incrementBytesRead(bytesRead);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "read",
  "errType" : [ "EOFException", "EOFException", "SocketTimeoutException", "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    checkNotClosed();\r\n    if (this.contentLength == 0 || (nextReadPos >= contentLength)) {\r\n        return -1;\r\n    }\r\n    try {\r\n        lazySeek(nextReadPos, 1);\r\n    } catch (EOFException e) {\r\n        return -1;\r\n    }\r\n    Invoker invoker = context.getReadInvoker();\r\n    int byteRead = invoker.retry(\"read\", pathStr, true, () -> {\r\n        int b;\r\n        if (wrappedStream == null) {\r\n            reopen(\"failure recovery\", getPos(), 1, false);\r\n        }\r\n        try {\r\n            b = wrappedStream.read();\r\n        } catch (EOFException e) {\r\n            return -1;\r\n        } catch (SocketTimeoutException e) {\r\n            onReadFailure(e, true);\r\n            throw e;\r\n        } catch (IOException e) {\r\n            onReadFailure(e, false);\r\n            throw e;\r\n        }\r\n        return b;\r\n    });\r\n    if (byteRead >= 0) {\r\n        pos++;\r\n        nextReadPos++;\r\n    }\r\n    if (byteRead >= 0) {\r\n        incrementBytesRead(1);\r\n    }\r\n    return byteRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "onReadFailure",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void onReadFailure(IOException ioe, boolean forceAbort)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Got exception while trying to read from stream {}, \" + \"client: {} object: {}, trying to recover: \", uri, client, object, ioe);\r\n    } else {\r\n        LOG.info(\"Got exception while trying to read from stream {}, \" + \"client: {} object: {}, trying to recover: \" + ioe, uri, client, object);\r\n    }\r\n    streamStatistics.readException();\r\n    closeStream(\"failure recovery\", contentRangeFinish, forceAbort);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "read",
  "errType" : [ "EOFException", "EOFException", "SocketTimeoutException", "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int read(byte[] buf, int off, int len) throws IOException\n{\r\n    checkNotClosed();\r\n    validatePositionedReadArgs(nextReadPos, buf, off, len);\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    if (this.contentLength == 0 || (nextReadPos >= contentLength)) {\r\n        return -1;\r\n    }\r\n    try {\r\n        lazySeek(nextReadPos, len);\r\n    } catch (EOFException e) {\r\n        return -1;\r\n    }\r\n    Invoker invoker = context.getReadInvoker();\r\n    streamStatistics.readOperationStarted(nextReadPos, len);\r\n    int bytesRead = invoker.retry(\"read\", pathStr, true, () -> {\r\n        int bytes;\r\n        if (wrappedStream == null) {\r\n            reopen(\"failure recovery\", getPos(), 1, false);\r\n        }\r\n        try {\r\n            bytes = wrappedStream.read(buf, off, len);\r\n        } catch (EOFException e) {\r\n            return -1;\r\n        } catch (SocketTimeoutException e) {\r\n            onReadFailure(e, true);\r\n            throw e;\r\n        } catch (IOException e) {\r\n            onReadFailure(e, false);\r\n            throw e;\r\n        }\r\n        return bytes;\r\n    });\r\n    if (bytesRead > 0) {\r\n        pos += bytesRead;\r\n        nextReadPos += bytesRead;\r\n    }\r\n    incrementBytesRead(bytesRead);\r\n    streamStatistics.readOperationCompleted(len, bytesRead);\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkNotClosed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkNotClosed() throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(uri + \": \" + FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (!closed) {\r\n        closed = true;\r\n        try {\r\n            closeStream(\"close() operation\", this.contentRangeFinish, false);\r\n            LOG.debug(\"Statistics of stream {}\\n{}\", key, streamStatistics);\r\n            client.close();\r\n            super.close();\r\n        } finally {\r\n            streamStatistics.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "closeStream",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void closeStream(String reason, long length, boolean forceAbort)\n{\r\n    if (!isObjectStreamOpen()) {\r\n        return;\r\n    }\r\n    long remaining = remainingInCurrentRequest();\r\n    LOG.debug(\"Closing stream {}: {}\", reason, forceAbort ? \"abort\" : \"soft\");\r\n    boolean shouldAbort = forceAbort || remaining > readahead;\r\n    try {\r\n        if (!shouldAbort) {\r\n            try {\r\n                long drained = 0;\r\n                while (wrappedStream.read() >= 0) {\r\n                    drained++;\r\n                }\r\n                LOG.debug(\"Drained stream of {} bytes\", drained);\r\n                wrappedStream.close();\r\n                streamStatistics.streamClose(false, drained);\r\n            } catch (Exception e) {\r\n                LOG.debug(\"When closing {} stream for {}, will abort the stream\", uri, reason, e);\r\n                shouldAbort = true;\r\n            }\r\n        }\r\n        if (shouldAbort) {\r\n            LOG.debug(\"Aborting stream {}\", uri);\r\n            try {\r\n                wrappedStream.abort();\r\n            } catch (Exception e) {\r\n                LOG.warn(\"When aborting {} stream after failing to close it for {}\", uri, reason, e);\r\n            }\r\n            streamStatistics.streamClose(true, remaining);\r\n        }\r\n        LOG.debug(\"Stream {} {}: {}; remaining={} streamPos={},\" + \" nextReadPos={},\" + \" request range {}-{} length={}\", uri, (shouldAbort ? \"aborted\" : \"closed\"), reason, remaining, pos, nextReadPos, contentRangeStart, contentRangeFinish, length);\r\n    } finally {\r\n        wrappedStream = null;\r\n        object = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "resetConnection",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean resetConnection() throws IOException\n{\r\n    checkNotClosed();\r\n    if (isObjectStreamOpen()) {\r\n        LOG.info(\"Forced reset of connection to {}\", uri);\r\n        closeStream(\"reset()\", contentRangeFinish, true);\r\n    }\r\n    return isObjectStreamOpen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int available() throws IOException\n{\r\n    checkNotClosed();\r\n    long remaining = remainingInFile();\r\n    if (remaining > Integer.MAX_VALUE) {\r\n        return Integer.MAX_VALUE;\r\n    }\r\n    return (int) remaining;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "remainingInFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long remainingInFile()\n{\r\n    return this.contentLength - this.pos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "remainingInCurrentRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long remainingInCurrentRequest()\n{\r\n    return this.contentRangeFinish - this.pos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getContentRangeFinish",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getContentRangeFinish()\n{\r\n    return contentRangeFinish;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getContentRangeStart",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getContentRangeStart()\n{\r\n    return contentRangeStart;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "markSupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean markSupported()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "String toString()\n{\r\n    String s = streamStatistics.toString();\r\n    synchronized (this) {\r\n        final StringBuilder sb = new StringBuilder(\"S3AInputStream{\");\r\n        sb.append(uri);\r\n        sb.append(\" wrappedStream=\").append(isObjectStreamOpen() ? \"open\" : \"closed\");\r\n        sb.append(\" read policy=\").append(inputPolicy);\r\n        sb.append(\" pos=\").append(pos);\r\n        sb.append(\" nextReadPos=\").append(nextReadPos);\r\n        sb.append(\" contentLength=\").append(contentLength);\r\n        sb.append(\" contentRangeStart=\").append(contentRangeStart);\r\n        sb.append(\" contentRangeFinish=\").append(contentRangeFinish);\r\n        sb.append(\" remainingInCurrentRequest=\").append(remainingInCurrentRequest());\r\n        sb.append(\" \").append(changeTracker);\r\n        sb.append('\\n').append(s);\r\n        sb.append('}');\r\n        return sb.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "readFully",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void readFully(long position, byte[] buffer, int offset, int length) throws IOException\n{\r\n    checkNotClosed();\r\n    validatePositionedReadArgs(position, buffer, offset, length);\r\n    streamStatistics.readFullyOperationStarted(position, length);\r\n    if (length == 0) {\r\n        return;\r\n    }\r\n    int nread = 0;\r\n    synchronized (this) {\r\n        long oldPos = getPos();\r\n        try {\r\n            seek(position);\r\n            while (nread < length) {\r\n                int nbytes = read(buffer, offset + nread, length - nread);\r\n                if (nbytes < 0) {\r\n                    throw new EOFException(FSExceptionMessages.EOF_IN_READ_FULLY);\r\n                }\r\n                nread += nbytes;\r\n            }\r\n        } finally {\r\n            seekQuietly(oldPos);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getS3AStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputStreamStatistics getS3AStreamStatistics()\n{\r\n    return streamStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setReadahead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadahead(Long readahead)\n{\r\n    this.readahead = validateReadahead(readahead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getReadahead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadahead()\n{\r\n    return readahead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "calculateRequestLimit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long calculateRequestLimit(S3AInputPolicy inputPolicy, long targetPos, long length, long contentLength, long readahead)\n{\r\n    long rangeLimit;\r\n    switch(inputPolicy) {\r\n        case Random:\r\n            rangeLimit = (length < 0) ? contentLength : targetPos + Math.max(readahead, length);\r\n            break;\r\n        case Sequential:\r\n            rangeLimit = contentLength;\r\n            break;\r\n        case Normal:\r\n        default:\r\n            rangeLimit = contentLength;\r\n    }\r\n    rangeLimit = Math.min(contentLength, rangeLimit);\r\n    return rangeLimit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "validateReadahead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long validateReadahead(@Nullable Long readahead)\n{\r\n    if (readahead == null) {\r\n        return Constants.DEFAULT_READAHEAD_RANGE;\r\n    } else {\r\n        Preconditions.checkArgument(readahead >= 0, E_NEGATIVE_READAHEAD_VALUE);\r\n        return readahead;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "unbuffer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void unbuffer()\n{\r\n    try {\r\n        closeStream(\"unbuffer()\", contentRangeFinish, false);\r\n    } finally {\r\n        streamStatistics.unbuffered();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    switch(toLowerCase(capability)) {\r\n        case StreamCapabilities.IOSTATISTICS:\r\n        case StreamCapabilities.READAHEAD:\r\n        case StreamCapabilities.UNBUFFER:\r\n            return true;\r\n        default:\r\n            return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isObjectStreamOpen",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isObjectStreamOpen()\n{\r\n    return wrappedStream != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PathOutputCommitter createTaskCommitter(S3AFileSystem fileSystem, Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new DirectoryStagingCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "initCustomSigners",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void initCustomSigners()\n{\r\n    String[] customSigners = ownerConf.getTrimmedStrings(CUSTOM_SIGNERS);\r\n    if (customSigners == null || customSigners.length == 0) {\r\n        LOG.debug(\"No custom signers specified\");\r\n        return;\r\n    }\r\n    for (String customSigner : customSigners) {\r\n        String[] parts = customSigner.split(\":\");\r\n        if (!(parts.length == 1 || parts.length == 2 || parts.length == 3)) {\r\n            String message = \"Invalid format (Expected name, name:SignerClass,\" + \" name:SignerClass:SignerInitializerClass)\" + \" for CustomSigner: [\" + customSigner + \"]\";\r\n            LOG.error(message);\r\n            throw new IllegalArgumentException(message);\r\n        }\r\n        if (parts.length == 1) {\r\n        } else {\r\n            maybeRegisterSigner(parts[0], parts[1], ownerConf);\r\n            if (parts.length == 3) {\r\n                Class<? extends AwsSignerInitializer> clazz = null;\r\n                try {\r\n                    clazz = (Class<? extends AwsSignerInitializer>) ownerConf.getClassByName(parts[2]);\r\n                } catch (ClassNotFoundException e) {\r\n                    throw new RuntimeException(String.format(\"SignerInitializer class\" + \" [%s] not found for signer [%s]\", parts[2], parts[0]), e);\r\n                }\r\n                LOG.debug(\"Creating signer initializer: [{}] for signer: [{}]\", parts[2], parts[0]);\r\n                AwsSignerInitializer signerInitializer = ReflectionUtils.newInstance(clazz, null);\r\n                initializers.add(signerInitializer);\r\n                signerInitializer.registerStore(bucketName, ownerConf, delegationTokenProvider, ownerUgi);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "maybeRegisterSigner",
  "errType" : [ "IllegalArgumentException", "ClassNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void maybeRegisterSigner(String signerName, String signerClassName, Configuration conf)\n{\r\n    try {\r\n        SignerFactory.getSignerByTypeAndService(signerName, null);\r\n    } catch (IllegalArgumentException e) {\r\n        Class<? extends Signer> clazz = null;\r\n        try {\r\n            clazz = (Class<? extends Signer>) conf.getClassByName(signerClassName);\r\n        } catch (ClassNotFoundException cnfe) {\r\n            throw new RuntimeException(String.format(\"Signer class [%s] not found for signer [%s]\", signerClassName, signerName), cnfe);\r\n        }\r\n        LOG.debug(\"Registering Custom Signer - [{}->{}]\", signerName, clazz.getName());\r\n        synchronized (SignerManager.class) {\r\n            SignerFactory.registerSigner(signerName, clazz);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    LOG.debug(\"Unregistering fs from {} initializers\", initializers.size());\r\n    for (AwsSignerInitializer initializer : initializers) {\r\n        initializer.unregisterStore(bucketName, ownerConf, delegationTokenProvider, ownerUgi);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "apply",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T apply() throws IOException\n{\r\n    return execute();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T execute() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "executeOnlyOnce",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void executeOnlyOnce()\n{\r\n    Preconditions.checkState(!executed.getAndSet(true), \"Operation attempted twice\");\r\n    activateAuditSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "verifyIsMagicCommitPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyIsMagicCommitPath(S3AFileSystem fs, Path path) throws PathCommitException\n{\r\n    verifyIsMagicCommitFS(fs);\r\n    if (!fs.isMagicCommitPath(path)) {\r\n        throw new PathCommitException(path, E_BAD_PATH);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "verifyIsMagicCommitFS",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyIsMagicCommitFS(S3AFileSystem fs) throws PathCommitException\n{\r\n    if (!fs.isMagicCommitEnabled()) {\r\n        String fsUri = fs.getUri().toString();\r\n        LOG.error(\"{}: {}:\\n{}\", E_NORMAL_FS, fsUri, fs);\r\n        throw new PathCommitException(fsUri, E_NORMAL_FS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "verifyIsS3AFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AFileSystem verifyIsS3AFS(FileSystem fs, Path path) throws PathCommitException\n{\r\n    if (!(fs instanceof S3AFileSystem)) {\r\n        throw new PathCommitException(path, E_WRONG_FS);\r\n    }\r\n    return (S3AFileSystem) fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getS3AFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3AFileSystem getS3AFileSystem(Path path, Configuration conf, boolean magicCommitRequired) throws PathCommitException, IOException\n{\r\n    S3AFileSystem s3AFS = verifyIsS3AFS(path.getFileSystem(conf), path);\r\n    if (magicCommitRequired) {\r\n        verifyIsMagicCommitFS(s3AFS);\r\n    }\r\n    return s3AFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "validateCollectionClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateCollectionClass(Iterable it, Class classname) throws ValidationFailure\n{\r\n    for (Object o : it) {\r\n        verify(o.getClass().equals(classname), \"Collection element is not a %s: %s\", classname, o.getClass());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "extractJobID",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String extractJobID(Configuration conf)\n{\r\n    String jobUUID = conf.getTrimmed(FS_S3A_COMMITTER_UUID, \"\");\r\n    if (!jobUUID.isEmpty()) {\r\n        return jobUUID;\r\n    }\r\n    jobUUID = conf.getTrimmed(SPARK_WRITE_UUID, \"\");\r\n    if (!jobUUID.isEmpty()) {\r\n        return jobUUID;\r\n    }\r\n    jobUUID = conf.getTrimmed(MR_JOB_ID, \"\");\r\n    if (!jobUUID.isEmpty()) {\r\n        return jobUUID;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getReadInvoker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Invoker getReadInvoker()\n{\r\n    return invoker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getInputPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputPolicy getInputPolicy()\n{\r\n    return inputPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getChangeDetectionPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ChangeDetectionPolicy getChangeDetectionPolicy()\n{\r\n    return changeDetectionPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getReadahead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadahead()\n{\r\n    return readahead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpan getAuditSpan()\n{\r\n    return auditSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3AReadOpContext{\");\r\n    sb.append(\"path=\").append(path);\r\n    sb.append(\", inputPolicy=\").append(inputPolicy);\r\n    sb.append(\", readahead=\").append(readahead);\r\n    sb.append(\", changeDetectionPolicy=\").append(changeDetectionPolicy);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createWrappedCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileOutputCommitter createWrappedCommitter(JobContext context, Configuration conf) throws IOException\n{\r\n    initFileOutputCommitterOptions(context);\r\n    commitsDirectory = Paths.getMultipartUploadCommitsDirectory(conf, getUUID());\r\n    return new FileOutputCommitter(commitsDirectory, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "initFileOutputCommitterOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initFileOutputCommitterOptions(JobContext context)\n{\r\n    context.getConfiguration().setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"StagingCommitter{\");\r\n    sb.append(super.toString());\r\n    sb.append(\", commitsDirectory=\").append(commitsDirectory);\r\n    sb.append(\", uniqueFilenames=\").append(uniqueFilenames);\r\n    sb.append(\", conflictResolution=\").append(conflictResolution);\r\n    sb.append(\", uploadPartSize=\").append(uploadPartSize);\r\n    if (wrappedCommitter != null) {\r\n        sb.append(\", wrappedCommitter=\").append(wrappedCommitter);\r\n    }\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "buildWorkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path buildWorkPath(JobContext context, String uuid) throws IOException\n{\r\n    if (context instanceof TaskAttemptContext) {\r\n        return taskAttemptWorkingPath((TaskAttemptContext) context, uuid);\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "useUniqueFilenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Boolean useUniqueFilenames()\n{\r\n    return uniqueFilenames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getJobAttemptFileSystem",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem getJobAttemptFileSystem(JobContext context) throws IOException\n{\r\n    Path p = getJobAttemptPath(context);\r\n    return p.getFileSystem(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(JobContext context, Path out)\n{\r\n    return getJobAttemptPath(getAppAttemptId(context), out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getJobAttemptPath(int appAttemptId, Path out)\n{\r\n    return new Path(getPendingJobAttemptsPath(out), String.valueOf(appAttemptId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getJobAttemptPath(int appAttemptId)\n{\r\n    return new Path(getPendingJobAttemptsPath(commitsDirectory), String.valueOf(appAttemptId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getPendingTaskAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPendingTaskAttemptsPath(JobContext context, Path out)\n{\r\n    return new Path(getJobAttemptPath(context, out), TEMPORARY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context, Path out)\n{\r\n    return new Path(getPendingTaskAttemptsPath(context, out), String.valueOf(context.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getPendingJobAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPendingJobAttemptsPath(Path out)\n{\r\n    checkNotNull(out, \"Null 'out' path\");\r\n    return new Path(out, TEMPORARY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getCommittedTaskPath(TaskAttemptContext context)\n{\r\n    return getCommittedTaskPath(getAppAttemptId(context), context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "validateContext",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateContext(TaskAttemptContext context)\n{\r\n    checkNotNull(context, \"null context\");\r\n    checkNotNull(context.getTaskAttemptID(), \"null task attempt ID\");\r\n    checkNotNull(context.getTaskAttemptID().getTaskID(), \"null task ID\");\r\n    checkNotNull(context.getTaskAttemptID().getJobID(), \"null job ID\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getCommittedTaskPath(int appAttemptId, TaskAttemptContext context)\n{\r\n    validateContext(context);\r\n    return new Path(getJobAttemptPath(appAttemptId), String.valueOf(context.getTaskAttemptID().getTaskID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getTempTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTempTaskAttemptPath(TaskAttemptContext context)\n{\r\n    throw new UnsupportedOperationException(\"Unimplemented\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getTaskOutput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<LocatedFileStatus> getTaskOutput(TaskAttemptContext context) throws IOException\n{\r\n    Path attemptPath = getTaskAttemptPath(context);\r\n    checkNotNull(attemptPath, \"No attemptPath path in {}\", this);\r\n    LOG.debug(\"Scanning {} for files to commit\", attemptPath);\r\n    return listAndFilter(getTaskAttemptFilesystem(context), attemptPath, true, HIDDEN_FILE_FILTER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getFinalKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getFinalKey(String relative, JobContext context)\n{\r\n    if (uniqueFilenames) {\r\n        return getS3KeyPrefix(context) + \"/\" + Paths.addUUID(relative, getUUID());\r\n    } else {\r\n        return getS3KeyPrefix(context) + \"/\" + relative;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getFinalPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getFinalPath(String relative, JobContext context) throws IOException\n{\r\n    return getDestS3AFS().keyToQualifiedPath(getFinalKey(relative, context));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getBaseTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getBaseTaskAttemptPath(TaskAttemptContext context)\n{\r\n    return getWorkPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(JobContext context)\n{\r\n    return wrappedCommitter.getJobAttemptPath(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupJob(JobContext context) throws IOException\n{\r\n    super.setupJob(context);\r\n    wrappedCommitter.setupJob(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "listPendingUploadsToCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ActiveCommit listPendingUploadsToCommit(JobContext context) throws IOException\n{\r\n    return listPendingUploads(context, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "listPendingUploadsToAbort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ActiveCommit listPendingUploadsToAbort(JobContext context) throws IOException\n{\r\n    return listPendingUploads(context, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "listPendingUploads",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ActiveCommit listPendingUploads(JobContext context, boolean suppressExceptions) throws IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Listing pending uploads\")) {\r\n        Path wrappedJobAttemptPath = getJobAttemptPath(context);\r\n        final FileSystem attemptFS = wrappedJobAttemptPath.getFileSystem(context.getConfiguration());\r\n        return ActiveCommit.fromStatusList(attemptFS, listAndFilter(attemptFS, wrappedJobAttemptPath, false, HIDDEN_FILE_FILTER));\r\n    } catch (FileNotFoundException e) {\r\n        maybeIgnore(suppressExceptions, \"Pending upload directory not found\", e);\r\n    } catch (IOException e) {\r\n        maybeIgnore(suppressExceptions, \"Listing pending uploads\", e);\r\n    }\r\n    return ActiveCommit.empty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "cleanupStagingDirs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanupStagingDirs()\n{\r\n    Path workPath = getWorkPath();\r\n    if (workPath != null) {\r\n        LOG.debug(\"Cleaning up work path {}\", workPath);\r\n        ignoreIOExceptions(LOG, \"cleaning up\", workPath.toString(), () -> deleteQuietly(workPath.getFileSystem(getConf()), workPath, true));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup(JobContext context, boolean suppressExceptions) throws IOException\n{\r\n    maybeIgnore(suppressExceptions, \"Cleanup wrapped committer\", () -> wrappedCommitter.cleanupJob(context));\r\n    maybeIgnore(suppressExceptions, \"Delete destination paths\", () -> deleteDestinationPaths(context));\r\n    super.cleanup(context, suppressExceptions);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "abortJobInternal",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void abortJobInternal(JobContext context, boolean suppressExceptions) throws IOException\n{\r\n    String r = getRole();\r\n    boolean failed = false;\r\n    try (DurationInfo d = new DurationInfo(LOG, \"%s: aborting job in state %s \", r, jobIdString(context))) {\r\n        ActiveCommit pending = listPendingUploadsToAbort(context);\r\n        abortPendingUploads(context, pending, suppressExceptions, true);\r\n    } catch (FileNotFoundException e) {\r\n        LOG.debug(\"No job directory to read uploads from\");\r\n    } catch (IOException e) {\r\n        failed = true;\r\n        maybeIgnore(suppressExceptions, \"aborting job\", e);\r\n    } finally {\r\n        super.abortJobInternal(context, failed || suppressExceptions);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "deleteDestinationPaths",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteDestinationPaths(JobContext context) throws IOException\n{\r\n    Path attemptPath = getJobAttemptPath(context);\r\n    ignoreIOExceptions(LOG, \"Deleting Job attempt Path\", attemptPath.toString(), () -> deleteWithWarning(getJobAttemptFileSystem(context), attemptPath, true));\r\n    deleteWithWarning(getDestFS(), new Path(getOutputPath(), TEMPORARY), true);\r\n    deleteTaskWorkingPathQuietly(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupTask(TaskAttemptContext context) throws IOException\n{\r\n    Path taskAttemptPath = getTaskAttemptPath(context);\r\n    try (DurationInfo d = new DurationInfo(LOG, \"%s: setup task attempt path %s \", getRole(), taskAttemptPath)) {\r\n        super.setupTask(context);\r\n        wrappedCommitter.setupTask(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "needsTaskCommit",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext context) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"%s: needsTaskCommit() Task %s\", getRole(), context.getTaskAttemptID())) {\r\n        Path attemptPath = getTaskAttemptPath(context);\r\n        FileSystem fs = getTaskAttemptFilesystem(context);\r\n        FileStatus[] stats = fs.listStatus(attemptPath);\r\n        LOG.debug(\"{} files to commit under {}\", stats.length, attemptPath);\r\n        return stats.length > 0;\r\n    } catch (FileNotFoundException e) {\r\n        LOG.info(\"No files to commit\");\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "commitTask",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void commitTask(TaskAttemptContext context) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"%s: commit task %s\", getRole(), context.getTaskAttemptID())) {\r\n        int count = commitTaskInternal(context, getTaskOutput(context));\r\n        LOG.info(\"{}: upload file count: {}\", getRole(), count);\r\n    } catch (IOException e) {\r\n        LOG.error(\"{}: commit of task {} failed\", getRole(), context.getTaskAttemptID(), e);\r\n        getCommitOperations().taskCompleted(false);\r\n        throw e;\r\n    } finally {\r\n        destroyThreadPool();\r\n        resetCommonContext();\r\n    }\r\n    getCommitOperations().taskCompleted(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "commitTaskInternal",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "int commitTaskInternal(final TaskAttemptContext context, List<? extends FileStatus> taskOutput) throws IOException\n{\r\n    LOG.debug(\"{}: commitTaskInternal\", getRole());\r\n    Configuration conf = context.getConfiguration();\r\n    final Path attemptPath = getTaskAttemptPath(context);\r\n    FileSystem attemptFS = getTaskAttemptFilesystem(context);\r\n    LOG.debug(\"{}: attempt path is {}\", getRole(), attemptPath);\r\n    Path commitsAttemptPath = wrappedCommitter.getTaskAttemptPath(context);\r\n    FileSystem commitsFS = commitsAttemptPath.getFileSystem(conf);\r\n    int commitCount = taskOutput.size();\r\n    final Queue<SinglePendingCommit> commits = new ConcurrentLinkedQueue<>();\r\n    LOG.info(\"{}: uploading from staging directory to S3 {}\", getRole(), attemptPath);\r\n    LOG.info(\"{}: Saving pending data information to {}\", getRole(), commitsAttemptPath);\r\n    if (taskOutput.isEmpty()) {\r\n        LOG.warn(\"{}: No files to commit\", getRole());\r\n    } else {\r\n        boolean threw = true;\r\n        context.progress();\r\n        PendingSet pendingCommits = new PendingSet(commitCount);\r\n        pendingCommits.putExtraData(TASK_ATTEMPT_ID, context.getTaskAttemptID().toString());\r\n        try {\r\n            Tasks.foreach(taskOutput).stopOnFailure().suppressExceptions(false).executeWith(buildSubmitter(context)).run(stat -> {\r\n                Path path = stat.getPath();\r\n                File localFile = new File(path.toUri().getPath());\r\n                String relative = Paths.getRelativePath(attemptPath, path);\r\n                String partition = Paths.getPartition(relative);\r\n                String key = getFinalKey(relative, context);\r\n                Path destPath = getDestS3AFS().keyToQualifiedPath(key);\r\n                SinglePendingCommit commit = getCommitOperations().uploadFileToPendingCommit(localFile, destPath, partition, uploadPartSize, context);\r\n                LOG.debug(\"{}: adding pending commit {}\", getRole(), commit);\r\n                commits.add(commit);\r\n            });\r\n            for (SinglePendingCommit commit : commits) {\r\n                pendingCommits.add(commit);\r\n            }\r\n            LOG.debug(\"Saving {} pending commit(s)) to file {}\", pendingCommits.size(), commitsAttemptPath);\r\n            pendingCommits.save(commitsFS, commitsAttemptPath, false);\r\n            threw = false;\r\n        } finally {\r\n            if (threw) {\r\n                LOG.error(\"{}: Exception during commit process, aborting {} commit(s)\", getRole(), commits.size());\r\n                try (CommitOperations.CommitContext commitContext = initiateCommitOperation();\r\n                    DurationInfo ignored = new DurationInfo(LOG, \"Aborting %s uploads\", commits.size())) {\r\n                    Tasks.foreach(commits).suppressExceptions().run(commitContext::abortSingleCommit);\r\n                }\r\n                deleteTaskAttemptPathQuietly(context);\r\n            }\r\n        }\r\n        Paths.clearTempFolderInfo(context.getTaskAttemptID());\r\n    }\r\n    LOG.debug(\"Committing wrapped task\");\r\n    wrappedCommitter.commitTask(context);\r\n    LOG.debug(\"Cleaning up attempt dir {}\", attemptPath);\r\n    attemptFS.delete(attemptPath, true);\r\n    return commits.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "abortTask",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void abortTask(TaskAttemptContext context) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Abort task %s\", context.getTaskAttemptID())) {\r\n        deleteTaskAttemptPathQuietly(context);\r\n        deleteTaskWorkingPathQuietly(context);\r\n        wrappedCommitter.abortTask(context);\r\n    } catch (IOException e) {\r\n        LOG.error(\"{}: exception when aborting task {}\", getRole(), context.getTaskAttemptID(), e);\r\n        throw e;\r\n    } finally {\r\n        destroyThreadPool();\r\n        resetCommonContext();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "taskAttemptWorkingPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path taskAttemptWorkingPath(TaskAttemptContext context, String uuid) throws IOException\n{\r\n    return getTaskAttemptPath(context, Paths.getLocalTaskAttemptTempDir(context.getConfiguration(), uuid, context.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "deleteTaskWorkingPathQuietly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteTaskWorkingPathQuietly(JobContext context)\n{\r\n    ignoreIOExceptions(LOG, \"Delete working path\", \"\", () -> {\r\n        Path path = buildWorkPath(context, getUUID());\r\n        if (path != null) {\r\n            deleteQuietly(path.getFileSystem(getConf()), path, true);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getS3KeyPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getS3KeyPrefix(JobContext context)\n{\r\n    return s3KeyPrefix;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getConflictResolutionMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ConflictResolution getConflictResolutionMode(JobContext context, Configuration fsConf)\n{\r\n    if (conflictResolution == null) {\r\n        this.conflictResolution = ConflictResolution.valueOf(getConfictModeOption(context, fsConf, DEFAULT_CONFLICT_MODE));\r\n    }\r\n    return conflictResolution;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "failDestinationExists",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "PathExistsException failDestinationExists(final Path path, final String description)\n{\r\n    LOG.error(\"{}: Failing commit by job {} to write\" + \" to existing output path {}.\", description, getJobContext().getJobID(), path);\r\n    try {\r\n        int limit = 10;\r\n        RemoteIterator<LocatedFileStatus> lf = getDestFS().listFiles(path, true);\r\n        LOG.info(\"Partial Directory listing\");\r\n        while (limit > 0 && lf.hasNext()) {\r\n            limit--;\r\n            LocatedFileStatus status = lf.next();\r\n            LOG.info(\"{}: {}\", status.getPath(), status.isDirectory() ? \" dir\" : (\"file size \" + status.getLen() + \" bytes\"));\r\n        }\r\n        cleanupRemoteIterator(lf);\r\n    } catch (IOException e) {\r\n        LOG.info(\"Discarding exception raised when listing {}: \" + e, path);\r\n        LOG.debug(\"stack trace \", e);\r\n    }\r\n    return new PathExistsException(path.toString(), description + \": \" + InternalCommitterConstants.E_DEST_EXISTS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getConfictModeOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getConfictModeOption(JobContext context, Configuration fsConf, String defVal)\n{\r\n    return getConfigurationOption(context, fsConf, FS_S3A_COMMITTER_STAGING_CONFLICT_MODE, defVal).toUpperCase(Locale.ENGLISH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "preCommitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void preCommitJob(final JobContext context, final ActiveCommit pending) throws IOException\n{\r\n    precommitCheckPendingFiles(context, pending);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getInnerStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getInnerStatistics()\n{\r\n    return innerStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "counters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> counters()\n{\r\n    return getInnerStatistics().counters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "gauges",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> gauges()\n{\r\n    return getInnerStatistics().gauges();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "minimums",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> minimums()\n{\r\n    return getInnerStatistics().minimums();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "maximums",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> maximums()\n{\r\n    return getInnerStatistics().maximums();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "meanStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, MeanStatistic> meanStatistics()\n{\r\n    return getInnerStatistics().meanStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "aggregate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean aggregate(@Nullable final IOStatistics statistics)\n{\r\n    return getInnerStatistics().aggregate(statistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incrementCounter(final String key, final long value)\n{\r\n    return getInnerStatistics().incrementCounter(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "setCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCounter(final String key, final long value)\n{\r\n    getInnerStatistics().setCounter(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "setGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setGauge(final String key, final long value)\n{\r\n    getInnerStatistics().setGauge(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incrementGauge(final String key, final long value)\n{\r\n    return getInnerStatistics().incrementGauge(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "setMaximum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaximum(final String key, final long value)\n{\r\n    getInnerStatistics().setMaximum(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementMaximum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incrementMaximum(final String key, final long value)\n{\r\n    return getInnerStatistics().incrementMaximum(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "setMinimum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMinimum(final String key, final long value)\n{\r\n    getInnerStatistics().setMinimum(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementMinimum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incrementMinimum(final String key, final long value)\n{\r\n    return getInnerStatistics().incrementMinimum(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "addMinimumSample",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addMinimumSample(final String key, final long value)\n{\r\n    getInnerStatistics().addMinimumSample(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "addMaximumSample",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addMaximumSample(final String key, final long value)\n{\r\n    getInnerStatistics().addMaximumSample(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "setMeanStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMeanStatistic(final String key, final MeanStatistic value)\n{\r\n    getInnerStatistics().setMeanStatistic(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "addMeanStatisticSample",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addMeanStatisticSample(final String key, final long value)\n{\r\n    getInnerStatistics().addMeanStatisticSample(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reset()\n{\r\n    getInnerStatistics().reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getCounterReference",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AtomicLong getCounterReference(final String key)\n{\r\n    return getInnerStatistics().getCounterReference(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getMaximumReference",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AtomicLong getMaximumReference(final String key)\n{\r\n    return getInnerStatistics().getMaximumReference(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getMinimumReference",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AtomicLong getMinimumReference(final String key)\n{\r\n    return getInnerStatistics().getMinimumReference(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getGaugeReference",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AtomicLong getGaugeReference(final String key)\n{\r\n    return getInnerStatistics().getGaugeReference(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getMeanStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MeanStatistic getMeanStatistic(final String key)\n{\r\n    return getInnerStatistics().getMeanStatistic(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "addTimedOperation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addTimedOperation(final String prefix, final long durationMillis)\n{\r\n    getInnerStatistics().addTimedOperation(prefix, durationMillis);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "addTimedOperation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addTimedOperation(final String prefix, final Duration duration)\n{\r\n    getInnerStatistics().addTimedOperation(prefix, duration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getThrottleProbability",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getThrottleProbability()\n{\r\n    return throttleProbability;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFailureLimit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailureLimit()\n{\r\n    return failureLimit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setFailureLimit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFailureLimit(int failureLimit)\n{\r\n    this.failureLimit = failureLimit;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setThrottleProbability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setThrottleProbability(float throttleProbability)\n{\r\n    this.throttleProbability = validProbability(throttleProbability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "trueWithProbability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean trueWithProbability(float p)\n{\r\n    return Math.random() < p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"FailureInjectionPolicy:\" + \" throttle probability %s\" + \"; failure limit %d\", throttleProbability, failureLimit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "validProbability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float validProbability(float p)\n{\r\n    Preconditions.checkArgument(p >= 0.0f && p <= 1.0f, \"Probability out of range 0 to 1 %s\", p);\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getMagicJobAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getMagicJobAttemptsPath(Path out)\n{\r\n    return new Path(out, MAGIC);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getAppAttemptId(JobContext context)\n{\r\n    return context.getConfiguration().getInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getMagicJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getMagicJobAttemptPath(String jobUUID, Path dest)\n{\r\n    return new Path(getMagicJobAttemptsPath(dest), formatAppAttemptDir(jobUUID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "formatAppAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String formatAppAttemptDir(String jobUUID)\n{\r\n    return String.format(\"job-%s\", jobUUID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getMagicTaskAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getMagicTaskAttemptsPath(String jobUUID, Path dest)\n{\r\n    return new Path(getMagicJobAttemptPath(jobUUID, dest), \"tasks\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getMagicTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getMagicTaskAttemptPath(TaskAttemptContext context, String jobUUID, Path dest)\n{\r\n    return new Path(getBaseMagicTaskAttemptPath(context, jobUUID, dest), BASE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getBaseMagicTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getBaseMagicTaskAttemptPath(TaskAttemptContext context, String jobUUID, Path dest)\n{\r\n    return new Path(getMagicTaskAttemptsPath(jobUUID, dest), String.valueOf(context.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTempJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTempJobAttemptPath(String jobUUID, Path out)\n{\r\n    return new Path(new Path(out, TEMP_DATA), formatAppAttemptDir(jobUUID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTempTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTempTaskAttemptPath(TaskAttemptContext context, final String jobUUID, Path out)\n{\r\n    return new Path(getTempJobAttemptPath(jobUUID, out), String.valueOf(context.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "jobIdString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String jobIdString(JobContext context)\n{\r\n    JobID jobID = context.getJobID();\r\n    return jobID != null ? jobID.toString() : \"(no job ID)\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "jobName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String jobName(JobContext context)\n{\r\n    String name = context.getJobName();\r\n    return (name != null && !name.isEmpty()) ? name : \"(anonymous)\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getConfigurationOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getConfigurationOption(JobContext context, Configuration fsConf, String key, String defVal)\n{\r\n    return context.getConfiguration().getTrimmed(key, fsConf.getTrimmed(key, defVal));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getUriDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getUriDefaultPort()\n{\r\n    return super.getUriDefaultPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3A{\");\r\n    sb.append(\"URI =\").append(fsImpl.getUri());\r\n    sb.append(\"; fsImpl=\").append(fsImpl);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(final DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    marshalledCredentials.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(final DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    marshalledCredentials.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getExpiryTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getExpiryTime()\n{\r\n    return marshalledCredentials.getExpiration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getMarshalledCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MarshalledCredentials getMarshalledCredentials()\n{\r\n    return marshalledCredentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return super.toString() + \"; \" + marshalledCredentials.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PathOutputCommitter createTaskCommitter(S3AFileSystem fileSystem, Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new StagingCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "createTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PathOutputCommitter createTaskCommitter(S3AFileSystem fileSystem, Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new MagicS3GuardCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "get",
  "errType" : [ "RuntimeException", "IOException", "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T get()\n{\r\n    try {\r\n        if (auditSpan != null) {\r\n            auditSpan.activate();\r\n        }\r\n        return call.call();\r\n    } catch (RuntimeException e) {\r\n        throw e;\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    } catch (Exception e) {\r\n        throw new UncheckedIOException(new IOException(e));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "submit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<T> submit(final Executor executor, final Callable<T> call)\n{\r\n    return CompletableFuture.supplyAsync(new CallableSupplier<T>(call), executor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "submit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<T> submit(final Executor executor, final AuditSpan auditSpan, final Callable<T> call)\n{\r\n    return CompletableFuture.supplyAsync(new CallableSupplier<T>(auditSpan, call), executor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "waitForCompletion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitForCompletion(final List<CompletableFuture<T>> futures) throws IOException\n{\r\n    if (futures.isEmpty()) {\r\n        return;\r\n    }\r\n    waitForCompletion(CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "waitForCompletion",
  "errType" : [ "CancellationException", "CompletionException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitForCompletion(final CompletableFuture<T> future) throws IOException\n{\r\n    try (DurationInfo ignore = new DurationInfo(LOG, false, \"Waiting for task completion\")) {\r\n        future.join();\r\n    } catch (CancellationException e) {\r\n        throw new IOException(e);\r\n    } catch (CompletionException e) {\r\n        raiseInnerCause(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "waitForCompletionIgnoringExceptions",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitForCompletionIgnoringExceptions(@Nullable final CompletableFuture<T> future)\n{\r\n    if (future != null) {\r\n        try (DurationInfo ignore = new DurationInfo(LOG, false, \"Waiting for task completion\")) {\r\n            future.join();\r\n        } catch (Exception e) {\r\n            LOG.debug(\"Ignoring exception raised in task completion: \");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "maybeAwaitCompletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeAwaitCompletion(@Nullable final CompletableFuture<Void> future) throws IOException\n{\r\n    if (future != null) {\r\n        waitForCompletion(future);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    Configuration conf = getConfig();\r\n    duration = conf.getTimeDuration(DELEGATION_TOKEN_DURATION, DEFAULT_DELEGATION_TOKEN_DURATION, TimeUnit.SECONDS);\r\n    endpoint = conf.getTrimmed(DELEGATION_TOKEN_ENDPOINT, DEFAULT_DELEGATION_TOKEN_ENDPOINT);\r\n    region = conf.getTrimmed(DELEGATION_TOKEN_REGION, DEFAULT_DELEGATION_TOKEN_REGION);\r\n    parentAuthChain = buildAWSProviderList(getCanonicalUri(), conf, AWS_CREDENTIALS_PROVIDER, STANDARD_AWS_PROVIDERS, new HashSet<>());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    super.serviceStop();\r\n    synchronized (this) {\r\n        this.stsClient.ifPresent(IOUtils::closeStream);\r\n        this.stsClient = Optional.empty();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "deployUnbonded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AWSCredentialProviderList deployUnbonded() throws IOException\n{\r\n    requireServiceStarted();\r\n    return parentAuthChain;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getInvoker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Invoker getInvoker()\n{\r\n    return invoker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AWSCredentialProviderList bindToTokenIdentifier(final AbstractS3ATokenIdentifier retrievedIdentifier) throws IOException\n{\r\n    final SessionTokenIdentifier identifier = convertTokenIdentifier(retrievedIdentifier, SessionTokenIdentifier.class);\r\n    setTokenIdentifier(Optional.of(identifier));\r\n    MarshalledCredentials marshalledCredentials = identifier.getMarshalledCredentials();\r\n    setExpirationDateTime(marshalledCredentials.getExpirationDateTime());\r\n    return new AWSCredentialProviderList(\"Session Token Binding\", new MarshalledCredentialProvider(SESSION_TOKEN, getStoreContext().getFsURI(), getConfig(), marshalledCredentials, MarshalledCredentials.CredentialTypeRequired.SessionOnly));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDescription()\n{\r\n    return String.format(\"%s token binding for user %s, \" + \"with STS endpoint \\\"%s\\\", region \\\"%s\\\"\" + \" and token duration %d:%02d\", bindingName(), getOwner().getShortUserName(), endpoint, region, TimeUnit.SECONDS.toMinutes(duration), duration % 60);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindingName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String bindingName()\n{\r\n    return \"Session\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getUserAgentField",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getUserAgentField()\n{\r\n    if (tokenIdentifier.isPresent()) {\r\n        return \"; session ID \" + tokenIdentifier.get().getUuid();\r\n    } else {\r\n        return \"\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "maybeInitSTS",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Optional<STSClientFactory.STSClient> maybeInitSTS() throws IOException\n{\r\n    if (stsInitAttempted.getAndSet(true)) {\r\n        return stsClient;\r\n    }\r\n    Configuration conf = getConfig();\r\n    URI uri = getCanonicalUri();\r\n    final AWSCredentials parentCredentials = once(\"get credentials\", \"\", () -> parentAuthChain.getCredentials());\r\n    hasSessionCreds = parentCredentials instanceof AWSSessionCredentials;\r\n    if (!hasSessionCreds) {\r\n        LOG.debug(\"Creating STS client for {}\", getDescription());\r\n        invoker = new Invoker(new S3ARetryPolicy(conf), LOG_EVENT);\r\n        ClientConfiguration awsConf = S3AUtils.createAwsConf(conf, uri.getHost(), Constants.AWS_SERVICE_IDENTIFIER_STS);\r\n        AWSSecurityTokenService tokenService = STSClientFactory.builder(parentAuthChain, awsConf, endpoint, region).build();\r\n        stsClient = Optional.of(STSClientFactory.createClientConnection(tokenService, invoker));\r\n    } else {\r\n        LOG.debug(\"Parent-provided session credentials will be propagated\");\r\n        stsClient = Optional.empty();\r\n    }\r\n    return stsClient;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "prepareSTSClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Optional<STSClientFactory.STSClient> prepareSTSClient() throws IOException\n{\r\n    return maybeInitSTS();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDuration()\n{\r\n    return duration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "SessionTokenIdentifier createTokenIdentifier(final Optional<RoleModel.Policy> policy, final EncryptionSecrets encryptionSecrets, final Text renewer) throws IOException\n{\r\n    requireServiceStarted();\r\n    final MarshalledCredentials marshalledCredentials;\r\n    String origin = AbstractS3ATokenIdentifier.createDefaultOriginMessage();\r\n    final Optional<STSClientFactory.STSClient> client = prepareSTSClient();\r\n    if (client.isPresent()) {\r\n        marshalledCredentials = fromSTSCredentials(client.get().requestSessionCredentials(duration, TimeUnit.SECONDS));\r\n    } else {\r\n        if (!forwardMessageLogged.getAndSet(true)) {\r\n            LOG.warn(\"Forwarding existing session credentials to {}\" + \" -duration unknown\", getCanonicalUri());\r\n        }\r\n        origin += \" \" + CREDENTIALS_CONVERTED_TO_DELEGATION_TOKEN;\r\n        final AWSCredentials awsCredentials = parentAuthChain.getCredentials();\r\n        if (awsCredentials instanceof AWSSessionCredentials) {\r\n            marshalledCredentials = fromAWSCredentials((AWSSessionCredentials) awsCredentials);\r\n        } else {\r\n            throw new DelegationTokenIOException(\"AWS Authentication chain is no longer supplying session secrets\");\r\n        }\r\n    }\r\n    return new SessionTokenIdentifier(getKind(), getOwnerText(), renewer, getCanonicalUri(), marshalledCredentials, encryptionSecrets, origin);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createEmptyIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SessionTokenIdentifier createEmptyIdentifier()\n{\r\n    return new SessionTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getExpirationDateTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Optional<OffsetDateTime> getExpirationDateTime()\n{\r\n    return expirationDateTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setExpirationDateTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setExpirationDateTime(Optional<OffsetDateTime> expirationDateTime)\n{\r\n    this.expirationDateTime = expirationDateTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Optional<SessionTokenIdentifier> getTokenIdentifier()\n{\r\n    return tokenIdentifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTokenIdentifier(Optional<SessionTokenIdentifier> tokenIdentifier)\n{\r\n    this.tokenIdentifier = tokenIdentifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "addUUID",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String addUUID(String pathStr, String uuid)\n{\r\n    Preconditions.checkArgument(StringUtils.isNotEmpty(pathStr), \"empty path\");\r\n    Preconditions.checkArgument(StringUtils.isNotEmpty(uuid), \"empty uuid\");\r\n    if (pathStr.contains(uuid)) {\r\n        return pathStr;\r\n    }\r\n    int dot;\r\n    int lastSlash = pathStr.lastIndexOf('/');\r\n    if (lastSlash >= 0) {\r\n        Preconditions.checkState(lastSlash + 1 < pathStr.length(), \"Bad path: \" + pathStr);\r\n        dot = pathStr.indexOf('.', lastSlash);\r\n    } else {\r\n        dot = pathStr.indexOf('.');\r\n    }\r\n    if (dot >= 0) {\r\n        return pathStr.substring(0, dot) + \"-\" + uuid + pathStr.substring(dot);\r\n    } else {\r\n        return pathStr + \"-\" + uuid;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getParent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getParent(String pathStr)\n{\r\n    int lastSlash = pathStr.lastIndexOf('/');\r\n    if (lastSlash >= 0) {\r\n        return pathStr.substring(0, lastSlash);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getRelativePath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRelativePath(Path basePath, Path fullPath)\n{\r\n    return basePath.toUri().relativize(fullPath.toUri()).getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "path",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path path(Path parent, String... child)\n{\r\n    Path p = parent;\r\n    for (String c : child) {\r\n        if (!c.isEmpty()) {\r\n            p = new Path(p, c);\r\n        }\r\n    }\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getLocalTaskAttemptTempDir",
  "errType" : [ "ExecutionException|UncheckedExecutionException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getLocalTaskAttemptTempDir(final Configuration conf, final String uuid, final TaskAttemptID attemptID) throws IOException\n{\r\n    try {\r\n        final LocalDirAllocator allocator = new LocalDirAllocator(Constants.BUFFER_DIR);\r\n        String name = uuid + \"-\" + attemptID;\r\n        return tempFolders.get(name, () -> {\r\n            return FileSystem.getLocal(conf).makeQualified(allocator.getLocalPathForWrite(name, conf));\r\n        });\r\n    } catch (ExecutionException | UncheckedExecutionException e) {\r\n        Throwable cause = e.getCause();\r\n        if (cause instanceof RuntimeException) {\r\n            throw (RuntimeException) cause;\r\n        }\r\n        if (cause instanceof IOException) {\r\n            throw (IOException) cause;\r\n        }\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "clearTempFolderInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearTempFolderInfo(final TaskAttemptID attemptID)\n{\r\n    tempFolders.invalidate(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "resetTempFolderCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetTempFolderCache()\n{\r\n    tempFolders.invalidateAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "tempDirForStaging",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path tempDirForStaging(FileSystem fs, Configuration conf)\n{\r\n    String fallbackPath = fs.getScheme().equals(\"file\") ? System.getProperty(JAVA_IO_TMPDIR) : FILESYSTEM_TEMP_PATH;\r\n    return fs.makeQualified(new Path(conf.getTrimmed(FS_S3A_COMMITTER_STAGING_TMP_PATH, fallbackPath)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getAppAttemptId(Configuration conf)\n{\r\n    return conf.getInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getMultipartUploadCommitsDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getMultipartUploadCommitsDirectory(Configuration conf, String uuid) throws IOException\n{\r\n    return getMultipartUploadCommitsDirectory(FileSystem.get(conf), conf, uuid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getMultipartUploadCommitsDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getMultipartUploadCommitsDirectory(FileSystem fs, Configuration conf, String uuid) throws IOException\n{\r\n    return path(tempDirForStaging(fs, conf), UserGroupInformation.getCurrentUser().getShortUserName(), uuid, STAGING_UPLOADS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPartition(String relative)\n{\r\n    return getParent(relative);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getPartitions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<String> getPartitions(Path attemptPath, List<? extends FileStatus> taskOutput) throws IOException\n{\r\n    Set<String> partitions = new LinkedHashSet<>();\r\n    for (FileStatus fileStatus : taskOutput) {\r\n        Path outputFile = fileStatus.getPath();\r\n        if (!fileStatus.isFile()) {\r\n            throw new PathIsDirectoryException(outputFile.toString());\r\n        }\r\n        String partition = getPartition(getRelativePath(attemptPath, outputFile));\r\n        partitions.add(partition != null ? partition : TABLE_ROOT);\r\n    }\r\n    return partitions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "init",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init() throws IOException\n{\r\n    if (initialized.getAndSet(true)) {\r\n        return;\r\n    }\r\n    try {\r\n        awsCredentials = Invoker.once(\"create credentials\", \"\", () -> createCredentials(getConf()));\r\n    } catch (IOException e) {\r\n        initializationException = e;\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "isInitialized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isInitialized()\n{\r\n    return initialized.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AWSCredentials createCredentials(Configuration config) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getCredentials",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AWSCredentials getCredentials() throws SdkBaseException\n{\r\n    try {\r\n        if (!isInitialized()) {\r\n            init();\r\n        }\r\n    } catch (IOException e) {\r\n        if (e.getCause() instanceof SdkBaseException) {\r\n            throw (SdkBaseException) e.getCause();\r\n        } else {\r\n            throw new CredentialInitializationException(e.getMessage(), e);\r\n        }\r\n    }\r\n    if (awsCredentials == null) {\r\n        throw new CredentialInitializationException(\"Provider \" + this + \" has no credentials\");\r\n    }\r\n    return awsCredentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "hasCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasCredentials()\n{\r\n    return awsCredentials == null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getInitializationException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOException getInitializationException()\n{\r\n    return initializationException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "execute",
  "errType" : [ "AccessDeniedException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Boolean execute() throws IOException\n{\r\n    LOG.debug(\"Making directory: {}\", dir);\r\n    if (dir.isRoot()) {\r\n        return true;\r\n    }\r\n    FileStatus fileStatus = getPathStatusExpectingDir(dir);\r\n    if (fileStatus != null) {\r\n        if (fileStatus.isDirectory()) {\r\n            return true;\r\n        } else {\r\n            throw new FileAlreadyExistsException(\"Path is a file: \" + dir);\r\n        }\r\n    }\r\n    Path fPart = dir.getParent();\r\n    try {\r\n        while (fPart != null && !fPart.isRoot()) {\r\n            fileStatus = getPathStatusExpectingDir(fPart);\r\n            if (fileStatus == null) {\r\n                fPart = fPart.getParent();\r\n                continue;\r\n            }\r\n            if (fileStatus.isDirectory()) {\r\n                break;\r\n            }\r\n            throw new FileAlreadyExistsException(String.format(\"Can't make directory for path '%s' since it is a file.\", fPart));\r\n        }\r\n    } catch (AccessDeniedException e) {\r\n        LOG.info(\"mkdirs({}}: Access denied when looking\" + \" for parent directory {}; skipping checks\", dir, fPart);\r\n        LOG.debug(\"{}\", e.toString(), e);\r\n    }\r\n    String key = getStoreContext().pathToKey(dir);\r\n    callbacks.createFakeDirectory(key);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "probePathStatusOrNull",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus probePathStatusOrNull(final Path path, final Set<StatusProbeEnum> probes) throws IOException\n{\r\n    try {\r\n        return callbacks.probePathStatus(path, probes);\r\n    } catch (FileNotFoundException fnfe) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getPathStatusExpectingDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3AFileStatus getPathStatusExpectingDir(final Path path) throws IOException\n{\r\n    S3AFileStatus status = probePathStatusOrNull(path, StatusProbeEnum.DIRECTORIES);\r\n    if (status == null) {\r\n        status = probePathStatusOrNull(path, StatusProbeEnum.FILE);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MultipartUtils.UploadIterator listMultipartUploads(final StoreContext storeContext, AmazonS3 s3, @Nullable String prefix, int maxKeys) throws IOException\n{\r\n    return new MultipartUtils.UploadIterator(storeContext, s3, maxKeys, prefix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "keepDirectoryMarkers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean keepDirectoryMarkers(final Path path)\n{\r\n    switch(markerPolicy) {\r\n        case Keep:\r\n            return true;\r\n        case Authoritative:\r\n            return authoritativeness.test(path);\r\n        case Delete:\r\n        default:\r\n            return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getMarkerPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MarkerPolicy getMarkerPolicy()\n{\r\n    return markerPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "describe",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String describe()\n{\r\n    return markerPolicy.getOptionName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"DirectoryMarkerRetention{\");\r\n    sb.append(\"policy='\").append(markerPolicy.getOptionName()).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasPathCapability(final Path path, final String capability)\n{\r\n    switch(capability) {\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_AWARE:\r\n            return true;\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_KEEP:\r\n            return markerPolicy == MarkerPolicy.Keep;\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_DELETE:\r\n            return markerPolicy == MarkerPolicy.Delete;\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_POLICY_AUTHORITATIVE:\r\n            return markerPolicy == MarkerPolicy.Authoritative;\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_KEEP:\r\n            return keepDirectoryMarkers(path);\r\n        case STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE:\r\n            return !keepDirectoryMarkers(path);\r\n        default:\r\n            throw new IllegalArgumentException(\"Unknown capability \" + capability);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getDirectoryPolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DirectoryPolicy getDirectoryPolicy(final Configuration conf, final Predicate<Path> authoritativeness)\n{\r\n    DirectoryPolicy policy;\r\n    String option = conf.getTrimmed(DIRECTORY_MARKER_POLICY, DEFAULT_DIRECTORY_MARKER_POLICY);\r\n    switch(option.toLowerCase(Locale.ENGLISH)) {\r\n        case DIRECTORY_MARKER_POLICY_DELETE:\r\n            LOG.debug(\"Directory markers will be deleted\");\r\n            policy = DELETE;\r\n            break;\r\n        case DIRECTORY_MARKER_POLICY_KEEP:\r\n            LOG.info(\"Directory markers will be kept\");\r\n            policy = KEEP;\r\n            break;\r\n        case DIRECTORY_MARKER_POLICY_AUTHORITATIVE:\r\n            LOG.info(\"Directory markers will be kept on authoritative\" + \" paths\");\r\n            policy = new DirectoryPolicyImpl(MarkerPolicy.Authoritative, authoritativeness);\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(UNKNOWN_MARKER_POLICY + option);\r\n    }\r\n    return policy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "availablePolicies",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<MarkerPolicy> availablePolicies()\n{\r\n    return AVAILABLE_POLICIES;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getFsURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getFsURI()\n{\r\n    return fsURI;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBucket()\n{\r\n    return bucket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getUsername",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUsername()\n{\r\n    return username;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getExecutor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExecutorService getExecutor()\n{\r\n    return executor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getInvoker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Invoker getInvoker()\n{\r\n    return invoker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getInstrumentation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AStatisticsContext getInstrumentation()\n{\r\n    return instrumentation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getInputPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputPolicy getInputPolicy()\n{\r\n    return inputPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getChangeDetectionPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ChangeDetectionPolicy getChangeDetectionPolicy()\n{\r\n    return changeDetectionPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "isMultiObjectDeleteEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMultiObjectDeleteEnabled()\n{\r\n    return multiObjectDeleteEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "isUseListV1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUseListV1()\n{\r\n    return useListV1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getContextAccessors",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContextAccessors getContextAccessors()\n{\r\n    return contextAccessors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "keyToPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path keyToPath(String key)\n{\r\n    return contextAccessors.keyToPath(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "pathToKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String pathToKey(Path path)\n{\r\n    return contextAccessors.pathToKey(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "makeQualified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path makeQualified(Path path)\n{\r\n    return contextAccessors.makeQualified(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getStorageStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AStorageStatistics getStorageStatistics()\n{\r\n    return storageStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic)\n{\r\n    incrementStatistic(statistic, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic, long count)\n{\r\n    instrumentation.incrementCounter(statistic, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "decrementGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void decrementGauge(Statistic statistic, long count)\n{\r\n    instrumentation.decrementGauge(statistic, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "incrementGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementGauge(Statistic statistic, long count)\n{\r\n    instrumentation.incrementGauge(statistic, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createThrottledExecutor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExecutorService createThrottledExecutor(int capacity)\n{\r\n    return new SemaphoredDelegatingExecutor(executor, capacity, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createThrottledExecutor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExecutorService createThrottledExecutor()\n{\r\n    return createThrottledExecutor(executorCapacity);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserGroupInformation getOwner()\n{\r\n    return owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createTempFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File createTempFile(String prefix, long size) throws IOException\n{\r\n    return contextAccessors.createTempFile(prefix, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getBucketLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBucketLocation() throws IOException\n{\r\n    return contextAccessors.getBucketLocation();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "fullKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String fullKey(final S3AFileStatus stat)\n{\r\n    String k = pathToKey(stat.getPath());\r\n    return (stat.isDirectory() && !k.endsWith(\"/\")) ? k + \"/\" : k;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "submit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<T> submit(final CompletableFuture<T> future, final Callable<T> call)\n{\r\n    getExecutor().submit(() -> LambdaUtils.eval(future, call));\r\n    return future;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getAuditor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanSource<AuditSpanS3A> getAuditor()\n{\r\n    return auditor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getActiveAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpan getActiveAuditSpan()\n{\r\n    return contextAccessors.getActiveAuditSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getRequestFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RequestFactory getRequestFactory()\n{\r\n    return contextAccessors.getRequestFactory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "isCSEEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCSEEnabled()\n{\r\n    return isCSEEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init(final OperationAuditorOptions opts)\n{\r\n    this.options = opts;\r\n    this.iostatistics = opts.getIoStatisticsStore();\r\n    init(opts.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getAuditorId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAuditorId()\n{\r\n    return auditorID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getOptions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationAuditorOptions getOptions()\n{\r\n    return options;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createSpanID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createSpanID()\n{\r\n    return String.format(\"%s-%08d\", auditorID, SPAN_ID_COUNTER.incrementAndGet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "waitFor",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void waitFor(Collection<Future<?>> futures)\n{\r\n    int size = futures.size();\r\n    LOG.debug(\"Waiting for {} tasks to complete\", size);\r\n    int oldNumFinished = 0;\r\n    while (true) {\r\n        int numFinished = (int) futures.stream().filter(Future::isDone).count();\r\n        if (oldNumFinished != numFinished) {\r\n            LOG.debug(\"Finished count -> {}/{}\", numFinished, size);\r\n            oldNumFinished = numFinished;\r\n        }\r\n        if (numFinished == size) {\r\n            break;\r\n        } else {\r\n            try {\r\n                Thread.sleep(10);\r\n            } catch (InterruptedException e) {\r\n                futures.forEach(future -> future.cancel(true));\r\n                Thread.currentThread().interrupt();\r\n                break;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "foreach",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Builder<I> foreach(Iterable<I> items)\n{\r\n    return new Builder<>(items);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "foreach",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Builder<I> foreach(I[] items)\n{\r\n    return new Builder<>(Arrays.asList(items));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "throwOne",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void throwOne(Collection<Exception> exceptions) throws E\n{\r\n    Iterator<Exception> iter = exceptions.iterator();\r\n    Exception e = iter.next();\r\n    Class<? extends Exception> exceptionClass = e.getClass();\r\n    while (iter.hasNext()) {\r\n        Exception other = iter.next();\r\n        if (!exceptionClass.isInstance(other)) {\r\n            e.addSuppressed(other);\r\n        }\r\n    }\r\n    Tasks.<E>castAndThrow(e);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "castAndThrow",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void castAndThrow(Exception e) throws E\n{\r\n    if (e instanceof RuntimeException) {\r\n        throw (RuntimeException) e;\r\n    }\r\n    throw (E) e;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getStoreContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContext getStoreContext()\n{\r\n    return storeContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpan getAuditSpan()\n{\r\n    return auditSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "activateAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void activateAuditSpan()\n{\r\n    auditSpan.activate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "updateAwsRequestCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateAwsRequestCount(final long count)\n{\r\n    countersAndGauges.incrementCounter(STORE_IO_REQUEST, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "updateAwsRetryCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateAwsRetryCount(final long count)\n{\r\n    countersAndGauges.incrementCounter(STORE_IO_RETRY, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "updateAwsThrottleExceptionsCount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateAwsThrottleExceptionsCount(final long count)\n{\r\n    countersAndGauges.incrementCounter(STORE_IO_THROTTLED, count);\r\n    countersAndGauges.addValueToQuantiles(STORE_IO_THROTTLE_RATE, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "noteAwsRequestTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void noteAwsRequestTime(final Duration duration)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "noteAwsClientExecuteTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void noteAwsClientExecuteTime(final Duration duration)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "noteRequestMarshallTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void noteRequestMarshallTime(final Duration duration)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "noteRequestSigningTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void noteRequestSigningTime(final Duration duration)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "noteResponseProcessingTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void noteResponseProcessingTime(final Duration duration)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newInputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputStreamStatistics newInputStreamStatistics()\n{\r\n    return EMPTY_INPUT_STREAM_STATISTICS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newCommitterStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommitterStatistics newCommitterStatistics()\n{\r\n    return EMPTY_COMMITTER_STATISTICS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newOutputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockOutputStreamStatistics newOutputStreamStatistics()\n{\r\n    return EMPTY_BLOCK_OUTPUT_STREAM_STATISTICS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newDelegationTokenStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationTokenStatistics newDelegationTokenStatistics()\n{\r\n    return EMPTY_DELEGATION_TOKEN_STATISTICS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newStatisticsFromAwsSdk",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StatisticsFromAwsSdk newStatisticsFromAwsSdk()\n{\r\n    return EMPTY_STATISTICS_FROM_AWS_SDK;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "createMultipartUploaderStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AMultipartUploaderStatistics createMultipartUploaderStatistics()\n{\r\n    return new EmptyMultipartUploaderStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementCounter(final Statistic op, final long count)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementGauge",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void incrementGauge(final Statistic op, final long count)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "decrementGauge",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void decrementGauge(final Statistic op, final long count)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "addValueToQuantiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addValueToQuantiles(final Statistic op, final long value)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "recordDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void recordDuration(final Statistic op, final boolean success, final Duration duration)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createSSECustomerKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Optional<SSECustomerKey> createSSECustomerKey(final EncryptionSecrets secrets)\n{\r\n    if (secrets.hasEncryptionKey() && secrets.getEncryptionMethod() == S3AEncryptionMethods.SSE_C) {\r\n        return Optional.of(new SSECustomerKey(secrets.getEncryptionKey()));\r\n    } else {\r\n        return Optional.empty();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createSSEAwsKeyManagementParams",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Optional<SSEAwsKeyManagementParams> createSSEAwsKeyManagementParams(final EncryptionSecrets secrets)\n{\r\n    if (secrets.getEncryptionMethod() == S3AEncryptionMethods.SSE_KMS) {\r\n        if (secrets.hasEncryptionKey()) {\r\n            return Optional.of(new SSEAwsKeyManagementParams(secrets.getEncryptionKey()));\r\n        } else {\r\n            return Optional.of(new SSEAwsKeyManagementParams());\r\n        }\r\n    } else {\r\n        return Optional.empty();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "allowS3Operations",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<Statement> allowS3Operations(String bucket, boolean write)\n{\r\n    ArrayList<Statement> statements = Lists.newArrayList(statement(true, bucketToArn(bucket), S3_GET_BUCKET_LOCATION, S3_BUCKET_ALL_LIST));\r\n    if (write) {\r\n        statements.add(statement(true, bucketObjectsToArn(bucket), S3_ROOT_RW_OPERATIONS));\r\n    } else {\r\n        statements.add(statement(true, bucketObjectsToArn(bucket), S3_ROOT_READ_OPERATIONS_LIST));\r\n    }\r\n    return statements;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "bucketObjectsToArn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String bucketObjectsToArn(String bucket)\n{\r\n    return String.format(\"arn:aws:s3:::%s/*\", bucket);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "bucketToArn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String bucketToArn(String bucket)\n{\r\n    return String.format(\"arn:aws:s3:::%s\", bucket);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return binding;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void refresh()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getAllocator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocalDirAllocator getAllocator(Configuration conf, String key)\n{\r\n    if (directoryAllocator != null) {\r\n        String bufferDir = conf.get(key) != null ? key : Constants.HADOOP_TMP_DIR;\r\n        directoryAllocator = new LocalDirAllocator(bufferDir);\r\n    }\r\n    return directoryAllocator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "tempFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File tempFile(Configuration conf, String prefix, long size) throws IOException\n{\r\n    return getAllocator(conf, BUFFER_DIR).createTmpFileForWrite(prefix, size, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "tempPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path tempPath(Configuration conf, String prefix, long size) throws IOException\n{\r\n    return getAllocator(conf, BUFFER_DIR).getLocalPathForWrite(prefix, size, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "inc",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void inc(Statistic op, long count)\n{\r\n    incrementCallback.accept(op, count);\r\n    incCounter(op.getSymbol(), count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "instantiated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void instantiated()\n{\r\n    inc(MULTIPART_UPLOAD_INSTANTIATED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "uploadStarted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void uploadStarted()\n{\r\n    inc(MULTIPART_UPLOAD_STARTED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "partPut",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void partPut(final long lengthInBytes)\n{\r\n    inc(MULTIPART_UPLOAD_PART_PUT, 1);\r\n    inc(MULTIPART_UPLOAD_PART_PUT_BYTES, lengthInBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "uploadCompleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void uploadCompleted()\n{\r\n    inc(MULTIPART_UPLOAD_COMPLETED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "uploadAborted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void uploadAborted()\n{\r\n    inc(MULTIPART_UPLOAD_ABORTED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "abortUploadsUnderPathInvoked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortUploadsUnderPathInvoked()\n{\r\n    inc(MULTIPART_UPLOAD_ABORT_UNDER_PATH_INVOKED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "createTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PathOutputCommitter createTaskCommitter(S3AFileSystem fileSystem, Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return new PartitionedStagingCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "close",
  "errType" : [ "IOException|AbortedException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    long skipped = 0;\r\n    boolean aborted = false;\r\n    if (!closed.getAndSet(true)) {\r\n        try {\r\n            boolean shouldAbort = wrappedStream.available() > readahead;\r\n            if (!shouldAbort) {\r\n                skipped = wrappedStream.skip(readahead);\r\n                shouldAbort = wrappedStream.read() >= 0;\r\n            }\r\n            if (shouldAbort) {\r\n                aborted = true;\r\n                wrappedStream.abort();\r\n            }\r\n        } catch (IOException | AbortedException e) {\r\n            LOG.debug(\"While closing stream\", e);\r\n        } finally {\r\n            IOUtils.cleanupWithLogger(LOG, wrappedStream);\r\n            streamStatistics.streamClose(aborted, skipped);\r\n            streamStatistics.close();\r\n            super.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "checkNotClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkNotClosed() throws IOException\n{\r\n    if (closed.get()) {\r\n        throw new PathIOException(uri, FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int available() throws IOException\n{\r\n    checkNotClosed();\r\n    return wrappedStream.available();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long skip(final long n) throws IOException\n{\r\n    checkNotClosed();\r\n    long skipped = once(\"skip\", uri, () -> wrappedStream.skip(n));\r\n    pos.addAndGet(skipped);\r\n    streamStatistics.seekForwards(skipped, skipped);\r\n    return skipped;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos()\n{\r\n    return pos.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "setReadahead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadahead(Long readahead)\n{\r\n    this.readahead = validateReadahead(readahead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getReadahead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadahead()\n{\r\n    return readahead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "read",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    checkNotClosed();\r\n    int byteRead;\r\n    try {\r\n        byteRead = once(\"read()\", uri, () -> wrappedStream.read());\r\n    } catch (EOFException e) {\r\n        if (completedSuccessfully.get()) {\r\n            return -1;\r\n        } else {\r\n            LOG.info(\"Reading of S3 Select data from {} failed before all results \" + \" were generated.\", uri);\r\n            streamStatistics.readException();\r\n            throw new PathIOException(uri, \"Read of S3 Select data did not complete\");\r\n        }\r\n    }\r\n    if (byteRead >= 0) {\r\n        incrementBytesRead(1);\r\n    }\r\n    return byteRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "read",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int read(final byte[] buf, final int off, final int len) throws IOException\n{\r\n    checkNotClosed();\r\n    validatePositionedReadArgs(pos.get(), buf, off, len);\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    int bytesRead;\r\n    try {\r\n        streamStatistics.readOperationStarted(pos.get(), len);\r\n        bytesRead = wrappedStream.read(buf, off, len);\r\n    } catch (EOFException e) {\r\n        streamStatistics.readException();\r\n        return -1;\r\n    }\r\n    incrementBytesRead(bytesRead);\r\n    streamStatistics.readOperationCompleted(len, bytesRead);\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void seek(long newPos) throws IOException\n{\r\n    long current = getPos();\r\n    long distance = newPos - current;\r\n    if (distance < 0) {\r\n        throw unsupported(SEEK_UNSUPPORTED + \" backwards from \" + current + \" to \" + newPos);\r\n    }\r\n    if (distance == 0) {\r\n        LOG.debug(\"ignoring seek to current position.\");\r\n    } else {\r\n        LOG.debug(\"Forward seek by reading {} bytes\", distance);\r\n        long bytesSkipped = 0;\r\n        while (distance > 0) {\r\n            int r = read();\r\n            if (r == -1) {\r\n                throw new EOFException(\"Seek to \" + newPos + \" reached End of File at offset \" + getPos());\r\n            }\r\n            distance--;\r\n            bytesSkipped++;\r\n        }\r\n        streamStatistics.seekForwards(bytesSkipped, bytesSkipped);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "unsupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathIOException unsupported(final String action)\n{\r\n    return new PathIOException(String.format(\"s3a://%s/%s\", bucket, key), action + \" not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean seekToNewSource(long targetPos) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "markSupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean markSupported()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "mark",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void mark(int readLimit)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reset() throws IOException\n{\r\n    throw unsupported(\"Mark\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "abort",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void abort()\n{\r\n    if (!closed.get()) {\r\n        LOG.debug(\"Aborting\");\r\n        wrappedStream.abort();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read(final long position, final byte[] buffer, final int offset, final int length) throws IOException\n{\r\n    seek(position);\r\n    return read(buffer, offset, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "incrementBytesRead",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void incrementBytesRead(long bytesRead)\n{\r\n    if (bytesRead > 0) {\r\n        pos.addAndGet(bytesRead);\r\n    }\r\n    streamStatistics.bytesRead(bytesRead);\r\n    if (readContext.getStats() != null && bytesRead > 0) {\r\n        readContext.getStats().incrementBytesRead(bytesRead);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getS3AStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputStreamStatistics getS3AStreamStatistics()\n{\r\n    return streamStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    String s = streamStatistics.toString();\r\n    synchronized (this) {\r\n        final StringBuilder sb = new StringBuilder(\"SelectInputStream{\");\r\n        sb.append(uri);\r\n        sb.append(\"; state \").append(!closed.get() ? \"open\" : \"closed\");\r\n        sb.append(\"; pos=\").append(getPos());\r\n        sb.append(\"; readahead=\").append(readahead);\r\n        sb.append('\\n').append(s);\r\n        sb.append('}');\r\n        return sb.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Void execute() throws IOException, PathExistsException\n{\r\n    LOG.debug(\"Copying local file from {} to {}\", source, destination);\r\n    File sourceFile = callbacks.pathToLocalFile(source);\r\n    updateDestStatus(destination);\r\n    if (getDestStatus().isPresent() && getDestStatus().get().isDirectory() && sourceFile.isDirectory()) {\r\n        destination = new Path(destination, sourceFile.getName());\r\n        LOG.debug(\"Destination updated to: {}\", destination);\r\n        updateDestStatus(destination);\r\n    }\r\n    checkSource(sourceFile);\r\n    checkDestination(destination, sourceFile, overwrite);\r\n    uploadSourceFromFS();\r\n    if (deleteSource) {\r\n        callbacks.deleteLocal(source, true);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "updateDestStatus",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateDestStatus(Path dest) throws IOException\n{\r\n    try {\r\n        destStatus = callbacks.getFileStatus(dest);\r\n    } catch (FileNotFoundException e) {\r\n        destStatus = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "uploadSourceFromFS",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void uploadSourceFromFS() throws IOException\n{\r\n    RemoteIterator<LocatedFileStatus> localFiles = listFilesAndDirs(source);\r\n    List<CompletableFuture<Void>> activeOps = new ArrayList<>();\r\n    Set<Path> emptyDirs = new HashSet<>();\r\n    List<UploadEntry> entries = new ArrayList<>();\r\n    while (localFiles.hasNext()) {\r\n        LocatedFileStatus sourceFile = localFiles.next();\r\n        Path sourceFilePath = sourceFile.getPath();\r\n        emptyDirs.remove(sourceFilePath.getParent());\r\n        if (sourceFile.isDirectory()) {\r\n            emptyDirs.add(sourceFilePath);\r\n            continue;\r\n        }\r\n        Path destPath = getFinalPath(sourceFilePath);\r\n        entries.add(new UploadEntry(sourceFilePath, destPath, sourceFile.getLen()));\r\n    }\r\n    if (localFiles instanceof Closeable) {\r\n        IOUtils.closeStream((Closeable) localFiles);\r\n    }\r\n    entries.sort(new ReverseComparator(new UploadEntry.SizeComparator()));\r\n    final int sortedUploadsCount = Math.min(LARGEST_N_FILES, entries.size());\r\n    List<UploadEntry> markedForUpload = new ArrayList<>();\r\n    for (int uploadNo = 0; uploadNo < sortedUploadsCount; uploadNo++) {\r\n        UploadEntry uploadEntry = entries.get(uploadNo);\r\n        File file = callbacks.pathToLocalFile(uploadEntry.source);\r\n        activeOps.add(submitUpload(file, uploadEntry));\r\n        markedForUpload.add(uploadEntry);\r\n    }\r\n    if (entries.isEmpty()) {\r\n        emptyDirs.add(source);\r\n    }\r\n    entries.removeAll(markedForUpload);\r\n    Collections.shuffle(entries);\r\n    for (UploadEntry uploadEntry : entries) {\r\n        File file = callbacks.pathToLocalFile(uploadEntry.source);\r\n        activeOps.add(submitUpload(file, uploadEntry));\r\n    }\r\n    for (Path emptyDir : emptyDirs) {\r\n        Path emptyDirPath = getFinalPath(emptyDir);\r\n        activeOps.add(submitCreateEmptyDir(emptyDirPath));\r\n    }\r\n    waitForCompletion(activeOps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "submitCreateEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<Void> submitCreateEmptyDir(Path dir)\n{\r\n    return submit(executor, callableWithinAuditSpan(getAuditSpan(), () -> {\r\n        callbacks.createEmptyDir(dir, getStoreContext());\r\n        return null;\r\n    }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "submitUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompletableFuture<Void> submitUpload(File file, UploadEntry uploadEntry)\n{\r\n    return submit(executor, callableWithinAuditSpan(getAuditSpan(), () -> {\r\n        callbacks.copyLocalFileFromTo(file, uploadEntry.source, uploadEntry.destination);\r\n        return null;\r\n    }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "checkSource",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkSource(File src) throws FileNotFoundException\n{\r\n    if (!src.exists()) {\r\n        throw new FileNotFoundException(\"No file: \" + src.getPath());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "checkDestination",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkDestination(Path dest, File src, boolean overwrite) throws PathExistsException, FileAlreadyExistsException\n{\r\n    if (!getDestStatus().isPresent()) {\r\n        return;\r\n    }\r\n    if (src.isDirectory() && getDestStatus().get().isFile()) {\r\n        throw new FileAlreadyExistsException(\"Source '\" + src.getPath() + \"' is directory and \" + \"destination '\" + dest + \"' is file\");\r\n    }\r\n    if (!overwrite) {\r\n        throw new PathExistsException(dest + \" already exists\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getFinalPath",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Path getFinalPath(Path src) throws PathIOException\n{\r\n    URI currentSrcUri = src.toUri();\r\n    URI relativeSrcUri = source.toUri().relativize(currentSrcUri);\r\n    if (relativeSrcUri.equals(currentSrcUri)) {\r\n        throw new PathIOException(\"Cannot get relative path for URI:\" + relativeSrcUri);\r\n    }\r\n    Optional<FileStatus> status = getDestStatus();\r\n    if (!relativeSrcUri.getPath().isEmpty()) {\r\n        return new Path(destination, relativeSrcUri.getPath());\r\n    } else if (status.isPresent() && status.get().isDirectory()) {\r\n        return new Path(destination, src.getName());\r\n    } else {\r\n        return destination;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getDestStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Optional<FileStatus> getDestStatus()\n{\r\n    return Optional.ofNullable(destStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "listFilesAndDirs",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listFilesAndDirs(Path path) throws IOException\n{\r\n    return new RemoteIterator<LocatedFileStatus>() {\r\n\r\n        private final Stack<RemoteIterator<LocatedFileStatus>> iterators = new Stack<>();\r\n\r\n        private RemoteIterator<LocatedFileStatus> current = callbacks.listLocalStatusIterator(path);\r\n\r\n        private LocatedFileStatus curFile;\r\n\r\n        @Override\r\n        public boolean hasNext() throws IOException {\r\n            while (curFile == null) {\r\n                if (current.hasNext()) {\r\n                    handleFileStat(current.next());\r\n                } else if (!iterators.empty()) {\r\n                    current = iterators.pop();\r\n                } else {\r\n                    return false;\r\n                }\r\n            }\r\n            return true;\r\n        }\r\n\r\n        private void handleFileStat(LocatedFileStatus stat) throws IOException {\r\n            if (stat.isFile()) {\r\n                curFile = stat;\r\n            } else {\r\n                curFile = stat;\r\n                iterators.push(current);\r\n                current = callbacks.listLocalStatusIterator(stat.getPath());\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public LocatedFileStatus next() throws IOException {\r\n            if (hasNext()) {\r\n                LocatedFileStatus result = curFile;\r\n                curFile = null;\r\n                return result;\r\n            }\r\n            throw new NoSuchElementException(\"No more entry in \" + path);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "setIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIOStatistics(final IOStatisticsStore statistics)\n{\r\n    this.ioStatistics = statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incCounter(String name)\n{\r\n    return incCounter(name, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incCounter(String name, long value)\n{\r\n    return ioStatistics.incrementCounter(name, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "lookupCounterValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long lookupCounterValue(final String name)\n{\r\n    return ioStatistics.counters().get(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "lookupGaugeValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long lookupGaugeValue(final String name)\n{\r\n    return ioStatistics.gauges().get(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incGauge(String name, long v)\n{\r\n    return ioStatistics.incrementGauge(name, v);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long incGauge(String name)\n{\r\n    return incGauge(name, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AbstractS3AStatisticsSource{\");\r\n    sb.append(ioStatistics);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "trackDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DurationTracker trackDuration(final String key, final long count)\n{\r\n    return getIOStatistics().trackDuration(key, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RetryPolicy getRetryPolicy()\n{\r\n    return retryPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRetryCallback",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Retried getRetryCallback()\n{\r\n    return retryCallback;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "once",
  "errType" : [ "AmazonClientException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T once(String action, String path, CallableRaisingIOE<T> operation) throws IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"%s\", action)) {\r\n        return operation.apply();\r\n    } catch (AmazonClientException e) {\r\n        throw S3AUtils.translateException(action, path, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "once",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void once(String action, String path, InvocationRaisingIOE operation) throws IOException\n{\r\n    once(action, path, () -> {\r\n        operation.apply();\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "onceInTheFuture",
  "errType" : [ "AmazonClientException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T onceInTheFuture(String action, String path, final Future<T> future) throws IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"%s\", action)) {\r\n        return FutureIO.awaitFuture(future);\r\n    } catch (AmazonClientException e) {\r\n        throw S3AUtils.translateException(action, path, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "ignoreIOExceptions",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void ignoreIOExceptions(Logger log, String action, String path, CallableRaisingIOE<T> operation)\n{\r\n    try {\r\n        once(action, path, operation);\r\n    } catch (IOException e) {\r\n        String description = toDescription(action, path);\r\n        String error = e.toString();\r\n        log.info(\"{}: {}\", description, error);\r\n        log.debug(\"{}\", description, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "ignoreIOExceptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void ignoreIOExceptions(Logger log, String action, String path, InvocationRaisingIOE operation)\n{\r\n    ignoreIOExceptions(log, action, path, () -> {\r\n        operation.apply();\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "retry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void retry(String action, String path, boolean idempotent, Retried retrying, InvocationRaisingIOE operation) throws IOException\n{\r\n    retry(action, path, idempotent, retrying, () -> {\r\n        operation.apply();\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeRetry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeRetry(boolean doRetry, String action, String path, boolean idempotent, Retried retrying, InvocationRaisingIOE operation) throws IOException\n{\r\n    maybeRetry(doRetry, action, path, idempotent, retrying, () -> {\r\n        operation.apply();\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "retry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void retry(String action, String path, boolean idempotent, InvocationRaisingIOE operation) throws IOException\n{\r\n    retry(action, path, idempotent, retryCallback, operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeRetry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeRetry(boolean doRetry, String action, String path, boolean idempotent, InvocationRaisingIOE operation) throws IOException\n{\r\n    maybeRetry(doRetry, action, path, idempotent, retryCallback, operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "retry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T retry(String action, @Nullable String path, boolean idempotent, CallableRaisingIOE<T> operation) throws IOException\n{\r\n    return retry(action, path, idempotent, retryCallback, operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "retry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T retry(String action, @Nullable String path, boolean idempotent, Retried retrying, CallableRaisingIOE<T> operation) throws IOException\n{\r\n    return retryUntranslated(toDescription(action, path), idempotent, retrying, () -> once(action, path, operation));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeRetry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T maybeRetry(boolean doRetry, String action, @Nullable String path, boolean idempotent, Retried retrying, CallableRaisingIOE<T> operation) throws IOException\n{\r\n    if (doRetry) {\r\n        return retryUntranslated(toDescription(action, path), idempotent, retrying, () -> once(action, path, operation));\r\n    } else {\r\n        return once(action, path, operation);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "retryUntranslated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T retryUntranslated(String text, boolean idempotent, CallableRaisingIOE<T> operation) throws IOException\n{\r\n    return retryUntranslated(text, idempotent, retryCallback, operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "retryUntranslated",
  "errType" : [ "IOException|SdkBaseException", "InterruptedException", "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "T retryUntranslated(String text, boolean idempotent, Retried retrying, CallableRaisingIOE<T> operation) throws IOException\n{\r\n    Preconditions.checkArgument(retrying != null, \"null retrying argument\");\r\n    int retryCount = 0;\r\n    Exception caught;\r\n    RetryPolicy.RetryAction retryAction;\r\n    boolean shouldRetry;\r\n    do {\r\n        try {\r\n            if (retryCount > 0) {\r\n                LOG.debug(\"retry #{}\", retryCount);\r\n            }\r\n            return operation.apply();\r\n        } catch (IOException | SdkBaseException e) {\r\n            caught = e;\r\n        }\r\n        IOException translated;\r\n        if (caught instanceof IOException) {\r\n            translated = (IOException) caught;\r\n        } else {\r\n            translated = S3AUtils.translateException(text, \"\", (SdkBaseException) caught);\r\n        }\r\n        try {\r\n            retryAction = retryPolicy.shouldRetry(translated, retryCount, 0, idempotent);\r\n            shouldRetry = retryAction.action.equals(RetryPolicy.RetryAction.RETRY.action);\r\n            if (shouldRetry) {\r\n                retrying.onFailure(text, translated, retryCount, idempotent);\r\n                Thread.sleep(retryAction.delayMillis);\r\n            }\r\n            retryCount++;\r\n        } catch (InterruptedException e) {\r\n            caught = new InterruptedIOException(\"Interrupted\");\r\n            caught.initCause(e);\r\n            shouldRetry = false;\r\n            Thread.currentThread().interrupt();\r\n        } catch (Exception e) {\r\n            LOG.warn(\"{}: exception in retry processing\", text, e);\r\n            shouldRetry = false;\r\n        }\r\n    } while (shouldRetry);\r\n    if (caught instanceof IOException) {\r\n        throw (IOException) caught;\r\n    } else {\r\n        throw (SdkBaseException) caught;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "quietly",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void quietly(String action, String path, InvocationRaisingIOE operation)\n{\r\n    try {\r\n        once(action, path, operation);\r\n    } catch (Exception e) {\r\n        LOG.debug(\"Action {} failed\", action, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "quietlyEval",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Optional<T> quietlyEval(String action, String path, CallableRaisingIOE<T> operation)\n{\r\n    try {\r\n        return Optional.of(once(action, path, operation));\r\n    } catch (Exception e) {\r\n        LOG.debug(\"Action {} failed\", action, e);\r\n        return Optional.empty();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toDescription",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toDescription(String action, @Nullable String path)\n{\r\n    return action + (StringUtils.isNotEmpty(path) ? (\" on \" + path) : \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A createSpan(final String operation, @Nullable final String path1, @Nullable final String path2)\n{\r\n    return new NoopSpan(createSpanID(), operation, path1, path2, activationCallbacks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getUnbondedSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanS3A getUnbondedSpan()\n{\r\n    return unbondedSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createAndStartNoopAuditor",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "NoopAuditor createAndStartNoopAuditor(Configuration conf, NoopSpan.SpanActivationCallbacks activationCallbacks)\n{\r\n    NoopAuditor noop = new NoopAuditor(activationCallbacks);\r\n    final OperationAuditorOptions options = OperationAuditorOptions.builder().withConfiguration(conf).withIoStatisticsStore(iostatisticsStore().build());\r\n    noop.init(options);\r\n    noop.start();\r\n    return noop;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getInstrumentation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AInstrumentation getInstrumentation()\n{\r\n    return statisticsSource.getInstrumentation();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "getInstanceStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem.Statistics getInstanceStatistics()\n{\r\n    return statisticsSource.getInstanceStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newInputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AInputStreamStatistics newInputStreamStatistics()\n{\r\n    return getInstrumentation().newInputStreamStatistics(statisticsSource.getInstanceStatistics());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newCommitterStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CommitterStatistics newCommitterStatistics()\n{\r\n    return getInstrumentation().newCommitterStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newOutputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockOutputStreamStatistics newOutputStreamStatistics()\n{\r\n    return getInstrumentation().newOutputStreamStatistics(getInstanceStatistics());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementCounter(Statistic op, long count)\n{\r\n    getInstrumentation().incrementCounter(op, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "incrementGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementGauge(Statistic op, long count)\n{\r\n    getInstrumentation().incrementGauge(op, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "decrementGauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void decrementGauge(Statistic op, long count)\n{\r\n    getInstrumentation().decrementGauge(op, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "addValueToQuantiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addValueToQuantiles(Statistic op, long value)\n{\r\n    getInstrumentation().addValueToQuantiles(op, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "recordDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void recordDuration(final Statistic op, final boolean success, final Duration duration)\n{\r\n    getInstrumentation().recordDuration(op, success, duration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newDelegationTokenStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DelegationTokenStatistics newDelegationTokenStatistics()\n{\r\n    return getInstrumentation().newDelegationTokenStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "newStatisticsFromAwsSdk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StatisticsFromAwsSdk newStatisticsFromAwsSdk()\n{\r\n    return new StatisticsFromAwsSdkImpl(getInstrumentation());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "createMultipartUploaderStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AMultipartUploaderStatistics createMultipartUploaderStatistics()\n{\r\n    return new S3AMultipartUploaderStatisticsImpl(this::incrementCounter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "trackDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DurationTracker trackDuration(final String key, final long count)\n{\r\n    return getInstrumentation().trackDuration(key, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setFsURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setFsURI(final URI fsURI)\n{\r\n    this.fsURI = fsURI;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setBucket(final String b)\n{\r\n    this.bucket = b;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setConfiguration(final Configuration conf)\n{\r\n    this.configuration = conf;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setUsername",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setUsername(final String user)\n{\r\n    this.username = user;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setOwner(final UserGroupInformation ugi)\n{\r\n    this.owner = ugi;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setExecutor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setExecutor(final ExecutorService ex)\n{\r\n    this.executor = ex;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setExecutorCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setExecutorCapacity(final int capacity)\n{\r\n    this.executorCapacity = capacity;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setInvoker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setInvoker(final Invoker invoke)\n{\r\n    this.invoker = invoke;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setInstrumentation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setInstrumentation(final S3AStatisticsContext instr)\n{\r\n    this.instrumentation = instr;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setStorageStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setStorageStatistics(final S3AStorageStatistics sstats)\n{\r\n    this.storageStatistics = sstats;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setInputPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setInputPolicy(final S3AInputPolicy policy)\n{\r\n    this.inputPolicy = policy;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setChangeDetectionPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setChangeDetectionPolicy(final ChangeDetectionPolicy policy)\n{\r\n    this.changeDetectionPolicy = policy;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setMultiObjectDeleteEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setMultiObjectDeleteEnabled(final boolean enabled)\n{\r\n    this.multiObjectDeleteEnabled = enabled;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setUseListV1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setUseListV1(final boolean useV1)\n{\r\n    this.useListV1 = useV1;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setContextAccessors",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setContextAccessors(final ContextAccessors accessors)\n{\r\n    this.contextAccessors = accessors;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setAuditor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setAuditor(final AuditSpanSource<AuditSpanS3A> value)\n{\r\n    auditor = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setEnableCSE",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContextBuilder setEnableCSE(boolean value)\n{\r\n    isCSEEnabled = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContext build()\n{\r\n    return new StoreContext(fsURI, bucket, configuration, username, owner, executor, executorCapacity, invoker, instrumentation, storageStatistics, inputPolicy, changeDetectionPolicy, multiObjectDeleteEnabled, useListV1, contextAccessors, auditor, isCSEEnabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUsage()\n{\r\n    return USAGE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getSelectDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationDuration getSelectDuration()\n{\r\n    return selectDuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesRead()\n{\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getLinesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLinesRead()\n{\r\n    return linesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "parseNaturalInt",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int parseNaturalInt(String option, String value)\n{\r\n    try {\r\n        int r = Integer.parseInt(value);\r\n        if (r < 0) {\r\n            throw invalidArgs(\"Negative value for option %s : %s\", option, value);\r\n        }\r\n        return r;\r\n    } catch (NumberFormatException e) {\r\n        throw invalidArgs(\"Invalid number for option %s : %s\", option, value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getOptValue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Optional<String> getOptValue(String key)\n{\r\n    String value = getCommandFormat().getOptValue(key);\r\n    return isNotEmpty(value) ? Optional.of(value) : Optional.empty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getIntValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Optional<Integer> getIntValue(String key)\n{\r\n    Optional<String> v = getOptValue(key);\r\n    return v.map(i -> parseNaturalInt(key, i));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "run",
  "errType" : [ "CommandFormat.UnknownOptionException", "FileNotFoundException" ],
  "containingMethodsNum" : 48,
  "sourceCodeText" : "int run(String[] args, PrintStream out) throws IOException, ExitUtil.ExitException\n{\r\n    final List<String> parsedArgs;\r\n    try {\r\n        parsedArgs = parseArgs(args);\r\n    } catch (CommandFormat.UnknownOptionException e) {\r\n        errorln(getUsage());\r\n        throw new ExitUtil.ExitException(EXIT_USAGE, e.getMessage(), e);\r\n    }\r\n    if (parsedArgs.size() < 2) {\r\n        errorln(getUsage());\r\n        throw new ExitUtil.ExitException(EXIT_USAGE, TOO_FEW_ARGUMENTS);\r\n    }\r\n    final String file = parsedArgs.get(0);\r\n    final Path path = new Path(file);\r\n    String expression = parsedArgs.get(1);\r\n    println(out, \"selecting file %s with query %s\", path, expression);\r\n    final Optional<String> header = getOptValue(OPT_HEADER);\r\n    header.ifPresent(h -> println(out, \"Using header option %s\", h));\r\n    Path destPath = getOptValue(OPT_OUTPUT).map(output -> {\r\n        println(out, \"Saving output to %s\", output);\r\n        return new Path(output);\r\n    }).orElse(null);\r\n    final boolean toConsole = destPath == null;\r\n    final Optional<Integer> expectedLines = toConsole ? getIntValue(OPT_EXPECTED) : Optional.empty();\r\n    final Optional<Integer> limit = getIntValue(OPT_LIMIT);\r\n    if (limit.isPresent()) {\r\n        final int l = limit.get();\r\n        println(out, \"Using line limit %s\", l);\r\n        if (expression.toLowerCase(Locale.ENGLISH).contains(\" limit \")) {\r\n            println(out, \"line limit already specified in SELECT expression\");\r\n        } else {\r\n            expression = expression + \" LIMIT \" + l;\r\n        }\r\n    }\r\n    FileSystem fs = bindFilesystem(path.getFileSystem(getConf()));\r\n    if (!fs.hasPathCapability(path, S3_SELECT_CAPABILITY)) {\r\n        throw new ExitUtil.ExitException(EXIT_SERVICE_UNAVAILABLE, SELECT_IS_DISABLED + \" for \" + file);\r\n    }\r\n    linesRead = 0;\r\n    selectDuration = new OperationDuration();\r\n    final FutureDataInputStreamBuilder builder = fs.openFile(path).must(SELECT_SQL, expression);\r\n    header.ifPresent(h -> builder.must(CSV_INPUT_HEADER, h));\r\n    getOptValue(OPT_COMPRESSION).ifPresent(compression -> builder.must(SELECT_INPUT_COMPRESSION, compression.toUpperCase(Locale.ENGLISH)));\r\n    getOptValue(OPT_INPUTFORMAT).ifPresent(opt -> {\r\n        if (!\"csv\".equalsIgnoreCase(opt)) {\r\n            throw invalidArgs(\"Unsupported input format %s\", opt);\r\n        }\r\n    });\r\n    getOptValue(OPT_OUTPUTFORMAT).ifPresent(opt -> {\r\n        if (!\"csv\".equalsIgnoreCase(opt)) {\r\n            throw invalidArgs(\"Unsupported output format %s\", opt);\r\n        }\r\n    });\r\n    builder.opt(SELECT_ERRORS_INCLUDE_SQL, true);\r\n    FSDataInputStream stream;\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Selecting stream\")) {\r\n        stream = FutureIOSupport.awaitFuture(builder.build());\r\n    } catch (FileNotFoundException e) {\r\n        throw notFound(e);\r\n    }\r\n    try {\r\n        if (toConsole) {\r\n            bytesRead = 0;\r\n            @SuppressWarnings(\"IOResourceOpenedButNotSafelyClosed\")\r\n            Scanner scanner = new Scanner(new BufferedReader(new InputStreamReader(stream, StandardCharsets.UTF_8)));\r\n            scanner.useDelimiter(\"\\n\");\r\n            while (scanner.hasNextLine()) {\r\n                linesRead++;\r\n                String l = scanner.nextLine();\r\n                bytesRead += l.length() + 1;\r\n                println(out, \"%s\", l);\r\n            }\r\n        } else {\r\n            FileSystem destFS = destPath.getFileSystem(getConf());\r\n            try (DurationInfo ignored = new DurationInfo(LOG, \"Copying File\");\r\n                OutputStream destStream = destFS.createFile(destPath).overwrite(true).build()) {\r\n                bytesRead = IOUtils.copy(stream, destStream);\r\n            }\r\n        }\r\n        try (DurationInfo ignored = new DurationInfo(LOG, \"Closing stream\")) {\r\n            stream.close();\r\n        }\r\n        String result = toConsole ? String.format(\"%s lines\", linesRead) : String.format(\"%s bytes\", bytesRead);\r\n        selectDuration.finished();\r\n        println(out, \"Read %s in time %s\", result, selectDuration.getDurationString());\r\n        println(out, \"Bytes Read: %,d bytes\", bytesRead);\r\n        println(out, \"Bandwidth: %,.1f MiB/s\", bandwidthMBs(bytesRead, selectDuration.value()));\r\n    } finally {\r\n        cleanupWithLogger(LOG, stream);\r\n    }\r\n    LOG.debug(\"Statistics {}\", stream);\r\n    expectedLines.ifPresent(l -> {\r\n        if (l != linesRead) {\r\n            throw exitException(EXIT_FAIL, \"Expected %d rows but the operation returned %d\", l, linesRead);\r\n        }\r\n    });\r\n    out.flush();\r\n    return EXIT_SUCCESS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "bandwidthMBs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double bandwidthMBs(long bytes, long durationMillisNS)\n{\r\n    return durationMillisNS > 0 ? (bytes / 1048576.0 * 1000 / durationMillisNS) : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "keyOfFinalDestination",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String keyOfFinalDestination(List<String> elements, String key)\n{\r\n    if (isMagicCommitPath(elements)) {\r\n        return elementsToKey(finalDestination(elements));\r\n    } else {\r\n        return key;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createTracker",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "PutTracker createTracker(Path path, String key)\n{\r\n    final List<String> elements = splitPathToElements(path);\r\n    PutTracker tracker;\r\n    if (isMagicFile(elements)) {\r\n        if (isMagicCommitPath(elements)) {\r\n            final String destKey = keyOfFinalDestination(elements, key);\r\n            String pendingsetPath = key + CommitConstants.PENDING_SUFFIX;\r\n            getStoreContext().incrementStatistic(Statistic.COMMITTER_MAGIC_FILES_CREATED);\r\n            tracker = new MagicCommitTracker(path, getStoreContext().getBucket(), key, destKey, pendingsetPath, owner.getWriteOperationHelper());\r\n            LOG.debug(\"Created {}\", tracker);\r\n        } else {\r\n            LOG.warn(\"File being created has a \\\"magic\\\" path, but the filesystem\" + \" has magic file support disabled: {}\", path);\r\n            tracker = new PutTracker(key);\r\n        }\r\n    } else {\r\n        tracker = new PutTracker(key);\r\n    }\r\n    return tracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "finalDestination",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> finalDestination(List<String> elements)\n{\r\n    return magicCommitEnabled ? MagicCommitPaths.finalDestination(elements) : elements;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isMagicCommitEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMagicCommitEnabled()\n{\r\n    return magicCommitEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isMagicCommitPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMagicCommitPath(Path path)\n{\r\n    return isMagicCommitPath(splitPathToElements(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isMagicCommitPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMagicCommitPath(List<String> elements)\n{\r\n    return magicCommitEnabled && isMagicFile(elements);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isMagicFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isMagicFile(List<String> elements)\n{\r\n    return isMagicPath(elements) && !isCommitMetadataFile(elements);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "isCommitMetadataFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isCommitMetadataFile(List<String> elements)\n{\r\n    String last = elements.get(elements.size() - 1);\r\n    return last.endsWith(CommitConstants.PENDING_SUFFIX) || last.endsWith(CommitConstants.PENDINGSET_SUFFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validate() throws ValidationFailure\n{\r\n    ValidationFailure.verify(name != null, \"Incompatible file format: no 'name' field\");\r\n    ValidationFailure.verify(NAME.equals(name), \"Incompatible file format: \" + name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] toBytes() throws IOException\n{\r\n    return serializer().toBytes(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void save(FileSystem fs, Path path, boolean overwrite) throws IOException\n{\r\n    name = NAME;\r\n    serializer().save(fs, path, this, overwrite);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"SuccessData{\");\r\n    sb.append(\"committer='\").append(committer).append('\\'');\r\n    sb.append(\", hostname='\").append(hostname).append('\\'');\r\n    sb.append(\", description='\").append(description).append('\\'');\r\n    sb.append(\", date='\").append(date).append('\\'');\r\n    sb.append(\", filenames=[\").append(StringUtils.join(filenames, \", \")).append(\"]\");\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "dumpMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dumpMetrics(String prefix, String middle, String suffix)\n{\r\n    return joinMap(metrics, prefix, middle, suffix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "dumpDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dumpDiagnostics(String prefix, String middle, String suffix)\n{\r\n    return joinMap(diagnostics, prefix, middle, suffix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "joinMap",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String joinMap(Map<String, ?> map, String prefix, String middle, String suffix)\n{\r\n    if (map == null) {\r\n        return \"\";\r\n    }\r\n    List<String> list = new ArrayList<>(map.keySet());\r\n    Collections.sort(list);\r\n    StringBuilder sb = new StringBuilder(list.size() * 32);\r\n    for (String k : list) {\r\n        sb.append(prefix).append(k).append(middle).append(map.get(k)).append(suffix);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SuccessData load(FileSystem fs, Path path) throws IOException\n{\r\n    LOG.debug(\"Reading success data from {}\", path);\r\n    SuccessData instance = serializer().load(fs, path);\r\n    instance.validate();\r\n    return instance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "serializer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JsonSerialization<SuccessData> serializer()\n{\r\n    return new JsonSerialization<>(SuccessData.class, false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setName(String name)\n{\r\n    this.name = name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimestamp()\n{\r\n    return timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimestamp(long timestamp)\n{\r\n    this.timestamp = timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDate()\n{\r\n    return date;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDate(String date)\n{\r\n    this.date = date;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHostname(String hostname)\n{\r\n    this.hostname = hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitter()\n{\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCommitter(String committer)\n{\r\n    this.committer = committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDescription()\n{\r\n    return description;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDescription(String description)\n{\r\n    this.description = description;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Long> getMetrics()\n{\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMetrics(Map<String, Long> metrics)\n{\r\n    this.metrics = metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getFilenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getFilenames()\n{\r\n    return filenames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setFilenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFilenames(List<String> filenames)\n{\r\n    this.filenames = filenames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getDiagnostics()\n{\r\n    return diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDiagnostics(Map<String, String> diagnostics)\n{\r\n    this.diagnostics = diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "addDiagnostic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDiagnostic(String key, String value)\n{\r\n    diagnostics.put(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobId(String jobId)\n{\r\n    this.jobId = jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getJobIdSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobIdSource()\n{\r\n    return jobIdSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setJobIdSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobIdSource(final String jobIdSource)\n{\r\n    this.jobIdSource = jobIdSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsSnapshot getIOStatistics()\n{\r\n    return iostats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIOStatistics(final IOStatisticsSnapshot ioStatistics)\n{\r\n    this.iostats = ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "buildFSURI",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "URI buildFSURI(URI uri)\n{\r\n    rejectSecretsInURIs(uri);\r\n    Objects.requireNonNull(uri, \"null uri\");\r\n    Objects.requireNonNull(uri.getScheme(), \"null uri.getScheme()\");\r\n    if (uri.getHost() == null && uri.getAuthority() != null) {\r\n        Objects.requireNonNull(uri.getHost(), \"null uri host.\");\r\n    }\r\n    Objects.requireNonNull(uri.getHost(), \"null uri host.\");\r\n    return URI.create(uri.getScheme() + \"://\" + uri.getHost());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString(URI pathUri)\n{\r\n    return pathUri != null ? String.format(\"%s://%s/%s\", pathUri.getScheme(), pathUri.getHost(), pathUri.getPath()) : \"(null URI)\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "rejectSecretsInURIs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void rejectSecretsInURIs(URI name)\n{\r\n    Login login = extractLoginDetails(name);\r\n    Preconditions.checkArgument(!login.hasLogin(), LOGIN_WARNING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "extractLoginDetails",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Login extractLoginDetails(URI name)\n{\r\n    if (name == null) {\r\n        return Login.EMPTY;\r\n    }\r\n    String authority = name.getAuthority();\r\n    if (authority == null) {\r\n        return Login.EMPTY;\r\n    }\r\n    int loginIndex = authority.indexOf('@');\r\n    if (loginIndex < 0) {\r\n        return Login.EMPTY;\r\n    }\r\n    String login = authority.substring(0, loginIndex);\r\n    int loginSplit = login.indexOf(':');\r\n    if (loginSplit > 0) {\r\n        String user = login.substring(0, loginSplit);\r\n        String encodedPassword = login.substring(loginSplit + 1);\r\n        return new Login(user, encodedPassword.isEmpty() ? \"\" : \"password removed\");\r\n    } else if (loginSplit == 0) {\r\n        return Login.EMPTY;\r\n    } else {\r\n        return new Login(login, \"\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "canonicalizeUri",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "URI canonicalizeUri(URI uri, int defaultPort)\n{\r\n    if (uri.getPort() == -1 && defaultPort > 0) {\r\n        try {\r\n            uri = new URI(uri.getScheme(), uri.getUserInfo(), uri.getHost(), defaultPort, uri.getPath(), uri.getQuery(), uri.getFragment());\r\n        } catch (URISyntaxException e) {\r\n            throw new AssertionError(\"Valid URI became unparseable: \" + uri);\r\n        }\r\n    }\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "checkPath",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void checkPath(Configuration conf, URI fsUri, Path path, int defaultPort)\n{\r\n    URI pathUri = path.toUri();\r\n    String thatScheme = pathUri.getScheme();\r\n    if (thatScheme == null) {\r\n        return;\r\n    }\r\n    URI thisUri = canonicalizeUri(fsUri, defaultPort);\r\n    String thisScheme = thisUri.getScheme();\r\n    if (equalsIgnoreCase(thisScheme, thatScheme)) {\r\n        String thisHost = thisUri.getHost();\r\n        String thatHost = pathUri.getHost();\r\n        if (thatHost == null && thisHost != null) {\r\n            URI defaultUri = FileSystem.getDefaultUri(conf);\r\n            if (equalsIgnoreCase(thisScheme, defaultUri.getScheme())) {\r\n                pathUri = defaultUri;\r\n            } else {\r\n                pathUri = null;\r\n            }\r\n        }\r\n        if (pathUri != null) {\r\n            pathUri = canonicalizeUri(pathUri, defaultPort);\r\n            thatHost = pathUri.getHost();\r\n            if (thisHost == thatHost || (thisHost != null && equalsIgnoreCase(thisHost, thatHost))) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n    throw new IllegalArgumentException(\"Wrong FS \" + S3xLoginHelper.toString(pathUri) + \" -expected \" + fsUri);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return \"s3n\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(URI uri, Configuration conf) throws IOException\n{\r\n    super.initialize(uri, conf);\r\n    throw new IOException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileStatus getFileStatus(Path f) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FSDataInputStream open(Path f, int bufferSize) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FSDataOutputStream append(Path f, int bufferSize, Progressable progress) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean rename(Path src, Path dst) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileStatus[] listStatus(Path f) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWorkingDirectory(Path new_dir)\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3native",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission) throws IOException\n{\r\n    throw new UnsupportedOperationException(UNSUPPORTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "operationRetried",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void operationRetried(String text, Exception ex, int retries, boolean idempotent)\n{\r\n    LOG.info(\"{}: Retried {}: {}\", text, retries, ex.toString());\r\n    LOG.debug(\"Stack\", ex);\r\n    owner.operationRetried(text, ex, retries, idempotent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "retry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T retry(String action, String path, boolean idempotent, CallableRaisingIOE<T> operation) throws IOException\n{\r\n    activateAuditSpan();\r\n    return invoker.retry(action, path, idempotent, operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpan getAuditSpan()\n{\r\n    return auditSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "activateAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpan activateAuditSpan()\n{\r\n    return auditSpan.activate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deactivateAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deactivateAuditSpan()\n{\r\n    auditSpan.deactivate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createPutObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "PutObjectRequest createPutObjectRequest(String destKey, InputStream inputStream, long length, final Map<String, String> headers)\n{\r\n    activateAuditSpan();\r\n    ObjectMetadata objectMetadata = newObjectMetadata(length);\r\n    if (headers != null) {\r\n        objectMetadata.setUserMetadata(headers);\r\n    }\r\n    return getRequestFactory().newPutObjectRequest(destKey, objectMetadata, inputStream);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createPutObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "PutObjectRequest createPutObjectRequest(String dest, File sourceFile)\n{\r\n    Preconditions.checkState(sourceFile.length() < Integer.MAX_VALUE, \"File length is too big for a single PUT upload\");\r\n    activateAuditSpan();\r\n    return getRequestFactory().newPutObjectRequest(dest, newObjectMetadata((int) sourceFile.length()), sourceFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "writeSuccessful",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void writeSuccessful(long length)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "writeFailed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeFailed(Exception ex)\n{\r\n    LOG.debug(\"Write to {} failed\", this, ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ObjectMetadata newObjectMetadata(long length)\n{\r\n    return getRequestFactory().newObjectMetadata(length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initiateMultiPartUpload",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String initiateMultiPartUpload(String destKey) throws IOException\n{\r\n    LOG.debug(\"Initiating Multipart upload to {}\", destKey);\r\n    try (AuditSpan span = activateAuditSpan()) {\r\n        return retry(\"initiate MultiPartUpload\", destKey, true, () -> {\r\n            final InitiateMultipartUploadRequest initiateMPURequest = getRequestFactory().newMultipartUploadRequest(destKey);\r\n            return owner.initiateMultipartUpload(initiateMPURequest).getUploadId();\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "finalizeMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CompleteMultipartUploadResult finalizeMultipartUpload(String destKey, String uploadId, List<PartETag> partETags, long length, Retried retrying) throws IOException\n{\r\n    if (partETags.isEmpty()) {\r\n        throw new PathIOException(destKey, \"No upload parts in multipart upload\");\r\n    }\r\n    try (AuditSpan span = activateAuditSpan()) {\r\n        CompleteMultipartUploadResult uploadResult;\r\n        uploadResult = invoker.retry(\"Completing multipart upload\", destKey, true, retrying, () -> {\r\n            final CompleteMultipartUploadRequest request = getRequestFactory().newCompleteMultipartUploadRequest(destKey, uploadId, partETags);\r\n            return owner.getAmazonS3Client().completeMultipartUpload(request);\r\n        });\r\n        owner.finishedWrite(destKey, length, uploadResult.getETag(), uploadResult.getVersionId());\r\n        return uploadResult;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "completeMPUwithRetries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CompleteMultipartUploadResult completeMPUwithRetries(String destKey, String uploadId, List<PartETag> partETags, long length, AtomicInteger errorCount) throws IOException\n{\r\n    checkNotNull(uploadId);\r\n    checkNotNull(partETags);\r\n    LOG.debug(\"Completing multipart upload {} with {} parts\", uploadId, partETags.size());\r\n    return finalizeMultipartUpload(destKey, uploadId, partETags, length, (text, e, r, i) -> errorCount.incrementAndGet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abortMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abortMultipartUpload(String destKey, String uploadId, boolean shouldRetry, Retried retrying) throws IOException\n{\r\n    if (shouldRetry) {\r\n        invoker.retry(\"Aborting multipart upload ID \" + uploadId, destKey, true, retrying, withinAuditSpan(getAuditSpan(), () -> owner.abortMultipartUpload(destKey, uploadId)));\r\n    } else {\r\n        once(\"Aborting multipart upload ID \" + uploadId, destKey, withinAuditSpan(getAuditSpan(), () -> owner.abortMultipartUpload(destKey, uploadId)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abortMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortMultipartUpload(MultipartUpload upload) throws IOException\n{\r\n    invoker.retry(\"Aborting multipart commit\", upload.getKey(), true, withinAuditSpan(getAuditSpan(), () -> owner.abortMultipartUpload(upload)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abortMultipartUploadsUnderPath",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int abortMultipartUploadsUnderPath(String prefix) throws IOException\n{\r\n    LOG.debug(\"Aborting multipart uploads under {}\", prefix);\r\n    int count = 0;\r\n    List<MultipartUpload> multipartUploads = listMultipartUploads(prefix);\r\n    LOG.debug(\"Number of outstanding uploads: {}\", multipartUploads.size());\r\n    for (MultipartUpload upload : multipartUploads) {\r\n        try {\r\n            abortMultipartUpload(upload);\r\n            count++;\r\n        } catch (FileNotFoundException e) {\r\n            LOG.debug(\"Already aborted: {}\", upload.getKey(), e);\r\n        }\r\n    }\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<MultipartUpload> listMultipartUploads(final String prefix) throws IOException\n{\r\n    activateAuditSpan();\r\n    return owner.listMultipartUploads(prefix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abortMultipartCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortMultipartCommit(String destKey, String uploadId) throws IOException\n{\r\n    abortMultipartUpload(destKey, uploadId, true, invoker.getRetryCallback());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newUploadPartRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UploadPartRequest newUploadPartRequest(String destKey, String uploadId, int partNumber, int size, InputStream uploadStream, File sourceFile, Long offset) throws IOException\n{\r\n    return once(\"upload part request\", destKey, withinAuditSpan(getAuditSpan(), () -> getRequestFactory().newUploadPartRequest(destKey, uploadId, partNumber, size, uploadStream, sourceFile, offset)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"WriteOperationHelper {bucket=\").append(bucket);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "putObject",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PutObjectResult putObject(PutObjectRequest putObjectRequest) throws IOException\n{\r\n    return retry(\"Writing Object\", putObjectRequest.getKey(), true, withinAuditSpan(getAuditSpan(), () -> owner.putObjectDirect(putObjectRequest)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "uploadObject",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UploadResult uploadObject(PutObjectRequest putObjectRequest) throws IOException\n{\r\n    return retry(\"Writing Object\", putObjectRequest.getKey(), true, withinAuditSpan(getAuditSpan(), () -> owner.executePut(putObjectRequest, null)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "revertCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void revertCommit(String destKey) throws IOException\n{\r\n    once(\"revert commit\", destKey, withinAuditSpan(getAuditSpan(), () -> {\r\n        Path destPath = owner.keyToQualifiedPath(destKey);\r\n        owner.deleteObjectAtPath(destPath, destKey, true);\r\n        owner.maybeCreateFakeParentDirectory(destPath);\r\n    }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "commitUpload",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CompleteMultipartUploadResult commitUpload(String destKey, String uploadId, List<PartETag> partETags, long length) throws IOException\n{\r\n    checkNotNull(uploadId);\r\n    checkNotNull(partETags);\r\n    LOG.debug(\"Completing multipart upload {} with {} parts\", uploadId, partETags.size());\r\n    return finalizeMultipartUpload(destKey, uploadId, partETags, length, Invoker.NO_OP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "uploadPart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UploadPartResult uploadPart(UploadPartRequest request) throws IOException\n{\r\n    return retry(\"upload part #\" + request.getPartNumber() + \" upload ID \" + request.getUploadId(), request.getKey(), true, withinAuditSpan(getAuditSpan(), () -> owner.uploadPart(request)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newSelectRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SelectObjectContentRequest newSelectRequest(Path path)\n{\r\n    try (AuditSpan span = getAuditSpan()) {\r\n        return getRequestFactory().newSelectRequest(storeContext.pathToKey(path));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "select",
  "errType" : [ "AmazonS3Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "SelectObjectContentResult select(final Path source, final SelectObjectContentRequest request, final String action) throws IOException\n{\r\n    String bucketName = request.getBucketName();\r\n    Preconditions.checkArgument(bucket.equals(bucketName), \"wrong bucket: %s\", bucketName);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Initiating select call {} {}\", source, request.getExpression());\r\n        LOG.debug(SelectBinding.toString(request));\r\n    }\r\n    return invoker.retry(action, source.toString(), true, withinAuditSpan(getAuditSpan(), () -> {\r\n        try (DurationInfo ignored = new DurationInfo(LOG, \"S3 Select operation\")) {\r\n            try {\r\n                return owner.getAmazonS3Client().selectObjectContent(request);\r\n            } catch (AmazonS3Exception e) {\r\n                LOG.error(\"Failure of S3 Select request against {}\", source);\r\n                LOG.debug(\"S3 Select request against {}:\\n{}\", source, SelectBinding.toString(request), e);\r\n                throw e;\r\n            }\r\n        }\r\n    }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpan createSpan(final String operation, @Nullable final String path1, @Nullable final String path2) throws IOException\n{\r\n    return auditSpanSource.createSpan(operation, path1, path2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementWriteOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementWriteOperations()\n{\r\n    owner.incrementWriteOperations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    deactivateAuditSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRequestFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RequestFactory getRequestFactory()\n{\r\n    return requestFactory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "setupJob",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setupJob(JobContext context) throws IOException\n{\r\n    Path outputPath = getOutputPath();\r\n    FileSystem fs = getDestFS();\r\n    ConflictResolution conflictResolution = getConflictResolutionMode(context, fs.getConf());\r\n    LOG.info(\"Conflict Resolution mode is {}\", conflictResolution);\r\n    try {\r\n        final FileStatus status = fs.getFileStatus(outputPath);\r\n        if (!status.isDirectory()) {\r\n            throw new PathExistsException(outputPath.toString(), \"output path is not a directory: \" + InternalCommitterConstants.E_DEST_EXISTS);\r\n        }\r\n        switch(conflictResolution) {\r\n            case FAIL:\r\n                throw failDestinationExists(outputPath, \"Setting job as \" + getRole());\r\n            case APPEND:\r\n            case REPLACE:\r\n                LOG.debug(\"Destination directory exists; conflict policy permits this\");\r\n        }\r\n    } catch (FileNotFoundException ignored) {\r\n    }\r\n    super.setupJob(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\staging",
  "methodName" : "preCommitJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void preCommitJob(final JobContext context, final ActiveCommit pending) throws IOException\n{\r\n    super.preCommitJob(context, pending);\r\n    Path outputPath = getOutputPath();\r\n    FileSystem fs = getDestFS();\r\n    Configuration fsConf = fs.getConf();\r\n    switch(getConflictResolutionMode(context, fsConf)) {\r\n        case FAIL:\r\n            break;\r\n        case APPEND:\r\n            break;\r\n        case REPLACE:\r\n            if (fs.delete(outputPath, true)) {\r\n                LOG.info(\"{}: removed output path to be replaced: {}\", getRole(), outputPath);\r\n            }\r\n            break;\r\n        default:\r\n            throw new IOException(getRole() + \": unknown conflict resolution mode: \" + getConflictResolutionMode(context, fsConf));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getServiceName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getServiceName()\n{\r\n    return new Text(SERVICE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "isTokenRequired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isTokenRequired()\n{\r\n    return UserGroupInformation.isSecurityEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "addDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<?> addDelegationTokens(Configuration conf, Credentials creds, String renewer, String url) throws Exception\n{\r\n    if (!url.startsWith(getServiceName().toString())) {\r\n        url = getServiceName().toString() + \"://\" + url;\r\n    }\r\n    FileSystem fs = FileSystem.get(URI.create(url), conf);\r\n    Token<?> token = fs.getDelegationToken(renewer);\r\n    if (token == null) {\r\n        throw new DelegationTokenIOException(FETCH_FAILED + \": \" + url);\r\n    }\r\n    creds.addToken(token.getService(), token);\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getETag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getETag()\n{\r\n    return getEtag();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEtag()\n{\r\n    return eTag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getVersionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getVersionId()\n{\r\n    return versionId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    return super.equals(o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toS3AFileStatus",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "S3AFileStatus toS3AFileStatus()\n{\r\n    return new S3AFileStatus(getPath(), isDirectory(), isEmptyDirectory, getLen(), getModificationTime(), getBlockSize(), getOwner(), getEtag(), getVersionId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(super.toString());\r\n    sb.append(\"[eTag='\").append(eTag != null ? eTag : \"\").append('\\'');\r\n    sb.append(\", versionId='\").append(versionId != null ? versionId : \"\").append('\\'');\r\n    sb.append(']');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "requiresDelayedCommitOutputInFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean requiresDelayedCommitOutputInFileSystem()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupJob(JobContext context) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Setup Job %s\", jobIdString(context))) {\r\n        super.setupJob(context);\r\n        Path jobAttemptPath = getJobAttemptPath(context);\r\n        getDestinationFS(jobAttemptPath, context.getConfiguration()).mkdirs(jobAttemptPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "listPendingUploadsToCommit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ActiveCommit listPendingUploadsToCommit(JobContext context) throws IOException\n{\r\n    FileSystem fs = getDestFS();\r\n    return ActiveCommit.fromStatusList(fs, listAndFilter(fs, getJobAttemptPath(context), false, CommitOperations.PENDINGSET_FILTER));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "cleanupStagingDirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanupStagingDirs()\n{\r\n    Path path = magicSubdir(getOutputPath());\r\n    try (DurationInfo ignored = new DurationInfo(LOG, true, \"Deleting magic directory %s\", path)) {\r\n        Invoker.ignoreIOExceptions(LOG, \"cleanup magic directory\", path.toString(), () -> deleteWithWarning(getDestFS(), path, true));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext context) throws IOException\n{\r\n    Path taskAttemptPath = getTaskAttemptPath(context);\r\n    try (DurationInfo d = new DurationInfo(LOG, \"needsTaskCommit task %s\", context.getTaskAttemptID())) {\r\n        return taskAttemptPath.getFileSystem(context.getConfiguration()).exists(taskAttemptPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "commitTask",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void commitTask(TaskAttemptContext context) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Commit task %s\", context.getTaskAttemptID())) {\r\n        PendingSet commits = innerCommitTask(context);\r\n        LOG.info(\"Task {} committed {} files\", context.getTaskAttemptID(), commits.size());\r\n    } catch (IOException e) {\r\n        getCommitOperations().taskCompleted(false);\r\n        throw e;\r\n    } finally {\r\n        deleteTaskAttemptPathQuietly(context);\r\n        destroyThreadPool();\r\n        resetCommonContext();\r\n    }\r\n    getCommitOperations().taskCompleted(true);\r\n    LOG.debug(\"aggregate statistics\\n{}\", demandStringifyIOStatistics(getIOStatistics()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "innerCommitTask",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "PendingSet innerCommitTask(TaskAttemptContext context) throws IOException\n{\r\n    Path taskAttemptPath = getTaskAttemptPath(context);\r\n    CommitOperations actions = getCommitOperations();\r\n    Pair<PendingSet, List<Pair<LocatedFileStatus, IOException>>> loaded = actions.loadSinglePendingCommits(taskAttemptPath, true);\r\n    PendingSet pendingSet = loaded.getKey();\r\n    List<Pair<LocatedFileStatus, IOException>> failures = loaded.getValue();\r\n    if (!failures.isEmpty()) {\r\n        LOG.error(\"At least one commit file could not be read: failing\");\r\n        abortPendingUploads(context, pendingSet.getCommits(), true);\r\n        throw failures.get(0).getValue();\r\n    }\r\n    String jobId = getUUID();\r\n    String taskId = String.valueOf(context.getTaskAttemptID());\r\n    for (SinglePendingCommit commit : pendingSet.getCommits()) {\r\n        commit.setJobId(jobId);\r\n        commit.setTaskId(taskId);\r\n    }\r\n    pendingSet.putExtraData(TASK_ATTEMPT_ID, taskId);\r\n    pendingSet.setJobId(jobId);\r\n    Path jobAttemptPath = getJobAttemptPath(context);\r\n    TaskAttemptID taskAttemptID = context.getTaskAttemptID();\r\n    Path taskOutcomePath = new Path(jobAttemptPath, taskAttemptID.getTaskID().toString() + CommitConstants.PENDINGSET_SUFFIX);\r\n    LOG.info(\"Saving work of {} to {}\", taskAttemptID, taskOutcomePath);\r\n    LOG.debug(\"task statistics\\n{}\", IOStatisticsLogging.demandStringifyIOStatisticsSource(pendingSet));\r\n    try {\r\n        pendingSet.save(getDestFS(), taskOutcomePath, true);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Failed to save task commit data to {} \", taskOutcomePath, e);\r\n        abortPendingUploads(context, pendingSet.getCommits(), true);\r\n        throw e;\r\n    }\r\n    return pendingSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void abortTask(TaskAttemptContext context) throws IOException\n{\r\n    Path attemptPath = getTaskAttemptPath(context);\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Abort task %s\", context.getTaskAttemptID())) {\r\n        getCommitOperations().abortAllSinglePendingCommits(attemptPath, true);\r\n    } finally {\r\n        deleteQuietly(attemptPath.getFileSystem(context.getConfiguration()), attemptPath, true);\r\n        destroyThreadPool();\r\n        resetCommonContext();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(int appAttemptId)\n{\r\n    return getMagicJobAttemptPath(getUUID(), getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context)\n{\r\n    return getMagicTaskAttemptPath(context, getUUID(), getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getBaseTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getBaseTaskAttemptPath(TaskAttemptContext context)\n{\r\n    return getBaseMagicTaskAttemptPath(context, getUUID(), getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "getTempTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTempTaskAttemptPath(TaskAttemptContext context)\n{\r\n    return CommitUtilsWithMR.getTempTaskAttemptPath(context, getUUID(), getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"MagicCommitter{\");\r\n    sb.append(super.toString());\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "serializer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JsonSerialization<SinglePendingCommit> serializer()\n{\r\n    return new JsonSerialization<>(SinglePendingCommit.class, false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SinglePendingCommit load(FileSystem fs, Path path) throws IOException\n{\r\n    SinglePendingCommit instance = serializer().load(fs, path);\r\n    instance.filename = path.toString();\r\n    instance.validate();\r\n    return instance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "readObject",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readObject(ObjectInputStream inStream) throws IOException, ClassNotFoundException\n{\r\n    inStream.defaultReadObject();\r\n    validate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "touch",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void touch(long millis)\n{\r\n    created = millis;\r\n    saved = millis;\r\n    date = new Date(millis).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "bindCommitData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void bindCommitData(List<PartETag> parts) throws ValidationFailure\n{\r\n    etags = new ArrayList<>(parts.size());\r\n    int counter = 1;\r\n    for (PartETag part : parts) {\r\n        verify(part.getPartNumber() == counter, \"Expected part number %s but got %s\", counter, part.getPartNumber());\r\n        etags.add(part.getETag());\r\n        counter++;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void validate() throws ValidationFailure\n{\r\n    verify(version == VERSION, \"Wrong version: %s\", version);\r\n    verify(StringUtils.isNotEmpty(bucket), \"Empty bucket\");\r\n    verify(StringUtils.isNotEmpty(destinationKey), \"Empty destination\");\r\n    verify(StringUtils.isNotEmpty(uploadId), \"Empty uploadId\");\r\n    verify(length >= 0, \"Invalid length: \" + length);\r\n    destinationPath();\r\n    verify(etags != null, \"No etag list\");\r\n    validateCollectionClass(etags, String.class);\r\n    for (String etag : etags) {\r\n        verify(StringUtils.isNotEmpty(etag), \"Empty etag\");\r\n    }\r\n    if (extraData != null) {\r\n        validateCollectionClass(extraData.keySet(), String.class);\r\n        validateCollectionClass(extraData.values(), String.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"SinglePendingCommit{\");\r\n    sb.append(\"version=\").append(version);\r\n    sb.append(\", uri='\").append(uri).append('\\'');\r\n    sb.append(\", destination='\").append(destinationKey).append('\\'');\r\n    sb.append(\", uploadId='\").append(uploadId).append('\\'');\r\n    sb.append(\", created=\").append(created);\r\n    sb.append(\", saved=\").append(saved);\r\n    sb.append(\", size=\").append(length);\r\n    sb.append(\", date='\").append(date).append('\\'');\r\n    sb.append(\", jobId='\").append(jobId).append('\\'');\r\n    sb.append(\", taskId='\").append(taskId).append('\\'');\r\n    sb.append(\", notes='\").append(text).append('\\'');\r\n    if (etags != null) {\r\n        sb.append(\", etags=[\");\r\n        sb.append(join(\",\", etags));\r\n        sb.append(']');\r\n    } else {\r\n        sb.append(\", etags=null\");\r\n    }\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] toBytes() throws IOException\n{\r\n    validate();\r\n    return serializer().toBytes(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void save(FileSystem fs, Path path, boolean overwrite) throws IOException\n{\r\n    serializer().save(fs, path, this, overwrite);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "destinationPath",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path destinationPath()\n{\r\n    Preconditions.checkState(StringUtils.isNotEmpty(uri), \"Empty uri\");\r\n    try {\r\n        return new Path(new URI(uri));\r\n    } catch (URISyntaxException e) {\r\n        throw new IllegalStateException(\"Cannot parse URI \" + uri);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getPartCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPartCount()\n{\r\n    return etags.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterator<String> iterator()\n{\r\n    return etags.iterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getVersion()\n{\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVersion(int version)\n{\r\n    this.version = version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getFilename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFilename()\n{\r\n    return filename;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setFilename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFilename(String filename)\n{\r\n    this.filename = filename;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUri(String uri)\n{\r\n    this.uri = uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getUploadId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUploadId()\n{\r\n    return uploadId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setUploadId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUploadId(String uploadId)\n{\r\n    this.uploadId = uploadId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBucket()\n{\r\n    return bucket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBucket(String bucket)\n{\r\n    this.bucket = bucket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getDestinationKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDestinationKey()\n{\r\n    return destinationKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setDestinationKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDestinationKey(String destinationKey)\n{\r\n    this.destinationKey = destinationKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCreated()\n{\r\n    return created;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCreated(long created)\n{\r\n    this.created = created;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getSaved",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSaved()\n{\r\n    return saved;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setSaved",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSaved(long saved)\n{\r\n    this.saved = saved;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDate()\n{\r\n    return date;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDate(String date)\n{\r\n    this.date = date;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobId(String jobId)\n{\r\n    this.jobId = jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskId(String taskId)\n{\r\n    this.taskId = taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getText",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getText()\n{\r\n    return text;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setText",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setText(String text)\n{\r\n    this.text = text;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getEtags",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getEtags()\n{\r\n    return etags;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setEtags",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEtags(List<String> etags)\n{\r\n    this.etags = etags;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getExtraData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getExtraData()\n{\r\n    return extraData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setExtraData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setExtraData(Map<String, String> extraData)\n{\r\n    this.extraData = extraData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "putExtraData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void putExtraData(String key, String value)\n{\r\n    extraData.put(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength()\n{\r\n    return length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLength(long length)\n{\r\n    this.length = length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsSnapshot getIOStatistics()\n{\r\n    return iostats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "setIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIOStatistics(final IOStatisticsSnapshot ioStatistics)\n{\r\n    this.iostats = ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "analyze",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "RequestInfo analyze(T request)\n{\r\n    if (request instanceof AbortMultipartUploadRequest) {\r\n        return writing(MULTIPART_UPLOAD_ABORTED, ((AbortMultipartUploadRequest) request).getKey(), 0);\r\n    } else if (request instanceof CompleteMultipartUploadRequest) {\r\n        CompleteMultipartUploadRequest r = (CompleteMultipartUploadRequest) request;\r\n        return writing(MULTIPART_UPLOAD_COMPLETED, r.getKey(), r.getPartETags().size());\r\n    } else if (request instanceof DeleteObjectRequest) {\r\n        return writing(OBJECT_DELETE_REQUEST, ((DeleteObjectRequest) request).getKey(), 1);\r\n    } else if (request instanceof DeleteObjectsRequest) {\r\n        DeleteObjectsRequest r = (DeleteObjectsRequest) request;\r\n        List<DeleteObjectsRequest.KeyVersion> keys = r.getKeys();\r\n        return writing(OBJECT_BULK_DELETE_REQUEST, keys.isEmpty() ? null : keys.get(0).getKey(), keys.size());\r\n    } else if (request instanceof GetBucketLocationRequest) {\r\n        GetBucketLocationRequest r = (GetBucketLocationRequest) request;\r\n        return reading(STORE_EXISTS_PROBE, r.getBucketName(), 0);\r\n    } else if (request instanceof GetObjectMetadataRequest) {\r\n        return reading(ACTION_HTTP_HEAD_REQUEST, ((GetObjectMetadataRequest) request).getKey(), 0);\r\n    } else if (request instanceof GetObjectRequest) {\r\n        GetObjectRequest r = (GetObjectRequest) request;\r\n        long[] range = r.getRange();\r\n        long size = range == null ? -1 : range[1] - range[0];\r\n        return reading(ACTION_HTTP_GET_REQUEST, r.getKey(), size);\r\n    } else if (request instanceof InitiateMultipartUploadRequest) {\r\n        return writing(MULTIPART_UPLOAD_STARTED, ((InitiateMultipartUploadRequest) request).getKey(), 0);\r\n    } else if (request instanceof ListMultipartUploadsRequest) {\r\n        ListMultipartUploadsRequest r = (ListMultipartUploadsRequest) request;\r\n        return reading(MULTIPART_UPLOAD_LIST, r.getPrefix(), r.getMaxUploads());\r\n    } else if (request instanceof ListObjectsRequest) {\r\n        ListObjectsRequest r = (ListObjectsRequest) request;\r\n        return reading(OBJECT_LIST_REQUEST, r.getPrefix(), r.getMaxKeys());\r\n    } else if (request instanceof ListNextBatchOfObjectsRequest) {\r\n        ListNextBatchOfObjectsRequest r = (ListNextBatchOfObjectsRequest) request;\r\n        ObjectListing l = r.getPreviousObjectListing();\r\n        String prefix = \"\";\r\n        int size = 0;\r\n        if (l != null) {\r\n            prefix = l.getPrefix();\r\n            size = l.getMaxKeys();\r\n        }\r\n        return reading(OBJECT_LIST_REQUEST, prefix, size);\r\n    } else if (request instanceof ListObjectsV2Request) {\r\n        ListObjectsV2Request r = (ListObjectsV2Request) request;\r\n        return reading(OBJECT_LIST_REQUEST, r.getPrefix(), r.getMaxKeys());\r\n    } else if (request instanceof PutObjectRequest) {\r\n        PutObjectRequest r = (PutObjectRequest) request;\r\n        return writing(OBJECT_PUT_REQUEST, r.getKey(), 0);\r\n    } else if (request instanceof SelectObjectContentRequest) {\r\n        SelectObjectContentRequest r = (SelectObjectContentRequest) request;\r\n        return reading(OBJECT_SELECT_REQUESTS, r.getKey(), 1);\r\n    } else if (request instanceof UploadPartRequest) {\r\n        UploadPartRequest r = (UploadPartRequest) request;\r\n        return writing(MULTIPART_UPLOAD_PART_PUT, r.getKey(), r.getPartSize());\r\n    }\r\n    return writing(request.getClass().getName(), null, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "request",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RequestInfo request(final String verb, final boolean mutating, final String key, final Number size)\n{\r\n    return new RequestInfo(verb, mutating, key, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "reading",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RequestInfo reading(final String verb, final String key, final Number size)\n{\r\n    return request(verb, false, key, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "writing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RequestInfo writing(final String verb, final String key, final Number size)\n{\r\n    return request(verb, true, key, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "isRequestNotAlwaysInSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRequestNotAlwaysInSpan(final Object request)\n{\r\n    return request instanceof CopyPartRequest || request instanceof CompleteMultipartUploadRequest || request instanceof GetBucketLocationRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "toSafeLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long toSafeLong(final Number size)\n{\r\n    return size != null ? size.longValue() : 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "configureSocketFactory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void configureSocketFactory(final ClientConfiguration awsConf, final DelegatingSSLSocketFactory.SSLChannelMode channelMode) throws IOException\n{\r\n    DelegatingSSLSocketFactory.initializeDefaultFactory(channelMode);\r\n    awsConf.getApacheHttpClientConfig().setSslSocketFactory(new SSLConnectionSocketFactory(DelegatingSSLSocketFactory.getDefaultFactory(), (HostnameVerifier) null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "v1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ListResult v1(ObjectListing result)\n{\r\n    return new S3ListResult(result, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "v2",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ListResult v2(ListObjectsV2Result result)\n{\r\n    return new S3ListResult(null, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isV1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isV1()\n{\r\n    return v1Result != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getV1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ObjectListing getV1()\n{\r\n    return v1Result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getV2",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListObjectsV2Result getV2()\n{\r\n    return v2Result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getObjectSummaries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<S3ObjectSummary> getObjectSummaries()\n{\r\n    if (isV1()) {\r\n        return v1Result.getObjectSummaries();\r\n    } else {\r\n        return v2Result.getObjectSummaries();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isTruncated",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isTruncated()\n{\r\n    if (isV1()) {\r\n        return v1Result.isTruncated();\r\n    } else {\r\n        return v2Result.isTruncated();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCommonPrefixes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> getCommonPrefixes()\n{\r\n    if (isV1()) {\r\n        return v1Result.getCommonPrefixes();\r\n    } else {\r\n        return v2Result.getCommonPrefixes();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "objectSummaryKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> objectSummaryKeys()\n{\r\n    return getObjectSummaries().stream().map(S3ObjectSummary::getKey).collect(Collectors.toList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hasPrefixesOrObjects",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasPrefixesOrObjects()\n{\r\n    return !(getCommonPrefixes()).isEmpty() || !getObjectSummaries().isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "representsEmptyDirectory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean representsEmptyDirectory(final String dirKey)\n{\r\n    List<String> keys = objectSummaryKeys();\r\n    return keys.size() == 1 && keys.contains(dirKey) && getCommonPrefixes().isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "logAtDebug",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void logAtDebug(Logger log)\n{\r\n    Collection<String> prefixes = getCommonPrefixes();\r\n    Collection<S3ObjectSummary> summaries = getObjectSummaries();\r\n    log.debug(\"Prefix count = {}; object count={}\", prefixes.size(), summaries.size());\r\n    for (S3ObjectSummary summary : summaries) {\r\n        log.debug(\"Summary: {} {}\", summary.getKey(), summary.getSize());\r\n    }\r\n    for (String prefix : prefixes) {\r\n        log.debug(\"Prefix: {}\", prefix);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCause",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonS3Exception getCause()\n{\r\n    return (AmazonS3Exception) super.getCause();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getErrorResponseXml",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getErrorResponseXml()\n{\r\n    return getCause().getErrorResponseXml();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getAdditionalDetails",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, String> getAdditionalDetails()\n{\r\n    return getCause().getAdditionalDetails();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getExtendedRequestId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getExtendedRequestId()\n{\r\n    return getCause().getExtendedRequestId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "listObjects",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<S3AFileStatus> listObjects(final Path path, final String key) throws IOException\n{\r\n    return operationCallbacks.listObjects(path, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "removeKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeKeys(final List<DeleteObjectsRequest.KeyVersion> keysToDelete, final boolean deleteFakeDir) throws MultiObjectDeleteException, AmazonClientException, IOException\n{\r\n    operationCallbacks.removeKeys(keysToDelete, deleteFakeDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "prepareRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T prepareRequest(T t)\n{\r\n    return requestPreparer != null ? requestPreparer.prepareRequest(t) : t;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getCannedACL",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CannedAccessControlList getCannedACL()\n{\r\n    return cannedACL;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBucket()\n{\r\n    return bucket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "generateSSEAwsKeyParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Optional<SSEAwsKeyManagementParams> generateSSEAwsKeyParams()\n{\r\n    return EncryptionSecretOperations.createSSEAwsKeyManagementParams(encryptionSecrets);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "generateSSECustomerKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Optional<SSECustomerKey> generateSSECustomerKey()\n{\r\n    return EncryptionSecretOperations.createSSECustomerKey(encryptionSecrets);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getServerSideEncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AEncryptionMethods getServerSideEncryptionAlgorithm()\n{\r\n    return encryptionSecrets.getEncryptionMethod();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getContentEncoding",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getContentEncoding()\n{\r\n    return contentEncoding;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setOptionalUploadPartRequestParameters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOptionalUploadPartRequestParameters(UploadPartRequest request)\n{\r\n    generateSSECustomerKey().ifPresent(request::setSSECustomerKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setOptionalGetObjectMetadataParameters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOptionalGetObjectMetadataParameters(GetObjectMetadataRequest request)\n{\r\n    generateSSECustomerKey().ifPresent(request::setSSECustomerKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setOptionalMultipartUploadRequestParameters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOptionalMultipartUploadRequestParameters(InitiateMultipartUploadRequest request)\n{\r\n    generateSSEAwsKeyParams().ifPresent(request::setSSEAwsKeyManagementParams);\r\n    generateSSECustomerKey().ifPresent(request::setSSECustomerKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setOptionalPutRequestParameters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOptionalPutRequestParameters(PutObjectRequest request)\n{\r\n    generateSSEAwsKeyParams().ifPresent(request::setSSEAwsKeyManagementParams);\r\n    generateSSECustomerKey().ifPresent(request::setSSECustomerKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setOptionalObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setOptionalObjectMetadata(ObjectMetadata metadata, boolean isDirectoryMarker)\n{\r\n    final S3AEncryptionMethods algorithm = getServerSideEncryptionAlgorithm();\r\n    if (S3AEncryptionMethods.SSE_S3 == algorithm) {\r\n        metadata.setSSEAlgorithm(algorithm.getMethod());\r\n    }\r\n    if (contentEncoding != null && !isDirectoryMarker) {\r\n        metadata.setContentEncoding(contentEncoding);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ObjectMetadata newObjectMetadata(long length)\n{\r\n    return createObjectMetadata(length, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ObjectMetadata createObjectMetadata(long length, boolean isDirectoryMarker)\n{\r\n    final ObjectMetadata om = new ObjectMetadata();\r\n    setOptionalObjectMetadata(om, isDirectoryMarker);\r\n    if (length >= 0) {\r\n        om.setContentLength(length);\r\n    }\r\n    return om;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newCopyObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "CopyObjectRequest newCopyObjectRequest(String srcKey, String dstKey, ObjectMetadata srcom)\n{\r\n    CopyObjectRequest copyObjectRequest = new CopyObjectRequest(getBucket(), srcKey, getBucket(), dstKey);\r\n    ObjectMetadata dstom = newObjectMetadata(srcom.getContentLength());\r\n    HeaderProcessing.cloneObjectMetadata(srcom, dstom);\r\n    setOptionalObjectMetadata(dstom, false);\r\n    copyEncryptionParameters(srcom, copyObjectRequest);\r\n    copyObjectRequest.setCannedAccessControlList(cannedACL);\r\n    copyObjectRequest.setNewObjectMetadata(dstom);\r\n    Optional.ofNullable(srcom.getStorageClass()).ifPresent(copyObjectRequest::setStorageClass);\r\n    return prepareRequest(copyObjectRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "copyEncryptionParameters",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void copyEncryptionParameters(ObjectMetadata srcom, CopyObjectRequest copyObjectRequest)\n{\r\n    String sourceKMSId = srcom.getSSEAwsKmsKeyId();\r\n    if (isNotEmpty(sourceKMSId)) {\r\n        LOG.debug(\"Propagating SSE-KMS settings from source {}\", sourceKMSId);\r\n        copyObjectRequest.setSSEAwsKeyManagementParams(new SSEAwsKeyManagementParams(sourceKMSId));\r\n    }\r\n    switch(getServerSideEncryptionAlgorithm()) {\r\n        case SSE_S3:\r\n            break;\r\n        case SSE_C:\r\n            generateSSECustomerKey().ifPresent(customerKey -> {\r\n                copyObjectRequest.setSourceSSECustomerKey(customerKey);\r\n                copyObjectRequest.setDestinationSSECustomerKey(customerKey);\r\n            });\r\n            break;\r\n        case SSE_KMS:\r\n            generateSSEAwsKeyParams().ifPresent(copyObjectRequest::setSSEAwsKeyManagementParams);\r\n            break;\r\n        default:\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newPutObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "PutObjectRequest newPutObjectRequest(String key, ObjectMetadata metadata, File srcfile)\n{\r\n    Preconditions.checkNotNull(srcfile);\r\n    PutObjectRequest putObjectRequest = new PutObjectRequest(getBucket(), key, srcfile);\r\n    setOptionalPutRequestParameters(putObjectRequest);\r\n    putObjectRequest.setCannedAcl(cannedACL);\r\n    putObjectRequest.setMetadata(metadata);\r\n    return prepareRequest(putObjectRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newPutObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "PutObjectRequest newPutObjectRequest(String key, ObjectMetadata metadata, InputStream inputStream)\n{\r\n    Preconditions.checkNotNull(inputStream);\r\n    Preconditions.checkArgument(isNotEmpty(key), \"Null/empty key\");\r\n    PutObjectRequest putObjectRequest = new PutObjectRequest(getBucket(), key, inputStream, metadata);\r\n    setOptionalPutRequestParameters(putObjectRequest);\r\n    putObjectRequest.setCannedAcl(cannedACL);\r\n    return prepareRequest(putObjectRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newDirectoryMarkerRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "PutObjectRequest newDirectoryMarkerRequest(String directory)\n{\r\n    String key = directory.endsWith(\"/\") ? directory : (directory + \"/\");\r\n    final InputStream im = new InputStream() {\r\n\r\n        @Override\r\n        public int read() throws IOException {\r\n            return -1;\r\n        }\r\n    };\r\n    final ObjectMetadata md = createObjectMetadata(0L, true);\r\n    md.setContentType(HeaderProcessing.CONTENT_TYPE_X_DIRECTORY);\r\n    PutObjectRequest putObjectRequest = newPutObjectRequest(key, md, im);\r\n    return putObjectRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newListMultipartUploadsRequest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ListMultipartUploadsRequest newListMultipartUploadsRequest(String prefix)\n{\r\n    ListMultipartUploadsRequest request = new ListMultipartUploadsRequest(getBucket());\r\n    if (prefix != null) {\r\n        request.setPrefix(prefix);\r\n    }\r\n    return prepareRequest(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newAbortMultipartUploadRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbortMultipartUploadRequest newAbortMultipartUploadRequest(String destKey, String uploadId)\n{\r\n    return prepareRequest(new AbortMultipartUploadRequest(getBucket(), destKey, uploadId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newMultipartUploadRequest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "InitiateMultipartUploadRequest newMultipartUploadRequest(String destKey)\n{\r\n    final InitiateMultipartUploadRequest initiateMPURequest = new InitiateMultipartUploadRequest(getBucket(), destKey, newObjectMetadata(-1));\r\n    initiateMPURequest.setCannedACL(getCannedACL());\r\n    setOptionalMultipartUploadRequestParameters(initiateMPURequest);\r\n    return prepareRequest(initiateMPURequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newCompleteMultipartUploadRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CompleteMultipartUploadRequest newCompleteMultipartUploadRequest(String destKey, String uploadId, List<PartETag> partETags)\n{\r\n    return prepareRequest(new CompleteMultipartUploadRequest(bucket, destKey, uploadId, new ArrayList<>(partETags)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newGetObjectMetadataRequest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "GetObjectMetadataRequest newGetObjectMetadataRequest(String key)\n{\r\n    GetObjectMetadataRequest request = new GetObjectMetadataRequest(getBucket(), key);\r\n    setOptionalGetObjectMetadataParameters(request);\r\n    return prepareRequest(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newGetObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetObjectRequest newGetObjectRequest(String key)\n{\r\n    GetObjectRequest request = new GetObjectRequest(bucket, key);\r\n    generateSSECustomerKey().ifPresent(request::setSSECustomerKey);\r\n    return prepareRequest(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newUploadPartRequest",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "UploadPartRequest newUploadPartRequest(String destKey, String uploadId, int partNumber, int size, InputStream uploadStream, File sourceFile, long offset) throws PathIOException\n{\r\n    checkNotNull(uploadId);\r\n    checkArgument((uploadStream != null) ^ (sourceFile != null), \"Data source\");\r\n    checkArgument(size >= 0, \"Invalid partition size %s\", size);\r\n    checkArgument(partNumber > 0, \"partNumber must be between 1 and %s inclusive, but is %s\", DEFAULT_UPLOAD_PART_COUNT_LIMIT, partNumber);\r\n    LOG.debug(\"Creating part upload request for {} #{} size {}\", uploadId, partNumber, size);\r\n    final String pathErrorMsg = \"Number of parts in multipart upload exceeded.\" + \" Current part count = %s, Part count limit = %s \";\r\n    if (partNumber > multipartPartCountLimit) {\r\n        throw new PathIOException(destKey, String.format(pathErrorMsg, partNumber, multipartPartCountLimit));\r\n    }\r\n    UploadPartRequest request = new UploadPartRequest().withBucketName(getBucket()).withKey(destKey).withUploadId(uploadId).withPartNumber(partNumber).withPartSize(size);\r\n    if (uploadStream != null) {\r\n        request.setInputStream(uploadStream);\r\n    } else {\r\n        checkArgument(sourceFile.exists(), \"Source file does not exist: %s\", sourceFile);\r\n        checkArgument(sourceFile.isFile(), \"Source is not a file: %s\", sourceFile);\r\n        checkArgument(offset >= 0, \"Invalid offset %s\", offset);\r\n        long length = sourceFile.length();\r\n        checkArgument(offset == 0 || offset < length, \"Offset %s beyond length of file %s\", offset, length);\r\n        request.setFile(sourceFile);\r\n        request.setFileOffset(offset);\r\n    }\r\n    setOptionalUploadPartRequestParameters(request);\r\n    return prepareRequest(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newSelectRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SelectObjectContentRequest newSelectRequest(String key)\n{\r\n    SelectObjectContentRequest request = new SelectObjectContentRequest();\r\n    request.setBucketName(bucket);\r\n    request.setKey(key);\r\n    generateSSECustomerKey().ifPresent(request::setSSECustomerKey);\r\n    return prepareRequest(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newListObjectsV1Request",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ListObjectsRequest newListObjectsV1Request(final String key, final String delimiter, final int maxKeys)\n{\r\n    ListObjectsRequest request = new ListObjectsRequest().withBucketName(bucket).withMaxKeys(maxKeys).withPrefix(key);\r\n    if (delimiter != null) {\r\n        request.setDelimiter(delimiter);\r\n    }\r\n    return prepareRequest(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newListNextBatchOfObjectsRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ListNextBatchOfObjectsRequest newListNextBatchOfObjectsRequest(ObjectListing prev)\n{\r\n    return prepareRequest(new ListNextBatchOfObjectsRequest(prev));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newListObjectsV2Request",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ListObjectsV2Request newListObjectsV2Request(final String key, final String delimiter, final int maxKeys)\n{\r\n    final ListObjectsV2Request request = new ListObjectsV2Request().withBucketName(bucket).withMaxKeys(maxKeys).withPrefix(key);\r\n    if (delimiter != null) {\r\n        request.setDelimiter(delimiter);\r\n    }\r\n    return prepareRequest(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newDeleteObjectRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DeleteObjectRequest newDeleteObjectRequest(String key)\n{\r\n    return prepareRequest(new DeleteObjectRequest(bucket, key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newBulkDeleteRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DeleteObjectsRequest newBulkDeleteRequest(List<DeleteObjectsRequest.KeyVersion> keysToDelete)\n{\r\n    return prepareRequest(new DeleteObjectsRequest(bucket).withKeys(keysToDelete).withQuiet(true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "setEncryptionSecrets",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEncryptionSecrets(final EncryptionSecrets secrets)\n{\r\n    encryptionSecrets = secrets;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RequestFactoryBuilder builder()\n{\r\n    return new RequestFactoryBuilder();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean initialize() throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "outputImmediatelyVisible",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean outputImmediatelyVisible()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "aboutToComplete",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "boolean aboutToComplete(String uploadId, List<PartETag> parts, long bytesWritten, final IOStatistics iostatistics) throws IOException\n{\r\n    Preconditions.checkArgument(StringUtils.isNotEmpty(uploadId), \"empty/null upload ID: \" + uploadId);\r\n    Preconditions.checkArgument(parts != null, \"No uploaded parts list\");\r\n    Preconditions.checkArgument(!parts.isEmpty(), \"No uploaded parts to save\");\r\n    SinglePendingCommit commitData = new SinglePendingCommit();\r\n    commitData.touch(System.currentTimeMillis());\r\n    commitData.setDestinationKey(getDestKey());\r\n    commitData.setBucket(bucket);\r\n    commitData.setUri(path.toUri().toString());\r\n    commitData.setUploadId(uploadId);\r\n    commitData.setText(\"\");\r\n    commitData.setLength(bytesWritten);\r\n    commitData.bindCommitData(parts);\r\n    commitData.setIOStatistics(new IOStatisticsSnapshot(iostatistics));\r\n    byte[] bytes = commitData.toBytes();\r\n    LOG.info(\"Uncommitted data pending to file {};\" + \" commit metadata for {} parts in {}. size: {} byte(s)\", path.toUri(), parts.size(), pendingPartKey, bytesWritten);\r\n    LOG.debug(\"Closed MPU to {}, saved commit information to {}; data=:\\n{}\", path, pendingPartKey, commitData);\r\n    PutObjectRequest put = writer.createPutObjectRequest(pendingPartKey, new ByteArrayInputStream(bytes), bytes.length, null);\r\n    writer.uploadObject(put);\r\n    Map<String, String> headers = new HashMap<>();\r\n    headers.put(X_HEADER_MAGIC_MARKER, Long.toString(bytesWritten));\r\n    PutObjectRequest originalDestPut = writer.createPutObjectRequest(originalDestKey, new ByteArrayInputStream(EMPTY), 0, headers);\r\n    writer.uploadObject(originalDestPut);\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\magic",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"MagicCommitTracker{\");\r\n    sb.append(\", destKey=\").append(getDestKey());\r\n    sb.append(\", pendingPartKey='\").append(pendingPartKey).append('\\'');\r\n    sb.append(\", path=\").append(path);\r\n    sb.append(\", writer=\").append(writer);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validate() throws ValidationFailure",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] toBytes() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit\\files",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void save(FileSystem fs, Path path, boolean overwrite) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "activate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A activate()\n{\r\n    if (activationCallbacks != null) {\r\n        activationCallbacks.activate(this);\r\n    }\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "deactivate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deactivate()\n{\r\n    if (activationCallbacks != null) {\r\n        activationCallbacks.deactivate(this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"NoopSpan{\");\r\n    sb.append(\"id='\").append(getSpanId()).append('\\'');\r\n    sb.append(\"name='\").append(getOperationName()).append('\\'');\r\n    sb.append(\", path1='\").append(path1).append('\\'');\r\n    sb.append(\", path2='\").append(path2).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getThisBuilder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AMultipartUploaderBuilder getThisBuilder()\n{\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AMultipartUploader build() throws IllegalArgumentException, IOException\n{\r\n    return new S3AMultipartUploader(this, writeOperations, context, statistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToFileSystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void bindToFileSystem(final URI uri, final StoreContext context, final DelegationOperations delegationOperations) throws IOException\n{\r\n    requireServiceState(STATE.NOTINITED);\r\n    Preconditions.checkState(canonicalUri == null, \"bindToFileSystem called twice\");\r\n    this.canonicalUri = requireNonNull(uri);\r\n    this.storeContext = requireNonNull(context);\r\n    this.owner = context.getOwner();\r\n    this.policyProvider = delegationOperations;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getCanonicalUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getCanonicalUri()\n{\r\n    return canonicalUri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserGroupInformation getOwner()\n{\r\n    return owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getStoreContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StoreContext getStoreContext()\n{\r\n    return storeContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getPolicyProvider",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationOperations getPolicyProvider()\n{\r\n    return policyProvider;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "requireServiceState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireServiceState(final STATE state) throws IllegalStateException\n{\r\n    Preconditions.checkState(isInState(state), \"Required State: %s; Actual State %s\", state, getServiceState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "requireServiceStarted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void requireServiceStarted() throws IllegalStateException\n{\r\n    requireServiceState(STATE.STARTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    requireNonNull(canonicalUri, \"service does not have a canonical URI\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "getUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUsage()\n{\r\n    return USAGE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return MARKERS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "resetBindings",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetBindings()\n{\r\n    super.resetBindings();\r\n    storeContext = null;\r\n    operations = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "run",
  "errType" : [ "CommandFormat.UnknownOptionException", "UnknownStoreException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "int run(final String[] args, final PrintStream stream) throws ExitUtil.ExitException, Exception\n{\r\n    this.out = stream;\r\n    final List<String> parsedArgs;\r\n    try {\r\n        parsedArgs = parseArgs(args);\r\n    } catch (CommandFormat.UnknownOptionException e) {\r\n        errorln(getUsage());\r\n        throw new ExitUtil.ExitException(EXIT_USAGE, e.getMessage(), e);\r\n    }\r\n    if (parsedArgs.size() != 1) {\r\n        errorln(getUsage());\r\n        println(out, \"Supplied arguments: [\" + String.join(\", \", parsedArgs) + \"]\");\r\n        throw new ExitUtil.ExitException(EXIT_USAGE, String.format(E_ARGUMENTS, parsedArgs.size()));\r\n    }\r\n    CommandFormat command = getCommandFormat();\r\n    verbose = command.getOpt(VERBOSE);\r\n    int expectedMin = getOptValue(OPT_MIN, 0);\r\n    int expectedMax = getOptValue(OPT_MAX, 0);\r\n    boolean audit = command.getOpt(OPT_AUDIT);\r\n    boolean clean = command.getOpt(OPT_CLEAN);\r\n    if (audit == clean) {\r\n        errorln(getUsage());\r\n        throw new ExitUtil.ExitException(EXIT_USAGE, \"Exactly one of \" + AUDIT + \" and \" + CLEAN);\r\n    }\r\n    int limit = getOptValue(OPT_LIMIT, UNLIMITED_LISTING);\r\n    final String dir = parsedArgs.get(0);\r\n    Path path = new Path(dir);\r\n    URI uri = path.toUri();\r\n    if (uri.getPath().isEmpty()) {\r\n        path = new Path(path, \"/\");\r\n    }\r\n    FileSystem fs = path.getFileSystem(getConf());\r\n    boolean nonAuth = command.getOpt(OPT_NONAUTH);\r\n    ScanResult result;\r\n    try {\r\n        result = execute(new ScanArgsBuilder().withSourceFS(fs).withPath(path).withDoPurge(clean).withMinMarkerCount(expectedMin).withMaxMarkerCount(expectedMax).withLimit(limit).withNonAuth(nonAuth).build());\r\n    } catch (UnknownStoreException ex) {\r\n        throw new ExitUtil.ExitException(EXIT_NOT_FOUND, ex.toString(), ex);\r\n    }\r\n    if (verbose) {\r\n        dumpFileSystemStatistics(out);\r\n    }\r\n    String saveFile = command.getOptValue(OPT_OUT);\r\n    if (saveFile != null && !saveFile.isEmpty()) {\r\n        println(out, \"Saving result to %s\", saveFile);\r\n        try (Writer writer = new OutputStreamWriter(new FileOutputStream(saveFile), StandardCharsets.UTF_8)) {\r\n            final List<String> surplus = result.getTracker().getSurplusMarkers().keySet().stream().map(p -> p.toString() + \"/\").sorted().collect(Collectors.toList());\r\n            IOUtils.writeLines(surplus, \"\\n\", writer);\r\n        }\r\n    }\r\n    return result.finish();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "getOptValue",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int getOptValue(String option, int defVal)\n{\r\n    CommandFormat command = getCommandFormat();\r\n    String value = command.getOptValue(option);\r\n    if (value != null && !value.isEmpty()) {\r\n        try {\r\n            return Integer.parseInt(value);\r\n        } catch (NumberFormatException e) {\r\n            throw new ExitUtil.ExitException(EXIT_USAGE, String.format(\"Argument for %s is not a number: %s\", option, value));\r\n        }\r\n    } else {\r\n        return defVal;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "execute",
  "errType" : [ "UnknownStoreException", "FileNotFoundException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "ScanResult execute(final ScanArgs scanArgs) throws IOException\n{\r\n    S3AFileSystem fs = bindFilesystem(scanArgs.getSourceFS());\r\n    storeContext = fs.createStoreContext();\r\n    DirectoryPolicy activePolicy = fs.getDirectoryMarkerPolicy();\r\n    DirectoryPolicy.MarkerPolicy policy = activePolicy.getMarkerPolicy();\r\n    println(out, \"The directory marker policy of %s is \\\"%s\\\"\", storeContext.getFsURI(), policy);\r\n    String authPath = storeContext.getConfiguration().getTrimmed(AUTHORITATIVE_PATH, \"\");\r\n    if (policy == DirectoryPolicy.MarkerPolicy.Authoritative) {\r\n        println(out, \"Authoritative path list is \\\"%s\\\"\", authPath);\r\n    }\r\n    Path path = scanArgs.getPath();\r\n    Path target = path.makeQualified(fs.getUri(), new Path(\"/\"));\r\n    try {\r\n        getFilesystem().getFileStatus(target);\r\n    } catch (UnknownStoreException ex) {\r\n        throw new ExitUtil.ExitException(EXIT_NOT_FOUND, ex.toString(), ex);\r\n    } catch (FileNotFoundException ex) {\r\n        throw new ExitUtil.ExitException(EXIT_NOT_FOUND, \"Not found: \" + target, ex);\r\n    }\r\n    DirectoryPolicy filterPolicy;\r\n    if (scanArgs.isNonAuth()) {\r\n        filterPolicy = new DirectoryPolicyImpl(DirectoryPolicy.MarkerPolicy.Authoritative, fs::allowAuthoritative);\r\n    } else {\r\n        filterPolicy = null;\r\n    }\r\n    int minMarkerCount = scanArgs.getMinMarkerCount();\r\n    int maxMarkerCount = scanArgs.getMaxMarkerCount();\r\n    if (minMarkerCount > maxMarkerCount) {\r\n        println(out, \"Swapping -min (%d) and -max (%d) values\", minMarkerCount, maxMarkerCount);\r\n        int m = minMarkerCount;\r\n        minMarkerCount = maxMarkerCount;\r\n        maxMarkerCount = m;\r\n    }\r\n    operations = fs.createMarkerToolOperations(target.toString());\r\n    return scan(target, scanArgs.isDoPurge(), minMarkerCount, maxMarkerCount, scanArgs.getLimit(), filterPolicy);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "scan",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "ScanResult scan(final Path path, final boolean doPurge, final int minMarkerCount, final int maxMarkerCount, final int limit, final DirectoryPolicy filterPolicy) throws IOException, ExitUtil.ExitException\n{\r\n    Preconditions.checkArgument(minMarkerCount <= maxMarkerCount, \"The min marker count of %d is greater than the max value of %d\", minMarkerCount, maxMarkerCount);\r\n    ScanResult result = new ScanResult();\r\n    result.exitCode = EXIT_SUCCESS;\r\n    DirMarkerTracker tracker = new DirMarkerTracker(path, true);\r\n    result.tracker = tracker;\r\n    boolean completed;\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"marker scan %s\", path)) {\r\n        completed = scanDirectoryTree(path, tracker, limit);\r\n    }\r\n    int objectsFound = tracker.getObjectsFound();\r\n    println(out, \"Listed %d object%s under %s%n\", objectsFound, suffix(objectsFound), path);\r\n    Map<Path, DirMarkerTracker.Marker> surplusMarkers = tracker.getSurplusMarkers();\r\n    Map<Path, DirMarkerTracker.Marker> leafMarkers = tracker.getLeafMarkers();\r\n    int markerCount = surplusMarkers.size();\r\n    result.totalMarkerCount = markerCount;\r\n    result.filteredMarkerCount = markerCount;\r\n    if (markerCount == 0) {\r\n        println(out, \"No surplus directory markers were found under %s\", path);\r\n    } else {\r\n        println(out, \"Found %d surplus directory marker%s under %s\", markerCount, suffix(markerCount), path);\r\n        for (Path markers : surplusMarkers.keySet()) {\r\n            println(out, \"    %s/\", markers);\r\n        }\r\n    }\r\n    if (!leafMarkers.isEmpty()) {\r\n        println(out, \"Found %d empty directory 'leaf' marker%s under %s\", leafMarkers.size(), suffix(leafMarkers.size()), path);\r\n        for (Path markers : leafMarkers.keySet()) {\r\n            println(out, \"    %s/\", markers);\r\n        }\r\n        println(out, \"These are required to indicate empty directories\");\r\n    }\r\n    if (doPurge) {\r\n        int deletePageSize = storeContext.getConfiguration().getInt(BULK_DELETE_PAGE_SIZE, BULK_DELETE_PAGE_SIZE_DEFAULT);\r\n        result.purgeSummary = purgeMarkers(tracker, deletePageSize);\r\n    } else {\r\n        if (filterPolicy != null) {\r\n            List<Path> allowed = tracker.removeAllowedMarkers(filterPolicy);\r\n            int allowedMarkers = allowed.size();\r\n            println(out, \"%nIgnoring %d marker%s in authoritative paths\", allowedMarkers, suffix(allowedMarkers));\r\n            if (verbose) {\r\n                allowed.forEach(p -> println(out, p.toString()));\r\n            }\r\n            markerCount = surplusMarkers.size();\r\n            result.filteredMarkerCount = markerCount;\r\n        }\r\n        if (markerCount < minMarkerCount || markerCount > maxMarkerCount) {\r\n            return failScan(result, EXIT_NOT_ACCEPTABLE, \"Marker count %d out of range \" + \"[%d - %d]\", markerCount, minMarkerCount, maxMarkerCount);\r\n        }\r\n    }\r\n    if (!completed) {\r\n        failScan(result, EXIT_INTERRUPTED, \"Listing limit (%d) reached before completing the scan\", limit);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "failScan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ScanResult failScan(ScanResult result, int code, String message, Object... args)\n{\r\n    String text = String.format(message, args);\r\n    result.exitCode = code;\r\n    result.exitText = text;\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "suffix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String suffix(final int size)\n{\r\n    return size == 1 ? \"\" : \"s\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "scanDirectoryTree",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean scanDirectoryTree(final Path path, final DirMarkerTracker tracker, final int limit) throws IOException\n{\r\n    int count = 0;\r\n    boolean result = true;\r\n    RemoteIterator<S3AFileStatus> listing = operations.listObjects(path, storeContext.pathToKey(path));\r\n    while (listing.hasNext()) {\r\n        count++;\r\n        S3AFileStatus status = listing.next();\r\n        Path statusPath = status.getPath();\r\n        S3ALocatedFileStatus locatedStatus = new S3ALocatedFileStatus(status, null);\r\n        String key = storeContext.pathToKey(statusPath);\r\n        if (status.isDirectory()) {\r\n            if (verbose) {\r\n                println(out, \"  Directory Marker %s/\", key);\r\n            }\r\n            LOG.debug(\"{}\", key);\r\n            tracker.markerFound(statusPath, key + \"/\", locatedStatus);\r\n        } else {\r\n            tracker.fileFound(statusPath, key, locatedStatus);\r\n        }\r\n        if ((count % 1000) == 0) {\r\n            println(out, \"Scanned %,d objects\", count);\r\n        }\r\n        if (limit > 0 && count >= limit) {\r\n            println(out, \"Limit of scan reached - %,d object%s\", limit, suffix(limit));\r\n            result = false;\r\n            break;\r\n        }\r\n    }\r\n    LOG.debug(\"Listing summary {}\", listing);\r\n    if (verbose) {\r\n        println(out, \"%nListing statistics:%n  %s%n\", ioStatisticsSourceToString(listing));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "purgeMarkers",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "MarkerPurgeSummary purgeMarkers(final DirMarkerTracker tracker, final int deletePageSize) throws MultiObjectDeleteException, AmazonClientException, IOException\n{\r\n    MarkerPurgeSummary summary = new MarkerPurgeSummary();\r\n    Map<Path, DirMarkerTracker.Marker> markers = tracker.getSurplusMarkers();\r\n    int size = markers.size();\r\n    List<DeleteObjectsRequest.KeyVersion> collect = markers.values().stream().map(p -> new DeleteObjectsRequest.KeyVersion(p.getKey())).collect(Collectors.toList());\r\n    List<DeleteObjectsRequest.KeyVersion> markerKeys = new ArrayList<>(collect);\r\n    Collections.shuffle(markerKeys);\r\n    int pages = size / deletePageSize;\r\n    if (size % deletePageSize > 0) {\r\n        pages += 1;\r\n    }\r\n    if (verbose) {\r\n        println(out, \"%n%d marker%s to delete in %d page%s of %d keys/page\", size, suffix(size), pages, suffix(pages), deletePageSize);\r\n    }\r\n    DurationInfo durationInfo = new DurationInfo(LOG, \"Deleting markers\");\r\n    int start = 0;\r\n    while (start < size) {\r\n        int end = Math.min(start + deletePageSize, size);\r\n        List<DeleteObjectsRequest.KeyVersion> page = markerKeys.subList(start, end);\r\n        once(\"Remove S3 Keys\", tracker.getBasePath().toString(), () -> operations.removeKeys(page, true));\r\n        summary.deleteRequests++;\r\n        start = end;\r\n    }\r\n    durationInfo.close();\r\n    summary.totalDeleteRequestDuration = durationInfo.value();\r\n    summary.markersDeleted = size;\r\n    return summary;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "isVerbose",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isVerbose()\n{\r\n    return verbose;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "setVerbose",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVerbose(final boolean verbose)\n{\r\n    this.verbose = verbose;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\tools",
  "methodName" : "execMarkerTool",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MarkerTool.ScanResult execMarkerTool(ScanArgs scanArgs) throws IOException\n{\r\n    MarkerTool tool = new MarkerTool(scanArgs.getSourceFS().getConf());\r\n    tool.setVerbose(LOG.isDebugEnabled());\r\n    return tool.execute(scanArgs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createProvidedFileStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<S3AFileStatus> createProvidedFileStatusIterator(S3AFileStatus[] fileStatuses, PathFilter filter, FileStatusAcceptor acceptor)\n{\r\n    return filteringRemoteIterator(remoteIteratorFromArray(fileStatuses), status -> filter.accept(status.getPath()) && acceptor.accept(status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toProvidedFileStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<S3AFileStatus> toProvidedFileStatusIterator(S3AFileStatus[] fileStatuses)\n{\r\n    return filteringRemoteIterator(remoteIteratorFromArray(fileStatuses), Listing.ACCEPT_ALL_BUT_S3N::accept);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFileStatusListingIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatusListingIterator createFileStatusListingIterator(Path listPath, S3ListRequest request, PathFilter filter, FileStatusAcceptor acceptor, AuditSpan span) throws IOException\n{\r\n    return new FileStatusListingIterator(createObjectListingIterator(listPath, request, span), filter, acceptor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createObjectListingIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ObjectListingIterator createObjectListingIterator(final Path listPath, final S3ListRequest request, final AuditSpan span) throws IOException\n{\r\n    return new ObjectListingIterator(listPath, request, span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createLocatedFileStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<S3ALocatedFileStatus> createLocatedFileStatusIterator(RemoteIterator<S3AFileStatus> statusIterator)\n{\r\n    return RemoteIterators.mappingRemoteIterator(statusIterator, listingOperationCallbacks::toLocatedFileStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createSingleStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<S3ALocatedFileStatus> createSingleStatusIterator(S3ALocatedFileStatus status)\n{\r\n    return remoteIteratorFromSingleton(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getListFilesAssumingDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RemoteIterator<S3ALocatedFileStatus> getListFilesAssumingDir(Path path, boolean recursive, FileStatusAcceptor acceptor, AuditSpan span) throws IOException\n{\r\n    String key = maybeAddTrailingSlash(pathToKey(path));\r\n    String delimiter = recursive ? null : \"/\";\r\n    if (recursive) {\r\n        LOG.debug(\"Recursive list of all entries under {}\", key);\r\n    } else {\r\n        LOG.debug(\"Requesting all entries under {} with delimiter '{}'\", key, delimiter);\r\n    }\r\n    return createLocatedFileStatusIterator(createFileStatusListingIterator(path, listingOperationCallbacks.createListObjectsRequest(key, delimiter, span), ACCEPT_ALL, acceptor, span));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getLocatedFileStatusIteratorForDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<S3ALocatedFileStatus> getLocatedFileStatusIteratorForDir(Path dir, PathFilter filter, AuditSpan span) throws IOException\n{\r\n    span.activate();\r\n    final String key = maybeAddTrailingSlash(pathToKey(dir));\r\n    return createLocatedFileStatusIterator(createFileStatusListingIterator(dir, listingOperationCallbacks.createListObjectsRequest(key, \"/\", span), filter, new AcceptAllButSelfAndS3nDirs(dir), span));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getFileStatusesAssumingNonEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RemoteIterator<S3AFileStatus> getFileStatusesAssumingNonEmptyDir(Path path, final AuditSpan span) throws IOException\n{\r\n    String key = pathToKey(path);\r\n    if (!key.isEmpty()) {\r\n        key = key + '/';\r\n    }\r\n    S3ListRequest request = createListObjectsRequest(key, \"/\", span);\r\n    LOG.debug(\"listStatus: doing listObjects for directory {}\", key);\r\n    return createFileStatusListingIterator(path, request, ACCEPT_ALL, new AcceptAllButSelfAndS3nDirs(path), span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createListObjectsRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3ListRequest createListObjectsRequest(String key, String delimiter, final AuditSpan span)\n{\r\n    return listingOperationCallbacks.createListObjectsRequest(key, delimiter, span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toLocatedFileStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> toLocatedFileStatusIterator(RemoteIterator<? extends LocatedFileStatus> iterator)\n{\r\n    return (RemoteIterator<LocatedFileStatus>) iterator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    OperationAuditorOptions options = OperationAuditorOptions.builder().withConfiguration(conf).withIoStatisticsStore(ioStatisticsStore);\r\n    auditor = AuditIntegration.createAndInitAuditor(getConfig(), S3AAuditConstants.AUDIT_SERVICE_CLASSNAME, options);\r\n    addService(auditor);\r\n    LOG.debug(\"Audit manager initialized with audit service {}\", auditor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    setUnbondedSpan(new WrappingAuditSpan(auditor.getUnbondedSpan(), false));\r\n    LOG.debug(\"Started audit service {}\", auditor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    activeSpanMap.clear();\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(super.toString());\r\n    sb.append(\", auditor=\").append(auditor);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getAuditor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationAuditor getAuditor()\n{\r\n    return auditor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getUnbondedSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WrappingAuditSpan getUnbondedSpan()\n{\r\n    return unbondedSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "setUnbondedSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUnbondedSpan(final WrappingAuditSpan unbondedSpan)\n{\r\n    this.unbondedSpan = unbondedSpan;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getActiveAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A getActiveAuditSpan()\n{\r\n    return activeSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "activeSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "WrappingAuditSpan activeSpan()\n{\r\n    return activeSpanMap.getForCurrentThread();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "setActiveThreadSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A setActiveThreadSpan(AuditSpanS3A span)\n{\r\n    return switchToActiveSpan(new WrappingAuditSpan(span, span.isValidSpan()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "switchToActiveSpan",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "WrappingAuditSpan switchToActiveSpan(WrappingAuditSpan span)\n{\r\n    if (span != null && span.isValidSpan()) {\r\n        activeSpanMap.setForCurrentThread(span);\r\n    } else {\r\n        activeSpanMap.removeForCurrentThread();\r\n    }\r\n    return activeSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "noteSpanReferenceLost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void noteSpanReferenceLost(long threadId)\n{\r\n    auditor.noteSpanReferenceLost(threadId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "prune",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int prune()\n{\r\n    return activeSpanMap.prune();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "removeActiveSpanFromMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean removeActiveSpanFromMap()\n{\r\n    activeSpanMap.removeForCurrentThread();\r\n    if (deactivationsBeforePrune.decrementAndGet() == 0) {\r\n        activeSpanMap.prune();\r\n        deactivationsBeforePrune.set(pruneThreshold);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getActiveSpanMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WeakReferenceThreadMap<WrappingAuditSpan> getActiveSpanMap()\n{\r\n    return activeSpanMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getSpanId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSpanId()\n{\r\n    return auditor != null ? auditor.getAuditorId() : \"(auditor not yet created)\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getOperationName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOperationName()\n{\r\n    return AUDIT_MANAGER_OPERATION;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createSpan",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AuditSpanS3A createSpan(final String operation, @Nullable final String path1, @Nullable final String path2) throws IOException\n{\r\n    Preconditions.checkState(isInState(STATE.STARTED), \"Audit Manager %s is in wrong state: %s\", this, getServiceState());\r\n    ioStatisticsStore.incrementCounter(Statistic.AUDIT_SPAN_CREATION.getSymbol());\r\n    return setActiveThreadSpan(auditor.createSpan(operation, path1, path2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createRequestHandlers",
  "errType" : [ "ExceptionInInitializerError", "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<RequestHandler2> createRequestHandlers() throws IOException\n{\r\n    List<RequestHandler2> requestHandlers = new ArrayList<>();\r\n    requestHandlers.add(new SdkRequestHandler());\r\n    final Class<?>[] handlers = getConfig().getClasses(AUDIT_REQUEST_HANDLERS);\r\n    if (handlers != null) {\r\n        for (Class<?> handler : handlers) {\r\n            try {\r\n                Constructor<?> ctor = handler.getConstructor();\r\n                requestHandlers.add((RequestHandler2) ctor.newInstance());\r\n            } catch (ExceptionInInitializerError e) {\r\n                throw FutureIO.unwrapInnerException(e);\r\n            } catch (Exception e) {\r\n                throw new IOException(e);\r\n            }\r\n        }\r\n    }\r\n    return requestHandlers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createStateChangeListener",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TransferStateChangeListener createStateChangeListener()\n{\r\n    final WrappingAuditSpan span = activeSpan();\r\n    return (transfer, state) -> switchToActiveSpan(span);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean checkAccess(final Path path, final S3AFileStatus status, final FsAction mode) throws IOException\n{\r\n    return auditor.checkAccess(path, status, mode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "requestCreated",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "T requestCreated(final T request)\n{\r\n    AuditSpanS3A span = getActiveAuditSpan();\r\n    if (LOG.isTraceEnabled()) {\r\n        LOG.trace(\"Created Request {} in span {}\", analyzer.analyze(request), span);\r\n    }\r\n    attachSpanToRequest(request, span);\r\n    try {\r\n        return span.requestCreated(request);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "beforeExecution",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T beforeExecution(final T request)\n{\r\n    ioStatisticsStore.incrementCounter(AUDIT_REQUEST_EXECUTION.getSymbol());\r\n    try {\r\n        return extractAndActivateSpanFromRequest(request).beforeExecution(request);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "afterResponse",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void afterResponse(final Request<?> request, final Response<?> response) throws AuditFailureException, SdkBaseException\n{\r\n    try {\r\n        extractAndActivateSpanFromRequest(request).afterResponse(request, response);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "extractAndActivateSpanFromRequest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AWSAuditEventCallbacks extractAndActivateSpanFromRequest(final T request)\n{\r\n    AWSAuditEventCallbacks span;\r\n    span = retrieveAttachedSpan(request);\r\n    if (span == null) {\r\n        LOG.debug(\"No audit span attached to request {}\", request);\r\n        span = getActiveAuditSpan();\r\n    } else {\r\n        if (span instanceof WrappingAuditSpan) {\r\n            switchToActiveSpan((WrappingAuditSpan) span);\r\n        } else {\r\n            WARN_OF_SPAN_TYPE.warn(NOT_A_WRAPPED_SPAN + \": {}\", span);\r\n            LOG.debug(NOT_A_WRAPPED_SPAN + \": {}\", span);\r\n        }\r\n    }\r\n    return span;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "afterError",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void afterError(final Request<?> request, final Response<?> response, final Exception exception) throws AuditFailureException, SdkBaseException\n{\r\n    try {\r\n        extractAndActivateSpanFromRequest(request).afterError(request, response, exception);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "beforeMarshalling",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AmazonWebServiceRequest beforeMarshalling(final AmazonWebServiceRequest request)\n{\r\n    try {\r\n        return extractAndActivateSpanFromRequest(request).beforeMarshalling(request);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "beforeRequest",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void beforeRequest(final Request<?> request)\n{\r\n    try {\r\n        extractAndActivateSpanFromRequest(request).beforeRequest(request);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "beforeAttempt",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void beforeAttempt(final HandlerBeforeAttemptContext context)\n{\r\n    try {\r\n        extractAndActivateSpanFromRequest(context.getRequest()).beforeAttempt(context);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "afterAttempt",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void afterAttempt(final HandlerAfterAttemptContext context)\n{\r\n    try {\r\n        extractAndActivateSpanFromRequest(context.getRequest()).afterAttempt(context);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "beforeUnmarshalling",
  "errType" : [ "AuditFailureException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HttpResponse beforeUnmarshalling(final Request<?> request, final HttpResponse httpResponse)\n{\r\n    try {\r\n        extractAndActivateSpanFromRequest(request.getOriginalRequest()).beforeUnmarshalling(request, httpResponse);\r\n    } catch (AuditFailureException e) {\r\n        ioStatisticsStore.incrementCounter(AUDIT_FAILURE.getSymbol());\r\n        throw e;\r\n    }\r\n    return httpResponse;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "v1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ListRequest v1(ListObjectsRequest request)\n{\r\n    return new S3ListRequest(request, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "v2",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ListRequest v2(ListObjectsV2Request request)\n{\r\n    return new S3ListRequest(null, request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isV1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isV1()\n{\r\n    return v1Request != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getV1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListObjectsRequest getV1()\n{\r\n    return v1Request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getV2",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListObjectsV2Request getV2()\n{\r\n    return v2Request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    if (isV1()) {\r\n        return String.format(DESCRIPTION, v1Request.getBucketName(), v1Request.getPrefix(), v1Request.getDelimiter(), v1Request.getMaxKeys(), v1Request.isRequesterPays());\r\n    } else {\r\n        return String.format(DESCRIPTION, v2Request.getBucketName(), v2Request.getPrefix(), v2Request.getDelimiter(), v2Request.getMaxKeys(), v2Request.isRequesterPays());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getRevisionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRevisionId()\n{\r\n    return revisionId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getSource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ChangeDetectionPolicy.Source getSource()\n{\r\n    return policy.getSource();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getVersionMismatches",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getVersionMismatches()\n{\r\n    return versionMismatches.getVersionMismatches();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "maybeApplyConstraint",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean maybeApplyConstraint(final GetObjectRequest request)\n{\r\n    if (policy.getMode() == ChangeDetectionPolicy.Mode.Server && revisionId != null) {\r\n        policy.applyRevisionConstraint(request, revisionId);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "maybeApplyConstraint",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean maybeApplyConstraint(final CopyObjectRequest request)\n{\r\n    if (policy.getMode() == ChangeDetectionPolicy.Mode.Server && revisionId != null) {\r\n        policy.applyRevisionConstraint(request, revisionId);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "maybeApplyConstraint",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean maybeApplyConstraint(final GetObjectMetadataRequest request)\n{\r\n    if (policy.getMode() == ChangeDetectionPolicy.Mode.Server && revisionId != null) {\r\n        policy.applyRevisionConstraint(request, revisionId);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "processResponse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void processResponse(final S3Object object, final String operation, final long pos) throws PathIOException\n{\r\n    if (object == null) {\r\n        if (revisionId != null) {\r\n            versionMismatches.versionMismatchError();\r\n            throw new RemoteFileChangedException(uri, operation, String.format(CHANGE_REPORTED_BY_S3 + \" during %s\" + \" at position %s.\" + \" %s %s was unavailable\", operation, pos, getSource(), getRevisionId()));\r\n        } else {\r\n            throw new PathIOException(uri, \"No data returned from GET request\");\r\n        }\r\n    }\r\n    processMetadata(object.getObjectMetadata(), operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "processResponse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void processResponse(final CopyResult copyResult) throws PathIOException\n{\r\n    String newRevisionId = policy.getRevisionId(copyResult);\r\n    LOG.debug(\"Copy result {}: {}\", policy.getSource(), newRevisionId);\r\n    if (newRevisionId == null && policy.isRequireVersion()) {\r\n        throw new NoVersionAttributeException(uri, String.format(\"Change detection policy requires %s\", policy.getSource()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "processException",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void processException(SdkBaseException e, String operation) throws RemoteFileChangedException\n{\r\n    if (e instanceof AmazonServiceException) {\r\n        AmazonServiceException serviceException = (AmazonServiceException) e;\r\n        if (serviceException.getStatusCode() == SC_PRECONDITION_FAILED) {\r\n            versionMismatches.versionMismatchError();\r\n            throw new RemoteFileChangedException(uri, operation, String.format(RemoteFileChangedException.PRECONDITIONS_FAILED + \" on %s.\" + \" Version %s was unavailable\", getSource(), getRevisionId()), serviceException);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "processMetadata",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void processMetadata(final ObjectMetadata metadata, final String operation) throws PathIOException\n{\r\n    final String newRevisionId = policy.getRevisionId(metadata, uri);\r\n    processNewRevision(newRevisionId, operation, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "processNewRevision",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void processNewRevision(final String newRevisionId, final String operation, final long pos) throws PathIOException\n{\r\n    if (newRevisionId == null && policy.isRequireVersion()) {\r\n        throw new NoVersionAttributeException(uri, String.format(\"Change detection policy requires %s\", policy.getSource()));\r\n    }\r\n    if (revisionId == null) {\r\n        LOG.debug(\"Setting revision ID for object at {}: {}\", uri, newRevisionId);\r\n        revisionId = newRevisionId;\r\n    } else if (!revisionId.equals(newRevisionId)) {\r\n        LOG.debug(\"Revision ID changed from {} to {}\", revisionId, newRevisionId);\r\n        ImmutablePair<Boolean, RemoteFileChangedException> pair = policy.onChangeDetected(revisionId, newRevisionId, uri, pos, operation, versionMismatches.getVersionMismatches());\r\n        if (pair.left) {\r\n            versionMismatches.versionMismatchError();\r\n        }\r\n        if (pair.right != null) {\r\n            throw pair.right;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"ChangeTracker{\");\r\n    sb.append(policy);\r\n    sb.append(\", revisionId='\").append(revisionId).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "PathOutputCommitter createTaskCommitter(S3AFileSystem fileSystem, Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    AbstractS3ACommitterFactory factory = chooseCommitterFactory(fileSystem, outputPath, context.getConfiguration());\r\n    if (factory != null) {\r\n        PathOutputCommitter committer = factory.createTaskCommitter(fileSystem, outputPath, context);\r\n        LOG.info(\"Using committer {} to output data to {}\", (committer instanceof AbstractS3ACommitter ? ((AbstractS3ACommitter) committer).getName() : committer.toString()), outputPath);\r\n        return committer;\r\n    } else {\r\n        LOG.warn(\"Using standard FileOutputCommitter to commit work.\" + \" This is slow and potentially unsafe.\");\r\n        return createFileOutputCommitter(outputPath, context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "chooseCommitterFactory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AbstractS3ACommitterFactory chooseCommitterFactory(S3AFileSystem fileSystem, Path outputPath, Configuration taskConf) throws PathCommitException\n{\r\n    AbstractS3ACommitterFactory factory;\r\n    Configuration fsConf = fileSystem.getConf();\r\n    String name = fsConf.getTrimmed(FS_S3A_COMMITTER_NAME, COMMITTER_NAME_FILE);\r\n    name = taskConf.getTrimmed(FS_S3A_COMMITTER_NAME, name);\r\n    LOG.debug(\"Committer option is {}\", name);\r\n    switch(name) {\r\n        case COMMITTER_NAME_FILE:\r\n            factory = null;\r\n            break;\r\n        case COMMITTER_NAME_DIRECTORY:\r\n            factory = new DirectoryStagingCommitterFactory();\r\n            break;\r\n        case COMMITTER_NAME_PARTITIONED:\r\n            factory = new PartitionedStagingCommitterFactory();\r\n            break;\r\n        case COMMITTER_NAME_MAGIC:\r\n            factory = new MagicS3GuardCommitterFactory();\r\n            break;\r\n        case InternalCommitterConstants.COMMITTER_NAME_STAGING:\r\n            factory = new StagingCommitterFactory();\r\n            break;\r\n        default:\r\n            throw new PathCommitException(outputPath, \"Unknown committer: \\\"\" + name + \"\\\"\");\r\n    }\r\n    return factory;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "e",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String e(String name, String pattern)\n{\r\n    return String.format(\"(?<%s>%s) \", name, pattern);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "eNoTrailing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String eNoTrailing(String name, String pattern)\n{\r\n    return String.format(\"(?<%s>%s)\", name, pattern);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "e",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String e(String name)\n{\r\n    return e(name, SIMPLE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "q",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String q(String name)\n{\r\n    return e(name, QUOTED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createBlockIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3ADataBlocks.DataBlock createBlockIfNeeded() throws IOException\n{\r\n    if (activeBlock == null) {\r\n        blockCount++;\r\n        if (blockCount >= Constants.MAX_MULTIPART_COUNT) {\r\n            LOG.error(\"Number of partitions in stream exceeds limit for S3: \" + Constants.MAX_MULTIPART_COUNT + \" write may fail.\");\r\n        }\r\n        activeBlock = blockFactory.create(blockCount, this.blockSize, statistics);\r\n    }\r\n    return activeBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getActiveBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ADataBlocks.DataBlock getActiveBlock()\n{\r\n    return activeBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hasActiveBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasActiveBlock()\n{\r\n    return activeBlock != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "clearActiveBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearActiveBlock()\n{\r\n    if (activeBlock != null) {\r\n        LOG.debug(\"Clearing active block\");\r\n    }\r\n    synchronized (this) {\r\n        activeBlock = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkOpen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkOpen() throws IOException\n{\r\n    if (closed.get()) {\r\n        throw new IOException(\"Filesystem \" + writeOperationHelper + \" closed\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "flush",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void flush() throws IOException\n{\r\n    try {\r\n        checkOpen();\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Stream closed: \" + e.getMessage());\r\n        return;\r\n    }\r\n    S3ADataBlocks.DataBlock dataBlock = getActiveBlock();\r\n    if (dataBlock != null) {\r\n        dataBlock.flush();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(int b) throws IOException\n{\r\n    singleCharWrite[0] = (byte) b;\r\n    write(singleCharWrite, 0, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void write(byte[] source, int offset, int len) throws IOException\n{\r\n    S3ADataBlocks.validateWriteArgs(source, offset, len);\r\n    checkOpen();\r\n    if (len == 0) {\r\n        return;\r\n    }\r\n    statistics.writeBytes(len);\r\n    S3ADataBlocks.DataBlock block = createBlockIfNeeded();\r\n    int written = block.write(source, offset, len);\r\n    int remainingCapacity = block.remainingCapacity();\r\n    if (written < len) {\r\n        LOG.debug(\"writing more data than block has capacity -triggering upload\");\r\n        uploadCurrentBlock(false);\r\n        this.write(source, offset + written, len - written);\r\n    } else {\r\n        if (remainingCapacity == 0 && !isCSEEnabled) {\r\n            uploadCurrentBlock(false);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "uploadCurrentBlock",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void uploadCurrentBlock(boolean isLast) throws IOException\n{\r\n    Preconditions.checkState(hasActiveBlock(), \"No active block\");\r\n    LOG.debug(\"Writing block # {}\", blockCount);\r\n    initMultipartUpload();\r\n    try {\r\n        multiPartUpload.uploadBlockAsync(getActiveBlock(), isLast);\r\n        bytesSubmitted += getActiveBlock().dataSize();\r\n    } finally {\r\n        clearActiveBlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initMultipartUpload() throws IOException\n{\r\n    if (multiPartUpload == null) {\r\n        LOG.debug(\"Initiating Multipart upload\");\r\n        multiPartUpload = new MultiPartUpload(key);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "close",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (closed.getAndSet(true)) {\r\n        LOG.debug(\"Ignoring close() as stream is already closed\");\r\n        return;\r\n    }\r\n    S3ADataBlocks.DataBlock block = getActiveBlock();\r\n    boolean hasBlock = hasActiveBlock();\r\n    LOG.debug(\"{}: Closing block #{}: current block= {}\", this, blockCount, hasBlock ? block : \"(none)\");\r\n    long bytes = 0;\r\n    try {\r\n        if (multiPartUpload == null) {\r\n            if (hasBlock) {\r\n                bytes = putObject();\r\n                bytesSubmitted = bytes;\r\n            }\r\n        } else {\r\n            if (hasBlock && (block.hasData() || multiPartUpload.getPartsSubmitted() == 0)) {\r\n                uploadCurrentBlock(true);\r\n            }\r\n            final List<PartETag> partETags = multiPartUpload.waitForAllPartUploads();\r\n            bytes = bytesSubmitted;\r\n            if (putTracker.aboutToComplete(multiPartUpload.getUploadId(), partETags, bytes, iostatistics)) {\r\n                multiPartUpload.complete(partETags);\r\n            } else {\r\n                LOG.info(\"File {} will be visible when the job is committed\", key);\r\n            }\r\n        }\r\n        if (!putTracker.outputImmediatelyVisible()) {\r\n            statistics.commitUploaded(bytes);\r\n        }\r\n        LOG.debug(\"Upload complete to {} by {}\", key, writeOperationHelper);\r\n    } catch (IOException ioe) {\r\n        maybeAbortMultipart();\r\n        writeOperationHelper.writeFailed(ioe);\r\n        throw ioe;\r\n    } finally {\r\n        cleanupOnClose();\r\n    }\r\n    writeOperationHelper.writeSuccessful(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "cleanupOnClose",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanupOnClose()\n{\r\n    cleanupWithLogger(LOG, getActiveBlock(), blockFactory);\r\n    LOG.debug(\"Statistics: {}\", statistics);\r\n    cleanupWithLogger(LOG, statistics);\r\n    clearActiveBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeAbortMultipart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException maybeAbortMultipart()\n{\r\n    if (multiPartUpload != null) {\r\n        final IOException ioe = multiPartUpload.abort();\r\n        multiPartUpload = null;\r\n        return ioe;\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "abort",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AbortableResult abort()\n{\r\n    if (closed.getAndSet(true)) {\r\n        LOG.debug(\"Ignoring abort() as stream is already closed\");\r\n        return new AbortableResultImpl(true, null);\r\n    }\r\n    try (DurationTracker d = statistics.trackDuration(INVOCATION_ABORT.getSymbol())) {\r\n        return new AbortableResultImpl(false, maybeAbortMultipart());\r\n    } finally {\r\n        cleanupOnClose();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "putObject",
  "errType" : [ "InterruptedException", "ExecutionException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "int putObject() throws IOException\n{\r\n    LOG.debug(\"Executing regular upload for {}\", writeOperationHelper);\r\n    final S3ADataBlocks.DataBlock block = getActiveBlock();\r\n    int size = block.dataSize();\r\n    final S3ADataBlocks.BlockUploadData uploadData = block.startUpload();\r\n    final PutObjectRequest putObjectRequest = uploadData.hasFile() ? writeOperationHelper.createPutObjectRequest(key, uploadData.getFile()) : writeOperationHelper.createPutObjectRequest(key, uploadData.getUploadStream(), size, null);\r\n    BlockUploadProgress callback = new BlockUploadProgress(block, progressListener, now());\r\n    putObjectRequest.setGeneralProgressListener(callback);\r\n    statistics.blockUploadQueued(size);\r\n    ListenableFuture<PutObjectResult> putObjectResult = executorService.submit(() -> {\r\n        try {\r\n            return writeOperationHelper.putObject(putObjectRequest);\r\n        } finally {\r\n            cleanupWithLogger(LOG, uploadData, block);\r\n        }\r\n    });\r\n    clearActiveBlock();\r\n    try {\r\n        putObjectResult.get();\r\n        return size;\r\n    } catch (InterruptedException ie) {\r\n        LOG.warn(\"Interrupted object upload\", ie);\r\n        Thread.currentThread().interrupt();\r\n        return 0;\r\n    } catch (ExecutionException ee) {\r\n        throw extractException(\"regular upload\", key, ee);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3ABlockOutputStream{\");\r\n    sb.append(writeOperationHelper.toString());\r\n    sb.append(\", blockSize=\").append(blockSize);\r\n    S3ADataBlocks.DataBlock block = activeBlock;\r\n    if (block != null) {\r\n        sb.append(\", activeBlock=\").append(block);\r\n    }\r\n    sb.append(\" Statistics=\").append(IOStatisticsLogging.ioStatisticsSourceToString(this));\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementWriteOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementWriteOperations()\n{\r\n    writeOperationHelper.incrementWriteOperations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "now",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Instant now()\n{\r\n    return Instant.now();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockOutputStreamStatistics getStatistics()\n{\r\n    return statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    switch(capability.toLowerCase(Locale.ENGLISH)) {\r\n        case CommitConstants.STREAM_CAPABILITY_MAGIC_OUTPUT:\r\n        case CommitConstants.STREAM_CAPABILITY_MAGIC_OUTPUT_OLD:\r\n            return !putTracker.outputImmediatelyVisible();\r\n        case StreamCapabilities.HFLUSH:\r\n        case StreamCapabilities.HSYNC:\r\n            return false;\r\n        case StreamCapabilities.IOSTATISTICS:\r\n            return true;\r\n        case StreamCapabilities.ABORTABLE_STREAM:\r\n            return true;\r\n        default:\r\n            return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hflush",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void hflush() throws IOException\n{\r\n    statistics.hflushInvoked();\r\n    handleSyncableInvocation();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void hsync() throws IOException\n{\r\n    statistics.hsyncInvoked();\r\n    handleSyncableInvocation();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "handleSyncableInvocation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void handleSyncableInvocation()\n{\r\n    final UnsupportedOperationException ex = new UnsupportedOperationException(E_NOT_SYNCABLE);\r\n    if (!downgradeSyncableExceptions) {\r\n        throw ex;\r\n    }\r\n    WARN_ON_SYNCABLE.warn(\"Application invoked the Syncable API against\" + \" stream writing to {}. This is Unsupported\", key);\r\n    LOG.debug(\"Downgrading Syncable call\", ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockOutputStreamBuilder builder()\n{\r\n    return new BlockOutputStreamBuilder();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "createCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AWSCredentials createCredentials(final Configuration config) throws IOException\n{\r\n    return toAWSCredentials(credentials, typeRequired, component);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMetricsSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MetricsSystem getMetricsSystem()\n{\r\n    synchronized (METRICS_SYSTEM_LOCK) {\r\n        if (metricsSystem == null) {\r\n            metricsSystem = new MetricsSystemImpl();\r\n            metricsSystem.init(METRICS_SYSTEM_NAME);\r\n        }\r\n    }\r\n    return metricsSystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "registerAsMetricsSource",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void registerAsMetricsSource(URI name)\n{\r\n    int number;\r\n    synchronized (METRICS_SYSTEM_LOCK) {\r\n        getMetricsSystem();\r\n        metricsSourceActiveCounter++;\r\n        number = ++metricsSourceNameCounter;\r\n    }\r\n    String msName = METRICS_SOURCE_BASENAME + number;\r\n    metricsSourceName = msName + \"-\" + name.getHost();\r\n    metricsSystem.register(metricsSourceName, \"\", this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "counter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableCounterLong counter(String name, String desc)\n{\r\n    return registry.newCounter(name, desc, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "counter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableCounterLong counter(Statistic op)\n{\r\n    return counter(op.getSymbol(), op.getDescription());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "duration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void duration(Statistic op)\n{\r\n    counter(op.getSymbol(), op.getDescription());\r\n    counter(op.getSymbol() + SUFFIX_FAILURES, op.getDescription());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "gauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableGaugeLong gauge(String name, String desc)\n{\r\n    return registry.newGauge(name, desc, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "gauge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableGaugeLong gauge(Statistic op)\n{\r\n    return gauge(op.getSymbol(), op.getDescription());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "quantiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableQuantiles quantiles(Statistic op, String sampleName, String valueName, int interval)\n{\r\n    return registry.newQuantiles(op.getSymbol(), op.getDescription(), sampleName, valueName, interval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRegistry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MetricsRegistry getRegistry()\n{\r\n    return registry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "dump",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String dump(String prefix, String separator, String suffix, boolean all)\n{\r\n    MetricStringBuilder metricBuilder = new MetricStringBuilder(null, prefix, separator, suffix);\r\n    registry.snapshot(metricBuilder, all);\r\n    return metricBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCounterValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCounterValue(Statistic statistic)\n{\r\n    return getCounterValue(statistic.getSymbol());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCounterValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getCounterValue(String name)\n{\r\n    MutableCounterLong counter = lookupCounter(name);\r\n    return counter == null ? 0 : counter.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MutableCounterLong lookupCounter(String name)\n{\r\n    MutableMetric metric = lookupMetric(name);\r\n    if (metric == null) {\r\n        return null;\r\n    }\r\n    if (!(metric instanceof MutableCounterLong)) {\r\n        throw new IllegalStateException(\"Metric \" + name + \" is not a MutableCounterLong: \" + metric + \" (type: \" + metric.getClass() + \")\");\r\n    }\r\n    return (MutableCounterLong) metric;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupGauge",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MutableGaugeLong lookupGauge(String name)\n{\r\n    MutableMetric metric = lookupMetric(name);\r\n    if (metric == null) {\r\n        LOG.debug(\"No gauge {}\", name);\r\n    }\r\n    return (MutableGaugeLong) metric;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupQuantiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MutableQuantiles lookupQuantiles(String name)\n{\r\n    MutableMetric metric = lookupMetric(name);\r\n    if (metric == null) {\r\n        LOG.debug(\"No quantiles {}\", name);\r\n    }\r\n    return (MutableQuantiles) metric;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "lookupMetric",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableMetric lookupMetric(String name)\n{\r\n    MutableMetric metric = getRegistry().get(name);\r\n    return metric;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return instanceIOStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getDurationTrackerFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DurationTrackerFactory getDurationTrackerFactory()\n{\r\n    return durationTrackerFactory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "trackDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DurationTracker trackDuration(final String key, final long count)\n{\r\n    return durationTrackerFactory.trackDuration(key, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createMetricsUpdatingStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore createMetricsUpdatingStore()\n{\r\n    return new MetricsUpdatingIOStatisticsStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3AInstrumentation{\");\r\n    if (LOG.isDebugEnabled()) {\r\n        sb.append(\"instanceIOStatistics=\").append(instanceIOStatistics);\r\n    }\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "fileCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fileCreated()\n{\r\n    incrementCounter(FILES_CREATED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "fileDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fileDeleted(int count)\n{\r\n    incrementCounter(FILES_DELETED, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "fakeDirsDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fakeDirsDeleted(int count)\n{\r\n    incrementCounter(FAKE_DIRECTORIES_DELETED, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "directoryCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void directoryCreated()\n{\r\n    incrementCounter(DIRECTORIES_CREATED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "directoryDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void directoryDeleted()\n{\r\n    incrementCounter(DIRECTORIES_DELETED, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "filesCopied",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void filesCopied(int files, long size)\n{\r\n    incrementCounter(FILES_COPIED, files);\r\n    incrementCounter(FILES_COPIED_BYTES, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "errorIgnored",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void errorIgnored()\n{\r\n    incrementCounter(IGNORED_ERRORS, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementCounter(Statistic op, long count)\n{\r\n    incrementNamedCounter(op.getSymbol(), count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementNamedCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long incrementNamedCounter(final String name, final long count)\n{\r\n    if (count != 0) {\r\n        incrementMutableCounter(name, count);\r\n        return instanceIOStatistics.incrementCounter(name, count);\r\n    } else {\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementMutableCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void incrementMutableCounter(final String name, final long count)\n{\r\n    if (count > 0) {\r\n        MutableCounterLong counter = lookupCounter(name);\r\n        if (counter != null) {\r\n            counter.incr(count);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "addValueToQuantiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addValueToQuantiles(Statistic op, long value)\n{\r\n    MutableQuantiles quantiles = lookupQuantiles(op.getSymbol());\r\n    if (quantiles != null) {\r\n        quantiles.add(value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementCounter(Statistic op, AtomicLong count)\n{\r\n    incrementCounter(op, count.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "incrementGauge",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incrementGauge(Statistic op, long count)\n{\r\n    MutableGaugeLong gauge = lookupGauge(op.getSymbol());\r\n    if (gauge != null) {\r\n        gauge.incr(count);\r\n    } else {\r\n        LOG.debug(\"No Gauge: \" + op);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "decrementGauge",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void decrementGauge(Statistic op, long count)\n{\r\n    MutableGaugeLong gauge = lookupGauge(op.getSymbol());\r\n    if (gauge != null) {\r\n        gauge.decr(count);\r\n    } else {\r\n        LOG.debug(\"No Gauge: {}\", op);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "recordDuration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void recordDuration(final Statistic op, final boolean success, final Duration duration)\n{\r\n    String name = op.getSymbol() + (success ? \"\" : SUFFIX_FAILURES);\r\n    instanceIOStatistics.addTimedOperation(name, duration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newInputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AInputStreamStatistics newInputStreamStatistics(@Nullable final FileSystem.Statistics filesystemStatistics)\n{\r\n    return new InputStreamStatistics(filesystemStatistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newCommitterStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommitterStatistics newCommitterStatistics()\n{\r\n    return new CommitterStatisticsImpl();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getMetrics(MetricsCollector collector, boolean all)\n{\r\n    registry.snapshot(collector.addRecord(registry.info().name()), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void close()\n{\r\n    synchronized (METRICS_SYSTEM_LOCK) {\r\n        throttleRateQuantile.stop();\r\n        metricsSystem.unregisterSource(metricsSourceName);\r\n        metricsSourceActiveCounter--;\r\n        int activeSources = metricsSourceActiveCounter;\r\n        if (activeSources == 0) {\r\n            LOG.debug(\"Shutting down metrics publisher\");\r\n            metricsSystem.publishMetricsNow();\r\n            metricsSystem.shutdown();\r\n            metricsSystem = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newOutputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockOutputStreamStatistics newOutputStreamStatistics(FileSystem.Statistics filesystemStatistics)\n{\r\n    return new OutputStreamStatistics(filesystemStatistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "mergeOutputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void mergeOutputStreamStatistics(OutputStreamStatistics source)\n{\r\n    incrementCounter(STREAM_WRITE_TOTAL_TIME, source.totalUploadDuration());\r\n    incrementCounter(STREAM_WRITE_QUEUE_DURATION, source.queueDuration);\r\n    incrementCounter(STREAM_WRITE_TOTAL_DATA, source.bytesUploaded);\r\n    incrementCounter(STREAM_WRITE_BLOCK_UPLOADS, source.blockUploadsCompleted);\r\n    incrementCounter(STREAM_WRITE_EXCEPTIONS, source.lookupCounterValue(StreamStatisticNames.STREAM_WRITE_EXCEPTIONS));\r\n    this.getIOStatistics().aggregate(source.getIOStatistics());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "newDelegationTokenStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationTokenStatistics newDelegationTokenStatistics()\n{\r\n    return new DelegationTokenStatisticsImpl();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toMap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, Long> toMap()\n{\r\n    MetricsToMap metricBuilder = new MetricsToMap(null);\r\n    registry.snapshot(metricBuilder, true);\r\n    return metricBuilder.getMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "collectMetrics",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void collectMetrics(final Request<?> request, final Response<?> response)\n{\r\n    TimingInfo timingInfo = request.getAWSRequestMetrics().getTimingInfo();\r\n    counter(timingInfo, HttpClientRetryCount.name(), collector::updateAwsRetryCount);\r\n    counter(timingInfo, RequestCount.name(), collector::updateAwsRequestCount);\r\n    counter(timingInfo, ThrottleException.name(), collector::updateAwsThrottleExceptionsCount);\r\n    timing(timingInfo, ClientExecuteTime.name(), collector::noteAwsClientExecuteTime);\r\n    timing(timingInfo, HttpRequestTime.name(), collector::noteAwsRequestTime);\r\n    timing(timingInfo, RequestMarshallTime.name(), collector::noteRequestMarshallTime);\r\n    timing(timingInfo, RequestSigningTime.name(), collector::noteRequestSigningTime);\r\n    timing(timingInfo, ResponseProcessingTime.name(), collector::noteResponseProcessingTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "timing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void timing(TimingInfo timingInfo, String subMeasurementName, Consumer<Duration> durationConsumer)\n{\r\n    TimingInfo t1 = timingInfo.getSubMeasurement(subMeasurementName);\r\n    if (t1 != null && t1.getTimeTakenMillisIfKnown() != null) {\r\n        durationConsumer.accept(Duration.ofMillis(t1.getTimeTakenMillisIfKnown().longValue()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\statistics\\impl",
  "methodName" : "counter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void counter(TimingInfo timingInfo, String subMeasurementName, LongConsumer consumer)\n{\r\n    Number n = timingInfo.getCounter(subMeasurementName);\r\n    if (n != null) {\r\n        consumer.accept(n.longValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "toJson",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toJson(Policy policy) throws JsonProcessingException\n{\r\n    return serialization.toJson(policy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "newSid",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String newSid()\n{\r\n    SID_COUNTER.incrementAndGet();\r\n    return SID_COUNTER.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "effect",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Effects effect(final boolean allowed)\n{\r\n    return allowed ? Effects.Allow : Effects.Deny;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "resource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String resource(String bucket, String key, boolean addWildcard)\n{\r\n    return String.format(BUCKET_RESOURCE_F, bucket, key + (addWildcard ? \"*\" : \"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "resource",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String resource(Path path, final boolean isDirectory, boolean addWildcard)\n{\r\n    String key = pathToKey(path);\r\n    if (isDirectory && !key.isEmpty()) {\r\n        key = key + \"/\";\r\n    }\r\n    return resource(path.toUri().getHost(), key, addWildcard);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "directory",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String[] directory(Path path)\n{\r\n    String host = path.toUri().getHost();\r\n    String key = pathToKey(path);\r\n    if (!key.isEmpty()) {\r\n        return new String[] { resource(host, key + \"/\", true), resource(host, key, false), resource(host, key + \"/\", false) };\r\n    } else {\r\n        return new String[] { resource(host, key, true) };\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "pathToKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String pathToKey(Path path)\n{\r\n    if (path.toUri().getScheme() != null && path.toUri().getPath().isEmpty()) {\r\n        return \"\";\r\n    }\r\n    return path.toUri().getPath().substring(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "statement",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Statement statement(boolean allow, String scope, String... actions)\n{\r\n    return new Statement(RoleModel.effect(allow)).addActions(actions).addResources(scope);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "statement",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Statement statement(boolean allow, String scope, Collection<String> actions)\n{\r\n    return new Statement(RoleModel.effect(allow)).addActions(actions).addResources(scope);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "statement",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Statement statement(final boolean allow, final Path path, final boolean isDirectory, final boolean wildcards, final String... actions)\n{\r\n    return new Statement(RoleModel.effect(allow)).addActions(actions).addResources(resource(path, isDirectory, wildcards));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "statement",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Statement statement(final boolean allow, final Path path, final boolean isDirectory, final boolean wildcards, final Collection<String> actions)\n{\r\n    return new Statement(RoleModel.effect(allow)).addActions(actions).addResources(resource(path, isDirectory, wildcards));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "policy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Policy policy(Statement... statements)\n{\r\n    return new Policy(statements);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth",
  "methodName" : "policy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Policy policy(final List<RoleModel.Statement> statements)\n{\r\n    return new Policy(statements);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "initializeIssueDate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initializeIssueDate()\n{\r\n    setIssueDate(Time.now());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getBucket",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBucket()\n{\r\n    return uri.getHost();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getOrigin",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOrigin()\n{\r\n    return origin;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "setOrigin",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOrigin(final String origin)\n{\r\n    this.origin = origin;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCreated()\n{\r\n    return created;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(final DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    Text.writeString(out, uri.toString());\r\n    Text.writeString(out, origin);\r\n    Text.writeString(out, uuid);\r\n    encryptionSecrets.write(out);\r\n    out.writeLong(created);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void readFields(final DataInput in) throws DelegationTokenIOException, IOException\n{\r\n    super.readFields(in);\r\n    uri = URI.create(Text.readString(in, MAX_TEXT_LENGTH));\r\n    origin = Text.readString(in, MAX_TEXT_LENGTH);\r\n    uuid = Text.readString(in, MAX_TEXT_LENGTH);\r\n    encryptionSecrets.readFields(in);\r\n    created = in.readLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validate() throws IOException\n{\r\n    if (uri == null) {\r\n        throw new DelegationTokenIOException(\"No URI in \" + this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3ATokenIdentifier{\");\r\n    sb.append(getKind());\r\n    sb.append(\"; uri=\").append(uri);\r\n    sb.append(\"; timestamp=\").append(created);\r\n    sb.append(\"; renewer=\").append(getRenewer());\r\n    sb.append(\"; encryption=\").append(encryptionSecrets.toString());\r\n    sb.append(\"; \").append(uuid);\r\n    sb.append(\"; \").append(origin);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean equals(final Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    if (!super.equals(o)) {\r\n        return false;\r\n    }\r\n    final AbstractS3ATokenIdentifier that = (AbstractS3ATokenIdentifier) o;\r\n    return Objects.equals(uuid, that.uuid) && Objects.equals(uri, that.uri);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hash(super.hashCode(), uri);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getExpiryTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpiryTime()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getUuid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUuid()\n{\r\n    return uuid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getEncryptionSecrets",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EncryptionSecrets getEncryptionSecrets()\n{\r\n    return encryptionSecrets;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createDefaultOriginMessage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createDefaultOriginMessage()\n{\r\n    return String.format(\"Created on %s at time %s.\", NetUtils.getHostname(), java.time.Instant.now());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean initialize() throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "outputImmediatelyVisible",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean outputImmediatelyVisible()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "aboutToComplete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean aboutToComplete(String uploadId, List<PartETag> parts, long bytesWritten, final IOStatistics iostatistics) throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getDestKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDestKey()\n{\r\n    return destKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"DefaultPutTracker{\");\r\n    sb.append(\"destKey='\").append(destKey).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ContentSummary execute() throws IOException\n{\r\n    FileStatus status = probePathStatusOrNull(path, StatusProbeEnum.FILE);\r\n    if (status != null && status.isFile()) {\r\n        long length = status.getLen();\r\n        return new ContentSummary.Builder().length(length).fileCount(1).directoryCount(0).spaceConsumed(length).build();\r\n    }\r\n    final ContentSummary summary = getDirSummary(path);\r\n    LOG.debug(\"IOStatistics of getContentSummary({}):\\n{}\", path, iostatistics);\r\n    return summary;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getDirSummary",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "ContentSummary getDirSummary(Path dir) throws IOException\n{\r\n    long totalLength = 0;\r\n    long fileCount = 0;\r\n    long dirCount = 1;\r\n    RemoteIterator<S3ALocatedFileStatus> it = callbacks.listFilesIterator(dir, true);\r\n    Set<Path> dirSet = new HashSet<>();\r\n    Set<Path> pathsTraversed = new HashSet<>();\r\n    while (it.hasNext()) {\r\n        S3ALocatedFileStatus fileStatus = it.next();\r\n        Path filePath = fileStatus.getPath();\r\n        if (fileStatus.isDirectory() && !filePath.equals(dir)) {\r\n            dirSet.add(filePath);\r\n            buildDirectorySet(dirSet, pathsTraversed, dir, filePath.getParent());\r\n        } else if (!fileStatus.isDirectory()) {\r\n            fileCount += 1;\r\n            totalLength += fileStatus.getLen();\r\n            buildDirectorySet(dirSet, pathsTraversed, dir, filePath.getParent());\r\n        }\r\n    }\r\n    iostatistics.aggregate(retrieveIOStatistics(it));\r\n    return new ContentSummary.Builder().length(totalLength).fileCount(fileCount).directoryCount(dirCount + dirSet.size()).spaceConsumed(totalLength).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "buildDirectorySet",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void buildDirectorySet(Set<Path> dirSet, Set<Path> pathsTraversed, Path basePath, Path parentPath)\n{\r\n    if (parentPath == null || pathsTraversed.contains(parentPath) || parentPath.equals(basePath)) {\r\n        return;\r\n    }\r\n    dirSet.add(parentPath);\r\n    buildDirectorySet(dirSet, pathsTraversed, basePath, parentPath.getParent());\r\n    pathsTraversed.add(parentPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "probePathStatusOrNull",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileStatus probePathStatusOrNull(final Path p, final Set<StatusProbeEnum> probes) throws IOException\n{\r\n    try {\r\n        return callbacks.probePathStatus(p, probes);\r\n    } catch (FileNotFoundException fnfe) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "completeActiveCopies",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void completeActiveCopies(String reason) throws IOException\n{\r\n    LOG.debug(\"Waiting for {} active copies to complete: {}\", activeCopies.size(), reason);\r\n    waitForCompletion(activeCopies);\r\n    activeCopies.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "queueToDelete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void queueToDelete(Path path, String key)\n{\r\n    LOG.debug(\"Queueing to delete {}\", path);\r\n    keysToDelete.add(new DeleteObjectsRequest.KeyVersion(key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "queueToDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void queueToDelete(List<DirMarkerTracker.Marker> markersToDelete)\n{\r\n    markersToDelete.forEach(m -> queueToDelete(null, m.getKey()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "queueToDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void queueToDelete(final DirMarkerTracker.Marker marker)\n{\r\n    queueToDelete(marker.getPath(), marker.getKey());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "completeActiveCopiesAndDeleteSources",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void completeActiveCopiesAndDeleteSources(String reason) throws IOException\n{\r\n    completeActiveCopies(reason);\r\n    removeSourceObjects(keysToDelete);\r\n    keysToDelete.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "execute",
  "errType" : [ "AmazonClientException|IOException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Long execute() throws IOException\n{\r\n    executeOnlyOnce();\r\n    Path destCreated = destPath;\r\n    try {\r\n        if (sourceStatus.isFile()) {\r\n            destCreated = renameFileToDest();\r\n        } else {\r\n            recursiveDirectoryRename();\r\n        }\r\n    } catch (AmazonClientException | IOException ex) {\r\n        try {\r\n            completeActiveCopies(\"failure handling\");\r\n        } catch (IOException e) {\r\n            LOG.warn(\"While completing all active copies\", e);\r\n        }\r\n        throw convertToIOException(ex);\r\n    }\r\n    callbacks.finishRename(sourcePath, destCreated);\r\n    return bytesCopied.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "renameFileToDest",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Path renameFileToDest() throws IOException\n{\r\n    final StoreContext storeContext = getStoreContext();\r\n    Path copyDestinationPath = destPath;\r\n    String copyDestinationKey = destKey;\r\n    S3ObjectAttributes sourceAttributes = callbacks.createObjectAttributes(sourceStatus);\r\n    S3AReadOpContext readContext = callbacks.createReadContext(sourceStatus);\r\n    if (destStatus != null && destStatus.isDirectory()) {\r\n        String newDestKey = maybeAddTrailingSlash(destKey);\r\n        String filename = sourceKey.substring(storeContext.pathToKey(sourcePath.getParent()).length() + 1);\r\n        newDestKey = newDestKey + filename;\r\n        copyDestinationKey = newDestKey;\r\n        copyDestinationPath = storeContext.keyToPath(newDestKey);\r\n    }\r\n    LOG.debug(\"rename: renaming file {} to {}\", sourcePath, copyDestinationPath);\r\n    copySource(sourceKey, sourceAttributes, readContext, copyDestinationPath, copyDestinationKey);\r\n    bytesCopied.addAndGet(sourceStatus.getLen());\r\n    callbacks.deleteObjectAtPath(sourcePath, sourceKey, true);\r\n    return copyDestinationPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "recursiveDirectoryRename",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void recursiveDirectoryRename() throws IOException\n{\r\n    final StoreContext storeContext = getStoreContext();\r\n    LOG.debug(\"rename: renaming directory {} to {}\", sourcePath, destPath);\r\n    String dstKey = maybeAddTrailingSlash(destKey);\r\n    String srcKey = maybeAddTrailingSlash(sourceKey);\r\n    if (dstKey.startsWith(srcKey)) {\r\n        throw new RenameFailedException(srcKey, dstKey, \"cannot rename a directory to a subdirectory of itself \");\r\n    }\r\n    if (destStatus != null && destStatus.isEmptyDirectory() == Tristate.TRUE) {\r\n        LOG.debug(\"Deleting fake directory marker at destination {}\", destStatus.getPath());\r\n        callbacks.deleteObjectAtPath(destStatus.getPath(), dstKey, false);\r\n    }\r\n    Path parentPath = storeContext.keyToPath(srcKey);\r\n    DirMarkerTracker dirMarkerTracker = new DirMarkerTracker(parentPath, false);\r\n    final RemoteIterator<S3ALocatedFileStatus> iterator = callbacks.listFilesAndDirectoryMarkers(parentPath, sourceStatus, true);\r\n    while (iterator.hasNext()) {\r\n        S3ALocatedFileStatus child = iterator.next();\r\n        LOG.debug(\"To rename {}\", child);\r\n        String k = storeContext.pathToKey(child.getPath());\r\n        String key = (child.isDirectory() && !k.endsWith(\"/\")) ? k + \"/\" : k;\r\n        Path childSourcePath = storeContext.keyToPath(key);\r\n        List<DirMarkerTracker.Marker> markersToDelete;\r\n        boolean isMarker = key.endsWith(\"/\");\r\n        if (isMarker) {\r\n            markersToDelete = dirMarkerTracker.markerFound(childSourcePath, key, child);\r\n        } else {\r\n            markersToDelete = dirMarkerTracker.fileFound(childSourcePath, key, child);\r\n            String newDestKey = dstKey + key.substring(srcKey.length());\r\n            Path childDestPath = storeContext.keyToPath(newDestKey);\r\n            queueToDelete(childSourcePath, key);\r\n            CompletableFuture<Path> copy = initiateCopy(child, key, newDestKey, childDestPath);\r\n            activeCopies.add(copy);\r\n            bytesCopied.addAndGet(sourceStatus.getLen());\r\n        }\r\n        queueToDelete(markersToDelete);\r\n        endOfLoopActions();\r\n    }\r\n    copyEmptyDirectoryMarkers(srcKey, dstKey, dirMarkerTracker);\r\n    completeActiveCopiesAndDeleteSources(\"final copy and delete\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "endOfLoopActions",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void endOfLoopActions() throws IOException\n{\r\n    if (keysToDelete.size() == pageSize) {\r\n        completeActiveCopiesAndDeleteSources(\"paged delete\");\r\n    } else {\r\n        if (activeCopies.size() == RENAME_PARALLEL_LIMIT) {\r\n            LOG.debug(\"Waiting for active copies to complete\");\r\n            completeActiveCopies(\"batch threshold reached\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "copyEmptyDirectoryMarkers",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "OperationDuration copyEmptyDirectoryMarkers(final String srcKey, final String dstKey, final DirMarkerTracker dirMarkerTracker) throws IOException\n{\r\n    LOG.debug(\"Copying markers from {}\", dirMarkerTracker);\r\n    final StoreContext storeContext = getStoreContext();\r\n    Map<Path, DirMarkerTracker.Marker> leafMarkers = dirMarkerTracker.getLeafMarkers();\r\n    Map<Path, DirMarkerTracker.Marker> surplus = dirMarkerTracker.getSurplusMarkers();\r\n    DurationInfo duration = new DurationInfo(LOG, false, \"copying %d leaf markers with %d surplus not copied\", leafMarkers.size(), surplus.size());\r\n    for (DirMarkerTracker.Marker entry : leafMarkers.values()) {\r\n        String key = entry.getKey();\r\n        String newDestKey = dstKey + key.substring(srcKey.length());\r\n        Path childDestPath = storeContext.keyToPath(newDestKey);\r\n        LOG.debug(\"copying dir marker from {} to {}\", key, newDestKey);\r\n        activeCopies.add(initiateCopy(entry.getStatus(), key, newDestKey, childDestPath));\r\n        queueToDelete(entry);\r\n        endOfLoopActions();\r\n    }\r\n    duration.close();\r\n    return duration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "initiateCopy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CompletableFuture<Path> initiateCopy(final S3ALocatedFileStatus source, final String key, final String newDestKey, final Path childDestPath)\n{\r\n    S3ObjectAttributes sourceAttributes = callbacks.createObjectAttributes(source.getPath(), source.getEtag(), source.getVersionId(), source.getLen());\r\n    return submit(getStoreContext().getExecutor(), callableWithinAuditSpan(getAuditSpan(), () -> copySource(key, sourceAttributes, callbacks.createReadContext(source), childDestPath, newDestKey)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "copySource",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path copySource(final String srcKey, final S3ObjectAttributes srcAttributes, final S3AReadOpContext readContext, final Path destination, final String destinationKey) throws IOException\n{\r\n    long len = srcAttributes.getLen();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Copy file from %s to %s (length=%d)\", srcKey, destinationKey, len)) {\r\n        callbacks.copyFile(srcKey, destinationKey, srcAttributes, readContext);\r\n    }\r\n    return destination;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "removeSourceObjects",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeSourceObjects(final List<DeleteObjectsRequest.KeyVersion> keys) throws IOException\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Initiating delete operation for {} objects\", keys.size());\r\n        for (DeleteObjectsRequest.KeyVersion key : keys) {\r\n            LOG.debug(\" {} {}\", key.getKey(), key.getVersion() != null ? key.getVersion() : \"\");\r\n        }\r\n    }\r\n    Invoker.once(\"rename \" + sourcePath + \" to \" + destPath, sourcePath.toString(), () -> callbacks.removeKeys(keys, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "maybeAddTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String maybeAddTrailingSlash(String key)\n{\r\n    if (!key.isEmpty() && !key.endsWith(\"/\")) {\r\n        return key + '/';\r\n    } else {\r\n        return key;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "convertToIOException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException convertToIOException(final Exception ex)\n{\r\n    if (ex instanceof IOException) {\r\n        return (IOException) ex;\r\n    } else if (ex instanceof SdkBaseException) {\r\n        return translateException(\"rename \" + sourcePath + \" to \" + destPath, sourcePath.toString(), (SdkBaseException) ex);\r\n    } else {\r\n        return new IOException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCause",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AmazonServiceException getCause()\n{\r\n    return (AmazonServiceException) super.getCause();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRequestId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRequestId()\n{\r\n    return getCause().getRequestId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getServiceName()\n{\r\n    return getCause().getServiceName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getErrorCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getErrorCode()\n{\r\n    return getCause().getErrorCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getStatusCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getStatusCode()\n{\r\n    return getCause().getStatusCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRawResponseContent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRawResponseContent()\n{\r\n    return getCause().getRawResponseContent();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isRetryable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRetryable()\n{\r\n    return getCause().isRetryable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createCredentials",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AWSCredentials createCredentials(Configuration config) throws IOException\n{\r\n    MarshalledCredentials creds = MarshalledCredentialBinding.fromFileSystem(getUri(), config);\r\n    MarshalledCredentials.CredentialTypeRequired sessionOnly = MarshalledCredentials.CredentialTypeRequired.SessionOnly;\r\n    if (!creds.isValid(sessionOnly)) {\r\n        throw new NoAwsCredentialsException(COMPONENT);\r\n    }\r\n    return MarshalledCredentialBinding.toAWSCredentials(creds, sessionOnly, COMPONENT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic)\n{\r\n    incrementStatistic(statistic, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementStatistic(Statistic statistic, long count)\n{\r\n    instrumentation.incrementCounter(statistic, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "bulkDeleteRetried",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void bulkDeleteRetried(DeleteObjectsRequest deleteRequest, Exception ex)\n{\r\n    LOG.debug(\"Retrying on error during bulk delete\", ex);\r\n    if (isThrottleException(ex)) {\r\n        onDeleteThrottled(deleteRequest);\r\n    } else if (isSymptomOfBrokenConnection(ex)) {\r\n        LOG.warn(\"Bulk delete operation interrupted: {}\", ex.getMessage());\r\n        onDeleteThrottled(deleteRequest);\r\n    } else {\r\n        incrementStatistic(IGNORED_ERRORS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "onDeleteThrottled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void onDeleteThrottled(final DeleteObjectsRequest deleteRequest)\n{\r\n    final List<DeleteObjectsRequest.KeyVersion> keys = deleteRequest.getKeys();\r\n    final int size = keys.size();\r\n    incrementStatistic(STORE_IO_THROTTLED, size);\r\n    instrumentation.addValueToQuantiles(STORE_IO_THROTTLE_RATE, size);\r\n    THROTTLE_LOG.info(\"Bulk delete {} keys throttled -first key = {}; last = {}\", size, keys.get(0).getKey(), keys.get(size - 1).getKey());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "isSymptomOfBrokenConnection",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isSymptomOfBrokenConnection(final Exception ex)\n{\r\n    return ex instanceof AWSClientIOException && ex.getCause() instanceof SdkClientException && ex.getMessage().contains(XML_PARSE_BROKEN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "PathOutputCommitter createOutputCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    FileSystem fs = getDestinationFileSystem(outputPath, context);\r\n    PathOutputCommitter outputCommitter;\r\n    if (fs instanceof S3AFileSystem) {\r\n        outputCommitter = createTaskCommitter((S3AFileSystem) fs, outputPath, context);\r\n    } else {\r\n        throw new PathCommitException(outputPath, \"Filesystem not supported by this committer\");\r\n    }\r\n    LOG.info(\"Using Committer {} for {}\", outputCommitter, outputPath);\r\n    return outputCommitter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getDestinationFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getDestinationFileSystem(Path outputPath, JobContext context) throws IOException\n{\r\n    return outputPath != null ? FileSystem.get(outputPath.toUri(), context.getConfiguration()) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createTaskCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PathOutputCommitter createTaskCommitter(S3AFileSystem fileSystem, Path outputPath, TaskAttemptContext context) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "toPartEtags",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<PartETag> toPartEtags(List<String> tagIds)\n{\r\n    return IntStream.range(0, tagIds.size()).mapToObj(i -> new PartETag(i + 1, tagIds.get(i))).collect(Collectors.toList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"CommitOperations{\" + fs.getUri() + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommitterStatistics getStatistics()\n{\r\n    return statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return statistics.getIOStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitOrFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitOrFail(final SinglePendingCommit commit) throws IOException\n{\r\n    commit(commit, commit.getFilename()).maybeRethrow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commit",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "MaybeIOE commit(final SinglePendingCommit commit, final String origin)\n{\r\n    LOG.debug(\"Committing single commit {}\", commit);\r\n    MaybeIOE outcome;\r\n    String destKey = \"unknown destination\";\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Committing file %s size %s\", commit.getDestinationKey(), commit.getLength())) {\r\n        commit.validate();\r\n        destKey = commit.getDestinationKey();\r\n        long l = trackDuration(statistics, COMMITTER_MATERIALIZE_FILE.getSymbol(), () -> innerCommit(commit));\r\n        LOG.debug(\"Successful commit of file length {}\", l);\r\n        outcome = MaybeIOE.NONE;\r\n        statistics.commitCompleted(commit.getLength());\r\n    } catch (IOException e) {\r\n        String msg = String.format(\"Failed to commit upload against %s: %s\", destKey, e);\r\n        LOG.warn(msg, e);\r\n        outcome = new MaybeIOE(e);\r\n        statistics.commitFailed();\r\n    } catch (Exception e) {\r\n        String msg = String.format(\"Failed to commit upload against %s,\" + \" described in %s: %s\", destKey, origin, e);\r\n        LOG.warn(msg, e);\r\n        outcome = new MaybeIOE(new PathCommitException(origin, msg, e));\r\n        statistics.commitFailed();\r\n    }\r\n    return outcome;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "innerCommit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long innerCommit(final SinglePendingCommit commit) throws IOException\n{\r\n    writeOperations.commitUpload(commit.getDestinationKey(), commit.getUploadId(), toPartEtags(commit.getEtags()), commit.getLength());\r\n    return commit.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "locateAllSinglePendingCommits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<LocatedFileStatus> locateAllSinglePendingCommits(Path pendingDir, boolean recursive) throws IOException\n{\r\n    return listAndFilter(fs, pendingDir, recursive, PENDING_FILTER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "loadSinglePendingCommits",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Pair<PendingSet, List<Pair<LocatedFileStatus, IOException>>> loadSinglePendingCommits(Path pendingDir, boolean recursive) throws IOException\n{\r\n    List<LocatedFileStatus> statusList = locateAllSinglePendingCommits(pendingDir, recursive);\r\n    PendingSet commits = new PendingSet(statusList.size());\r\n    List<Pair<LocatedFileStatus, IOException>> failures = new ArrayList<>(1);\r\n    for (LocatedFileStatus status : statusList) {\r\n        try {\r\n            commits.add(SinglePendingCommit.load(fs, status.getPath()));\r\n        } catch (IOException e) {\r\n            LOG.warn(\"Failed to load commit file {}\", status.getPath(), e);\r\n            failures.add(Pair.of(status, e));\r\n        }\r\n    }\r\n    return Pair.of(commits, failures);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "makeIOE",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException makeIOE(String key, Exception ex)\n{\r\n    return ex instanceof IOException ? (IOException) ex : new PathCommitException(key, ex.toString(), ex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortSingleCommit",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void abortSingleCommit(SinglePendingCommit commit) throws IOException\n{\r\n    String destKey = commit.getDestinationKey();\r\n    String origin = commit.getFilename() != null ? (\" defined in \" + commit.getFilename()) : \"\";\r\n    String uploadId = commit.getUploadId();\r\n    LOG.info(\"Aborting commit ID {} to object {}{}\", uploadId, destKey, origin);\r\n    abortMultipartCommit(destKey, uploadId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortMultipartCommit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abortMultipartCommit(String destKey, String uploadId) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Aborting commit ID %s to path %s\", uploadId, destKey)) {\r\n        writeOperations.abortMultipartCommit(destKey, uploadId);\r\n    } finally {\r\n        statistics.commitAborted();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortAllSinglePendingCommits",
  "errType" : [ "FileNotFoundException", "FileNotFoundException", "IOException|IllegalArgumentException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "MaybeIOE abortAllSinglePendingCommits(Path pendingDir, boolean recursive) throws IOException\n{\r\n    Preconditions.checkArgument(pendingDir != null, \"null pendingDir\");\r\n    LOG.debug(\"Aborting all pending commit filess under {}\" + \" (recursive={}\", pendingDir, recursive);\r\n    RemoteIterator<LocatedFileStatus> pendingFiles;\r\n    try {\r\n        pendingFiles = ls(pendingDir, recursive);\r\n    } catch (FileNotFoundException fnfe) {\r\n        LOG.info(\"No directory to abort {}\", pendingDir);\r\n        return MaybeIOE.NONE;\r\n    }\r\n    MaybeIOE outcome = MaybeIOE.NONE;\r\n    if (!pendingFiles.hasNext()) {\r\n        LOG.debug(\"No files to abort under {}\", pendingDir);\r\n    }\r\n    while (pendingFiles.hasNext()) {\r\n        Path pendingFile = pendingFiles.next().getPath();\r\n        if (pendingFile.getName().endsWith(CommitConstants.PENDING_SUFFIX)) {\r\n            try {\r\n                abortSingleCommit(SinglePendingCommit.load(fs, pendingFile));\r\n            } catch (FileNotFoundException e) {\r\n                LOG.debug(\"listed file already deleted: {}\", pendingFile);\r\n            } catch (IOException | IllegalArgumentException e) {\r\n                if (MaybeIOE.NONE.equals(outcome)) {\r\n                    outcome = new MaybeIOE(makeIOE(pendingFile.toString(), e));\r\n                }\r\n            } finally {\r\n                S3AUtils.deleteQuietly(fs, pendingFile, false);\r\n            }\r\n        }\r\n    }\r\n    cleanupRemoteIterator(pendingFiles);\r\n    return outcome;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "ls",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> ls(Path path, boolean recursive) throws IOException\n{\r\n    return fs.listFiles(path, recursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "listPendingUploadsUnderPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<MultipartUpload> listPendingUploadsUnderPath(Path dest) throws IOException\n{\r\n    return writeOperations.listMultipartUploads(fs.pathToKey(dest));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortPendingUploadsUnderPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int abortPendingUploadsUnderPath(Path dest) throws IOException\n{\r\n    return writeOperations.abortMultipartUploadsUnderPath(fs.pathToKey(dest));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "deleteSuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteSuccessMarker(Path outputPath) throws IOException\n{\r\n    fs.delete(new Path(outputPath, _SUCCESS), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "createSuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createSuccessMarker(Path outputPath, SuccessData successData, boolean addMetrics) throws IOException\n{\r\n    Preconditions.checkArgument(outputPath != null, \"null outputPath\");\r\n    if (addMetrics) {\r\n        addFileSystemStatistics(successData.getMetrics());\r\n    }\r\n    Path markerPath = new Path(outputPath, _SUCCESS);\r\n    LOG.debug(\"Touching success marker for job {}: {}\", markerPath, successData);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Writing success file %s\", markerPath)) {\r\n        successData.save(fs, markerPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "revertCommit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void revertCommit(SinglePendingCommit commit) throws IOException\n{\r\n    LOG.info(\"Revert {}\", commit);\r\n    try {\r\n        writeOperations.revertCommit(commit.getDestinationKey());\r\n    } finally {\r\n        statistics.commitReverted();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "uploadFileToPendingCommit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "SinglePendingCommit uploadFileToPendingCommit(File localFile, Path destPath, String partition, long uploadPartSize, Progressable progress) throws IOException\n{\r\n    LOG.debug(\"Initiating multipart upload from {} to {}\", localFile, destPath);\r\n    Preconditions.checkArgument(destPath != null);\r\n    if (!localFile.isFile()) {\r\n        throw new FileNotFoundException(\"Not a file: \" + localFile);\r\n    }\r\n    String destURI = destPath.toUri().toString();\r\n    String destKey = fs.pathToKey(destPath);\r\n    String uploadId = null;\r\n    boolean threw = true;\r\n    final DurationTracker tracker = statistics.trackDuration(COMMITTER_STAGE_FILE_UPLOAD.getSymbol());\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Upload staged file from %s to %s\", localFile.getAbsolutePath(), destPath)) {\r\n        statistics.commitCreated();\r\n        uploadId = writeOperations.initiateMultiPartUpload(destKey);\r\n        long length = localFile.length();\r\n        SinglePendingCommit commitData = new SinglePendingCommit();\r\n        commitData.setDestinationKey(destKey);\r\n        commitData.setBucket(fs.getBucket());\r\n        commitData.touch(System.currentTimeMillis());\r\n        commitData.setUploadId(uploadId);\r\n        commitData.setUri(destURI);\r\n        commitData.setText(partition != null ? \"partition: \" + partition : \"\");\r\n        commitData.setLength(length);\r\n        long offset = 0;\r\n        long numParts = (length / uploadPartSize + ((length % uploadPartSize) > 0 ? 1 : 0));\r\n        if (numParts == 0) {\r\n            numParts = 1;\r\n        }\r\n        if (numParts > InternalConstants.DEFAULT_UPLOAD_PART_COUNT_LIMIT) {\r\n            throw new PathIOException(destPath.toString(), String.format(\"File to upload (size %d)\" + \" is too big to be uploaded in parts of size %d\", numParts, length));\r\n        }\r\n        List<PartETag> parts = new ArrayList<>((int) numParts);\r\n        LOG.debug(\"File size is {}, number of parts to upload = {}\", length, numParts);\r\n        for (int partNumber = 1; partNumber <= numParts; partNumber += 1) {\r\n            progress.progress();\r\n            long size = Math.min(length - offset, uploadPartSize);\r\n            UploadPartRequest part;\r\n            part = writeOperations.newUploadPartRequest(destKey, uploadId, partNumber, (int) size, null, localFile, offset);\r\n            part.setLastPart(partNumber == numParts);\r\n            UploadPartResult partResult = writeOperations.uploadPart(part);\r\n            offset += uploadPartSize;\r\n            parts.add(partResult.getPartETag());\r\n        }\r\n        commitData.bindCommitData(parts);\r\n        statistics.commitUploaded(length);\r\n        threw = false;\r\n        return commitData;\r\n    } finally {\r\n        if (threw && uploadId != null) {\r\n            try {\r\n                abortMultipartCommit(destKey, uploadId);\r\n            } catch (IOException e) {\r\n                LOG.error(\"Failed to abort upload {} to {}\", uploadId, destKey, e);\r\n            }\r\n        }\r\n        if (threw) {\r\n            tracker.failed();\r\n        }\r\n        tracker.close();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "addFileSystemStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFileSystemStatistics(Map<String, Long> dest)\n{\r\n    dest.putAll(fs.getInstrumentation().toMap());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "taskCompleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void taskCompleted(boolean success)\n{\r\n    statistics.taskCompleted(success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "jobCompleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void jobCompleted(boolean success)\n{\r\n    statistics.jobCompleted(success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "initiateCommitOperation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommitContext initiateCommitOperation(Path path) throws IOException\n{\r\n    return new CommitContext();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "extractMagicFileLength",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Optional<Long> extractMagicFileLength(FileSystem fs, Path path) throws IOException\n{\r\n    byte[] bytes;\r\n    try {\r\n        bytes = fs.getXAttr(path, XA_MAGIC_MARKER);\r\n    } catch (UnsupportedOperationException e) {\r\n        LOG.debug(\"Filesystem {} doesn't support XAttr API\", fs);\r\n        return Optional.empty();\r\n    }\r\n    return HeaderProcessing.extractXAttrLongValue(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSCredentials getCredentials()\n{\r\n    if (!StringUtils.isEmpty(accessKey) && !StringUtils.isEmpty(secretKey)) {\r\n        return new BasicAWSCredentials(accessKey, secretKey);\r\n    }\r\n    throw new NoAwsCredentialsException(\"SimpleAWSCredentialsProvider\", \"No AWS credentials in the Hadoop configuration\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void refresh()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "translateDeleteException",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "IOException translateDeleteException(final String message, final MultiObjectDeleteException deleteException)\n{\r\n    List<MultiObjectDeleteException.DeleteError> errors = deleteException.getErrors();\r\n    LOG.info(\"Bulk delete operation failed to delete all objects;\" + \" failure count = {}\", errors.size());\r\n    final StringBuilder result = new StringBuilder(errors.size() * 256);\r\n    result.append(message).append(\": \");\r\n    String exitCode = \"\";\r\n    for (MultiObjectDeleteException.DeleteError error : deleteException.getErrors()) {\r\n        String code = error.getCode();\r\n        String item = String.format(\"%s: %s%s: %s%n\", code, error.getKey(), (error.getVersionId() != null ? (\" (\" + error.getVersionId() + \")\") : \"\"), error.getMessage());\r\n        LOG.info(item);\r\n        result.append(item);\r\n        if (exitCode == null || exitCode.isEmpty() || ACCESS_DENIED.equals(code)) {\r\n            exitCode = code;\r\n        }\r\n    }\r\n    if (ACCESS_DENIED.equals(exitCode)) {\r\n        return (IOException) new AccessDeniedException(result.toString()).initCause(deleteException);\r\n    } else {\r\n        return new AWSS3IOException(result.toString(), deleteException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "withConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationAuditorOptions withConfiguration(final Configuration value)\n{\r\n    configuration = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "getIoStatisticsStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIoStatisticsStore()\n{\r\n    return ioStatisticsStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "withIoStatisticsStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationAuditorOptions withIoStatisticsStore(final IOStatisticsStore value)\n{\r\n    ioStatisticsStore = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit",
  "methodName" : "builder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationAuditorOptions builder()\n{\r\n    return new OperationAuditorOptions();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "loadAWSCredentials",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void loadAWSCredentials() throws IOException\n{\r\n    credentialOrigin = AbstractS3ATokenIdentifier.createDefaultOriginMessage();\r\n    Configuration conf = getConfig();\r\n    URI uri = getCanonicalUri();\r\n    S3xLoginHelper.Login secrets = S3AUtils.getAWSAccessKeys(uri, conf);\r\n    if (secrets.hasLogin()) {\r\n        awsCredentials = new MarshalledCredentials(secrets.getUser(), secrets.getPassword(), \"\");\r\n        credentialOrigin += \"; source = Hadoop configuration data\";\r\n    } else {\r\n        awsCredentials = MarshalledCredentialBinding.fromEnvironment(System.getenv());\r\n        if (awsCredentials.isValid(MarshalledCredentials.CredentialTypeRequired.AnyNonEmpty)) {\r\n            credentialOrigin += \"; source = Environment variables\";\r\n        } else {\r\n            credentialOrigin = \"no credentials in configuration or\" + \" environment variables\";\r\n        }\r\n    }\r\n    awsCredentials.validate(credentialOrigin + \": \", MarshalledCredentials.CredentialTypeRequired.AnyNonEmpty);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "deployUnbonded",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AWSCredentialProviderList deployUnbonded() throws IOException\n{\r\n    requireServiceStarted();\r\n    loadAWSCredentials();\r\n    return new AWSCredentialProviderList(\"Full Credentials Token Binding\", new MarshalledCredentialProvider(FULL_TOKEN, getStoreContext().getFsURI(), getConfig(), awsCredentials, MarshalledCredentials.CredentialTypeRequired.AnyNonEmpty));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AbstractS3ATokenIdentifier createTokenIdentifier(final Optional<RoleModel.Policy> policy, final EncryptionSecrets encryptionSecrets, final Text renewer) throws IOException\n{\r\n    requireServiceStarted();\r\n    Preconditions.checkNotNull(awsCredentials, \"No AWS credentials to use for a delegation token\");\r\n    return new FullCredentialsTokenIdentifier(getCanonicalUri(), getOwnerText(), renewer, awsCredentials, encryptionSecrets, credentialOrigin);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AWSCredentialProviderList bindToTokenIdentifier(final AbstractS3ATokenIdentifier retrievedIdentifier) throws IOException\n{\r\n    FullCredentialsTokenIdentifier tokenIdentifier = convertTokenIdentifier(retrievedIdentifier, FullCredentialsTokenIdentifier.class);\r\n    return new AWSCredentialProviderList(\"Full Credentials Token Binding\", new MarshalledCredentialProvider(FULL_TOKEN, getStoreContext().getFsURI(), getConfig(), tokenIdentifier.getMarshalledCredentials(), MarshalledCredentials.CredentialTypeRequired.AnyNonEmpty));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createEmptyIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ATokenIdentifier createEmptyIdentifier()\n{\r\n    return new FullCredentialsTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "bindSSLChannelMode",
  "errType" : [ "ClassNotFoundException|NoSuchMethodException|IllegalAccessException|InstantiationException|InvocationTargetException|LinkageError" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void bindSSLChannelMode(Configuration conf, ClientConfiguration awsConf) throws IOException\n{\r\n    String channelModeString = conf.getTrimmed(SSL_CHANNEL_MODE, DEFAULT_SSL_CHANNEL_MODE.name());\r\n    DelegatingSSLSocketFactory.SSLChannelMode channelMode = null;\r\n    for (DelegatingSSLSocketFactory.SSLChannelMode mode : DelegatingSSLSocketFactory.SSLChannelMode.values()) {\r\n        if (mode.name().equalsIgnoreCase(channelModeString)) {\r\n            channelMode = mode;\r\n        }\r\n    }\r\n    if (channelMode == null) {\r\n        throw new IllegalArgumentException(channelModeString + \" is not a valid value for \" + SSL_CHANNEL_MODE);\r\n    }\r\n    DelegatingSSLSocketFactory.initializeDefaultFactory(channelMode);\r\n    try {\r\n        Class<? extends ConfigureAWSSocketFactory> clazz = (Class<? extends ConfigureAWSSocketFactory>) Class.forName(BINDING_CLASSNAME);\r\n        clazz.getConstructor().newInstance().configureSocketFactory(awsConf, channelMode);\r\n    } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException | InstantiationException | InvocationTargetException | LinkageError e) {\r\n        LOG.debug(\"Unable to create class {}, value of {} will be ignored\", BINDING_CLASSNAME, SSL_CHANNEL_MODE, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "fixBucketRegion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String fixBucketRegion(final String region)\n{\r\n    return region == null || region.equals(\"US\") ? \"us-east-1\" : region;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "logDnsLookup",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void logDnsLookup(Configuration conf)\n{\r\n    String endPoint = conf.getTrimmed(ENDPOINT, DEFAULT_ENDPOINT);\r\n    String hostName = endPoint;\r\n    if (!endPoint.isEmpty() && LOG.isDebugEnabled()) {\r\n        if (endPoint.contains(\"://\")) {\r\n            try {\r\n                URI uri = new URI(endPoint);\r\n                hostName = uri.getHost();\r\n            } catch (URISyntaxException e) {\r\n                LOG.debug(\"Got URISyntaxException, ignoring\");\r\n            }\r\n        }\r\n        LOG.debug(\"Bucket endpoint : {}, Hostname : {}, DNSAddress : {}\", endPoint, hostName, NetUtils.normalizeHostName(hostName));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "progressChanged",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void progressChanged(ProgressEvent progressEvent)\n{\r\n    if (progress != null) {\r\n        progress.progress();\r\n    }\r\n    ProgressEventType pet = progressEvent.getEventType();\r\n    if (pet == TRANSFER_PART_STARTED_EVENT || pet == TRANSFER_COMPLETED_EVENT) {\r\n        fs.incrementWriteOperations();\r\n    }\r\n    long transferred = upload.getProgress().getBytesTransferred();\r\n    long delta = transferred - lastBytesTransferred;\r\n    fs.incrementPutProgressStatistics(key, delta);\r\n    lastBytesTransferred = transferred;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "uploadCompleted",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long uploadCompleted()\n{\r\n    long delta = upload.getProgress().getBytesTransferred() - lastBytesTransferred;\r\n    if (delta > 0) {\r\n        LOG.debug(\"S3A write delta changed after finished: {} bytes\", delta);\r\n        fs.incrementPutProgressStatistics(key, delta);\r\n    }\r\n    return delta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildAmazonS3Client",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AmazonS3 buildAmazonS3Client(final ClientConfiguration awsConf, final S3ClientCreationParameters parameters)\n{\r\n    LOG.warn(\"** FAILURE INJECTION ENABLED.  Do not run in production! **\");\r\n    LOG.warn(\"List inconsistency is no longer emulated; only throttling and read errors\");\r\n    InconsistentAmazonS3Client s3 = new InconsistentAmazonS3Client(parameters.getCredentialSet(), awsConf, getConf());\r\n    configureAmazonS3Client(s3, parameters.getEndpoint(), parameters.isPathStyleAccess());\r\n    return s3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return operations.getConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "isEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnabled()\n{\r\n    return enabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "isSelectEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSelectEnabled(Configuration conf)\n{\r\n    return conf.getBoolean(FS_S3A_SELECT_ENABLED, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "select",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataInputStream select(final S3AReadOpContext readContext, final String expression, final Configuration builderOptions, final S3ObjectAttributes objectAttributes) throws IOException\n{\r\n    return new FSDataInputStream(executeSelect(readContext, objectAttributes, builderOptions, buildSelectRequest(readContext.getPath(), expression, builderOptions)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "buildSelectRequest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SelectObjectContentRequest buildSelectRequest(final Path path, final String expression, final Configuration builderOptions) throws IOException\n{\r\n    Preconditions.checkState(isEnabled(), \"S3 Select is not enabled for %s\", path);\r\n    SelectObjectContentRequest request = operations.newSelectRequest(path);\r\n    buildRequest(request, expression, builderOptions);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "executeSelect",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SelectInputStream executeSelect(final S3AReadOpContext readContext, final S3ObjectAttributes objectAttributes, final Configuration builderOptions, final SelectObjectContentRequest request) throws IOException\n{\r\n    Path path = readContext.getPath();\r\n    if (readContext.getDstFileStatus().isDirectory()) {\r\n        throw new PathIOException(path.toString(), \"Can't select \" + path + \" because it is a directory\");\r\n    }\r\n    boolean sqlInErrors = builderOptions.getBoolean(SELECT_ERRORS_INCLUDE_SQL, errorsIncludeSql);\r\n    String expression = request.getExpression();\r\n    final String errorText = sqlInErrors ? expression : \"Select\";\r\n    if (sqlInErrors) {\r\n        LOG.info(\"Issuing SQL request {}\", expression);\r\n    }\r\n    return new SelectInputStream(readContext, objectAttributes, operations.select(path, request, errorText));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "buildRequest",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void buildRequest(final SelectObjectContentRequest request, final String expression, final Configuration builderOptions) throws IllegalArgumentException, IOException\n{\r\n    Preconditions.checkArgument(StringUtils.isNotEmpty(expression), \"No expression provided in parameter \" + SELECT_SQL);\r\n    final Configuration ownerConf = operations.getConf();\r\n    String inputFormat = builderOptions.get(SELECT_INPUT_FORMAT, SELECT_FORMAT_CSV).toLowerCase(Locale.ENGLISH);\r\n    Preconditions.checkArgument(SELECT_FORMAT_CSV.equals(inputFormat), \"Unsupported input format %s\", inputFormat);\r\n    String outputFormat = builderOptions.get(SELECT_OUTPUT_FORMAT, SELECT_FORMAT_CSV).toLowerCase(Locale.ENGLISH);\r\n    Preconditions.checkArgument(SELECT_FORMAT_CSV.equals(outputFormat), \"Unsupported output format %s\", outputFormat);\r\n    request.setExpressionType(ExpressionType.SQL);\r\n    request.setExpression(expandBackslashChars(expression));\r\n    InputSerialization inputSerialization = buildCsvInputRequest(ownerConf, builderOptions);\r\n    String compression = opt(builderOptions, ownerConf, SELECT_INPUT_COMPRESSION, COMPRESSION_OPT_NONE, true).toUpperCase(Locale.ENGLISH);\r\n    if (isNotEmpty(compression)) {\r\n        inputSerialization.setCompressionType(compression);\r\n    }\r\n    request.setInputSerialization(inputSerialization);\r\n    request.setOutputSerialization(buildCSVOutput(ownerConf, builderOptions));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "buildCsvInputRequest",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "InputSerialization buildCsvInputRequest(final Configuration ownerConf, final Configuration builderOptions) throws IllegalArgumentException, IOException\n{\r\n    String headerInfo = opt(builderOptions, ownerConf, CSV_INPUT_HEADER, CSV_INPUT_HEADER_OPT_DEFAULT, true).toUpperCase(Locale.ENGLISH);\r\n    String commentMarker = xopt(builderOptions, ownerConf, CSV_INPUT_COMMENT_MARKER, CSV_INPUT_COMMENT_MARKER_DEFAULT);\r\n    String fieldDelimiter = xopt(builderOptions, ownerConf, CSV_INPUT_INPUT_FIELD_DELIMITER, CSV_INPUT_FIELD_DELIMITER_DEFAULT);\r\n    String recordDelimiter = xopt(builderOptions, ownerConf, CSV_INPUT_RECORD_DELIMITER, CSV_INPUT_RECORD_DELIMITER_DEFAULT);\r\n    String quoteCharacter = xopt(builderOptions, ownerConf, CSV_INPUT_QUOTE_CHARACTER, CSV_INPUT_QUOTE_CHARACTER_DEFAULT);\r\n    String quoteEscapeCharacter = xopt(builderOptions, ownerConf, CSV_INPUT_QUOTE_ESCAPE_CHARACTER, CSV_INPUT_QUOTE_ESCAPE_CHARACTER_DEFAULT);\r\n    CSVInput csv = new CSVInput();\r\n    csv.setFieldDelimiter(fieldDelimiter);\r\n    csv.setRecordDelimiter(recordDelimiter);\r\n    csv.setComments(commentMarker);\r\n    csv.setQuoteCharacter(quoteCharacter);\r\n    if (StringUtils.isNotEmpty(quoteEscapeCharacter)) {\r\n        csv.setQuoteEscapeCharacter(quoteEscapeCharacter);\r\n    }\r\n    csv.setFileHeaderInfo(headerInfo);\r\n    InputSerialization inputSerialization = new InputSerialization();\r\n    inputSerialization.setCsv(csv);\r\n    return inputSerialization;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "buildCSVOutput",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "OutputSerialization buildCSVOutput(final Configuration ownerConf, final Configuration builderOptions) throws IllegalArgumentException, IOException\n{\r\n    String fieldDelimiter = xopt(builderOptions, ownerConf, CSV_OUTPUT_FIELD_DELIMITER, CSV_OUTPUT_FIELD_DELIMITER_DEFAULT);\r\n    String recordDelimiter = xopt(builderOptions, ownerConf, CSV_OUTPUT_RECORD_DELIMITER, CSV_OUTPUT_RECORD_DELIMITER_DEFAULT);\r\n    String quoteCharacter = xopt(builderOptions, ownerConf, CSV_OUTPUT_QUOTE_CHARACTER, CSV_OUTPUT_QUOTE_CHARACTER_DEFAULT);\r\n    String quoteEscapeCharacter = xopt(builderOptions, ownerConf, CSV_OUTPUT_QUOTE_ESCAPE_CHARACTER, CSV_OUTPUT_QUOTE_ESCAPE_CHARACTER_DEFAULT);\r\n    String quoteFields = xopt(builderOptions, ownerConf, CSV_OUTPUT_QUOTE_FIELDS, CSV_OUTPUT_QUOTE_FIELDS_ALWAYS).toUpperCase(Locale.ENGLISH);\r\n    OutputSerialization outputSerialization = new OutputSerialization();\r\n    CSVOutput csvOut = new CSVOutput();\r\n    csvOut.setQuoteCharacter(quoteCharacter);\r\n    csvOut.setQuoteFields(QuoteFields.fromValue(quoteFields));\r\n    csvOut.setFieldDelimiter(fieldDelimiter);\r\n    csvOut.setRecordDelimiter(recordDelimiter);\r\n    if (!quoteEscapeCharacter.isEmpty()) {\r\n        csvOut.setQuoteEscapeCharacter(quoteEscapeCharacter);\r\n    }\r\n    outputSerialization.setCsv(csvOut);\r\n    return outputSerialization;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString(final SelectObjectContentRequest request)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"SelectObjectContentRequest{\").append(\"bucket name=\").append(request.getBucketName()).append(\"; key=\").append(request.getKey()).append(\"; expressionType=\").append(request.getExpressionType()).append(\"; expression=\").append(request.getExpression());\r\n    InputSerialization input = request.getInputSerialization();\r\n    if (input != null) {\r\n        sb.append(\"; Input\").append(input.toString());\r\n    } else {\r\n        sb.append(\"; Input Serialization: none\");\r\n    }\r\n    OutputSerialization out = request.getOutputSerialization();\r\n    if (out != null) {\r\n        sb.append(\"; Output\").append(out.toString());\r\n    } else {\r\n        sb.append(\"; Output Serialization: none\");\r\n    }\r\n    return sb.append(\"}\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "opt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String opt(Configuration builderOptions, Configuration fsConf, String base, String defVal, boolean trim)\n{\r\n    String r = builderOptions.get(base, fsConf.get(base, defVal));\r\n    return trim ? r.trim() : r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "xopt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String xopt(Configuration selectOpts, Configuration fsConf, String base, String defVal)\n{\r\n    return expandBackslashChars(opt(selectOpts, fsConf, base, defVal, false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\select",
  "methodName" : "expandBackslashChars",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String expandBackslashChars(String src)\n{\r\n    return src.replace(\"\\\\n\", \"\\n\").replace(\"\\\\\\\"\", \"\\\"\").replace(\"\\\\t\", \"\\t\").replace(\"\\\\r\", \"\\r\").replace(\"\\\\\\\"\", \"\\\"\").replace(\"\\\\\\\\\", \"\\\\\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createS3Client",
  "errType" : [ "SdkClientException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "AmazonS3 createS3Client(final URI uri, final S3ClientCreationParameters parameters) throws IOException\n{\r\n    Configuration conf = getConf();\r\n    bucket = uri.getHost();\r\n    final ClientConfiguration awsConf = S3AUtils.createAwsConf(conf, bucket, Constants.AWS_SERVICE_IDENTIFIER_S3);\r\n    parameters.getHeaders().forEach((h, v) -> awsConf.addHeader(h, v));\r\n    if (parameters.isRequesterPays()) {\r\n        awsConf.addHeader(REQUESTER_PAYS_HEADER, REQUESTER_PAYS_HEADER_VALUE);\r\n    }\r\n    awsConf.setUseThrottleRetries(conf.getBoolean(EXPERIMENTAL_AWS_INTERNAL_THROTTLING, EXPERIMENTAL_AWS_INTERNAL_THROTTLING_DEFAULT));\r\n    if (!StringUtils.isEmpty(parameters.getUserAgentSuffix())) {\r\n        awsConf.setUserAgentSuffix(parameters.getUserAgentSuffix());\r\n    }\r\n    S3AEncryptionMethods encryptionMethods = getEncryptionAlgorithm(bucket, conf);\r\n    try {\r\n        if (S3AEncryptionMethods.CSE_KMS.getMethod().equals(encryptionMethods.getMethod())) {\r\n            return buildAmazonS3EncryptionClient(awsConf, parameters);\r\n        } else {\r\n            return buildAmazonS3Client(awsConf, parameters);\r\n        }\r\n    } catch (SdkClientException e) {\r\n        throw translateException(\"creating AWS S3 client\", uri.toString(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildAmazonS3EncryptionClient",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "AmazonS3 buildAmazonS3EncryptionClient(final ClientConfiguration awsConf, final S3ClientCreationParameters parameters) throws IOException\n{\r\n    AmazonS3 client;\r\n    AmazonS3EncryptionClientV2Builder builder = new AmazonS3EncryptionClientV2Builder();\r\n    Configuration conf = getConf();\r\n    String kmsKeyId = getS3EncryptionKey(bucket, conf, true);\r\n    Preconditions.checkArgument(!StringUtils.isBlank(kmsKeyId), \"CSE-KMS \" + \"method requires KMS key ID. Use \" + S3_ENCRYPTION_KEY + \" property to set it. \");\r\n    EncryptionMaterialsProvider materialsProvider = new KMSEncryptionMaterialsProvider(kmsKeyId);\r\n    builder.withEncryptionMaterialsProvider(materialsProvider);\r\n    configureBasicParams(builder, awsConf, parameters);\r\n    AmazonS3EncryptionClientV2Builder.EndpointConfiguration epr = createEndpointConfiguration(parameters.getEndpoint(), awsConf, getConf().getTrimmed(AWS_REGION));\r\n    configureEndpoint(builder, epr);\r\n    CryptoConfigurationV2 cryptoConfigurationV2 = new CryptoConfigurationV2(CryptoMode.AuthenticatedEncryption).withRangeGetMode(CryptoRangeGetMode.ALL);\r\n    if (epr != null) {\r\n        cryptoConfigurationV2.withAwsKmsRegion(RegionUtils.getRegion(epr.getSigningRegion()));\r\n        LOG.debug(\"KMS region used: {}\", cryptoConfigurationV2.getAwsKmsRegion());\r\n    }\r\n    builder.withCryptoConfiguration(cryptoConfigurationV2);\r\n    client = builder.build();\r\n    IGNORE_CSE_WARN.info(\"S3 client-side encryption enabled: Ignore S3-CSE \" + \"Warnings.\");\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "buildAmazonS3Client",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AmazonS3 buildAmazonS3Client(final ClientConfiguration awsConf, final S3ClientCreationParameters parameters)\n{\r\n    AmazonS3ClientBuilder b = AmazonS3Client.builder();\r\n    configureBasicParams(b, awsConf, parameters);\r\n    AwsClientBuilder.EndpointConfiguration epr = createEndpointConfiguration(parameters.getEndpoint(), awsConf, getConf().getTrimmed(AWS_REGION));\r\n    configureEndpoint(b, epr);\r\n    final AmazonS3 client = b.build();\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "configureBasicParams",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void configureBasicParams(AmazonS3Builder builder, ClientConfiguration awsConf, S3ClientCreationParameters parameters)\n{\r\n    builder.withCredentials(parameters.getCredentialSet());\r\n    builder.withClientConfiguration(awsConf);\r\n    builder.withPathStyleAccessEnabled(parameters.isPathStyleAccess());\r\n    if (parameters.getMetrics() != null) {\r\n        builder.withMetricsCollector(new AwsStatisticsCollector(parameters.getMetrics()));\r\n    }\r\n    if (parameters.getRequestHandlers() != null) {\r\n        builder.withRequestHandlers(parameters.getRequestHandlers().toArray(new RequestHandler2[0]));\r\n    }\r\n    if (parameters.getMonitoringListener() != null) {\r\n        builder.withMonitoringListener(parameters.getMonitoringListener());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "configureEndpoint",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void configureEndpoint(AmazonS3Builder builder, AmazonS3Builder.EndpointConfiguration epr)\n{\r\n    if (epr != null) {\r\n        builder.withEndpointConfiguration(epr);\r\n    } else {\r\n        builder.withForceGlobalBucketAccessEnabled(true);\r\n        String region = getConf().getTrimmed(AWS_REGION, AWS_S3_CENTRAL_REGION);\r\n        LOG.debug(\"fs.s3a.endpoint.region=\\\"{}\\\"\", region);\r\n        if (!region.isEmpty()) {\r\n            LOG.debug(\"Using default endpoint; setting region to {}\", region);\r\n            builder.setRegion(region);\r\n        } else {\r\n            WARN_OF_DEFAULT_REGION_CHAIN.warn(SDK_REGION_CHAIN_IN_USE);\r\n            LOG.debug(SDK_REGION_CHAIN_IN_USE);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "configureAmazonS3Client",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AmazonS3 configureAmazonS3Client(AmazonS3 s3, final String endPoint, final boolean pathStyleAccess) throws IllegalArgumentException\n{\r\n    if (!endPoint.isEmpty()) {\r\n        try {\r\n            s3.setEndpoint(endPoint);\r\n        } catch (IllegalArgumentException e) {\r\n            String msg = \"Incorrect endpoint: \" + e.getMessage();\r\n            LOG.error(msg);\r\n            throw new IllegalArgumentException(msg, e);\r\n        }\r\n    }\r\n    if (pathStyleAccess) {\r\n        LOG.debug(\"Enabling path style access!\");\r\n        s3.setS3ClientOptions(S3ClientOptions.builder().setPathStyleAccess(true).build());\r\n    }\r\n    return s3;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createEndpointConfiguration",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AwsClientBuilder.EndpointConfiguration createEndpointConfiguration(final String endpoint, final ClientConfiguration awsConf, String awsRegion)\n{\r\n    LOG.debug(\"Creating endpoint configuration for \\\"{}\\\"\", endpoint);\r\n    if (endpoint == null || endpoint.isEmpty()) {\r\n        LOG.debug(\"Using default endpoint -no need to generate a configuration\");\r\n        return null;\r\n    }\r\n    final URI epr = RuntimeHttpUtils.toUri(endpoint, awsConf);\r\n    LOG.debug(\"Endpoint URI = {}\", epr);\r\n    String region = awsRegion;\r\n    if (StringUtils.isBlank(region)) {\r\n        if (!ServiceUtils.isS3USStandardEndpoint(endpoint)) {\r\n            LOG.debug(\"Endpoint {} is not the default; parsing\", epr);\r\n            region = AwsHostNameUtils.parseRegion(epr.getHost(), S3_SERVICE_NAME);\r\n        } else {\r\n            LOG.debug(\"Endpoint {} is the standard one; declare region as null\", epr);\r\n            region = null;\r\n        }\r\n    }\r\n    LOG.debug(\"Region for endpoint {}, URI {} is determined as {}\", endpoint, epr, region);\r\n    return new AwsClientBuilder.EndpointConfiguration(endpoint, region);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"ActiveOperation{\");\r\n    sb.append(\"operationId=\").append(operationId);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getOperationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOperationId()\n{\r\n    return operationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getS3AStatisticsContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AStatisticsContext getS3AStatisticsContext()\n{\r\n    return statisticsContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "newOperationId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long newOperationId()\n{\r\n    return NEXT_OPERATION_ID.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getBucket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBucket()\n{\r\n    return bucket;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getServerSideEncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getServerSideEncryptionAlgorithm()\n{\r\n    return serverSideEncryptionAlgorithm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getServerSideEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getServerSideEncryptionKey()\n{\r\n    return serverSideEncryptionKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getETag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getETag()\n{\r\n    return eTag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getVersionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getVersionId()\n{\r\n    return versionId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getLen",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLen()\n{\r\n    return len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void bindToFileSystem(final URI uri, final StoreContext context, final DelegationOperations delegationOperations) throws IOException\n{\r\n    super.bindToFileSystem(uri, context, delegationOperations);\r\n    service = getTokenService(getCanonicalUri());\r\n    stats = context.getInstrumentation().newDelegationTokenStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    checkState(hasDelegationTokenBinding(conf), E_DELEGATION_TOKENS_DISABLED);\r\n    Class<? extends AbstractDelegationTokenBinding> binding = conf.getClass(DelegationConstants.DELEGATION_TOKEN_BINDING, SessionTokenBinding.class, AbstractDelegationTokenBinding.class);\r\n    tokenBinding = binding.newInstance();\r\n    tokenBinding.bindToFileSystem(getCanonicalUri(), getStoreContext(), getPolicyProvider());\r\n    tokenBinding.init(conf);\r\n    tokenBindingName = tokenBinding.getKind().toString();\r\n    LOG.debug(\"Filesystem {} is using delegation tokens of kind {}\", getCanonicalUri(), tokenBindingName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    tokenBinding.start();\r\n    bindToAnyDelegationToken();\r\n    LOG.debug(\"S3A Delegation support token {} with {}\", identifierToString(), tokenBinding.getDescription());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "identifierToString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String identifierToString()\n{\r\n    return decodedIdentifier.map(Objects::toString).orElse(\"(none)\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    LOG.debug(\"Stopping delegation tokens\");\r\n    try {\r\n        super.serviceStop();\r\n    } finally {\r\n        ServiceOperations.stopQuietly(LOG, tokenBinding);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "deployUnbonded",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deployUnbonded() throws IOException\n{\r\n    requireServiceStarted();\r\n    checkState(!isBoundToDT(), \"Already Bound to a delegation token\");\r\n    LOG.debug(\"No delegation tokens present: using direct authentication\");\r\n    credentialProviders = Optional.of(tokenBinding.deployUnbonded());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToAnyDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void bindToAnyDelegationToken() throws IOException\n{\r\n    checkState(!credentialProviders.isPresent(), E_ALREADY_DEPLOYED);\r\n    Token<AbstractS3ATokenIdentifier> token = selectTokenFromFSOwner();\r\n    if (token != null) {\r\n        bindToDelegationToken(token);\r\n    } else {\r\n        deployUnbonded();\r\n    }\r\n    if (credentialProviders.get().size() == 0) {\r\n        throw new DelegationTokenIOException(\"No AWS credential providers\" + \" created by Delegation Token Binding \" + tokenBinding.getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "resetTokenBindingToDT",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void resetTokenBindingToDT(final Token<AbstractS3ATokenIdentifier> token) throws IOException\n{\r\n    credentialProviders = Optional.empty();\r\n    bindToDelegationToken(token);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void bindToDelegationToken(final Token<AbstractS3ATokenIdentifier> token) throws IOException\n{\r\n    checkState(!credentialProviders.isPresent(), E_ALREADY_DEPLOYED);\r\n    boundDT = Optional.of(token);\r\n    AbstractS3ATokenIdentifier dti = extractIdentifier(token);\r\n    LOG.info(\"Using delegation token {}\", dti);\r\n    decodedIdentifier = Optional.of(dti);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, DURATION_LOG_AT_INFO, \"Creating Delegation Token\")) {\r\n        credentialProviders = Optional.of(tokenBinding.bindToTokenIdentifier(dti));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "isBoundToDT",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isBoundToDT()\n{\r\n    return boundDT.isPresent();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getBoundDT",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Optional<Token<AbstractS3ATokenIdentifier>> getBoundDT()\n{\r\n    return boundDT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenIssuingPolicy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TokenIssuingPolicy getTokenIssuingPolicy()\n{\r\n    return isBoundToDT() ? TokenIssuingPolicy.ReturnExistingToken : tokenBinding.getTokenIssuingPolicy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getBoundOrNewDT",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> getBoundOrNewDT(final EncryptionSecrets encryptionSecrets, final Text renewer) throws IOException\n{\r\n    LOG.debug(\"Delegation token requested\");\r\n    if (isBoundToDT()) {\r\n        LOG.debug(\"Returning current token\");\r\n        return getBoundDT().get();\r\n    } else {\r\n        return createDelegationToken(encryptionSecrets, renewer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getCreationCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCreationCount()\n{\r\n    return creationCount.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> createDelegationToken(final EncryptionSecrets encryptionSecrets, final Text renewer) throws IOException\n{\r\n    requireServiceStarted();\r\n    checkArgument(encryptionSecrets != null, \"Null encryption secrets\");\r\n    List<RoleModel.Statement> statements = getPolicyProvider().listAWSPolicyRules(ACCESS_POLICY);\r\n    Optional<RoleModel.Policy> rolePolicy = statements.isEmpty() ? Optional.empty() : Optional.of(new RoleModel.Policy(statements));\r\n    try (DurationInfo ignored = new DurationInfo(LOG, DURATION_LOG_AT_INFO, \"Creating New Delegation Token\", tokenBinding.getKind())) {\r\n        Token<AbstractS3ATokenIdentifier> token = trackDuration(stats, DELEGATION_TOKENS_ISSUED.getSymbol(), () -> tokenBinding.createDelegationToken(rolePolicy, encryptionSecrets, renewer));\r\n        if (token != null) {\r\n            token.setService(service);\r\n            noteTokenCreated(token);\r\n        }\r\n        return token;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "noteTokenCreated",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void noteTokenCreated(final Token<AbstractS3ATokenIdentifier> token)\n{\r\n    LOG.info(\"Created S3A Delegation Token: {}\", token);\r\n    creationCount.incrementAndGet();\r\n    stats.tokenIssued();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getAdditionalTokenIssuers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationTokenIssuer[] getAdditionalTokenIssuers() throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getCredentialProviders",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AWSCredentialProviderList getCredentialProviders() throws IOException\n{\r\n    return credentialProviders.orElseThrow(() -> new DelegationTokenIOException(\"Not yet bonded\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getEncryptionSecrets",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Optional<EncryptionSecrets> getEncryptionSecrets()\n{\r\n    return decodedIdentifier.map(AbstractS3ATokenIdentifier::getEncryptionSecrets);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDecodedIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Optional<AbstractS3ATokenIdentifier> getDecodedIdentifier()\n{\r\n    return decodedIdentifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getService()\n{\r\n    return service;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    return getCanonicalUri().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "selectTokenFromFSOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> selectTokenFromFSOwner() throws IOException\n{\r\n    return lookupToken(user.getCredentials(), service, tokenBinding.getKind());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Text getTokenService(final URI fsURI)\n{\r\n    return getTokenService(fsURI.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"S3ADelegationTokens{\");\r\n    sb.append(\"canonicalServiceURI=\").append(getCanonicalUri());\r\n    sb.append(\"; owner=\").append(user.getShortUserName());\r\n    sb.append(\"; isBoundToDT=\").append(isBoundToDT());\r\n    sb.append(\"; token creation count=\").append(getCreationCount());\r\n    sb.append(\"; tokenManager=\").append(tokenBinding);\r\n    sb.append(\"; token=\").append(identifierToString());\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenKind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Text getTokenKind()\n{\r\n    return tokenBinding.getKind();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getTokenService(final String fsURI)\n{\r\n    return new Text(fsURI);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "extractIdentifier",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AbstractS3ATokenIdentifier extractIdentifier(final Token<? extends AbstractS3ATokenIdentifier> token) throws IOException\n{\r\n    checkArgument(token != null, \"null token\");\r\n    AbstractS3ATokenIdentifier identifier;\r\n    try {\r\n        identifier = token.decodeIdentifier();\r\n    } catch (RuntimeException e) {\r\n        Throwable cause = e.getCause();\r\n        if (cause != null) {\r\n            throw new DelegationTokenIOException(\"Decoding S3A token \" + cause, cause);\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n    if (identifier == null) {\r\n        throw new DelegationTokenIOException(\"Failed to unmarshall token for \" + getCanonicalUri());\r\n    }\r\n    identifier.validate();\r\n    return identifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getUserAgentField",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUserAgentField()\n{\r\n    return tokenBinding.getUserAgentField();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "lookupToken",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> lookupToken(final Credentials credentials, final Text service, final Text kind) throws DelegationTokenIOException\n{\r\n    LOG.debug(\"Looking for token for service {} in credentials\", service);\r\n    Token<?> token = credentials.getToken(service);\r\n    if (token != null) {\r\n        Text tokenKind = token.getKind();\r\n        LOG.debug(\"Found token of kind {}\", tokenKind);\r\n        if (kind.equals(tokenKind)) {\r\n            return (Token<AbstractS3ATokenIdentifier>) token;\r\n        } else {\r\n            throw new DelegationTokenIOException(DelegationTokenIOException.TOKEN_MISMATCH + \": expected token\" + \" for \" + service + \" of type \" + kind + \" but got a token of type \" + tokenKind);\r\n        }\r\n    }\r\n    LOG.debug(\"No token for {} found\", service);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "lookupToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> lookupToken(final Credentials credentials, final Text service)\n{\r\n    return (Token<AbstractS3ATokenIdentifier>) credentials.getToken(service);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "lookupS3ADelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> lookupS3ADelegationToken(final Credentials credentials, final URI uri)\n{\r\n    return lookupToken(credentials, getTokenService(uri.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "hasDelegationTokenBinding",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasDelegationTokenBinding(Configuration conf)\n{\r\n    return StringUtils.isNotEmpty(conf.getTrimmed(DELEGATION_TOKEN_BINDING, DEFAULT_DELEGATION_TOKEN_BINDING));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "initOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initOutput(Path out) throws IOException\n{\r\n    FileSystem fs = getDestinationFS(out, getConf());\r\n    setDestFS(fs);\r\n    setOutputPath(fs.makeQualified(out));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getJobContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobContext getJobContext()\n{\r\n    return jobContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    return outputPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputPath(Path outputPath)\n{\r\n    Preconditions.checkNotNull(outputPath, \"Null output path\");\r\n    this.outputPath = outputPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getWorkPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkPath()\n{\r\n    return workPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setWorkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWorkPath(Path workPath)\n{\r\n    LOG.debug(\"Setting work path to {}\", workPath);\r\n    this.workPath = workPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getDestFS",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystem getDestFS() throws IOException\n{\r\n    if (destFS == null) {\r\n        FileSystem fs = getDestinationFS(outputPath, getConf());\r\n        setDestFS(fs);\r\n    }\r\n    return destFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getDestS3AFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "S3AFileSystem getDestS3AFS() throws IOException\n{\r\n    return (S3AFileSystem) getDestFS();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setDestFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDestFS(FileSystem destFS)\n{\r\n    this.destFS = destFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(JobContext context)\n{\r\n    return getJobAttemptPath(getAppAttemptId(context));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobAttemptPath(int appAttemptId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context)\n{\r\n    return getBaseTaskAttemptPath(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getBaseTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getBaseTaskAttemptPath(TaskAttemptContext context)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTempTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTempTaskAttemptPath(TaskAttemptContext context)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getUUID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUUID()\n{\r\n    return uuid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getUUIDSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobUUIDSource getUUIDSource()\n{\r\n    return uuidSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AbstractS3ACommitter{\");\r\n    sb.append(\"role=\").append(role);\r\n    sb.append(\", name=\").append(getName());\r\n    sb.append(\", outputPath=\").append(getOutputPath());\r\n    sb.append(\", workPath=\").append(workPath);\r\n    sb.append(\", uuid='\").append(getUUID()).append('\\'');\r\n    sb.append(\", uuid source=\").append(getUUIDSource());\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getDestinationFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getDestinationFS(Path out, Configuration config) throws IOException\n{\r\n    return getS3AFileSystem(out, config, requiresDelayedCommitOutputInFileSystem());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "requiresDelayedCommitOutputInFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean requiresDelayedCommitOutputInFileSystem()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "recoverTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void recoverTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    LOG.warn(\"Cannot recover task {}\", taskContext.getTaskAttemptID());\r\n    throw new PathCommitException(outputPath, String.format(\"Unable to recover task %s\", taskContext.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "maybeCreateSuccessMarkerFromCommits",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void maybeCreateSuccessMarkerFromCommits(JobContext context, ActiveCommit pending) throws IOException\n{\r\n    List<String> filenames = new ArrayList<>(pending.size());\r\n    filenames.addAll(pending.committedObjects);\r\n    IOStatisticsSnapshot snapshot = new IOStatisticsSnapshot(pending.getIOStatistics());\r\n    snapshot.aggregate(getIOStatistics());\r\n    maybeCreateSuccessMarker(context, filenames, snapshot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "maybeCreateSuccessMarker",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void maybeCreateSuccessMarker(JobContext context, List<String> filenames, final IOStatisticsSnapshot ioStatistics) throws IOException\n{\r\n    if (createJobMarker) {\r\n        SuccessData successData = new SuccessData();\r\n        successData.setCommitter(getName());\r\n        successData.setJobId(uuid);\r\n        successData.setJobIdSource(uuidSource.getText());\r\n        successData.setDescription(getRole());\r\n        successData.setHostname(NetUtils.getLocalHostname());\r\n        Date now = new Date();\r\n        successData.setTimestamp(now.getTime());\r\n        successData.setDate(now.toString());\r\n        successData.setFilenames(filenames);\r\n        successData.getIOStatistics().aggregate(ioStatistics);\r\n        commitOperations.createSuccessMarker(getOutputPath(), successData, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setupJob(JobContext context) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Job %s setting up\", getUUID())) {\r\n        jobSetup = true;\r\n        Configuration c = context.getConfiguration();\r\n        c.set(FS_S3A_COMMITTER_UUID, getUUID());\r\n        c.set(FS_S3A_COMMITTER_UUID_SOURCE, getUUIDSource().getText());\r\n        Path dest = getOutputPath();\r\n        if (createJobMarker) {\r\n            commitOperations.deleteSuccessMarker(dest);\r\n        }\r\n        getDestFS().mkdirs(dest);\r\n        warnOnActiveUploads(dest);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setupTask(TaskAttemptContext context) throws IOException\n{\r\n    TaskAttemptID attemptID = context.getTaskAttemptID();\r\n    updateCommonContext();\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Setup Task %s\", attemptID)) {\r\n        if (!jobSetup && getUUIDSource() == JobUUIDSource.GeneratedLocally) {\r\n            throw new PathCommitException(getOutputPath().toString(), \"Task attempt \" + attemptID + \" \" + E_SELF_GENERATED_JOB_UUID);\r\n        }\r\n        Path taskAttemptPath = getTaskAttemptPath(context);\r\n        FileSystem fs = taskAttemptPath.getFileSystem(getConf());\r\n        fs.mkdirs(taskAttemptPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getTaskAttemptFilesystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getTaskAttemptFilesystem(TaskAttemptContext context) throws IOException\n{\r\n    return getTaskAttemptPath(context).getFileSystem(getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitPendingUploads",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void commitPendingUploads(final JobContext context, final ActiveCommit pending) throws IOException\n{\r\n    if (pending.isEmpty()) {\r\n        LOG.warn(\"{}: No pending uploads to commit\", getRole());\r\n    }\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"committing the output of %s task(s)\", pending.size());\r\n        CommitOperations.CommitContext commitContext = initiateCommitOperation()) {\r\n        Tasks.foreach(pending.getSourceFiles()).stopOnFailure().suppressExceptions(false).executeWith(buildSubmitter(context)).abortWith(status -> loadAndAbort(commitContext, pending, status, true, false)).revertWith(status -> loadAndRevert(commitContext, pending, status)).run(status -> loadAndCommit(commitContext, pending, status));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "precommitCheckPendingFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void precommitCheckPendingFiles(final JobContext context, final ActiveCommit pending) throws IOException\n{\r\n    FileSystem sourceFS = pending.getSourceFS();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Preflight Load of pending files\")) {\r\n        Tasks.foreach(pending.getSourceFiles()).stopOnFailure().suppressExceptions(false).executeWith(buildSubmitter(context)).run(status -> PendingSet.load(sourceFS, status));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "loadAndCommit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void loadAndCommit(final CommitOperations.CommitContext commitContext, final ActiveCommit activeCommit, final FileStatus status) throws IOException\n{\r\n    final Path path = status.getPath();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Loading and committing files in pendingset %s\", path)) {\r\n        PendingSet pendingSet = PendingSet.load(activeCommit.getSourceFS(), status);\r\n        String jobId = pendingSet.getJobId();\r\n        if (!StringUtils.isEmpty(jobId) && !getUUID().equals(jobId)) {\r\n            throw new PathCommitException(path, String.format(\"Mismatch in Job ID (%s) and commit job ID (%s)\", getUUID(), jobId));\r\n        }\r\n        Tasks.foreach(pendingSet.getCommits()).stopOnFailure().suppressExceptions(false).executeWith(singleThreadSubmitter()).onFailure((commit, exception) -> commitContext.abortSingleCommit(commit)).abortWith(commitContext::abortSingleCommit).revertWith(commitContext::revertCommit).run(commit -> {\r\n            commitContext.commitOrFail(commit);\r\n            activeCommit.uploadCommitted(commit.getDestinationKey(), commit.getLength());\r\n        });\r\n        activeCommit.pendingsetCommitted(pendingSet.getIOStatistics());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "loadAndRevert",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void loadAndRevert(final CommitOperations.CommitContext commitContext, final ActiveCommit activeCommit, final FileStatus status) throws IOException\n{\r\n    final Path path = status.getPath();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Committing %s\", path)) {\r\n        PendingSet pendingSet = PendingSet.load(activeCommit.getSourceFS(), status);\r\n        Tasks.foreach(pendingSet.getCommits()).suppressExceptions(true).run(commitContext::revertCommit);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "loadAndAbort",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void loadAndAbort(final CommitOperations.CommitContext commitContext, final ActiveCommit activeCommit, final FileStatus status, final boolean suppressExceptions, final boolean deleteRemoteFiles) throws IOException\n{\r\n    final Path path = status.getPath();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Aborting %s\", path)) {\r\n        PendingSet pendingSet = PendingSet.load(activeCommit.getSourceFS(), status);\r\n        FileSystem fs = getDestFS();\r\n        Tasks.foreach(pendingSet.getCommits()).executeWith(singleThreadSubmitter()).suppressExceptions(suppressExceptions).run(commit -> {\r\n            try {\r\n                commitContext.abortSingleCommit(commit);\r\n            } catch (FileNotFoundException e) {\r\n                if (deleteRemoteFiles) {\r\n                    fs.delete(commit.destinationPath(), false);\r\n                }\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "initiateCommitOperation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CommitOperations.CommitContext initiateCommitOperation() throws IOException\n{\r\n    return getCommitOperations().initiateCommitOperation(getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitJobInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJobInternal(JobContext context, ActiveCommit pending) throws IOException\n{\r\n    trackDurationOfInvocation(committerStatistics, COMMITTER_COMMIT_JOB.getSymbol(), () -> commitPendingUploads(context, pending));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abortJob(JobContext context, JobStatus.State state) throws IOException\n{\r\n    LOG.info(\"{}: aborting job {} in state {}\", getRole(), jobIdString(context), state);\r\n    abortJobInternal(context, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortJobInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJobInternal(JobContext context, boolean suppressExceptions) throws IOException\n{\r\n    cleanup(context, suppressExceptions);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortPendingUploadsInCleanup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void abortPendingUploadsInCleanup(boolean suppressExceptions) throws IOException\n{\r\n    if (!shouldAbortUploadsInCleanup()) {\r\n        LOG.debug(\"Not cleanup up pending uploads to {} as {} is false \", getOutputPath(), FS_S3A_COMMITTER_ABORT_PENDING_UPLOADS);\r\n        return;\r\n    }\r\n    Path dest = getOutputPath();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, \"Aborting all pending commits under %s\", dest);\r\n        CommitOperations.CommitContext commitContext = initiateCommitOperation()) {\r\n        CommitOperations ops = getCommitOperations();\r\n        List<MultipartUpload> pending;\r\n        try {\r\n            pending = ops.listPendingUploadsUnderPath(dest);\r\n        } catch (IOException e) {\r\n            LOG.debug(\"Failed to list pending uploads under {}\", dest, e);\r\n            return;\r\n        }\r\n        if (!pending.isEmpty()) {\r\n            LOG.warn(\"{} pending uploads were found -aborting\", pending.size());\r\n            LOG.warn(\"If other tasks/jobs are writing to {},\" + \"this action may cause them to fail\", dest);\r\n            Tasks.foreach(pending).executeWith(buildSubmitter(getJobContext())).suppressExceptions(suppressExceptions).run(u -> commitContext.abortMultipartCommit(u.getKey(), u.getUploadId()));\r\n        } else {\r\n            LOG.info(\"No pending uploads were found\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "shouldAbortUploadsInCleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldAbortUploadsInCleanup()\n{\r\n    return getConf().getBoolean(FS_S3A_COMMITTER_ABORT_PENDING_UPLOADS, DEFAULT_FS_S3A_COMMITTER_ABORT_PENDING_UPLOADS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "preCommitJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void preCommitJob(JobContext context, ActiveCommit pending) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "commitJob",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void commitJob(JobContext context) throws IOException\n{\r\n    String id = jobIdString(context);\r\n    try (DurationInfo d = new DurationInfo(LOG, \"%s: commitJob(%s)\", getRole(), id)) {\r\n        ActiveCommit pending = listPendingUploadsToCommit(context);\r\n        preCommitJob(context, pending);\r\n        commitJobInternal(context, pending);\r\n        jobCompleted(true);\r\n        maybeCreateSuccessMarkerFromCommits(context, pending);\r\n        cleanup(context, false);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Commit failure for job {}\", id, e);\r\n        jobCompleted(false);\r\n        abortJobInternal(context, true);\r\n        throw e;\r\n    } finally {\r\n        resetCommonContext();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "jobCompleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void jobCompleted(boolean success)\n{\r\n    getCommitOperations().jobCompleted(success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "cleanupStagingDirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void cleanupStagingDirs()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "listPendingUploadsToCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ActiveCommit listPendingUploadsToCommit(JobContext context) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanup(JobContext context, boolean suppressExceptions) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, \"Cleanup job %s\", jobIdString(context))) {\r\n        abortPendingUploadsInCleanup(suppressExceptions);\r\n    } finally {\r\n        destroyThreadPool();\r\n        cleanupStagingDirs();\r\n        resetCommonContext();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanupJob(JobContext context) throws IOException\n{\r\n    String r = getRole();\r\n    String id = jobIdString(context);\r\n    LOG.warn(\"{}: using deprecated cleanupJob call for {}\", r, id);\r\n    try (DurationInfo d = new DurationInfo(LOG, \"%s: cleanup Job %s\", r, id)) {\r\n        cleanup(context, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "maybeIgnore",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeIgnore(boolean suppress, String action, InvocationRaisingIOE operation) throws IOException\n{\r\n    if (suppress) {\r\n        ignoreIOExceptions(LOG, action, \"\", operation);\r\n    } else {\r\n        operation.apply();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "maybeIgnore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeIgnore(boolean suppress, String action, IOException ex) throws IOException\n{\r\n    if (suppress) {\r\n        LOG.debug(action, ex);\r\n    } else {\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getCommitOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommitOperations getCommitOperations()\n{\r\n    return commitOperations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getRole",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRole()\n{\r\n    return role;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "buildSubmitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Tasks.Submitter buildSubmitter(JobContext context)\n{\r\n    if (getThreadCount(context) > 0) {\r\n        return new PoolSubmitter(context);\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "buildThreadPool",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ExecutorService buildThreadPool(JobContext context, int numThreads)\n{\r\n    Preconditions.checkArgument(numThreads > 0, \"Cannot create a thread pool with no threads\");\r\n    if (threadPool == null) {\r\n        LOG.debug(\"{}: creating thread pool of size {}\", getRole(), numThreads);\r\n        threadPool = HadoopExecutors.newFixedThreadPool(numThreads, new ThreadFactoryBuilder().setDaemon(true).setNameFormat(THREAD_PREFIX + context.getJobID() + \"-%d\").build());\r\n    }\r\n    return threadPool;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getThreadCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getThreadCount(final JobContext context)\n{\r\n    return context.getConfiguration().getInt(FS_S3A_COMMITTER_THREADS, DEFAULT_COMMITTER_THREADS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "submitRunnable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Future<?> submitRunnable(final JobContext context, final Runnable task)\n{\r\n    return buildThreadPool(context, getThreadCount(context)).submit(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "destroyThreadPool",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroyThreadPool()\n{\r\n    ExecutorService pool;\r\n    synchronized (this) {\r\n        pool = this.threadPool;\r\n        threadPool = null;\r\n    }\r\n    if (pool != null) {\r\n        LOG.debug(\"Destroying thread pool\");\r\n        HadoopExecutors.shutdown(pool, LOG, THREAD_POOL_SHUTDOWN_DELAY_SECONDS, TimeUnit.SECONDS);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "singleThreadSubmitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Tasks.Submitter singleThreadSubmitter()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "hasThreadPool",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasThreadPool()\n{\r\n    return threadPool != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "deleteTaskAttemptPathQuietly",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteTaskAttemptPathQuietly(TaskAttemptContext context)\n{\r\n    Path attemptPath = getBaseTaskAttemptPath(context);\r\n    ignoreIOExceptions(LOG, \"Delete task attempt path\", attemptPath.toString(), () -> deleteQuietly(getTaskAttemptFilesystem(context), attemptPath, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortPendingUploads",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void abortPendingUploads(JobContext context, List<SinglePendingCommit> pending, boolean suppressExceptions) throws IOException\n{\r\n    if (pending == null || pending.isEmpty()) {\r\n        LOG.info(\"{}: no pending commits to abort\", getRole());\r\n    } else {\r\n        try (DurationInfo d = new DurationInfo(LOG, \"Aborting %s uploads\", pending.size());\r\n            CommitOperations.CommitContext commitContext = initiateCommitOperation()) {\r\n            Tasks.foreach(pending).executeWith(buildSubmitter(context)).suppressExceptions(suppressExceptions).run(commitContext::abortSingleCommit);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "abortPendingUploads",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void abortPendingUploads(final JobContext context, final ActiveCommit pending, final boolean suppressExceptions, final boolean deleteRemoteFiles) throws IOException\n{\r\n    if (pending.isEmpty()) {\r\n        LOG.info(\"{}: no pending commits to abort\", getRole());\r\n    } else {\r\n        try (DurationInfo d = new DurationInfo(LOG, \"Aborting %s uploads\", pending.size());\r\n            CommitOperations.CommitContext commitContext = initiateCommitOperation()) {\r\n            Tasks.foreach(pending.getSourceFiles()).executeWith(buildSubmitter(context)).suppressExceptions(suppressExceptions).run(path -> loadAndAbort(commitContext, pending, path, suppressExceptions, deleteRemoteFiles));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return committerStatistics.getIOStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "warnOnActiveUploads",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void warnOnActiveUploads(final Path path)\n{\r\n    List<MultipartUpload> pending;\r\n    try {\r\n        pending = getCommitOperations().listPendingUploadsUnderPath(path);\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to list uploads under {}\", path, e);\r\n        return;\r\n    }\r\n    if (!pending.isEmpty()) {\r\n        LOG.warn(\"{} active upload(s) in progress under {}\", pending.size(), path);\r\n        LOG.warn(\"Either jobs are running concurrently\" + \" or failed jobs are not being cleaned up\");\r\n        DateFormat df = DateFormat.getDateTimeInstance();\r\n        pending.forEach(u -> LOG.info(\"[{}] {}\", df.format(u.getInitiated()), u.getKey()));\r\n        if (shouldAbortUploadsInCleanup()) {\r\n            LOG.warn(\"This committer will abort these uploads in job cleanup\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "buildJobUUID",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Pair<String, JobUUIDSource> buildJobUUID(Configuration conf, JobID jobId) throws PathCommitException\n{\r\n    String jobUUID = conf.getTrimmed(FS_S3A_COMMITTER_UUID, \"\");\r\n    if (!jobUUID.isEmpty()) {\r\n        return Pair.of(jobUUID, JobUUIDSource.CommitterUUIDProperty);\r\n    }\r\n    jobUUID = conf.getTrimmed(SPARK_WRITE_UUID, \"\");\r\n    if (!jobUUID.isEmpty()) {\r\n        return Pair.of(jobUUID, JobUUIDSource.SparkWriteUUID);\r\n    }\r\n    if (conf.getBoolean(FS_S3A_COMMITTER_REQUIRE_UUID, DEFAULT_S3A_COMMITTER_REQUIRE_UUID)) {\r\n        throw new PathCommitException(\"\", E_NO_SPARK_UUID);\r\n    }\r\n    if (conf.getBoolean(FS_S3A_COMMITTER_GENERATE_UUID, DEFAULT_S3A_COMMITTER_GENERATE_UUID)) {\r\n        String newId = UUID.randomUUID().toString();\r\n        LOG.warn(\"No job ID in configuration; generating a random ID: {}\", newId);\r\n        return Pair.of(newId, JobUUIDSource.GeneratedLocally);\r\n    }\r\n    return Pair.of(jobId.toString(), JobUUIDSource.JobID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "updateCommonContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateCommonContext()\n{\r\n    currentAuditContext().put(AuditConstants.PARAM_JOB_ID, uuid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "resetCommonContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetCommonContext()\n{\r\n    currentAuditContext().remove(AuditConstants.PARAM_JOB_ID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "getAuditSpanSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanSource getAuditSpanSource()\n{\r\n    return auditSpanSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "startOperation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpan startOperation(String name, @Nullable String path1, @Nullable String path2) throws IOException\n{\r\n    return getAuditSpanSource().createSpan(name, path1, path2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\commit",
  "methodName" : "verify",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void verify(boolean expression, String message, Object... args) throws ValidationFailure\n{\r\n    if (!expression) {\r\n        throw new ValidationFailure(message, args);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setName(final String name)\n{\r\n    if (!name.isEmpty() && !name.endsWith(\": \")) {\r\n        this.name = name + \": \";\r\n    } else {\r\n        this.name = name;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(AWSCredentialsProvider p)\n{\r\n    providers.add(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "addAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addAll(AWSCredentialProviderList other)\n{\r\n    providers.addAll(other.providers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refresh()\n{\r\n    if (isClosed()) {\r\n        return;\r\n    }\r\n    for (AWSCredentialsProvider provider : providers) {\r\n        provider.refresh();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCredentials",
  "errType" : [ "NoAwsCredentialsException", "AmazonClientException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "AWSCredentials getCredentials()\n{\r\n    if (isClosed()) {\r\n        LOG.warn(CREDENTIALS_REQUESTED_WHEN_CLOSED);\r\n        throw new NoAuthWithAWSException(name + CREDENTIALS_REQUESTED_WHEN_CLOSED);\r\n    }\r\n    checkNotEmpty();\r\n    if (reuseLastProvider && lastProvider != null) {\r\n        return lastProvider.getCredentials();\r\n    }\r\n    AmazonClientException lastException = null;\r\n    for (AWSCredentialsProvider provider : providers) {\r\n        try {\r\n            AWSCredentials credentials = provider.getCredentials();\r\n            Preconditions.checkNotNull(credentials, \"Null credentials returned by %s\", provider);\r\n            if ((credentials.getAWSAccessKeyId() != null && credentials.getAWSSecretKey() != null) || (credentials instanceof AnonymousAWSCredentials)) {\r\n                lastProvider = provider;\r\n                LOG.debug(\"Using credentials from {}\", provider);\r\n                return credentials;\r\n            }\r\n        } catch (NoAwsCredentialsException e) {\r\n            if (lastException == null) {\r\n                lastException = e;\r\n            }\r\n            LOG.debug(\"No credentials from {}: {}\", provider, e.toString());\r\n        } catch (AmazonClientException e) {\r\n            lastException = e;\r\n            LOG.debug(\"No credentials provided by {}: {}\", provider, e.toString(), e);\r\n        }\r\n    }\r\n    String message = name + \"No AWS Credentials provided by \" + listProviderNames();\r\n    if (lastException != null) {\r\n        message += \": \" + lastException;\r\n    }\r\n    if (lastException instanceof CredentialInitializationException) {\r\n        throw lastException;\r\n    } else {\r\n        throw new NoAuthWithAWSException(message, lastException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getProviders",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<AWSCredentialsProvider> getProviders()\n{\r\n    return providers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "checkNotEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkNotEmpty()\n{\r\n    if (providers.isEmpty()) {\r\n        throw new NoAuthWithAWSException(name + NO_AWS_CREDENTIAL_PROVIDERS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listProviderNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String listProviderNames()\n{\r\n    return providers.stream().map(provider -> provider.getClass().getSimpleName() + ' ').collect(Collectors.joining());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return \"AWSCredentialProviderList[\" + name + \"refcount= \" + refCount.get() + \": [\" + StringUtils.join(providers, \", \") + ']' + (lastProvider != null ? (\" last provider: \" + lastProvider) : \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "share",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AWSCredentialProviderList share()\n{\r\n    Preconditions.checkState(!closed.get(), \"Provider list is closed\");\r\n    refCount.incrementAndGet();\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getRefCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRefCount()\n{\r\n    return refCount.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return closed.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void close()\n{\r\n    synchronized (this) {\r\n        if (closed.get()) {\r\n            return;\r\n        }\r\n        int remainder = refCount.decrementAndGet();\r\n        if (remainder != 0) {\r\n            LOG.debug(\"Not closing {}\", this);\r\n            return;\r\n        }\r\n        LOG.debug(\"Closing {}\", this);\r\n        closed.set(true);\r\n    }\r\n    for (AWSCredentialsProvider p : providers) {\r\n        if (p instanceof Closeable) {\r\n            IOUtils.closeStream((Closeable) p);\r\n        } else if (p instanceof AutoCloseable) {\r\n            S3AUtils.closeAutocloseables(LOG, (AutoCloseable) p);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int size()\n{\r\n    return providers.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "validateWriteArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateWriteArgs(byte[] b, int off, int len) throws IOException\n{\r\n    Preconditions.checkNotNull(b);\r\n    if ((off < 0) || (off > b.length) || (len < 0) || ((off + len) > b.length) || ((off + len) < 0)) {\r\n        throw new IndexOutOfBoundsException(\"write (b[\" + b.length + \"], \" + off + \", \" + len + ')');\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "createFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlockFactory createFactory(S3AFileSystem owner, String name)\n{\r\n    switch(name) {\r\n        case Constants.FAST_UPLOAD_BUFFER_ARRAY:\r\n            return new ArrayBlockFactory(owner);\r\n        case Constants.FAST_UPLOAD_BUFFER_DISK:\r\n            return new DiskBlockFactory(owner);\r\n        case Constants.FAST_UPLOAD_BYTEBUFFER:\r\n            return new ByteBufferBlockFactory(owner);\r\n        default:\r\n            throw new IllegalArgumentException(\"Unsupported block buffer\" + \" \\\"\" + name + '\"');\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "fromFileStatus",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "S3AFileStatus fromFileStatus(FileStatus source, Tristate isEmptyDirectory, String eTag, String versionId)\n{\r\n    if (source.isDirectory()) {\r\n        return new S3AFileStatus(isEmptyDirectory, source.getPath(), source.getOwner());\r\n    } else {\r\n        return new S3AFileStatus(source.getLen(), source.getModificationTime(), source.getPath(), source.getBlockSize(), source.getOwner(), eTag, versionId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isEmptyDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Tristate isEmptyDirectory()\n{\r\n    return isEmptyDirectory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setIsEmptyDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIsEmptyDirectory(Tristate isEmptyDirectory)\n{\r\n    this.isEmptyDirectory = isEmptyDirectory;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getETag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getETag()\n{\r\n    return getEtag();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEtag()\n{\r\n    return eTag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getVersionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getVersionId()\n{\r\n    return versionId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setVersionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVersionId(final String versionId)\n{\r\n    this.versionId = versionId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    return super.equals(o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getModificationTime",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getModificationTime()\n{\r\n    if (isDirectory()) {\r\n        return System.currentTimeMillis();\r\n    } else {\r\n        return super.getModificationTime();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return super.toString() + String.format(\" isEmptyDirectory=%s\", isEmptyDirectory().name() + String.format(\" eTag=%s\", eTag) + String.format(\" versionId=%s\", versionId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AWSCredentials getCredentials()\n{\r\n    return new AnonymousAWSCredentials();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void refresh()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getClass().getSimpleName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return kind;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getOwnerText",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Text getOwnerText()\n{\r\n    return new Text(getOwner().getUserName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getTokenIssuingPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3ADelegationTokens.TokenIssuingPolicy getTokenIssuingPolicy()\n{\r\n    return S3ADelegationTokens.TokenIssuingPolicy.RequestNewToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Token<AbstractS3ATokenIdentifier> createDelegationToken(final Optional<RoleModel.Policy> policy, final EncryptionSecrets encryptionSecrets, final Text renewer) throws IOException\n{\r\n    requireServiceStarted();\r\n    final AbstractS3ATokenIdentifier tokenIdentifier = createTokenIdentifier(policy, encryptionSecrets, renewer);\r\n    if (tokenIdentifier != null) {\r\n        Token<AbstractS3ATokenIdentifier> token = new Token<>(tokenIdentifier, secretManager);\r\n        token.setKind(getKind());\r\n        LOG.debug(\"Created token {} with token identifier {}\", token, tokenIdentifier);\r\n        return token;\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ATokenIdentifier createTokenIdentifier(Optional<RoleModel.Policy> policy, EncryptionSecrets encryptionSecrets, Text renewer) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "convertTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T convertTokenIdentifier(final AbstractS3ATokenIdentifier identifier, final Class<T> expectedClass) throws DelegationTokenIOException\n{\r\n    if (!identifier.getClass().equals(expectedClass)) {\r\n        throw new DelegationTokenIOException(DelegationTokenIOException.TOKEN_WRONG_CLASS + \"; expected a token identifier of type \" + expectedClass + \" but got \" + identifier.getClass() + \" and kind \" + identifier.getKind());\r\n    }\r\n    return (T) identifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "deployUnbonded",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AWSCredentialProviderList deployUnbonded() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AWSCredentialProviderList bindToTokenIdentifier(AbstractS3ATokenIdentifier retrievedIdentifier) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createEmptyIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractS3ATokenIdentifier createEmptyIdentifier()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return super.toString() + \" token kind = \" + getKind();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    secretManager = createSecretMananger();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDescription()\n{\r\n    return \"Token binding \" + getKind().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createSecretMananger",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SecretManager<AbstractS3ATokenIdentifier> createSecretMananger() throws IOException\n{\r\n    return new TokenSecretManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getUserAgentField",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUserAgentField()\n{\r\n    return \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getSecretManagerPasssword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getSecretManagerPasssword()\n{\r\n    return \"non-password\".getBytes(Charset.forName(\"UTF-8\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "retrieveHeaders",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "Map<String, byte[]> retrieveHeaders(final Path path, final Statistic statistic) throws IOException\n{\r\n    StoreContext context = getStoreContext();\r\n    String objectKey = context.pathToKey(path);\r\n    ObjectMetadata md;\r\n    String symbol = statistic.getSymbol();\r\n    S3AStatisticsContext instrumentation = context.getInstrumentation();\r\n    try {\r\n        md = trackDuration(instrumentation, symbol, () -> callbacks.getObjectMetadata(objectKey));\r\n    } catch (FileNotFoundException e) {\r\n        md = trackDuration(instrumentation, symbol, () -> callbacks.getObjectMetadata(objectKey + \"/\"));\r\n    }\r\n    Map<String, String> rawHeaders = md.getUserMetadata();\r\n    Map<String, byte[]> headers = new TreeMap<>();\r\n    rawHeaders.forEach((key, value) -> headers.put(XA_HEADER_PREFIX + key, encodeBytes(value)));\r\n    maybeSetHeader(headers, XA_CACHE_CONTROL, md.getCacheControl());\r\n    maybeSetHeader(headers, XA_CONTENT_DISPOSITION, md.getContentDisposition());\r\n    maybeSetHeader(headers, XA_CONTENT_ENCODING, md.getContentEncoding());\r\n    maybeSetHeader(headers, XA_CONTENT_LANGUAGE, md.getContentLanguage());\r\n    if (md.getUserMetaDataOf(Headers.CRYPTO_CEK_ALGORITHM) != null && md.getUserMetaDataOf(Headers.UNENCRYPTED_CONTENT_LENGTH) != null) {\r\n        maybeSetHeader(headers, XA_CONTENT_LENGTH, md.getUserMetaDataOf(Headers.UNENCRYPTED_CONTENT_LENGTH));\r\n    } else {\r\n        maybeSetHeader(headers, XA_CONTENT_LENGTH, md.getContentLength());\r\n    }\r\n    maybeSetHeader(headers, XA_CONTENT_MD5, md.getContentMD5());\r\n    maybeSetHeader(headers, XA_CONTENT_RANGE, md.getContentRange());\r\n    maybeSetHeader(headers, XA_CONTENT_TYPE, md.getContentType());\r\n    maybeSetHeader(headers, XA_ETAG, md.getETag());\r\n    maybeSetHeader(headers, XA_LAST_MODIFIED, md.getLastModified());\r\n    maybeSetHeader(headers, XA_ARCHIVE_STATUS, md.getArchiveStatus());\r\n    maybeSetHeader(headers, XA_OBJECT_LOCK_LEGAL_HOLD_STATUS, md.getObjectLockLegalHoldStatus());\r\n    maybeSetHeader(headers, XA_OBJECT_LOCK_MODE, md.getObjectLockMode());\r\n    maybeSetHeader(headers, XA_OBJECT_LOCK_RETAIN_UNTIL_DATE, md.getObjectLockRetainUntilDate());\r\n    maybeSetHeader(headers, XA_OBJECT_REPLICATION_STATUS, md.getReplicationStatus());\r\n    maybeSetHeader(headers, XA_S3_VERSION_ID, md.getVersionId());\r\n    maybeSetHeader(headers, XA_SERVER_SIDE_ENCRYPTION, md.getSSEAlgorithm());\r\n    maybeSetHeader(headers, XA_STORAGE_CLASS, md.getStorageClass());\r\n    maybeSetHeader(headers, XA_STORAGE_CLASS, md.getReplicationStatus());\r\n    return headers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "maybeSetHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeSetHeader(final Map<String, byte[]> headers, final String name, final Object value)\n{\r\n    if (value != null) {\r\n        headers.put(name, encodeBytes(value));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "encodeBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] encodeBytes(@Nullable Object s)\n{\r\n    return s == null ? EMPTY : s.toString().getBytes(StandardCharsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "decodeBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String decodeBytes(byte[] bytes)\n{\r\n    return bytes == null ? null : new String(bytes, StandardCharsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getXAttr(Path path, String name) throws IOException\n{\r\n    return retrieveHeaders(path, INVOCATION_XATTR_GET_NAMED).get(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path) throws IOException\n{\r\n    return retrieveHeaders(path, INVOCATION_XATTR_GET_MAP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> listXAttrs(final Path path) throws IOException\n{\r\n    return new ArrayList<>(retrieveHeaders(path, INVOCATION_OP_XATTR_LIST).keySet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(Path path, List<String> names) throws IOException\n{\r\n    Map<String, byte[]> headers = retrieveHeaders(path, INVOCATION_XATTR_GET_NAMED_MAP);\r\n    Map<String, byte[]> result = new TreeMap<>();\r\n    headers.entrySet().stream().filter(entry -> names.contains(entry.getKey())).forEach(entry -> result.put(entry.getKey(), entry.getValue()));\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "extractXAttrLongValue",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Optional<Long> extractXAttrLongValue(byte[] data)\n{\r\n    String xAttr;\r\n    xAttr = HeaderProcessing.decodeBytes(data);\r\n    if (StringUtils.isNotEmpty(xAttr)) {\r\n        try {\r\n            long l = Long.parseLong(xAttr);\r\n            if (l >= 0) {\r\n                return Optional.of(l);\r\n            }\r\n        } catch (NumberFormatException ex) {\r\n            LOG.warn(\"Not a number: {}\", xAttr, ex);\r\n        }\r\n    }\r\n    return Optional.empty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "cloneObjectMetadata",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void cloneObjectMetadata(ObjectMetadata source, ObjectMetadata dest)\n{\r\n    if (source.getCacheControl() != null) {\r\n        dest.setCacheControl(source.getCacheControl());\r\n    }\r\n    if (source.getContentDisposition() != null) {\r\n        dest.setContentDisposition(source.getContentDisposition());\r\n    }\r\n    if (source.getContentEncoding() != null) {\r\n        dest.setContentEncoding(source.getContentEncoding());\r\n    }\r\n    if (source.getContentMD5() != null) {\r\n        dest.setContentMD5(source.getContentMD5());\r\n    }\r\n    if (source.getContentType() != null) {\r\n        dest.setContentType(source.getContentType());\r\n    }\r\n    if (source.getExpirationTime() != null) {\r\n        dest.setExpirationTime(source.getExpirationTime());\r\n    }\r\n    if (source.getExpirationTimeRuleId() != null) {\r\n        dest.setExpirationTimeRuleId(source.getExpirationTimeRuleId());\r\n    }\r\n    if (source.getHttpExpiresDate() != null) {\r\n        dest.setHttpExpiresDate(source.getHttpExpiresDate());\r\n    }\r\n    if (source.getLastModified() != null) {\r\n        dest.setLastModified(source.getLastModified());\r\n    }\r\n    if (source.getOngoingRestore() != null) {\r\n        dest.setOngoingRestore(source.getOngoingRestore());\r\n    }\r\n    if (source.getRestoreExpirationTime() != null) {\r\n        dest.setRestoreExpirationTime(source.getRestoreExpirationTime());\r\n    }\r\n    if (source.getSSEAlgorithm() != null) {\r\n        dest.setSSEAlgorithm(source.getSSEAlgorithm());\r\n    }\r\n    if (source.getSSECustomerAlgorithm() != null) {\r\n        dest.setSSECustomerAlgorithm(source.getSSECustomerAlgorithm());\r\n    }\r\n    if (source.getSSECustomerKeyMd5() != null) {\r\n        dest.setSSECustomerKeyMd5(source.getSSECustomerKeyMd5());\r\n    }\r\n    source.getUserMetadata().entrySet().stream().filter(e -> !e.getKey().equals(X_HEADER_MAGIC_MARKER)).forEach(e -> dest.addUserMetadata(e.getKey(), e.getValue()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getBasePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getBasePath()\n{\r\n    return basePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "markerFound",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<Marker> markerFound(Path path, final String key, final S3ALocatedFileStatus source)\n{\r\n    markersFound++;\r\n    leafMarkers.put(path, new Marker(path, key, source));\r\n    return pathFound(path, key, source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "fileFound",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Marker> fileFound(Path path, final String key, final S3ALocatedFileStatus source)\n{\r\n    filesFound++;\r\n    return pathFound(path, key, source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "pathFound",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<Marker> pathFound(Path path, final String key, final S3ALocatedFileStatus source)\n{\r\n    objectsFound++;\r\n    List<Marker> removed = new ArrayList<>();\r\n    final Path parent = path.getParent();\r\n    if (parent == null || parent.equals(lastDirChecked)) {\r\n        return removed;\r\n    }\r\n    removeParentMarkers(parent, removed);\r\n    lastDirChecked = parent;\r\n    return removed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "removeParentMarkers",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeParentMarkers(final Path path, List<Marker> removed)\n{\r\n    if (path == null || path.isRoot()) {\r\n        return;\r\n    }\r\n    scanCount++;\r\n    removeParentMarkers(path.getParent(), removed);\r\n    final Marker value = leafMarkers.remove(path);\r\n    if (value != null) {\r\n        removed.add(value);\r\n        if (recordSurplusMarkers) {\r\n            surplusMarkers.put(path, value);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getLeafMarkers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<Path, Marker> getLeafMarkers()\n{\r\n    return leafMarkers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getSurplusMarkers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<Path, Marker> getSurplusMarkers()\n{\r\n    return surplusMarkers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getLastDirChecked",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getLastDirChecked()\n{\r\n    return lastDirChecked;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getObjectsFound",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getObjectsFound()\n{\r\n    return objectsFound;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getScanCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getScanCount()\n{\r\n    return scanCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getFilesFound",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFilesFound()\n{\r\n    return filesFound;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getMarkersFound",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMarkersFound()\n{\r\n    return markersFound;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DirMarkerTracker{\" + \"leafMarkers=\" + leafMarkers.size() + \", surplusMarkers=\" + surplusMarkers.size() + \", lastDirChecked=\" + lastDirChecked + \", filesFound=\" + filesFound + \", scanCount=\" + scanCount + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "removeAllowedMarkers",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<Path> removeAllowedMarkers(DirectoryPolicy policy)\n{\r\n    List<Path> removed = new ArrayList<>();\r\n    Iterator<Map.Entry<Path, Marker>> entries = surplusMarkers.entrySet().iterator();\r\n    while (entries.hasNext()) {\r\n        Map.Entry<Path, Marker> entry = entries.next();\r\n        Path path = entry.getKey();\r\n        if (policy.keepDirectoryMarkers(path)) {\r\n            entries.remove();\r\n            LOG.debug(\"Removing {}\", entry.getValue());\r\n            removed.add(path);\r\n        }\r\n    }\r\n    return removed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "checkNoS3Guard",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean checkNoS3Guard(URI fsURI, Configuration conf) throws PathIOException\n{\r\n    final String classname = conf.getTrimmed(S3_METADATA_STORE_IMPL, \"\");\r\n    if (classname.isEmpty()) {\r\n        return false;\r\n    }\r\n    final String[] sources = conf.getPropertySources(S3_METADATA_STORE_IMPL);\r\n    final String origin = sources == null ? \"unknown\" : sources[0];\r\n    final String fsPath = fsURI.toString();\r\n    switch(classname) {\r\n        case NULL_METADATA_STORE:\r\n            LOG.debug(\"Ignoring S3Guard store option of {} -no longer needed \" + \"Origin {}\", NULL_METADATA_STORE, origin);\r\n            break;\r\n        case S3GUARD_METASTORE_LOCAL:\r\n            LOG.warn(\"Ignoring S3Guard store option of {} -no longer needed or supported. \" + \"Origin {}\", S3GUARD_METASTORE_LOCAL, origin);\r\n            break;\r\n        case S3GUARD_METASTORE_DYNAMO:\r\n            final String message = String.format(\"S3Guard is no longer needed/supported,\" + \" yet %s is configured to use DynamoDB as the S3Guard metadata store.\" + \" This is no longer needed or supported. \" + \"Origin of setting is %s\", fsPath, origin);\r\n            LOG.error(message);\r\n            throw new PathIOException(fsPath, message);\r\n        default:\r\n            throw new PathIOException(fsPath, \"Filesystem is configured to use unknown S3Guard store \" + classname + \" origin \" + origin);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getAuthoritativePaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> getAuthoritativePaths(S3AFileSystem fs)\n{\r\n    return getAuthoritativePaths(fs.getUri(), fs.getConf(), p -> fs.maybeAddTrailingSlash(fs.qualify(p).toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "getAuthoritativePaths",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Collection<String> getAuthoritativePaths(final URI uri, final Configuration conf, final Function<Path, String> qualifyToDir)\n{\r\n    String[] rawAuthoritativePaths = conf.getTrimmedStrings(AUTHORITATIVE_PATH, DEFAULT_AUTHORITATIVE_PATH);\r\n    Collection<String> authoritativePaths = new ArrayList<>();\r\n    if (rawAuthoritativePaths.length > 0) {\r\n        for (int i = 0; i < rawAuthoritativePaths.length; i++) {\r\n            Path path = new Path(rawAuthoritativePaths[i]);\r\n            URI pathURI = path.toUri();\r\n            if (pathURI.getAuthority() != null && !pathURI.getAuthority().equals(uri.getAuthority())) {\r\n                continue;\r\n            }\r\n            if (pathURI.getScheme() != null && !pathURI.getScheme().equals(uri.getScheme())) {\r\n                continue;\r\n            }\r\n            authoritativePaths.add(qualifyToDir.apply(path));\r\n        }\r\n    }\r\n    return authoritativePaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\s3guard",
  "methodName" : "allowAuthoritative",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean allowAuthoritative(Path p, S3AFileSystem fs, Collection<String> authPaths)\n{\r\n    String haystack = fs.maybeAddTrailingSlash(fs.qualify(p).toString());\r\n    if (!authPaths.isEmpty()) {\r\n        for (String needle : authPaths) {\r\n            if (haystack.startsWith(needle)) {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getFilesDeleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFilesDeleted()\n{\r\n    return filesDeleted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "Boolean execute() throws IOException\n{\r\n    executeOnlyOnce();\r\n    StoreContext context = getStoreContext();\r\n    Path path = status.getPath();\r\n    LOG.debug(\"Delete path {} - recursive {}\", path, recursive);\r\n    LOG.debug(\"Type = {}\", status.isFile() ? \"File\" : (status.isEmptyDirectory() == Tristate.TRUE ? \"Empty Directory\" : \"Directory\"));\r\n    String key = context.pathToKey(path);\r\n    if (status.isDirectory()) {\r\n        LOG.debug(\"delete: Path is a directory: {}\", path);\r\n        checkArgument(status.isEmptyDirectory() != Tristate.UNKNOWN, \"File status must have directory emptiness computed\");\r\n        if (!key.endsWith(\"/\")) {\r\n            key = key + \"/\";\r\n        }\r\n        if (\"/\".equals(key)) {\r\n            LOG.error(\"S3A: Cannot delete the root directory.\" + \" Path: {}. Recursive: {}\", status.getPath(), recursive);\r\n            return false;\r\n        }\r\n        if (!recursive && status.isEmptyDirectory() == Tristate.FALSE) {\r\n            throw new PathIsNotEmptyDirectoryException(path.toString());\r\n        }\r\n        if (status.isEmptyDirectory() == Tristate.TRUE) {\r\n            LOG.debug(\"deleting empty directory {}\", path);\r\n            deleteObjectAtPath(path, key, false);\r\n        } else {\r\n            deleteDirectoryTree(path, key);\r\n        }\r\n    } else {\r\n        LOG.debug(\"deleting simple file {}\", path);\r\n        deleteObjectAtPath(path, key, true);\r\n    }\r\n    LOG.debug(\"Deleted {} objects\", filesDeleted);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "deleteDirectoryTree",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void deleteDirectoryTree(final Path path, final String dirKey) throws IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"deleting %s\", dirKey)) {\r\n        resetDeleteList();\r\n        deleteFuture = null;\r\n        LOG.debug(\"Getting objects for directory prefix {} to delete\", dirKey);\r\n        final RemoteIterator<S3ALocatedFileStatus> locatedFiles = callbacks.listFilesAndDirectoryMarkers(path, status, true);\r\n        while (locatedFiles.hasNext()) {\r\n            S3AFileStatus child = locatedFiles.next().toS3AFileStatus();\r\n            queueForDeletion(child);\r\n        }\r\n        LOG.debug(\"Deleting final batch of listed files\");\r\n        submitNextBatch();\r\n        maybeAwaitCompletion(deleteFuture);\r\n    }\r\n    LOG.debug(\"Delete \\\"{}\\\" completed; deleted {} objects\", path, filesDeleted);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "deletionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String deletionKey(final S3AFileStatus stat)\n{\r\n    return getStoreContext().fullKey(stat);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "queueForDeletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void queueForDeletion(final S3AFileStatus stat) throws IOException\n{\r\n    queueForDeletion(deletionKey(stat), stat.isDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "queueForDeletion",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void queueForDeletion(final String key, boolean isDirMarker) throws IOException\n{\r\n    LOG.debug(\"Adding object to delete: \\\"{}\\\"\", key);\r\n    keys.add(new DeleteEntry(key, isDirMarker));\r\n    if (keys.size() == pageSize) {\r\n        submitNextBatch();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "submitNextBatch",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void submitNextBatch() throws IOException\n{\r\n    maybeAwaitCompletion(deleteFuture);\r\n    deleteFuture = submitDelete(keys);\r\n    resetDeleteList();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "resetDeleteList",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetDeleteList()\n{\r\n    keys = new ArrayList<>(pageSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "deleteObjectAtPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteObjectAtPath(final Path path, final String key, final boolean isFile) throws IOException\n{\r\n    LOG.debug(\"delete: {} {}\", (isFile ? \"file\" : \"dir marker\"), key);\r\n    filesDeleted++;\r\n    callbacks.deleteObjectAtPath(path, key, isFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "submitDelete",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CompletableFuture<Void> submitDelete(final List<DeleteEntry> keyList)\n{\r\n    if (keyList.isEmpty()) {\r\n        return null;\r\n    }\r\n    filesDeleted += keyList.size();\r\n    return submit(executor, callableWithinAuditSpan(getAuditSpan(), () -> {\r\n        asyncDeleteAction(keyList);\r\n        return null;\r\n    }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "asyncDeleteAction",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void asyncDeleteAction(final List<DeleteEntry> keyList) throws IOException\n{\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Delete page of %d keys\", keyList.size())) {\r\n        if (!keyList.isEmpty()) {\r\n            List<DeleteObjectsRequest.KeyVersion> files = keyList.stream().filter(e -> !e.isDirMarker).map(e -> e.keyVersion).collect(Collectors.toList());\r\n            LOG.debug(\"Deleting of {} file objects\", files.size());\r\n            Invoker.once(\"Remove S3 Files\", status.getPath().toString(), () -> callbacks.removeKeys(files, false));\r\n            List<DeleteObjectsRequest.KeyVersion> dirs = keyList.stream().filter(e -> e.isDirMarker).map(e -> e.keyVersion).collect(Collectors.toList());\r\n            LOG.debug(\"Deleting of {} directory markers\", dirs.size());\r\n            Invoker.once(\"Remove S3 Dir Markers\", status.getPath().toString(), () -> callbacks.removeKeys(dirs, true));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(final DataOutput out) throws IOException\n{\r\n    new LongWritable(serialVersionUID).write(out);\r\n    Text.writeString(out, encryptionAlgorithm);\r\n    Text.writeString(out, encryptionKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(final DataInput in) throws IOException\n{\r\n    final LongWritable version = new LongWritable();\r\n    version.readFields(in);\r\n    if (version.get() != serialVersionUID) {\r\n        throw new DelegationTokenIOException(\"Incompatible EncryptionSecrets version\");\r\n    }\r\n    encryptionAlgorithm = Text.readString(in, MAX_SECRET_LENGTH);\r\n    encryptionKey = Text.readString(in, MAX_SECRET_LENGTH);\r\n    init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "readObject",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException\n{\r\n    in.defaultReadObject();\r\n    init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init() throws IOException\n{\r\n    encryptionMethod = S3AEncryptionMethods.getMethod(encryptionAlgorithm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getEncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEncryptionAlgorithm()\n{\r\n    return encryptionAlgorithm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEncryptionKey()\n{\r\n    return encryptionKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "hasEncryptionAlgorithm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasEncryptionAlgorithm()\n{\r\n    return StringUtils.isNotEmpty(encryptionAlgorithm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "hasEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasEncryptionKey()\n{\r\n    return StringUtils.isNotEmpty(encryptionKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean equals(final Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    final EncryptionSecrets that = (EncryptionSecrets) o;\r\n    return Objects.equals(encryptionAlgorithm, that.encryptionAlgorithm) && Objects.equals(encryptionKey, that.encryptionKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hash(encryptionAlgorithm, encryptionKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getEncryptionMethod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "S3AEncryptionMethods getEncryptionMethod()\n{\r\n    return encryptionMethod;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return S3AEncryptionMethods.NONE.equals(encryptionMethod) ? \"(no encryption)\" : encryptionMethod.getMethod();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Mode getMode()\n{\r\n    return mode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Source getSource()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "isRequireVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRequireVersion()\n{\r\n    return requireVersion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getLogNoVersionSupport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LogExactlyOnce getLogNoVersionSupport()\n{\r\n    return logNoVersionSupport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getPolicy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ChangeDetectionPolicy getPolicy(Configuration configuration)\n{\r\n    Mode mode = Mode.fromConfiguration(configuration);\r\n    Source source = Source.fromConfiguration(configuration);\r\n    boolean requireVersion = configuration.getBoolean(CHANGE_DETECT_REQUIRE_VERSION, CHANGE_DETECT_REQUIRE_VERSION_DEFAULT);\r\n    return createPolicy(mode, source, requireVersion);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "createPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ChangeDetectionPolicy createPolicy(final Mode mode, final Source source, final boolean requireVersion)\n{\r\n    switch(source) {\r\n        case ETag:\r\n            return new ETagChangeDetectionPolicy(mode, requireVersion);\r\n        case VersionId:\r\n            return new VersionIdChangeDetectionPolicy(mode, requireVersion);\r\n        default:\r\n            return new NoChangeDetection();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return \"Policy \" + getSource() + \"/\" + getMode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getRevisionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRevisionId(ObjectMetadata objectMetadata, String uri)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getRevisionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRevisionId(S3ObjectAttributes s3Attributes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "getRevisionId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRevisionId(CopyResult copyResult)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "applyRevisionConstraint",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void applyRevisionConstraint(GetObjectRequest request, String revisionId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "applyRevisionConstraint",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void applyRevisionConstraint(CopyObjectRequest request, String revisionId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "applyRevisionConstraint",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void applyRevisionConstraint(GetObjectMetadataRequest request, String revisionId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\impl",
  "methodName" : "onChangeDetected",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ImmutablePair<Boolean, RemoteFileChangedException> onChangeDetected(String revisionId, String newRevisionId, String uri, long position, String operation, long timesAlreadyDetected)\n{\r\n    String positionText = position >= 0 ? (\" at \" + position) : \"\";\r\n    switch(mode) {\r\n        case None:\r\n            return new ImmutablePair<>(false, null);\r\n        case Warn:\r\n            if (timesAlreadyDetected == 0) {\r\n                LOG.warn(String.format(\"%s change detected on %s %s%s. Expected %s got %s\", getSource(), operation, uri, positionText, revisionId, newRevisionId));\r\n                return new ImmutablePair<>(true, null);\r\n            }\r\n            return new ImmutablePair<>(false, null);\r\n        case Client:\r\n        case Server:\r\n        default:\r\n            return new ImmutablePair<>(true, new RemoteFileChangedException(uri, operation, String.format(\"%s \" + CHANGE_DETECTED + \" during %s%s.\" + \" Expected %s got %s\", getSource(), operation, positionText, revisionId, newRevisionId)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "isRetryable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRetryable()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setFailureInjectionPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setFailureInjectionPolicy(S3AFileSystem fs, FailureInjectionPolicy policy) throws Exception\n{\r\n    AmazonS3 s3 = fs.getAmazonS3ClientForTesting(\"s3guard\");\r\n    InconsistentAmazonS3Client ic = InconsistentAmazonS3Client.castFrom(s3);\r\n    ic.replacePolicy(policy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "replacePolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void replacePolicy(FailureInjectionPolicy pol)\n{\r\n    this.policy = pol;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"Inconsistent S3 Client: %s; failure count %d\", policy, failureCounter.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "castFrom",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InconsistentAmazonS3Client castFrom(AmazonS3 c) throws Exception\n{\r\n    InconsistentAmazonS3Client ic = null;\r\n    if (c instanceof InconsistentAmazonS3Client) {\r\n        ic = (InconsistentAmazonS3Client) c;\r\n    }\r\n    Preconditions.checkNotNull(ic, \"Not an instance of \" + \"InconsistentAmazonS3Client\");\r\n    return ic;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteObjects",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DeleteObjectsResult deleteObjects(DeleteObjectsRequest deleteObjectsRequest) throws AmazonClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.deleteObjects(deleteObjectsRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "deleteObject",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteObject(DeleteObjectRequest deleteObjectRequest) throws AmazonClientException, AmazonServiceException\n{\r\n    String key = deleteObjectRequest.getKey();\r\n    LOG.debug(\"key {}\", key);\r\n    maybeFail();\r\n    super.deleteObject(deleteObjectRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "putObject",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "PutObjectResult putObject(PutObjectRequest putObjectRequest) throws AmazonClientException, AmazonServiceException\n{\r\n    LOG.debug(\"key {}\", putObjectRequest.getKey());\r\n    maybeFail();\r\n    return super.putObject(putObjectRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listObjects",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ObjectListing listObjects(ListObjectsRequest listObjectsRequest) throws AmazonClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.listObjects(listObjectsRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listObjectsV2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ListObjectsV2Result listObjectsV2(ListObjectsV2Request request) throws AmazonClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.listObjectsV2(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "completeMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CompleteMultipartUploadResult completeMultipartUpload(CompleteMultipartUploadRequest completeMultipartUploadRequest) throws SdkClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.completeMultipartUpload(completeMultipartUploadRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "uploadPart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "UploadPartResult uploadPart(UploadPartRequest uploadPartRequest) throws SdkClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.uploadPart(uploadPartRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "initiateMultipartUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InitiateMultipartUploadResult initiateMultipartUpload(InitiateMultipartUploadRequest initiateMultipartUploadRequest) throws SdkClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.initiateMultipartUpload(initiateMultipartUploadRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "listMultipartUploads",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MultipartUploadListing listMultipartUploads(ListMultipartUploadsRequest listMultipartUploadsRequest) throws SdkClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.listMultipartUploads(listMultipartUploadsRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setThrottleProbability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setThrottleProbability(float throttleProbability)\n{\r\n    policy.setThrottleProbability(throttleProbability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeFail",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void maybeFail(String errorMsg, int statusCode) throws AmazonClientException\n{\r\n    AmazonServiceException ex = null;\r\n    if (FailureInjectionPolicy.trueWithProbability(policy.getThrottleProbability())) {\r\n        ex = new AmazonServiceException(errorMsg + \" count = \" + (failureCounter.get() + 1), null);\r\n        ex.setStatusCode(statusCode);\r\n    }\r\n    int failureLimit = policy.getFailureLimit();\r\n    if (ex != null) {\r\n        long count = failureCounter.incrementAndGet();\r\n        if (failureLimit == 0 || (failureLimit > 0 && count < failureLimit)) {\r\n            throw ex;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "maybeFail",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeFail()\n{\r\n    maybeFail(\"throttled\", 503);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "setFailureLimit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setFailureLimit(int limit)\n{\r\n    policy.setFailureLimit(limit);\r\n    failureCounter.set(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getObject",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3Object getObject(GetObjectRequest var1) throws SdkClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.getObject(var1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a",
  "methodName" : "getObject",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "S3Object getObject(String bucketName, String key) throws SdkClientException, AmazonServiceException\n{\r\n    maybeFail();\r\n    return super.getObject(bucketName, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    NoopAuditor audit = new NoopAuditor(this);\r\n    final OperationAuditorOptions options = OperationAuditorOptions.builder().withConfiguration(conf).withIoStatisticsStore(iostatisticsStore().build());\r\n    addService(audit);\r\n    audit.init(options);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getSpanId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSpanId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getOperationName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getOperationName()\n{\r\n    return getName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getAuditor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OperationAuditor getAuditor()\n{\r\n    return auditor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getUnbondedSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A getUnbondedSpan()\n{\r\n    return auditor.getUnbondedSpan();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "getActiveAuditSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanS3A getActiveAuditSpan()\n{\r\n    return NoopSpan.INSTANCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createSpan",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuditSpanS3A createSpan(final String operation, @Nullable final String path1, @Nullable final String path2) throws IOException\n{\r\n    return createNewSpan(operation, path1, path2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createRequestHandlers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<RequestHandler2> createRequestHandlers() throws IOException\n{\r\n    return new ArrayList<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createStateChangeListener",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TransferStateChangeListener createStateChangeListener()\n{\r\n    return new TransferStateChangeListener() {\r\n\r\n        public void transferStateChanged(final Transfer transfer, final Transfer.TransferState state) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean checkAccess(final Path path, final S3AFileStatus status, final FsAction mode) throws IOException\n{\r\n    return auditor.checkAccess(path, status, mode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "activate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void activate(final AuditSpanS3A span)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "deactivate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deactivate(final AuditSpanS3A span)\n{\r\n    activate(getUnbondedSpan());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\audit\\impl",
  "methodName" : "createNewSpan",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuditSpanS3A createNewSpan(final String name, final String path1, final String path2)\n{\r\n    return NoopSpan.INSTANCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceInit(final Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    roleArn = getConfig().getTrimmed(DELEGATION_TOKEN_ROLE_ARN, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindToTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AWSCredentialProviderList bindToTokenIdentifier(final AbstractS3ATokenIdentifier retrievedIdentifier) throws IOException\n{\r\n    RoleTokenIdentifier tokenIdentifier = convertTokenIdentifier(retrievedIdentifier, RoleTokenIdentifier.class);\r\n    setTokenIdentifier(Optional.of(tokenIdentifier));\r\n    MarshalledCredentials marshalledCredentials = tokenIdentifier.getMarshalledCredentials();\r\n    setExpirationDateTime(marshalledCredentials.getExpirationDateTime());\r\n    return new AWSCredentialProviderList(\"Role Token Binding\", new MarshalledCredentialProvider(COMPONENT, getStoreContext().getFsURI(), getConfig(), marshalledCredentials, MarshalledCredentials.CredentialTypeRequired.SessionOnly));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "RoleTokenIdentifier createTokenIdentifier(final Optional<RoleModel.Policy> policy, final EncryptionSecrets encryptionSecrets, final Text renewer) throws IOException\n{\r\n    requireServiceStarted();\r\n    Preconditions.checkState(!roleArn.isEmpty(), E_NO_ARN);\r\n    String policyJson = policy.isPresent() ? MODEL.toJson(policy.get()) : \"\";\r\n    final STSClientFactory.STSClient client = prepareSTSClient().orElseThrow(() -> {\r\n        LOG.error(\"Cannot issue delegation tokens because the credential\" + \" providers listed in \" + DELEGATION_TOKEN_CREDENTIALS_PROVIDER + \" are returning session tokens\");\r\n        return new DelegationTokenIOException(E_NO_SESSION_TOKENS_FOR_ROLE_BINDING);\r\n    });\r\n    Credentials credentials = client.requestRole(roleArn, UUID.randomUUID().toString(), policyJson, getDuration(), TimeUnit.SECONDS);\r\n    return new RoleTokenIdentifier(getCanonicalUri(), getOwnerText(), renewer, fromSTSCredentials(credentials), encryptionSecrets, AbstractS3ATokenIdentifier.createDefaultOriginMessage() + \" Role ARN=\" + roleArn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "createEmptyIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RoleTokenIdentifier createEmptyIdentifier()\n{\r\n    return new RoleTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getDescription()\n{\r\n    return super.getDescription() + \" Role ARN=\" + (roleArn.isEmpty() ? \"(none)\" : ('\"' + roleArn + '\"'));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-aws\\src\\main\\java\\org\\apache\\hadoop\\fs\\s3a\\auth\\delegation",
  "methodName" : "bindingName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String bindingName()\n{\r\n    return \"Role\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
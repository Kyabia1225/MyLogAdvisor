[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setMaxBlockSize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMaxBlockSize(int size)\n{\r\n    maxBlockSize.set(size);\r\n    this.outBuffer = ByteBuffer.allocate(maxBlockSize.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setCompactionBlockCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCompactionBlockCount(int activationCount)\n{\r\n    activateCompactionBlockCount = activationCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBlockList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<BlockEntry> getBlockList() throws StorageException, IOException\n{\r\n    return blob.downloadBlockList(BlockListingFilter.COMMITTED, new BlobRequestOptions(), opContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(final int byteVal) throws IOException\n{\r\n    write(new byte[] { (byte) (byteVal & 0xFF) });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(final byte[] data, int offset, int length) throws IOException\n{\r\n    Preconditions.checkArgument(data != null, \"null data\");\r\n    if (offset < 0 || length < 0 || length > data.length - offset) {\r\n        throw new IndexOutOfBoundsException();\r\n    }\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n    while (outBuffer.remaining() < length) {\r\n        int remaining = outBuffer.remaining();\r\n        outBuffer.put(data, offset, remaining);\r\n        addBlockUploadCommand();\r\n        offset += remaining;\r\n        length -= remaining;\r\n    }\r\n    outBuffer.put(data, offset, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "flush",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void flush() throws IOException\n{\r\n    if (closed) {\r\n        return;\r\n    }\r\n    addBlockUploadCommand();\r\n    if (committedBlobLength.get() < blobLength) {\r\n        try {\r\n            addFlushCommand().await();\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hsync() throws IOException\n{\r\n    if (compactionEnabled) {\r\n        flush();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hflush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hflush() throws IOException\n{\r\n    if (compactionEnabled) {\r\n        flush();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    if (!compactionEnabled) {\r\n        return false;\r\n    }\r\n    return StoreImplementationUtils.isProbeForSyncable(capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : [ "InterruptedException", "StorageException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    LOG.debug(\"close {} \", key);\r\n    if (closed) {\r\n        return;\r\n    }\r\n    flush();\r\n    ioThreadPool.shutdown();\r\n    try {\r\n        if (!ioThreadPool.awaitTermination(CLOSE_UPLOAD_DELAY, TimeUnit.MINUTES)) {\r\n            LOG.error(\"Time out occurred while close() is waiting for IO request to\" + \" finish in append\" + \" for blob : {}\", key);\r\n            NativeAzureFileSystemHelper.logAllLiveStackTraces();\r\n            throw new AzureException(\"Timed out waiting for IO requests to finish\");\r\n        }\r\n    } catch (InterruptedException ex) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n    if (firstError.get() == null && blobExist) {\r\n        try {\r\n            lease.free();\r\n        } catch (StorageException ex) {\r\n            LOG.debug(\"Lease free update blob {} encountered Storage Exception:\" + \" {} Error Code : {}\", key, ex, ex.getErrorCode());\r\n            maybeSetFirstError(new AzureException(ex));\r\n        }\r\n    }\r\n    closed = true;\r\n    if (firstError.get() != null && !firstErrorThrown) {\r\n        throw firstError.get();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setBlocksCountAndBlockIdPrefix",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setBlocksCountAndBlockIdPrefix(List<BlockEntry> blockEntries)\n{\r\n    if (nextBlockCount == UNSET_BLOCKS_COUNT && blockIdPrefix == null) {\r\n        Random sequenceGenerator = new Random();\r\n        String blockZeroBlockId = (!blockEntries.isEmpty()) ? blockEntries.get(0).getId() : \"\";\r\n        String prefix = UUID.randomUUID().toString() + \"-\";\r\n        String sampleNewerVersionBlockId = generateNewerVersionBlockId(prefix, 0);\r\n        if (!blockEntries.isEmpty() && blockZeroBlockId.length() < sampleNewerVersionBlockId.length()) {\r\n            this.blockIdPrefix = \"\";\r\n            nextBlockCount = (long) (sequenceGenerator.nextInt(Integer.MAX_VALUE)) + sequenceGenerator.nextInt(Integer.MAX_VALUE - MAX_BLOCK_COUNT);\r\n            nextBlockCount += blockEntries.size();\r\n        } else {\r\n            this.blockIdPrefix = prefix;\r\n            nextBlockCount = blockEntries.size();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "generateBlockId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String generateBlockId() throws IOException\n{\r\n    if (nextBlockCount == UNSET_BLOCKS_COUNT || blockIdPrefix == null) {\r\n        throw new AzureException(\"Append Stream in invalid state. nextBlockCount not set correctly\");\r\n    }\r\n    return (!blockIdPrefix.isEmpty()) ? generateNewerVersionBlockId(blockIdPrefix, nextBlockCount++) : generateOlderVersionBlockId(nextBlockCount++);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "generateOlderVersionBlockId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateOlderVersionBlockId(long id)\n{\r\n    byte[] blockIdInBytes = new byte[8];\r\n    for (int m = 0; m < 8; m++) {\r\n        blockIdInBytes[7 - m] = (byte) ((id >> (8 * m)) & 0xFF);\r\n    }\r\n    return new String(Base64.encodeBase64(blockIdInBytes), StandardCharsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "generateNewerVersionBlockId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String generateNewerVersionBlockId(String prefix, long id)\n{\r\n    String blockIdSuffix = String.format(\"%06d\", id);\r\n    byte[] blockIdInBytes = (prefix + blockIdSuffix).getBytes(StandardCharsets.UTF_8);\r\n    return new String(Base64.encodeBase64(blockIdInBytes), StandardCharsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeBlockRequestInternal",
  "errType" : [ "Exception", "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void writeBlockRequestInternal(String blockId, ByteBuffer dataPayload, boolean bufferPoolBuffer)\n{\r\n    IOException lastLocalException = null;\r\n    int uploadRetryAttempts = 0;\r\n    while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {\r\n        try {\r\n            long startTime = System.nanoTime();\r\n            blob.uploadBlock(blockId, accessCondition, new ByteArrayInputStream(dataPayload.array()), dataPayload.position(), new BlobRequestOptions(), opContext);\r\n            LOG.debug(\"upload block finished for {} ms. block {} \", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime), blockId);\r\n            break;\r\n        } catch (Exception ioe) {\r\n            LOG.debug(\"Encountered exception during uploading block for Blob {}\" + \" Exception : {}\", key, ioe);\r\n            uploadRetryAttempts++;\r\n            lastLocalException = new AzureException(\"Encountered Exception while uploading block: \" + ioe, ioe);\r\n            try {\r\n                Thread.sleep(BLOCK_UPLOAD_RETRY_INTERVAL * (uploadRetryAttempts + 1));\r\n            } catch (InterruptedException ie) {\r\n                Thread.currentThread().interrupt();\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    if (bufferPoolBuffer) {\r\n        poolReadyByteBuffers.putBuffer(dataPayload);\r\n    }\r\n    if (uploadRetryAttempts == MAX_BLOCK_UPLOAD_RETRIES) {\r\n        maybeSetFirstError(lastLocalException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "maybeSetFirstError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeSetFirstError(IOException exception)\n{\r\n    firstError.compareAndSet(null, exception);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "maybeThrowFirstError",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeThrowFirstError() throws IOException\n{\r\n    if (firstError.get() != null) {\r\n        firstErrorThrown = true;\r\n        throw firstError.get();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeBlockListRequestInternal",
  "errType" : [ "Exception", "InterruptedException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void writeBlockListRequestInternal()\n{\r\n    IOException lastLocalException = null;\r\n    int uploadRetryAttempts = 0;\r\n    while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {\r\n        try {\r\n            long startTime = System.nanoTime();\r\n            blob.commitBlockList(blockEntries, accessCondition, new BlobRequestOptions(), opContext);\r\n            LOG.debug(\"Upload block list took {} ms for blob {} \", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime), key);\r\n            break;\r\n        } catch (Exception ioe) {\r\n            LOG.debug(\"Encountered exception during uploading block for Blob {}\" + \" Exception : {}\", key, ioe);\r\n            uploadRetryAttempts++;\r\n            lastLocalException = new AzureException(\"Encountered Exception while uploading block: \" + ioe, ioe);\r\n            try {\r\n                Thread.sleep(BLOCK_UPLOAD_RETRY_INTERVAL * (uploadRetryAttempts + 1));\r\n            } catch (InterruptedException ie) {\r\n                Thread.currentThread().interrupt();\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    if (uploadRetryAttempts == MAX_BLOCK_UPLOAD_RETRIES) {\r\n        maybeSetFirstError(lastLocalException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addBlockUploadCommand",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void addBlockUploadCommand() throws IOException\n{\r\n    maybeThrowFirstError();\r\n    if (blobExist && lease.isFreed()) {\r\n        throw new AzureException(String.format(\"Attempting to upload a block on blob : %s \" + \" that does not have lease on the Blob. Failing upload\", key));\r\n    }\r\n    int blockSize = outBuffer.position();\r\n    if (blockSize > 0) {\r\n        UploadCommand command = new UploadBlockCommand(generateBlockId(), outBuffer);\r\n        activeBlockCommands.add(command);\r\n        blobLength += blockSize;\r\n        outBuffer = poolReadyByteBuffers.getBuffer(false, maxBlockSize.get());\r\n        ioThreadPool.execute(new WriteRequest(command));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addFlushCommand",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "UploadCommand addFlushCommand() throws IOException\n{\r\n    maybeThrowFirstError();\r\n    if (blobExist && lease.isFreed()) {\r\n        throw new AzureException(String.format(\"Attempting to upload block list on blob : %s\" + \" that does not have lease on the Blob. Failing upload\", key));\r\n    }\r\n    UploadCommand command = new UploadBlockListCommand();\r\n    activeBlockCommands.add(command);\r\n    ioThreadPool.execute(new WriteRequest(command));\r\n    return command;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hook",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void hook(OperationContext operationContext, float readFactor, float writeFactor)\n{\r\n    SelfThrottlingIntercept throttler = new SelfThrottlingIntercept(operationContext, readFactor, writeFactor);\r\n    ResponseReceivedListener responseListener = throttler.new ResponseReceivedListener();\r\n    SendingRequestListener sendingListener = throttler.new SendingRequestListener();\r\n    operationContext.getResponseReceivedEventHandler().addListener(responseListener);\r\n    operationContext.getSendingRequestEventHandler().addListener(sendingListener);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "responseReceived",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void responseReceived(ResponseReceivedEvent event)\n{\r\n    RequestResult result = event.getRequestResult();\r\n    Date startDate = result.getStartDate();\r\n    Date stopDate = result.getStopDate();\r\n    long elapsed = stopDate.getTime() - startDate.getTime();\r\n    synchronized (this) {\r\n        this.lastE2Elatency = elapsed;\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        int statusCode = result.getStatusCode();\r\n        String etag = result.getEtag();\r\n        HttpURLConnection urlConnection = (HttpURLConnection) event.getConnectionObject();\r\n        int contentLength = urlConnection.getContentLength();\r\n        String requestMethod = urlConnection.getRequestMethod();\r\n        long threadId = Thread.currentThread().getId();\r\n        LOG.debug(String.format(\"SelfThrottlingIntercept:: ResponseReceived: threadId=%d, Status=%d, Elapsed(ms)=%d, ETAG=%s, contentLength=%d, requestMethod=%s\", threadId, statusCode, elapsed, etag, contentLength, requestMethod));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "sendingRequest",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void sendingRequest(SendingRequestEvent sendEvent)\n{\r\n    long lastLatency;\r\n    boolean operationIsRead;\r\n    synchronized (this) {\r\n        lastLatency = this.lastE2Elatency;\r\n    }\r\n    float sleepMultiple;\r\n    HttpURLConnection urlConnection = (HttpURLConnection) sendEvent.getConnectionObject();\r\n    if (urlConnection.getRequestMethod().equalsIgnoreCase(\"PUT\")) {\r\n        operationIsRead = false;\r\n        sleepMultiple = (1 / writeFactor) - 1;\r\n    } else {\r\n        operationIsRead = true;\r\n        sleepMultiple = (1 / readFactor) - 1;\r\n    }\r\n    long sleepDuration = (long) (sleepMultiple * lastLatency);\r\n    if (sleepDuration < 0) {\r\n        sleepDuration = 0;\r\n    }\r\n    if (sleepDuration > 0) {\r\n        try {\r\n            Thread.sleep(sleepDuration);\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n        sendEvent.getRequestResult().setStartDate(new Date());\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        boolean isFirstRequest = (lastLatency == 0);\r\n        long threadId = Thread.currentThread().getId();\r\n        LOG.debug(String.format(\" SelfThrottlingIntercept:: SendingRequest:   threadId=%d, requestType=%s, isFirstRequest=%b, sleepDuration=%d\", threadId, operationIsRead ? \"read \" : \"write\", isFirstRequest, sleepDuration));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bind(OperationContext opContext)\n{\r\n    opContext.getSendingRequestEventHandler().addListener(new SendRequestIntercept());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "eventOccurred",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void eventOccurred(SendingRequestEvent sendEvent)\n{\r\n    if (!(sendEvent.getConnectionObject() instanceof HttpURLConnection)) {\r\n        return;\r\n    }\r\n    HttpURLConnection urlConnection = (HttpURLConnection) sendEvent.getConnectionObject();\r\n    if (urlConnection.getRequestMethod().equalsIgnoreCase(\"GET\")) {\r\n        urlConnection.setRequestProperty(HeaderConstants.IF_MATCH, ALLOW_ALL_REQUEST_PRECONDITIONS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createOutputStreamId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createOutputStreamId()\n{\r\n    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    return supportFlush && isProbeForSyncable(capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(final int byteVal) throws IOException\n{\r\n    write(new byte[] { (byte) (byteVal & 0xFF) });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void write(final byte[] data, final int off, final int length) throws IOException\n{\r\n    DataBlocks.validateWriteArgs(data, off, length);\r\n    maybeThrowLastError();\r\n    if (off < 0 || length < 0 || length > data.length - off) {\r\n        throw new IndexOutOfBoundsException();\r\n    }\r\n    if (hasLease() && isLeaseFreed()) {\r\n        throw new PathIOException(path, ERR_WRITE_WITHOUT_LEASE);\r\n    }\r\n    DataBlocks.DataBlock block = createBlockIfNeeded();\r\n    int written = block.write(data, off, length);\r\n    int remainingCapacity = block.remainingCapacity();\r\n    if (written < length) {\r\n        LOG.debug(\"writing more data than block capacity -triggering upload\");\r\n        uploadCurrentBlock();\r\n        this.write(data, off + written, length - written);\r\n    } else {\r\n        if (remainingCapacity == 0) {\r\n            uploadCurrentBlock();\r\n        }\r\n    }\r\n    incrementWriteOps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createBlockIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DataBlocks.DataBlock createBlockIfNeeded() throws IOException\n{\r\n    if (activeBlock == null) {\r\n        blockCount++;\r\n        activeBlock = blockFactory.create(blockCount, this.blockSize, outputStreamStatistics);\r\n    }\r\n    return activeBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "uploadCurrentBlock",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void uploadCurrentBlock() throws IOException\n{\r\n    checkState(hasActiveBlock(), \"No active block\");\r\n    LOG.debug(\"Writing block # {}\", blockCount);\r\n    try {\r\n        uploadBlockAsync(getActiveBlock(), false, false);\r\n    } finally {\r\n        clearActiveBlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "uploadBlockAsync",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void uploadBlockAsync(DataBlocks.DataBlock blockToUpload, boolean isFlush, boolean isClose) throws IOException\n{\r\n    if (this.isAppendBlob) {\r\n        writeAppendBlobCurrentBufferToService();\r\n        return;\r\n    }\r\n    if (!blockToUpload.hasData()) {\r\n        return;\r\n    }\r\n    numOfAppendsToServerSinceLastFlush++;\r\n    final int bytesLength = blockToUpload.dataSize();\r\n    final long offset = position;\r\n    position += bytesLength;\r\n    outputStreamStatistics.bytesToUpload(bytesLength);\r\n    outputStreamStatistics.writeCurrentBuffer();\r\n    DataBlocks.BlockUploadData blockUploadData = blockToUpload.startUpload();\r\n    final Future<Void> job = executorService.submit(() -> {\r\n        AbfsPerfTracker tracker = client.getAbfsPerfTracker();\r\n        try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"writeCurrentBufferToService\", \"append\")) {\r\n            AppendRequestParameters.Mode mode = APPEND_MODE;\r\n            if (isFlush & isClose) {\r\n                mode = FLUSH_CLOSE_MODE;\r\n            } else if (isFlush) {\r\n                mode = FLUSH_MODE;\r\n            }\r\n            AppendRequestParameters reqParams = new AppendRequestParameters(offset, 0, bytesLength, mode, false, leaseId);\r\n            AbfsRestOperation op = client.append(path, blockUploadData.toByteArray(), reqParams, cachedSasToken.get(), new TracingContext(tracingContext));\r\n            cachedSasToken.update(op.getSasToken());\r\n            perfInfo.registerResult(op.getResult());\r\n            perfInfo.registerSuccess(true);\r\n            outputStreamStatistics.uploadSuccessful(bytesLength);\r\n            return null;\r\n        } finally {\r\n            IOUtils.close(blockUploadData);\r\n        }\r\n    });\r\n    writeOperations.add(new WriteOperation(job, offset, bytesLength));\r\n    shrinkWriteOperationQueue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "failureWhileSubmit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void failureWhileSubmit(Exception ex) throws IOException\n{\r\n    if (ex instanceof AbfsRestOperationException) {\r\n        if (((AbfsRestOperationException) ex).getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\r\n            throw new FileNotFoundException(ex.getMessage());\r\n        }\r\n    }\r\n    if (ex instanceof IOException) {\r\n        lastError = (IOException) ex;\r\n    } else {\r\n        lastError = new IOException(ex);\r\n    }\r\n    throw lastError;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getActiveBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataBlocks.DataBlock getActiveBlock()\n{\r\n    return activeBlock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hasActiveBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasActiveBlock()\n{\r\n    return activeBlock != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hasActiveBlockDataToUpload",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasActiveBlockDataToUpload()\n{\r\n    return hasActiveBlock() && getActiveBlock().hasData();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "clearActiveBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearActiveBlock()\n{\r\n    if (activeBlock != null) {\r\n        LOG.debug(\"Clearing active block\");\r\n    }\r\n    synchronized (this) {\r\n        activeBlock = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "incrementWriteOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementWriteOps()\n{\r\n    if (statistics != null) {\r\n        statistics.incrementWriteOps(1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "maybeThrowLastError",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void maybeThrowLastError() throws IOException\n{\r\n    if (lastError != null) {\r\n        throw lastError;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "flush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void flush() throws IOException\n{\r\n    if (!disableOutputStreamFlush) {\r\n        flushInternalAsync();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hsync() throws IOException\n{\r\n    if (supportFlush) {\r\n        flushInternal(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hflush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hflush() throws IOException\n{\r\n    if (supportFlush) {\r\n        flushInternal(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStreamID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStreamID()\n{\r\n    return outputStreamId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "registerListener",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerListener(Listener listener1)\n{\r\n    listener = listener1;\r\n    tracingContext.setListener(listener);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "close",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (closed) {\r\n        return;\r\n    }\r\n    try {\r\n        flushInternal(true);\r\n    } catch (IOException e) {\r\n        throw wrapException(path, e.getMessage(), e);\r\n    } finally {\r\n        if (hasLease()) {\r\n            lease.free();\r\n            lease = null;\r\n        }\r\n        lastError = new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n        buffer = null;\r\n        bufferIndex = 0;\r\n        closed = true;\r\n        writeOperations.clear();\r\n        if (hasActiveBlock()) {\r\n            clearActiveBlock();\r\n        }\r\n    }\r\n    LOG.debug(\"Closing AbfsOutputStream : {}\", this);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "flushInternal",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void flushInternal(boolean isClose) throws IOException\n{\r\n    maybeThrowLastError();\r\n    if (!isAppendBlob && enableSmallWriteOptimization && (numOfAppendsToServerSinceLastFlush == 0) && (writeOperations.size() == 0) && hasActiveBlockDataToUpload()) {\r\n        smallWriteOptimizedflushInternal(isClose);\r\n        return;\r\n    }\r\n    if (hasActiveBlockDataToUpload()) {\r\n        uploadCurrentBlock();\r\n    }\r\n    flushWrittenBytesToService(isClose);\r\n    numOfAppendsToServerSinceLastFlush = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "smallWriteOptimizedflushInternal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void smallWriteOptimizedflushInternal(boolean isClose) throws IOException\n{\r\n    uploadBlockAsync(getActiveBlock(), true, isClose);\r\n    waitForAppendsToComplete();\r\n    shrinkWriteOperationQueue();\r\n    maybeThrowLastError();\r\n    numOfAppendsToServerSinceLastFlush = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "flushInternalAsync",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void flushInternalAsync() throws IOException\n{\r\n    maybeThrowLastError();\r\n    if (hasActiveBlockDataToUpload()) {\r\n        uploadCurrentBlock();\r\n    }\r\n    waitForAppendsToComplete();\r\n    flushWrittenBytesToServiceAsync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "writeAppendBlobCurrentBufferToService",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void writeAppendBlobCurrentBufferToService() throws IOException\n{\r\n    DataBlocks.DataBlock activeBlock = getActiveBlock();\r\n    if (!hasActiveBlockDataToUpload()) {\r\n        return;\r\n    }\r\n    final int bytesLength = activeBlock.dataSize();\r\n    DataBlocks.BlockUploadData uploadData = activeBlock.startUpload();\r\n    clearActiveBlock();\r\n    outputStreamStatistics.writeCurrentBuffer();\r\n    outputStreamStatistics.bytesToUpload(bytesLength);\r\n    final long offset = position;\r\n    position += bytesLength;\r\n    AbfsPerfTracker tracker = client.getAbfsPerfTracker();\r\n    try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"writeCurrentBufferToService\", \"append\")) {\r\n        AppendRequestParameters reqParams = new AppendRequestParameters(offset, 0, bytesLength, APPEND_MODE, true, leaseId);\r\n        AbfsRestOperation op = client.append(path, uploadData.toByteArray(), reqParams, cachedSasToken.get(), new TracingContext(tracingContext));\r\n        cachedSasToken.update(op.getSasToken());\r\n        outputStreamStatistics.uploadSuccessful(bytesLength);\r\n        perfInfo.registerResult(op.getResult());\r\n        perfInfo.registerSuccess(true);\r\n        return;\r\n    } catch (Exception ex) {\r\n        outputStreamStatistics.uploadFailed(bytesLength);\r\n        failureWhileSubmit(ex);\r\n    } finally {\r\n        IOUtils.close(uploadData);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "waitForAppendsToComplete",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void waitForAppendsToComplete() throws IOException\n{\r\n    for (WriteOperation writeOperation : writeOperations) {\r\n        try {\r\n            writeOperation.task.get();\r\n        } catch (Exception ex) {\r\n            outputStreamStatistics.uploadFailed(writeOperation.length);\r\n            if (ex.getCause() instanceof AbfsRestOperationException) {\r\n                if (((AbfsRestOperationException) ex.getCause()).getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\r\n                    throw new FileNotFoundException(ex.getMessage());\r\n                }\r\n            }\r\n            if (ex.getCause() instanceof AzureBlobFileSystemException) {\r\n                ex = (AzureBlobFileSystemException) ex.getCause();\r\n            }\r\n            lastError = new IOException(ex);\r\n            throw lastError;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "flushWrittenBytesToService",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void flushWrittenBytesToService(boolean isClose) throws IOException\n{\r\n    waitForAppendsToComplete();\r\n    flushWrittenBytesToServiceInternal(position, false, isClose);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "flushWrittenBytesToServiceAsync",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void flushWrittenBytesToServiceAsync() throws IOException\n{\r\n    shrinkWriteOperationQueue();\r\n    if (this.lastTotalAppendOffset > this.lastFlushOffset) {\r\n        this.flushWrittenBytesToServiceInternal(this.lastTotalAppendOffset, true, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "flushWrittenBytesToServiceInternal",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void flushWrittenBytesToServiceInternal(final long offset, final boolean retainUncommitedData, final boolean isClose) throws IOException\n{\r\n    if (this.isAppendBlob && !isClose) {\r\n        return;\r\n    }\r\n    AbfsPerfTracker tracker = client.getAbfsPerfTracker();\r\n    try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"flushWrittenBytesToServiceInternal\", \"flush\")) {\r\n        AbfsRestOperation op = client.flush(path, offset, retainUncommitedData, isClose, cachedSasToken.get(), leaseId, new TracingContext(tracingContext));\r\n        cachedSasToken.update(op.getSasToken());\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        if (ex instanceof AbfsRestOperationException) {\r\n            if (((AbfsRestOperationException) ex).getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\r\n                throw new FileNotFoundException(ex.getMessage());\r\n            }\r\n        }\r\n        lastError = new IOException(ex);\r\n        throw lastError;\r\n    }\r\n    this.lastFlushOffset = offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "shrinkWriteOperationQueue",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void shrinkWriteOperationQueue() throws IOException\n{\r\n    try {\r\n        WriteOperation peek = writeOperations.peek();\r\n        while (peek != null && peek.task.isDone()) {\r\n            peek.task.get();\r\n            lastTotalAppendOffset += peek.length;\r\n            writeOperations.remove();\r\n            peek = writeOperations.peek();\r\n            outputStreamStatistics.queueShrunk();\r\n        }\r\n    } catch (Exception e) {\r\n        if (e.getCause() instanceof AzureBlobFileSystemException) {\r\n            lastError = (AzureBlobFileSystemException) e.getCause();\r\n        } else {\r\n            lastError = new IOException(e);\r\n        }\r\n        throw lastError;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "waitForPendingUploads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForPendingUploads() throws IOException\n{\r\n    waitForAppendsToComplete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getOutputStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamStatistics getOutputStreamStatistics()\n{\r\n    return outputStreamStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getWriteOperationsSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getWriteOperationsSize()\n{\r\n    return writeOperations.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMaxConcurrentRequestCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxConcurrentRequestCount()\n{\r\n    return this.maxConcurrentRequestCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMaxRequestsThatCanBeQueued",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxRequestsThatCanBeQueued()\n{\r\n    return maxRequestsThatCanBeQueued;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isAppendBlobStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Boolean isAppendBlobStream()\n{\r\n    return isAppendBlob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isLeaseFreed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isLeaseFreed()\n{\r\n    if (lease == null) {\r\n        return true;\r\n    }\r\n    return lease.isFreed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hasLease",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasLease()\n{\r\n    return lease != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(super.toString());\r\n    sb.append(\"AbfsOutputStream@\").append(this.hashCode());\r\n    sb.append(\"){\");\r\n    sb.append(outputStreamStatistics.toString());\r\n    sb.append(\"}\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "lookupMetric",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableMetric lookupMetric(String name)\n{\r\n    return getRegistry().get(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "lookupCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableCounterLong lookupCounter(String name)\n{\r\n    MutableMetric metric = lookupMetric(name);\r\n    if (metric == null) {\r\n        return null;\r\n    }\r\n    if (!(metric instanceof MutableCounterLong)) {\r\n        throw new IllegalStateException(\"Metric \" + name + \" is not a MutableCounterLong: \" + metric);\r\n    }\r\n    return (MutableCounterLong) metric;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MutableCounterLong createCounter(AbfsStatistic stats)\n{\r\n    return registry.newCounter(stats.getStatName(), stats.getStatDescription(), 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incrementCounter(AbfsStatistic statistic, long value)\n{\r\n    ioStatisticsStore.incrementCounter(statistic.getStatName(), value);\r\n    MutableCounterLong counter = lookupCounter(statistic.getStatName());\r\n    if (counter != null) {\r\n        counter.incr(value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRegistry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MetricsRegistry getRegistry()\n{\r\n    return registry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "formString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String formString(String prefix, String separator, String suffix, boolean all)\n{\r\n    MetricStringBuilder metricStringBuilder = new MetricStringBuilder(null, prefix, separator, suffix);\r\n    registry.snapshot(metricStringBuilder, all);\r\n    return metricStringBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "toMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> toMap()\n{\r\n    return ioStatisticsStore.counters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return ioStatisticsStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "trackDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DurationTracker trackDuration(String key)\n{\r\n    return ioStatisticsStore.trackDuration(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureADToken getToken() throws IOException\n{\r\n    if (isTokenAboutToExpire()) {\r\n        LOG.debug(\"AAD Token is missing or expired:\" + \" Calling refresh-token from abstract base class\");\r\n        token = refreshToken();\r\n    }\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "refreshToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureADToken refreshToken() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "isTokenAboutToExpire",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean isTokenAboutToExpire()\n{\r\n    if (token == null) {\r\n        LOG.debug(\"AADToken: no token. Returning expiring=true\");\r\n        return true;\r\n    }\r\n    boolean expiring = false;\r\n    long approximatelyNow = System.currentTimeMillis() + FIVE_MINUTES;\r\n    if (token.getExpiry().getTime() < approximatelyNow) {\r\n        expiring = true;\r\n    }\r\n    if (expiring) {\r\n        LOG.debug(\"AADToken: token expiring: \" + token.getExpiry().toString() + \" : Five-minute window: \" + new Date(approximatelyNow).toString());\r\n    }\r\n    return expiring;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withExponentialRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsClientContextBuilder withExponentialRetryPolicy(final ExponentialRetryPolicy exponentialRetryPolicy)\n{\r\n    this.exponentialRetryPolicy = exponentialRetryPolicy;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withAbfsPerfTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsClientContextBuilder withAbfsPerfTracker(final AbfsPerfTracker abfsPerfTracker)\n{\r\n    this.abfsPerfTracker = abfsPerfTracker;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withAbfsCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsClientContextBuilder withAbfsCounters(final AbfsCounters abfsCounters)\n{\r\n    this.abfsCounters = abfsCounters;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsClientContext build()\n{\r\n    return new AbfsClientContext(exponentialRetryPolicy, abfsPerfTracker, abfsCounters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "cleanup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup(Logger log, java.io.Closeable closeable)\n{\r\n    if (closeable != null) {\r\n        try {\r\n            closeable.close();\r\n        } catch (IOException e) {\r\n            if (log != null) {\r\n                log.debug(\"Exception in closing {}\", closeable, e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "checkForAzureStorageException",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Throwable checkForAzureStorageException(Exception e)\n{\r\n    Throwable innerException = e.getCause();\r\n    while (innerException != null && !(innerException instanceof StorageException)) {\r\n        innerException = innerException.getCause();\r\n    }\r\n    return innerException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isFileNotFoundException",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean isFileNotFoundException(StorageException e)\n{\r\n    String errorCode = e.getErrorCode();\r\n    if (errorCode != null && (errorCode.equals(StorageErrorCodeStrings.BLOB_NOT_FOUND) || errorCode.equals(StorageErrorCodeStrings.RESOURCE_NOT_FOUND) || errorCode.equals(StorageErrorCodeStrings.CONTAINER_NOT_FOUND) || errorCode.equals(StorageErrorCode.BLOB_NOT_FOUND.toString()) || errorCode.equals(StorageErrorCode.RESOURCE_NOT_FOUND.toString()) || errorCode.equals(StorageErrorCode.CONTAINER_NOT_FOUND.toString()))) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isBlobAlreadyExistsConflict",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isBlobAlreadyExistsConflict(StorageException e)\n{\r\n    if (e.getHttpStatusCode() == HttpURLConnection.HTTP_CONFLICT && StorageErrorCodeStrings.BLOB_ALREADY_EXISTS.equals(e.getErrorCode())) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "logAllLiveStackTraces",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void logAllLiveStackTraces()\n{\r\n    for (Map.Entry<Thread, StackTraceElement[]> entry : Thread.getAllStackTraces().entrySet()) {\r\n        LOG.debug(\"Thread \" + entry.getKey().getName());\r\n        StackTraceElement[] trace = entry.getValue();\r\n        for (int j = 0; j < trace.length; j++) {\r\n            LOG.debug(\"\\tat \" + trace[j]);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateReadArgs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateReadArgs(byte[] buffer, int offset, int length) throws EOFException\n{\r\n    Preconditions.checkArgument(length >= 0, \"length is negative\");\r\n    Preconditions.checkArgument(buffer != null, \"Null buffer\");\r\n    if (buffer.length - offset < length) {\r\n        throw new IndexOutOfBoundsException(FSExceptionMessages.TOO_MANY_BYTES_FOR_DEST_BUFFER + \": request length=\" + length + \", with offset =\" + offset + \"; buffer capacity =\" + (buffer.length - offset));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStorageAccountKey",
  "errType" : [ "IllegalAccessException|IOException", "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getStorageAccountKey(String accountName, Configuration rawConfig) throws KeyProviderException\n{\r\n    String envelope = super.getStorageAccountKey(accountName, rawConfig);\r\n    AbfsConfiguration abfsConfig;\r\n    try {\r\n        abfsConfig = new AbfsConfiguration(rawConfig, accountName);\r\n    } catch (IllegalAccessException | IOException e) {\r\n        throw new KeyProviderException(\"Unable to get key from credential providers.\", e);\r\n    }\r\n    final String command = abfsConfig.get(ConfigurationKeys.AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT);\r\n    if (command == null) {\r\n        throw new KeyProviderException(\"Script path is not specified via fs.azure.shellkeyprovider.script\");\r\n    }\r\n    String[] cmd = command.split(\" \");\r\n    String[] cmdWithEnvelope = Arrays.copyOf(cmd, cmd.length + 1);\r\n    cmdWithEnvelope[cmdWithEnvelope.length - 1] = envelope;\r\n    String decryptedKey = null;\r\n    try {\r\n        decryptedKey = Shell.execCommand(cmdWithEnvelope);\r\n    } catch (IOException ex) {\r\n        throw new KeyProviderException(ex);\r\n    }\r\n    return decryptedKey.trim();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getUriDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getUriDefaultPort()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "addPoint",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addPoint(long value)\n{\r\n    currentPoints.offer(new DataPoint(new Date(), value));\r\n    cleanupOldPoints();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentAverage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getCurrentAverage()\n{\r\n    cleanupOldPoints();\r\n    if (currentPoints.isEmpty()) {\r\n        return 0;\r\n    }\r\n    long sum = 0;\r\n    for (DataPoint current : currentPoints) {\r\n        sum += current.getValue();\r\n    }\r\n    return sum / currentPoints.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "cleanupOldPoints",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanupOldPoints()\n{\r\n    Date cutoffTime = new Date(new Date().getTime() - windowSizeMs);\r\n    while (!currentPoints.isEmpty() && currentPoints.peekFirst().getEventTime().before(cutoffTime)) {\r\n        currentPoints.removeFirst();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "isNearExpiry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isNearExpiry(OffsetDateTime expiry, long minExpiryInSeconds)\n{\r\n    if (expiry == OffsetDateTime.MIN) {\r\n        return true;\r\n    }\r\n    OffsetDateTime utcNow = OffsetDateTime.now(ZoneOffset.UTC);\r\n    return utcNow.until(expiry, SECONDS) <= minExpiryInSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "getExpiry",
  "errType" : [ "UnsupportedEncodingException", "DateTimeParseException", "UnsupportedEncodingException", "DateTimeParseException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "OffsetDateTime getExpiry(String token)\n{\r\n    if (token == null) {\r\n        return OffsetDateTime.MIN;\r\n    }\r\n    String signedExpiry = \"se=\";\r\n    int signedExpiryLen = 3;\r\n    int start = token.indexOf(signedExpiry);\r\n    if (start == -1) {\r\n        return OffsetDateTime.MIN;\r\n    }\r\n    start += signedExpiryLen;\r\n    int end = token.indexOf(\"&\", start);\r\n    String seValue = (end == -1) ? token.substring(start) : token.substring(start, end);\r\n    try {\r\n        seValue = URLDecoder.decode(seValue, \"utf-8\");\r\n    } catch (UnsupportedEncodingException ex) {\r\n        LOG.error(\"Error decoding se query parameter ({}) from SAS.\", seValue, ex);\r\n        return OffsetDateTime.MIN;\r\n    }\r\n    OffsetDateTime seDate = OffsetDateTime.MIN;\r\n    try {\r\n        seDate = OffsetDateTime.parse(seValue, DateTimeFormatter.ISO_DATE_TIME);\r\n    } catch (DateTimeParseException ex) {\r\n        LOG.error(\"Error parsing se query parameter ({}) from SAS.\", seValue, ex);\r\n    }\r\n    String signedKeyExpiry = \"ske=\";\r\n    int signedKeyExpiryLen = 4;\r\n    start = token.indexOf(signedKeyExpiry);\r\n    if (start == -1) {\r\n        return seDate;\r\n    }\r\n    start += signedKeyExpiryLen;\r\n    end = token.indexOf(\"&\", start);\r\n    String skeValue = (end == -1) ? token.substring(start) : token.substring(start, end);\r\n    try {\r\n        skeValue = URLDecoder.decode(skeValue, \"utf-8\");\r\n    } catch (UnsupportedEncodingException ex) {\r\n        LOG.error(\"Error decoding ske query parameter ({}) from SAS.\", skeValue, ex);\r\n        return OffsetDateTime.MIN;\r\n    }\r\n    OffsetDateTime skeDate = OffsetDateTime.MIN;\r\n    try {\r\n        skeDate = OffsetDateTime.parse(skeValue, DateTimeFormatter.ISO_DATE_TIME);\r\n    } catch (DateTimeParseException ex) {\r\n        LOG.error(\"Error parsing ske query parameter ({}) from SAS.\", skeValue, ex);\r\n        return OffsetDateTime.MIN;\r\n    }\r\n    return skeDate.isBefore(seDate) ? skeDate : seDate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "update",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void update(String token)\n{\r\n    if (token == sasToken) {\r\n        return;\r\n    }\r\n    OffsetDateTime newExpiry = getExpiry(token);\r\n    boolean isInvalid = isNearExpiry(newExpiry, minExpirationInSeconds);\r\n    synchronized (this) {\r\n        if (isInvalid) {\r\n            sasToken = null;\r\n            sasExpiry = OffsetDateTime.MIN;\r\n        } else {\r\n            sasToken = token;\r\n            sasExpiry = newExpiry;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String get()\n{\r\n    if (sasToken == null) {\r\n        return null;\r\n    }\r\n    String token;\r\n    OffsetDateTime exp;\r\n    synchronized (this) {\r\n        token = sasToken;\r\n        exp = sasExpiry;\r\n    }\r\n    boolean isInvalid = isNearExpiry(exp, minExpirationInSeconds);\r\n    return isInvalid ? null : token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setForTesting(String token, OffsetDateTime expiry)\n{\r\n    synchronized (this) {\r\n        sasToken = token;\r\n        sasExpiry = expiry;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "acquireLease",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void acquireLease(RetryPolicy retryPolicy, int numRetries, int retryInterval, long delay, TracingContext tracingContext) throws LeaseException\n{\r\n    LOG.debug(\"Attempting to acquire lease on {}, retry {}\", path, numRetries);\r\n    if (future != null && !future.isDone()) {\r\n        throw new LeaseException(ERR_LEASE_FUTURE_EXISTS);\r\n    }\r\n    future = client.schedule(() -> client.acquireLease(path, INFINITE_LEASE_DURATION, tracingContext), delay, TimeUnit.SECONDS);\r\n    client.addCallback(future, new FutureCallback<AbfsRestOperation>() {\r\n\r\n        @Override\r\n        public void onSuccess(@Nullable AbfsRestOperation op) {\r\n            leaseID = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_LEASE_ID);\r\n            LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\r\n        }\r\n\r\n        @Override\r\n        public void onFailure(Throwable throwable) {\r\n            try {\r\n                if (RetryPolicy.RetryAction.RetryDecision.RETRY == retryPolicy.shouldRetry(null, numRetries, 0, true).action) {\r\n                    LOG.debug(\"Failed to acquire lease on {}, retrying: {}\", path, throwable);\r\n                    acquireRetryCount++;\r\n                    acquireLease(retryPolicy, numRetries + 1, retryInterval, retryInterval, tracingContext);\r\n                } else {\r\n                    exception = throwable;\r\n                }\r\n            } catch (Exception e) {\r\n                exception = throwable;\r\n            }\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "free",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void free()\n{\r\n    if (leaseFreed) {\r\n        return;\r\n    }\r\n    try {\r\n        LOG.debug(\"Freeing lease: path {}, lease id {}\", path, leaseID);\r\n        if (future != null && !future.isDone()) {\r\n            future.cancel(true);\r\n        }\r\n        TracingContext tracingContext = new TracingContext(this.tracingContext);\r\n        tracingContext.setOperation(FSOperationType.RELEASE_LEASE);\r\n        client.releaseLease(path, leaseID, tracingContext);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception when trying to release lease {} on {}. Lease will need to be broken: {}\", leaseID, path, e.getMessage());\r\n    } finally {\r\n        leaseFreed = true;\r\n        LOG.debug(\"Freed lease {} on {}\", leaseID, path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isFreed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFreed()\n{\r\n    return leaseFreed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLeaseID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLeaseID()\n{\r\n    return leaseID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAcquireRetryCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getAcquireRetryCount()\n{\r\n    return acquireRetryCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTracingContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TracingContext getTracingContext()\n{\r\n    return tracingContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setTimeoutInMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimeoutInMs(int timeoutInMs)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setRetryPolicyFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRetryPolicyFactory(final RetryPolicyFactory retryPolicyFactory)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void createBlobClient(CloudStorageAccount account)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void createBlobClient(URI baseUri)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void createBlobClient(URI baseUri, StorageCredentials credentials)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StorageCredentials getCredentials()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerReference",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CloudBlobContainerWrapper getContainerReference(String name) throws URISyntaxException, StorageException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "containsAbfsUrl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean containsAbfsUrl(final String string)\n{\r\n    if (string == null || string.isEmpty()) {\r\n        return false;\r\n    }\r\n    return ABFS_URI_PATTERN.matcher(string).matches();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "extractAccountNameFromHostName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String extractAccountNameFromHostName(final String hostName)\n{\r\n    if (hostName == null || hostName.isEmpty()) {\r\n        return null;\r\n    }\r\n    if (!containsAbfsUrl(hostName)) {\r\n        return null;\r\n    }\r\n    String[] splitByDot = hostName.split(\"\\\\.\");\r\n    if (splitByDot.length == 0) {\r\n        return null;\r\n    }\r\n    return splitByDot[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "generateUniqueTestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateUniqueTestPath()\n{\r\n    String testUniqueForkId = System.getProperty(\"test.unique.fork.id\");\r\n    return testUniqueForkId == null ? \"/test\" : \"/\" + testUniqueForkId + \"/test\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "maskUrlQueryParameters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String maskUrlQueryParameters(List<NameValuePair> keyValueList, Set<String> queryParamsForFullMask, Set<String> queryParamsForPartialMask)\n{\r\n    return maskUrlQueryParameters(keyValueList, queryParamsForFullMask, queryParamsForPartialMask, DEFAULT_QUERY_STRINGBUILDER_CAPACITY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "maskUrlQueryParameters",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "String maskUrlQueryParameters(List<NameValuePair> keyValueList, Set<String> queryParamsForFullMask, Set<String> queryParamsForPartialMask, int queryLen)\n{\r\n    StringBuilder maskedUrl = new StringBuilder(queryLen);\r\n    for (NameValuePair keyValuePair : keyValueList) {\r\n        String key = keyValuePair.getName();\r\n        if (key.isEmpty()) {\r\n            throw new IllegalArgumentException(\"Query param key should not be empty\");\r\n        }\r\n        String value = keyValuePair.getValue();\r\n        maskedUrl.append(key);\r\n        maskedUrl.append(EQUAL);\r\n        if (value != null && !value.isEmpty()) {\r\n            if (queryParamsForFullMask.contains(key)) {\r\n                maskedUrl.append(FULL_MASK);\r\n            } else if (queryParamsForPartialMask.contains(key)) {\r\n                int valueLen = value.length();\r\n                int maskedLen = valueLen > PARTIAL_MASK_VISIBLE_LEN ? PARTIAL_MASK_VISIBLE_LEN : valueLen / 2;\r\n                maskedUrl.append(value, 0, valueLen - maskedLen);\r\n                maskedUrl.append(StringUtils.repeat(CHAR_MASK, maskedLen));\r\n            } else {\r\n                maskedUrl.append(value);\r\n            }\r\n        }\r\n        maskedUrl.append(AND_MARK);\r\n    }\r\n    maskedUrl.deleteCharAt(maskedUrl.length() - 1);\r\n    return maskedUrl.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "encodedUrlStr",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String encodedUrlStr(String url)\n{\r\n    try {\r\n        return URLEncoder.encode(url, \"UTF-8\");\r\n    } catch (UnsupportedEncodingException e) {\r\n        return \"https%3A%2F%2Ffailed%2Fto%2Fencode%2Furl\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "getMaskedUrl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getMaskedUrl(URL url)\n{\r\n    String queryString = url.getQuery();\r\n    if (queryString == null) {\r\n        return url.toString();\r\n    }\r\n    List<NameValuePair> queryKeyValueList = URLEncodedUtils.parse(queryString, StandardCharsets.UTF_8);\r\n    String maskedQueryString = maskUrlQueryParameters(queryKeyValueList, FULL_MASK_PARAM_KEYS, PARTIAL_MASK_PARAM_KEYS, queryString.length());\r\n    return url.toString().replace(queryString, maskedQueryString);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "blockUploaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void blockUploaded(Date startDate, Date endDate, long length)\n{\r\n    synchronized (blocksWrittenLock) {\r\n        allBlocksWritten.add(new BlockTransferWindow(startDate, endDate, length));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "blockDownloaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void blockDownloaded(Date startDate, Date endDate, long length)\n{\r\n    synchronized (blocksReadLock) {\r\n        allBlocksRead.add(new BlockTransferWindow(startDate, endDate, length));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "createNewToProcessQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ArrayList<BlockTransferWindow> createNewToProcessQueue()\n{\r\n    return new ArrayList<BlockTransferWindow>(PROCESS_QUEUE_INITIAL_CAPACITY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "updateBytesTransferred",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateBytesTransferred(boolean updateWrite, long bytes)\n{\r\n    if (updateWrite) {\r\n        instrumentation.updateBytesWrittenInLastSecond(bytes);\r\n    } else {\r\n        instrumentation.updateBytesReadInLastSecond(bytes);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "updateBytesTransferRate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateBytesTransferRate(boolean updateWrite, long bytesPerSecond)\n{\r\n    if (updateWrite) {\r\n        instrumentation.currentUploadBytesPerSecond(bytesPerSecond);\r\n    } else {\r\n        instrumentation.currentDownloadBytesPerSecond(bytesPerSecond);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "suppressAutoUpdate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void suppressAutoUpdate()\n{\r\n    suppressAutoUpdate = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "resumeAutoUpdate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resumeAutoUpdate()\n{\r\n    suppressAutoUpdate = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "triggerUpdate",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void triggerUpdate(boolean updateWrite)\n{\r\n    ArrayList<BlockTransferWindow> toProcess = null;\r\n    synchronized (updateWrite ? blocksWrittenLock : blocksReadLock) {\r\n        if (updateWrite && !allBlocksWritten.isEmpty()) {\r\n            toProcess = allBlocksWritten;\r\n            allBlocksWritten = createNewToProcessQueue();\r\n        } else if (!updateWrite && !allBlocksRead.isEmpty()) {\r\n            toProcess = allBlocksRead;\r\n            allBlocksRead = createNewToProcessQueue();\r\n        }\r\n    }\r\n    if (toProcess == null) {\r\n        updateBytesTransferred(updateWrite, 0);\r\n        updateBytesTransferRate(updateWrite, 0);\r\n        return;\r\n    }\r\n    long cutoffTime = new Date().getTime() - windowSizeMs;\r\n    long maxSingleBlockTransferRate = 0;\r\n    long bytesInLastSecond = 0;\r\n    for (BlockTransferWindow currentWindow : toProcess) {\r\n        long windowDuration = currentWindow.getEndDate().getTime() - currentWindow.getStartDate().getTime();\r\n        if (windowDuration == 0) {\r\n            windowDuration = 1;\r\n        }\r\n        if (currentWindow.getStartDate().getTime() > cutoffTime) {\r\n            bytesInLastSecond += currentWindow.bytesTransferred;\r\n        } else if (currentWindow.getEndDate().getTime() > cutoffTime) {\r\n            long adjustedBytes = (currentWindow.getBytesTransferred() * (currentWindow.getEndDate().getTime() - cutoffTime)) / windowDuration;\r\n            bytesInLastSecond += adjustedBytes;\r\n        }\r\n        long currentBlockTransferRate = (currentWindow.getBytesTransferred() * 1000) / windowDuration;\r\n        maxSingleBlockTransferRate = Math.max(maxSingleBlockTransferRate, currentBlockTransferRate);\r\n    }\r\n    updateBytesTransferred(updateWrite, bytesInLastSecond);\r\n    long aggregateTransferRate = bytesInLastSecond;\r\n    long maxObservedTransferRate = Math.max(aggregateTransferRate, maxSingleBlockTransferRate);\r\n    updateBytesTransferRate(updateWrite, maxObservedTransferRate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "close",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close()\n{\r\n    if (uploadBandwidthUpdater != null) {\r\n        uploadBandwidthUpdater.interrupt();\r\n        try {\r\n            uploadBandwidthUpdater.join();\r\n        } catch (InterruptedException e) {\r\n        }\r\n        uploadBandwidthUpdater = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAclBit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getAclBit()\n{\r\n    return aclBit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj instanceof FsPermission) {\r\n        FsPermission that = (FsPermission) obj;\r\n        return this.getUserAction() == that.getUserAction() && this.getGroupAction() == that.getGroupAction() && this.getOtherAction() == that.getOtherAction() && this.getStickyBit() == that.getStickyBit();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "valueOf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AbfsPermission valueOf(final String abfsSymbolicPermission)\n{\r\n    if (abfsSymbolicPermission == null) {\r\n        return null;\r\n    }\r\n    final boolean isExtendedAcl = abfsSymbolicPermission.charAt(abfsSymbolicPermission.length() - 1) == '+';\r\n    final String abfsRawSymbolicPermission = isExtendedAcl ? abfsSymbolicPermission.substring(0, abfsSymbolicPermission.length() - 1) : abfsSymbolicPermission;\r\n    int n = 0;\r\n    for (int i = 0; i < abfsRawSymbolicPermission.length(); i++) {\r\n        n = n << 1;\r\n        char c = abfsRawSymbolicPermission.charAt(i);\r\n        n += (c == '-' || c == 'T' || c == 'S') ? 0 : 1;\r\n    }\r\n    if (abfsRawSymbolicPermission.charAt(abfsRawSymbolicPermission.length() - 1) == 't' || abfsRawSymbolicPermission.charAt(abfsRawSymbolicPermission.length() - 1) == 'T') {\r\n        n += STICKY_BIT_OCTAL_VALUE;\r\n    }\r\n    return new AbfsPermission((short) n, isExtendedAcl);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isExtendedAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isExtendedAcl(final String abfsSymbolicPermission)\n{\r\n    if (abfsSymbolicPermission == null) {\r\n        return false;\r\n    }\r\n    return abfsSymbolicPermission.charAt(abfsSymbolicPermission.length() - 1) == '+';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return toShort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "refreshToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureADToken refreshToken() throws IOException\n{\r\n    LOG.debug(\"AADToken: refreshing client-credential based token\");\r\n    return AzureADAuthenticator.getTokenUsingClientCreds(authEndpoint, clientId, clientSecret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\constants",
  "methodName" : "accountProperty",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String accountProperty(String property, String account)\n{\r\n    return property + \".\" + account;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStream getStream()\n{\r\n    return stream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStream(AbfsInputStream stream)\n{\r\n    this.stream = stream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setTracingContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTracingContext(TracingContext tracingContext)\n{\r\n    this.tracingContext = tracingContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTracingContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TracingContext getTracingContext()\n{\r\n    return tracingContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOffset()\n{\r\n    return offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOffset(long offset)\n{\r\n    this.offset = offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLength()\n{\r\n    return length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLength(int length)\n{\r\n    this.length = length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRequestedLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRequestedLength()\n{\r\n    return requestedLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setRequestedLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRequestedLength(int requestedLength)\n{\r\n    this.requestedLength = requestedLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getBuffer()\n{\r\n    return buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBuffer(byte[] buffer)\n{\r\n    this.buffer = buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBufferindex",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBufferindex()\n{\r\n    return bufferindex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setBufferindex",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBufferindex(int bufferindex)\n{\r\n    this.bufferindex = bufferindex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getErrException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOException getErrException()\n{\r\n    return errException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setErrException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setErrException(final IOException errException)\n{\r\n    this.errException = errException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReadBufferStatus getStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(ReadBufferStatus status)\n{\r\n    this.status = status;\r\n    if (status == READ_FAILED) {\r\n        bufferindex = -1;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLatch",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CountDownLatch getLatch()\n{\r\n    return latch;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setLatch",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLatch(CountDownLatch latch)\n{\r\n    this.latch = latch;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTimeStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimeStamp()\n{\r\n    return timeStamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setTimeStamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimeStamp(long timeStamp)\n{\r\n    this.timeStamp = timeStamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isFirstByteConsumed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFirstByteConsumed()\n{\r\n    return isFirstByteConsumed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setFirstByteConsumed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFirstByteConsumed(boolean isFirstByteConsumed)\n{\r\n    this.isFirstByteConsumed = isFirstByteConsumed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isLastByteConsumed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLastByteConsumed()\n{\r\n    return isLastByteConsumed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setLastByteConsumed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastByteConsumed(boolean isLastByteConsumed)\n{\r\n    this.isLastByteConsumed = isLastByteConsumed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isAnyByteConsumed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isAnyByteConsumed()\n{\r\n    return isAnyByteConsumed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setAnyByteConsumed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAnyByteConsumed(boolean isAnyByteConsumed)\n{\r\n    this.isAnyByteConsumed = isAnyByteConsumed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "push",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void push(E element)\n{\r\n    AzureLinkedNode<E> newNode = new AzureLinkedNode<E>(element, top);\r\n    top = newNode;\r\n    count++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "pop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "E pop() throws Exception\n{\r\n    if (isEmpty()) {\r\n        throw new Exception(\"AzureStackEmpty\");\r\n    }\r\n    E element = top.getElement();\r\n    top = top.getNext();\r\n    count--;\r\n    return element;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "peek",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "E peek() throws Exception\n{\r\n    if (isEmpty()) {\r\n        throw new Exception(\"AzureStackEmpty\");\r\n    }\r\n    E element = top.getElement();\r\n    return element;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isEmpty()\n{\r\n    if (0 == size()) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int size()\n{\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    AzureLinkedNode<E> current = top;\r\n    for (int i = 0; i < size(); i++) {\r\n        E element = current.getElement();\r\n        sb.append(element.toString());\r\n        current = current.getNext();\r\n        if (size() - 1 > i) {\r\n            sb.append(\", \");\r\n        }\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "hook",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hook(OperationContext operationContext, AzureFileSystemInstrumentation instrumentation)\n{\r\n    ErrorMetricUpdater listener = new ErrorMetricUpdater(operationContext, instrumentation);\r\n    operationContext.getResponseReceivedEventHandler().addListener(listener);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "eventOccurred",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void eventOccurred(ResponseReceivedEvent eventArg)\n{\r\n    RequestResult currentResult = operationContext.getLastResult();\r\n    int statusCode = currentResult.getStatusCode();\r\n    if (statusCode >= HTTP_BAD_REQUEST && statusCode < HTTP_INTERNAL_ERROR && statusCode != HTTP_NOT_FOUND) {\r\n        instrumentation.clientErrorEncountered();\r\n    } else if (statusCode >= HTTP_INTERNAL_ERROR) {\r\n        instrumentation.serverErrorEncountered();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSHA256Hash",
  "errType" : [ "NoSuchAlgorithmException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] getSHA256Hash(String key) throws IOException\n{\r\n    try {\r\n        final MessageDigest digester = MessageDigest.getInstance(\"SHA-256\");\r\n        return digester.digest(key.getBytes(StandardCharsets.UTF_8));\r\n    } catch (NoSuchAlgorithmException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBase64EncodedString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBase64EncodedString(String key)\n{\r\n    return getBase64EncodedString(key.getBytes(StandardCharsets.UTF_8));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBase64EncodedString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBase64EncodedString(byte[] bytes)\n{\r\n    return Base64.getEncoder().encodeToString(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (tokenProvider instanceof Closeable) {\r\n        IOUtils.cleanupWithLogger(LOG, (Closeable) tokenProvider);\r\n    }\r\n    HadoopExecutors.shutdown(executorService, LOG, 0, TimeUnit.SECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFileSystem()\n{\r\n    return filesystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsPerfTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfTracker getAbfsPerfTracker()\n{\r\n    return abfsPerfTracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExponentialRetryPolicy getRetryPolicy()\n{\r\n    return retryPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSharedKeyCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SharedKeyCredentials getSharedKeyCredentials()\n{\r\n    return sharedKeyCredentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createDefaultHeaders",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<AbfsHttpHeader> createDefaultHeaders()\n{\r\n    final List<AbfsHttpHeader> requestHeaders = new ArrayList<AbfsHttpHeader>();\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_VERSION, xMsVersion));\r\n    requestHeaders.add(new AbfsHttpHeader(ACCEPT, APPLICATION_JSON + COMMA + SINGLE_WHITE_SPACE + APPLICATION_OCTET_STREAM));\r\n    requestHeaders.add(new AbfsHttpHeader(ACCEPT_CHARSET, UTF_8));\r\n    requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, EMPTY_STRING));\r\n    requestHeaders.add(new AbfsHttpHeader(USER_AGENT, userAgent));\r\n    return requestHeaders;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "addCustomerProvidedKeyHeaders",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addCustomerProvidedKeyHeaders(final List<AbfsHttpHeader> requestHeaders)\n{\r\n    if (clientProvidedEncryptionKey != null) {\r\n        requestHeaders.add(new AbfsHttpHeader(X_MS_ENCRYPTION_KEY, clientProvidedEncryptionKey));\r\n        requestHeaders.add(new AbfsHttpHeader(X_MS_ENCRYPTION_KEY_SHA256, clientProvidedEncryptionKeySHA));\r\n        requestHeaders.add(new AbfsHttpHeader(X_MS_ENCRYPTION_ALGORITHM, SERVER_SIDE_ENCRYPTION_ALGORITHM));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createDefaultUriQueryBuilder",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsUriQueryBuilder createDefaultUriQueryBuilder()\n{\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = new AbfsUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_TIMEOUT, DEFAULT_TIMEOUT);\r\n    return abfsUriQueryBuilder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createFilesystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AbfsRestOperation createFilesystem(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = new AbfsUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, FILESYSTEM);\r\n    final URL url = createRequestUrl(abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.CreateFileSystem, this, HTTP_METHOD_PUT, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setFilesystemProperties",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AbfsRestOperation setFilesystemProperties(final String properties, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_PROPERTIES, properties));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, FILESYSTEM);\r\n    final URL url = createRequestUrl(abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.SetFileSystemProperties, this, HTTP_METHOD_PUT, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "listPath",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AbfsRestOperation listPath(final String relativePath, final boolean recursive, final int listMaxResults, final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, FILESYSTEM);\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_DIRECTORY, getDirectoryQueryParameter(relativePath));\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RECURSIVE, String.valueOf(recursive));\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_MAXRESULTS, String.valueOf(listMaxResults));\r\n    abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_UPN, String.valueOf(abfsConfiguration.isUpnUsed()));\r\n    appendSASTokenToQuery(relativePath, SASTokenProvider.LIST_OPERATION, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.ListPaths, this, HTTP_METHOD_GET, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFilesystemProperties",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AbfsRestOperation getFilesystemProperties(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, FILESYSTEM);\r\n    final URL url = createRequestUrl(abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.GetFileSystemProperties, this, HTTP_METHOD_HEAD, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "deleteFilesystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AbfsRestOperation deleteFilesystem(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, FILESYSTEM);\r\n    final URL url = createRequestUrl(abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.DeleteFileSystem, this, HTTP_METHOD_DELETE, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createPath",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "AbfsRestOperation createPath(final String path, final boolean isFile, final boolean overwrite, final String permission, final String umask, final boolean isAppendBlob, final String eTag, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    if (isFile) {\r\n        addCustomerProvidedKeyHeaders(requestHeaders);\r\n    }\r\n    if (!overwrite) {\r\n        requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, AbfsHttpConstants.STAR));\r\n    }\r\n    if (permission != null && !permission.isEmpty()) {\r\n        requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_PERMISSIONS, permission));\r\n    }\r\n    if (umask != null && !umask.isEmpty()) {\r\n        requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_UMASK, umask));\r\n    }\r\n    if (eTag != null && !eTag.isEmpty()) {\r\n        requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.IF_MATCH, eTag));\r\n    }\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RESOURCE, isFile ? FILE : DIRECTORY);\r\n    if (isAppendBlob) {\r\n        abfsUriQueryBuilder.addQuery(QUERY_PARAM_BLOBTYPE, APPEND_BLOB_TYPE);\r\n    }\r\n    String operation = isFile ? SASTokenProvider.CREATE_FILE_OPERATION : SASTokenProvider.CREATE_DIRECTORY_OPERATION;\r\n    appendSASTokenToQuery(path, operation, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.CreatePath, this, HTTP_METHOD_PUT, url, requestHeaders);\r\n    try {\r\n        op.execute(tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        if (!op.hasResult()) {\r\n            throw ex;\r\n        }\r\n        if (!isFile && op.getResult().getStatusCode() == HttpURLConnection.HTTP_CONFLICT) {\r\n            String existingResource = op.getResult().getResponseHeader(X_MS_EXISTING_RESOURCE_TYPE);\r\n            if (existingResource != null && existingResource.equals(DIRECTORY)) {\r\n                return op;\r\n            }\r\n        }\r\n        throw ex;\r\n    }\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "acquireLease",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AbfsRestOperation acquireLease(final String path, int duration, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, ACQUIRE_LEASE_ACTION));\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_DURATION, Integer.toString(duration)));\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_PROPOSED_LEASE_ID, UUID.randomUUID().toString()));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.LeasePath, this, HTTP_METHOD_POST, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "renewLease",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AbfsRestOperation renewLease(final String path, final String leaseId, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, RENEW_LEASE_ACTION));\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.LeasePath, this, HTTP_METHOD_POST, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "releaseLease",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AbfsRestOperation releaseLease(final String path, final String leaseId, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, RELEASE_LEASE_ACTION));\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.LeasePath, this, HTTP_METHOD_POST, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "breakLease",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AbfsRestOperation breakLease(final String path, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ACTION, BREAK_LEASE_ACTION));\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_BREAK_PERIOD, DEFAULT_LEASE_BREAK_PERIOD));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.LeasePath, this, HTTP_METHOD_POST, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "renamePath",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Pair<AbfsRestOperation, Boolean> renamePath(final String source, final String destination, final String continuation, final TracingContext tracingContext, final String sourceEtag) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    String encodedRenameSource = urlEncode(FORWARD_SLASH + this.getFileSystem() + source);\r\n    if (authType == AuthType.SAS) {\r\n        final AbfsUriQueryBuilder srcQueryBuilder = new AbfsUriQueryBuilder();\r\n        appendSASTokenToQuery(source, SASTokenProvider.RENAME_SOURCE_OPERATION, srcQueryBuilder);\r\n        encodedRenameSource += srcQueryBuilder.toString();\r\n    }\r\n    LOG.trace(\"Rename source queryparam added {}\", encodedRenameSource);\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_RENAME_SOURCE, encodedRenameSource));\r\n    requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, STAR));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\r\n    appendSASTokenToQuery(destination, SASTokenProvider.RENAME_DESTINATION_OPERATION, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(destination, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.RenamePath, this, HTTP_METHOD_PUT, url, requestHeaders);\r\n    try {\r\n        op.execute(tracingContext);\r\n        return Pair.of(op, false);\r\n    } catch (AzureBlobFileSystemException e) {\r\n        if (!op.hasResult()) {\r\n            throw e;\r\n        }\r\n        boolean etagCheckSucceeded = renameIdempotencyCheckOp(source, sourceEtag, op, destination, tracingContext);\r\n        if (!etagCheckSucceeded) {\r\n            throw e;\r\n        }\r\n        return Pair.of(op, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "renameIdempotencyCheckOp",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean renameIdempotencyCheckOp(final String source, final String sourceEtag, final AbfsRestOperation op, final String destination, TracingContext tracingContext)\n{\r\n    Preconditions.checkArgument(op.hasResult(), \"Operations has null HTTP response\");\r\n    if ((op.isARetriedRequest()) && (op.getResult().getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) && isNotEmpty(sourceEtag)) {\r\n        LOG.debug(\"rename {} to {} failed, checking etag of destination\", source, destination);\r\n        try {\r\n            final AbfsRestOperation destStatusOp = getPathStatus(destination, false, tracingContext);\r\n            final AbfsHttpOperation result = destStatusOp.getResult();\r\n            return result.getStatusCode() == HttpURLConnection.HTTP_OK && sourceEtag.equals(extractEtagHeader(result));\r\n        } catch (AzureBlobFileSystemException ignored) {\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "append",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "AbfsRestOperation append(final String path, final byte[] buffer, AppendRequestParameters reqParams, final String cachedSasToken, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    addCustomerProvidedKeyHeaders(requestHeaders);\r\n    requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));\r\n    if (reqParams.getLeaseId() != null) {\r\n        requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, reqParams.getLeaseId()));\r\n    }\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, APPEND_ACTION);\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(reqParams.getPosition()));\r\n    if ((reqParams.getMode() == AppendRequestParameters.Mode.FLUSH_MODE) || (reqParams.getMode() == AppendRequestParameters.Mode.FLUSH_CLOSE_MODE)) {\r\n        abfsUriQueryBuilder.addQuery(QUERY_PARAM_FLUSH, TRUE);\r\n        if (reqParams.getMode() == AppendRequestParameters.Mode.FLUSH_CLOSE_MODE) {\r\n            abfsUriQueryBuilder.addQuery(QUERY_PARAM_CLOSE, TRUE);\r\n        }\r\n    }\r\n    String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.WRITE_OPERATION, abfsUriQueryBuilder, cachedSasToken);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.Append, this, HTTP_METHOD_PUT, url, requestHeaders, buffer, reqParams.getoffset(), reqParams.getLength(), sasTokenForReuse);\r\n    try {\r\n        op.execute(tracingContext);\r\n    } catch (AzureBlobFileSystemException e) {\r\n        if (!op.hasResult()) {\r\n            throw e;\r\n        }\r\n        if (reqParams.isAppendBlob() && appendSuccessCheckOp(op, path, (reqParams.getPosition() + reqParams.getLength()), tracingContext)) {\r\n            final AbfsRestOperation successOp = new AbfsRestOperation(AbfsRestOperationType.Append, this, HTTP_METHOD_PUT, url, requestHeaders, buffer, reqParams.getoffset(), reqParams.getLength(), sasTokenForReuse);\r\n            successOp.hardSetResult(HttpURLConnection.HTTP_OK);\r\n            return successOp;\r\n        }\r\n        throw e;\r\n    }\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "appendSuccessCheckOp",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean appendSuccessCheckOp(AbfsRestOperation op, final String path, final long length, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if ((op.isARetriedRequest()) && (op.getResult().getStatusCode() == HttpURLConnection.HTTP_BAD_REQUEST)) {\r\n        final AbfsRestOperation destStatusOp = getPathStatus(path, false, tracingContext);\r\n        if (destStatusOp.getResult().getStatusCode() == HttpURLConnection.HTTP_OK) {\r\n            String fileLength = destStatusOp.getResult().getResponseHeader(HttpHeaderConfigurations.CONTENT_LENGTH);\r\n            if (length <= Long.parseLong(fileLength)) {\r\n                LOG.debug(\"Returning success response from append blob idempotency code\");\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "flush",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "AbfsRestOperation flush(final String path, final long position, boolean retainUncommittedData, boolean isClose, final String cachedSasToken, final String leaseId, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    addCustomerProvidedKeyHeaders(requestHeaders);\r\n    requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));\r\n    if (leaseId != null) {\r\n        requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\r\n    }\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, FLUSH_ACTION);\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_POSITION, Long.toString(position));\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RETAIN_UNCOMMITTED_DATA, String.valueOf(retainUncommittedData));\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CLOSE, String.valueOf(isClose));\r\n    String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.WRITE_OPERATION, abfsUriQueryBuilder, cachedSasToken);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.Flush, this, HTTP_METHOD_PUT, url, requestHeaders, sasTokenForReuse);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setPathProperties",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "AbfsRestOperation setPathProperties(final String path, final String properties, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    addCustomerProvidedKeyHeaders(requestHeaders);\r\n    requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));\r\n    requestHeaders.add(new AbfsHttpHeader(X_MS_PROPERTIES, properties));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, SET_PROPERTIES_ACTION);\r\n    appendSASTokenToQuery(path, SASTokenProvider.SET_PROPERTIES_OPERATION, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.SetPathProperties, this, HTTP_METHOD_PUT, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getPathStatus",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "AbfsRestOperation getPathStatus(final String path, final boolean includeProperties, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    String operation = SASTokenProvider.GET_PROPERTIES_OPERATION;\r\n    if (!includeProperties) {\r\n        abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_ACTION, AbfsHttpConstants.GET_STATUS);\r\n        operation = SASTokenProvider.GET_STATUS_OPERATION;\r\n    } else {\r\n        addCustomerProvidedKeyHeaders(requestHeaders);\r\n    }\r\n    abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_UPN, String.valueOf(abfsConfiguration.isUpnUsed()));\r\n    appendSASTokenToQuery(path, operation, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.GetPathStatus, this, HTTP_METHOD_HEAD, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "AbfsRestOperation read(final String path, final long position, final byte[] buffer, final int bufferOffset, final int bufferLength, final String eTag, String cachedSasToken, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    addCustomerProvidedKeyHeaders(requestHeaders);\r\n    requestHeaders.add(new AbfsHttpHeader(RANGE, String.format(\"bytes=%d-%d\", position, position + bufferLength - 1)));\r\n    requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    String sasTokenForReuse = appendSASTokenToQuery(path, SASTokenProvider.READ_OPERATION, abfsUriQueryBuilder, cachedSasToken);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.ReadFile, this, HTTP_METHOD_GET, url, requestHeaders, buffer, bufferOffset, bufferLength, sasTokenForReuse);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "deletePath",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AbfsRestOperation deletePath(final String path, final boolean recursive, final String continuation, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_RECURSIVE, String.valueOf(recursive));\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\r\n    String operation = recursive ? SASTokenProvider.DELETE_RECURSIVE_OPERATION : SASTokenProvider.DELETE_OPERATION;\r\n    appendSASTokenToQuery(path, operation, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.DeletePath, this, HTTP_METHOD_DELETE, url, requestHeaders);\r\n    try {\r\n        op.execute(tracingContext);\r\n    } catch (AzureBlobFileSystemException e) {\r\n        if (!op.hasResult()) {\r\n            throw e;\r\n        }\r\n        final AbfsRestOperation idempotencyOp = deleteIdempotencyCheckOp(op);\r\n        if (idempotencyOp.getResult().getStatusCode() == op.getResult().getStatusCode()) {\r\n            throw e;\r\n        } else {\r\n            return idempotencyOp;\r\n        }\r\n    }\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "deleteIdempotencyCheckOp",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AbfsRestOperation deleteIdempotencyCheckOp(final AbfsRestOperation op)\n{\r\n    Preconditions.checkArgument(op.hasResult(), \"Operations has null HTTP response\");\r\n    if ((op.isARetriedRequest()) && (op.getResult().getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) && DEFAULT_DELETE_CONSIDERED_IDEMPOTENT) {\r\n        final AbfsRestOperation successOp = new AbfsRestOperation(AbfsRestOperationType.DeletePath, this, HTTP_METHOD_DELETE, op.getUrl(), op.getRequestHeaders());\r\n        successOp.hardSetResult(HttpURLConnection.HTTP_OK);\r\n        LOG.debug(\"Returning success response from delete idempotency logic\");\r\n        return successOp;\r\n    }\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AbfsRestOperation setOwner(final String path, final String owner, final String group, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));\r\n    if (owner != null && !owner.isEmpty()) {\r\n        requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_OWNER, owner));\r\n    }\r\n    if (group != null && !group.isEmpty()) {\r\n        requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_GROUP, group));\r\n    }\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_ACTION, AbfsHttpConstants.SET_ACCESS_CONTROL);\r\n    appendSASTokenToQuery(path, SASTokenProvider.SET_OWNER_OPERATION, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.SetOwner, this, AbfsHttpConstants.HTTP_METHOD_PUT, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "AbfsRestOperation setPermission(final String path, final String permission, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));\r\n    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_PERMISSIONS, permission));\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_ACTION, AbfsHttpConstants.SET_ACCESS_CONTROL);\r\n    appendSASTokenToQuery(path, SASTokenProvider.SET_PERMISSION_OPERATION, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.SetPermissions, this, AbfsHttpConstants.HTTP_METHOD_PUT, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsRestOperation setAcl(final String path, final String aclSpecString, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    return setAcl(path, aclSpecString, AbfsHttpConstants.EMPTY_STRING, tracingContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "AbfsRestOperation setAcl(final String path, final String aclSpecString, final String eTag, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    requestHeaders.add(new AbfsHttpHeader(X_HTTP_METHOD_OVERRIDE, HTTP_METHOD_PATCH));\r\n    requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_ACL, aclSpecString));\r\n    if (eTag != null && !eTag.isEmpty()) {\r\n        requestHeaders.add(new AbfsHttpHeader(HttpHeaderConfigurations.IF_MATCH, eTag));\r\n    }\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_ACTION, AbfsHttpConstants.SET_ACCESS_CONTROL);\r\n    appendSASTokenToQuery(path, SASTokenProvider.SET_ACL_OPERATION, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.SetAcl, this, AbfsHttpConstants.HTTP_METHOD_PUT, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsRestOperation getAclStatus(final String path, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    return getAclStatus(path, abfsConfiguration.isUpnUsed(), tracingContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AbfsRestOperation getAclStatus(final String path, final boolean useUPN, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\r\n    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_ACTION, AbfsHttpConstants.GET_ACCESS_CONTROL);\r\n    abfsUriQueryBuilder.addQuery(HttpQueryParams.QUERY_PARAM_UPN, String.valueOf(useUPN));\r\n    appendSASTokenToQuery(path, SASTokenProvider.GET_ACL_OPERATION, abfsUriQueryBuilder);\r\n    final URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    final AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.GetAcl, this, AbfsHttpConstants.HTTP_METHOD_HEAD, url, requestHeaders);\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AbfsRestOperation checkAccess(String path, String rwx, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\r\n    abfsUriQueryBuilder.addQuery(QUERY_PARAM_ACTION, CHECK_ACCESS);\r\n    abfsUriQueryBuilder.addQuery(QUERY_FS_ACTION, rwx);\r\n    appendSASTokenToQuery(path, SASTokenProvider.CHECK_ACCESS_OPERATION, abfsUriQueryBuilder);\r\n    URL url = createRequestUrl(path, abfsUriQueryBuilder.toString());\r\n    AbfsRestOperation op = new AbfsRestOperation(AbfsRestOperationType.CheckAccess, this, AbfsHttpConstants.HTTP_METHOD_HEAD, url, createDefaultHeaders());\r\n    op.execute(tracingContext);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getDirectoryQueryParameter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getDirectoryQueryParameter(final String path)\n{\r\n    String directory = path;\r\n    if (Strings.isNullOrEmpty(directory)) {\r\n        directory = AbfsHttpConstants.EMPTY_STRING;\r\n    } else if (directory.charAt(0) == '/') {\r\n        directory = directory.substring(1);\r\n    }\r\n    return directory;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "appendSASTokenToQuery",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String appendSASTokenToQuery(String path, String operation, AbfsUriQueryBuilder queryBuilder) throws SASTokenProviderException\n{\r\n    return appendSASTokenToQuery(path, operation, queryBuilder, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "appendSASTokenToQuery",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String appendSASTokenToQuery(String path, String operation, AbfsUriQueryBuilder queryBuilder, String cachedSasToken) throws SASTokenProviderException\n{\r\n    String sasToken = null;\r\n    if (this.authType == AuthType.SAS) {\r\n        try {\r\n            LOG.trace(\"Fetch SAS token for {} on {}\", operation, path);\r\n            if (cachedSasToken == null) {\r\n                sasToken = sasTokenProvider.getSASToken(this.accountName, this.filesystem, path, operation);\r\n                if ((sasToken == null) || sasToken.isEmpty()) {\r\n                    throw new UnsupportedOperationException(\"SASToken received is empty or null\");\r\n                }\r\n            } else {\r\n                sasToken = cachedSasToken;\r\n                LOG.trace(\"Using cached SAS token.\");\r\n            }\r\n            queryBuilder.setSASToken(sasToken);\r\n            LOG.trace(\"SAS token fetch complete for {} on {}\", operation, path);\r\n        } catch (Exception ex) {\r\n            throw new SASTokenProviderException(String.format(\"Failed to acquire a SAS token for %s on %s due to %s\", operation, path, ex.toString()));\r\n        }\r\n    }\r\n    return sasToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createRequestUrl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URL createRequestUrl(final String query) throws AzureBlobFileSystemException\n{\r\n    return createRequestUrl(EMPTY_STRING, query);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createRequestUrl",
  "errType" : [ "AzureBlobFileSystemException", "MalformedURLException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "URL createRequestUrl(final String path, final String query) throws AzureBlobFileSystemException\n{\r\n    final String base = baseUrl.toString();\r\n    String encodedPath = path;\r\n    try {\r\n        encodedPath = urlEncode(path);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        LOG.debug(\"Unexpected error.\", ex);\r\n        throw new InvalidUriException(path);\r\n    }\r\n    final StringBuilder sb = new StringBuilder();\r\n    sb.append(base);\r\n    sb.append(encodedPath);\r\n    sb.append(query);\r\n    final URL url;\r\n    try {\r\n        url = new URL(sb.toString());\r\n    } catch (MalformedURLException ex) {\r\n        throw new InvalidUriException(sb.toString());\r\n    }\r\n    return url;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "urlEncode",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String urlEncode(final String value) throws AzureBlobFileSystemException\n{\r\n    String encodedString;\r\n    try {\r\n        encodedString = URLEncoder.encode(value, UTF_8).replace(PLUS, PLUS_ENCODE).replace(FORWARD_SLASH_ENCODE, FORWARD_SLASH);\r\n    } catch (UnsupportedEncodingException ex) {\r\n        throw new InvalidUriException(value);\r\n    }\r\n    return encodedString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAccessToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAccessToken() throws IOException\n{\r\n    if (tokenProvider != null) {\r\n        return \"Bearer \" + tokenProvider.getToken().getAccessToken();\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAuthType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuthType getAuthType()\n{\r\n    return authType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "initializeUserAgent",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "String initializeUserAgent(final AbfsConfiguration abfsConfiguration, final String sslProviderName)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(APN_VERSION);\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(CLIENT_VERSION);\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(\"(\");\r\n    sb.append(System.getProperty(JAVA_VENDOR).replaceAll(SINGLE_WHITE_SPACE, EMPTY_STRING));\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(\"JavaJRE\");\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(System.getProperty(JAVA_VERSION));\r\n    sb.append(SEMICOLON);\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(System.getProperty(OS_NAME).replaceAll(SINGLE_WHITE_SPACE, EMPTY_STRING));\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(System.getProperty(OS_VERSION));\r\n    sb.append(FORWARD_SLASH);\r\n    sb.append(System.getProperty(OS_ARCH));\r\n    sb.append(SEMICOLON);\r\n    appendIfNotEmpty(sb, sslProviderName, true);\r\n    appendIfNotEmpty(sb, ExtensionHelper.getUserAgentSuffix(tokenProvider, EMPTY_STRING), true);\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(abfsConfiguration.getClusterName());\r\n    sb.append(FORWARD_SLASH);\r\n    sb.append(abfsConfiguration.getClusterType());\r\n    sb.append(\")\");\r\n    appendIfNotEmpty(sb, abfsConfiguration.getCustomUserAgentPrefix(), false);\r\n    return String.format(Locale.ROOT, sb.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "appendIfNotEmpty",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void appendIfNotEmpty(StringBuilder sb, String regEx, boolean shouldAppendSemiColon)\n{\r\n    if (regEx == null || regEx.trim().isEmpty()) {\r\n        return;\r\n    }\r\n    sb.append(SINGLE_WHITE_SPACE);\r\n    sb.append(regEx);\r\n    if (shouldAppendSemiColon) {\r\n        sb.append(SEMICOLON);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBaseUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URL getBaseUrl()\n{\r\n    return baseUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSasTokenProvider",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SASTokenProvider getSasTokenProvider()\n{\r\n    return this.sasTokenProvider;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsCounters getAbfsCounters()\n{\r\n    return abfsCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getNumLeaseThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumLeaseThreads()\n{\r\n    return abfsConfiguration.getNumLeaseThreads();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "schedule",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ListenableScheduledFuture<V> schedule(Callable<V> callable, long delay, TimeUnit timeUnit)\n{\r\n    return executorService.schedule(callable, delay, timeUnit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "submit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ListenableFuture<?> submit(Runnable runnable)\n{\r\n    return executorService.submit(runnable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "addCallback",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCallback(ListenableFuture<V> future, FutureCallback<V> callback)\n{\r\n    Futures.addCallback(future, callback, executorService);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "makeRemoteRequest",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String makeRemoteRequest(final String[] urls, final String path, final List<NameValuePair> queryParams, final String httpMethod) throws IOException\n{\r\n    final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    UserGroupInformation connectUgi = ugi.getRealUser();\r\n    if (connectUgi != null) {\r\n        queryParams.add(new NameValuePair() {\r\n\r\n            @Override\r\n            public String getName() {\r\n                return Constants.DOAS_PARAM;\r\n            }\r\n\r\n            @Override\r\n            public String getValue() {\r\n                return ugi.getShortUserName();\r\n            }\r\n        });\r\n    } else {\r\n        connectUgi = ugi;\r\n    }\r\n    final Token delegationToken = getDelegationToken(ugi);\r\n    if (!alwaysRequiresKerberosAuth && delegationToken != null) {\r\n        final String delegationTokenEncodedUrlString = delegationToken.encodeToUrlString();\r\n        queryParams.add(new NameValuePair() {\r\n\r\n            @Override\r\n            public String getName() {\r\n                return DELEGATION_TOKEN_QUERY_PARAM_NAME;\r\n            }\r\n\r\n            @Override\r\n            public String getValue() {\r\n                return delegationTokenEncodedUrlString;\r\n            }\r\n        });\r\n    }\r\n    if (delegationToken == null) {\r\n        connectUgi.checkTGTAndReloginFromKeytab();\r\n    }\r\n    String s = null;\r\n    try {\r\n        s = connectUgi.doAs(new PrivilegedExceptionAction<String>() {\r\n\r\n            @Override\r\n            public String run() throws Exception {\r\n                return retryableRequest(urls, path, queryParams, httpMethod);\r\n            }\r\n        });\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n        throw new IOException(e.getMessage(), e);\r\n    }\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getHttpRequest",
  "errType" : [ "AuthenticationException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "HttpUriRequest getHttpRequest(String[] urls, String path, List<NameValuePair> queryParams, int urlIndex, String httpMethod, boolean requiresNewAuth) throws URISyntaxException, IOException\n{\r\n    URIBuilder uriBuilder = new URIBuilder(urls[urlIndex]).setPath(path).setParameters(queryParams);\r\n    if (uriBuilder.getHost().equals(\"localhost\")) {\r\n        uriBuilder.setHost(InetAddress.getLocalHost().getCanonicalHostName());\r\n    }\r\n    HttpUriRequest httpUriRequest = null;\r\n    switch(httpMethod) {\r\n        case HttpPut.METHOD_NAME:\r\n            httpUriRequest = new HttpPut(uriBuilder.build());\r\n            break;\r\n        case HttpPost.METHOD_NAME:\r\n            httpUriRequest = new HttpPost(uriBuilder.build());\r\n            break;\r\n        default:\r\n            httpUriRequest = new HttpGet(uriBuilder.build());\r\n            break;\r\n    }\r\n    LOG.debug(\"SecureWasbRemoteCallHelper#getHttpRequest() {}\", uriBuilder.build().toURL());\r\n    if (alwaysRequiresKerberosAuth || delegationToken == null) {\r\n        AuthenticatedURL.Token token = null;\r\n        final Authenticator kerberosAuthenticator = new KerberosDelegationTokenAuthenticator();\r\n        try {\r\n            if (isSpnegoTokenCachingEnabled && !requiresNewAuth && spnegoToken != null && spnegoToken.isTokenValid()) {\r\n                token = spnegoToken.getToken();\r\n            } else {\r\n                token = new AuthenticatedURL.Token();\r\n                kerberosAuthenticator.authenticate(uriBuilder.build().toURL(), token);\r\n                spnegoToken = new SpnegoToken(token);\r\n            }\r\n        } catch (AuthenticationException e) {\r\n            throw new WasbRemoteCallException(Constants.AUTHENTICATION_FAILED_ERROR_MESSAGE, e);\r\n        }\r\n        Validate.isTrue(token.isSet(), \"Authenticated Token is NOT present. The request cannot proceed.\");\r\n        httpUriRequest.setHeader(\"Cookie\", AuthenticatedURL.AUTH_COOKIE + \"=\" + token);\r\n    }\r\n    return httpUriRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Token<?> getDelegationToken(UserGroupInformation userGroupInformation) throws IOException\n{\r\n    if (this.delegationToken == null) {\r\n        Token<?> token = null;\r\n        for (Token iterToken : userGroupInformation.getTokens()) {\r\n            if (iterToken.getKind().equals(WasbDelegationTokenIdentifier.TOKEN_KIND)) {\r\n                token = iterToken;\r\n                LOG.debug(\"{} token found in cache : {}\", WasbDelegationTokenIdentifier.TOKEN_KIND, iterToken);\r\n                break;\r\n            }\r\n        }\r\n        LOG.debug(\"UGI Information: {}\", userGroupInformation.toString());\r\n        if (token != null) {\r\n            LOG.debug(\"Using UGI token: {}\", token);\r\n            setDelegationToken(token);\r\n        }\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Delegation token from cache - {}\", delegationToken != null ? delegationToken.encodeToUrlString() : \"null\");\r\n    }\r\n    return this.delegationToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDelegationToken(final Token<T> token)\n{\r\n    synchronized (this) {\r\n        this.delegationToken = token;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "initializeSingleton",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initializeSingleton()\n{\r\n    if (singleton == null) {\r\n        singleton = new ClientThrottlingIntercept();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hook",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void hook(OperationContext context)\n{\r\n    context.getErrorReceivingResponseEventHandler().addListener(new ErrorReceivingResponseEventHandler());\r\n    context.getSendingRequestEventHandler().addListener(new SendingRequestEventHandler());\r\n    context.getResponseReceivedEventHandler().addListener(new ResponseReceivedEventHandler());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateMetrics",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void updateMetrics(HttpURLConnection conn, RequestResult result)\n{\r\n    BlobOperationDescriptor.OperationType operationType = BlobOperationDescriptor.getOperationType(conn);\r\n    int status = result.getStatusCode();\r\n    long contentLength = 0;\r\n    boolean isFailedOperation = (status < HttpURLConnection.HTTP_OK || status >= java.net.HttpURLConnection.HTTP_INTERNAL_ERROR);\r\n    switch(operationType) {\r\n        case AppendBlock:\r\n        case PutBlock:\r\n        case PutPage:\r\n            contentLength = BlobOperationDescriptor.getContentLengthIfKnown(conn, operationType);\r\n            if (contentLength > 0) {\r\n                singleton.writeThrottler.addBytesTransferred(contentLength, isFailedOperation);\r\n            }\r\n            break;\r\n        case GetBlob:\r\n            contentLength = BlobOperationDescriptor.getContentLengthIfKnown(conn, operationType);\r\n            if (contentLength > 0) {\r\n                singleton.readThrottler.addBytesTransferred(contentLength, isFailedOperation);\r\n            }\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "errorReceivingResponse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void errorReceivingResponse(ErrorReceivingResponseEvent event)\n{\r\n    updateMetrics((HttpURLConnection) event.getConnectionObject(), event.getRequestResult());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "sendingRequest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void sendingRequest(SendingRequestEvent event)\n{\r\n    BlobOperationDescriptor.OperationType operationType = BlobOperationDescriptor.getOperationType((HttpURLConnection) event.getConnectionObject());\r\n    switch(operationType) {\r\n        case GetBlob:\r\n            singleton.readThrottler.suspendIfNecessary();\r\n            break;\r\n        case AppendBlock:\r\n        case PutBlock:\r\n        case PutPage:\r\n            singleton.writeThrottler.suspendIfNecessary();\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "responseReceived",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void responseReceived(ResponseReceivedEvent event)\n{\r\n    updateMetrics((HttpURLConnection) event.getConnectionObject(), event.getRequestResult());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "initialize",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void initialize(URI uri, Configuration configuration) throws IOException\n{\r\n    uri = ensureAuthority(uri, configuration);\r\n    super.initialize(uri, configuration);\r\n    setConf(configuration);\r\n    LOG.debug(\"Initializing AzureBlobFileSystem for {}\", uri);\r\n    this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\r\n    abfsCounters = new AbfsCountersImpl(uri);\r\n    this.blockOutputBuffer = configuration.getTrimmed(DATA_BLOCKS_BUFFER, DATA_BLOCKS_BUFFER_DEFAULT);\r\n    this.blockFactory = DataBlocks.createFactory(FS_AZURE_BLOCK_UPLOAD_BUFFER_DIR, configuration, blockOutputBuffer);\r\n    this.blockOutputActiveBlocks = configuration.getInt(FS_AZURE_BLOCK_UPLOAD_ACTIVE_BLOCKS, BLOCK_UPLOAD_ACTIVE_BLOCKS_DEFAULT);\r\n    if (blockOutputActiveBlocks < 1) {\r\n        blockOutputActiveBlocks = 1;\r\n    }\r\n    AzureBlobFileSystemStore.AzureBlobFileSystemStoreBuilder systemStoreBuilder = new AzureBlobFileSystemStore.AzureBlobFileSystemStoreBuilder().withUri(uri).withSecureScheme(this.isSecureScheme()).withConfiguration(configuration).withAbfsCounters(abfsCounters).withBlockFactory(blockFactory).withBlockOutputActiveBlocks(blockOutputActiveBlocks).build();\r\n    this.abfsStore = new AzureBlobFileSystemStore(systemStoreBuilder);\r\n    LOG.trace(\"AzureBlobFileSystemStore init complete\");\r\n    final AbfsConfiguration abfsConfiguration = abfsStore.getAbfsConfiguration();\r\n    clientCorrelationId = TracingContext.validateClientCorrelationID(abfsConfiguration.getClientCorrelationId());\r\n    tracingHeaderFormat = abfsConfiguration.getTracingHeaderFormat();\r\n    this.setWorkingDirectory(this.getHomeDirectory());\r\n    if (abfsConfiguration.getCreateRemoteFileSystemDuringInitialization()) {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.CREATE_FILESYSTEM, tracingHeaderFormat, listener);\r\n        if (this.tryGetFileStatus(new Path(AbfsHttpConstants.ROOT_PATH), tracingContext) == null) {\r\n            try {\r\n                this.createFileSystem(tracingContext);\r\n            } catch (AzureBlobFileSystemException ex) {\r\n                checkException(null, ex, AzureServiceErrorCode.FILE_SYSTEM_ALREADY_EXISTS);\r\n            }\r\n        }\r\n    }\r\n    LOG.trace(\"Initiate check for delegation token manager\");\r\n    if (UserGroupInformation.isSecurityEnabled()) {\r\n        this.delegationTokenEnabled = abfsConfiguration.isDelegationTokenManagerEnabled();\r\n        if (this.delegationTokenEnabled) {\r\n            LOG.debug(\"Initializing DelegationTokenManager for {}\", uri);\r\n            this.delegationTokenManager = abfsConfiguration.getDelegationTokenManager();\r\n            delegationTokenManager.bind(getUri(), configuration);\r\n            LOG.debug(\"Created DelegationTokenManager {}\", delegationTokenManager);\r\n        }\r\n    }\r\n    AbfsClientThrottlingIntercept.initializeSingleton(abfsConfiguration.isAutoThrottlingEnabled());\r\n    rateLimiting = RateLimitingFactory.create(abfsConfiguration.getRateLimit());\r\n    LOG.debug(\"Initializing AzureBlobFileSystem for {} complete\", uri);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AzureBlobFileSystem{\");\r\n    sb.append(\"uri=\").append(uri);\r\n    sb.append(\", user='\").append(abfsStore.getUser()).append('\\'');\r\n    sb.append(\", primaryUserGroup='\").append(abfsStore.getPrimaryGroup()).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isSecureScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSecureScheme()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return this.uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "registerListener",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void registerListener(Listener listener1)\n{\r\n    listener = listener1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream open(final Path path, final int bufferSize) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.open path: {} bufferSize: {}\", path, bufferSize);\r\n    return open(path, Optional.empty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "open",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataInputStream open(final Path path, final Optional<OpenFileParameters> parameters) throws IOException\n{\r\n    statIncrement(CALL_OPEN);\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.OPEN, tracingHeaderFormat, listener);\r\n        InputStream inputStream = abfsStore.openFileForRead(qualifiedPath, parameters, statistics, tracingContext);\r\n        return new FSDataInputStream(inputStream);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "openFileWithOptions",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CompletableFuture<FSDataInputStream> openFileWithOptions(final Path path, final OpenFileParameters parameters) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.openFileWithOptions path: {}\", path);\r\n    AbstractFSBuilderImpl.rejectUnknownMandatoryKeys(parameters.getMandatoryKeys(), Collections.emptySet(), \"for \" + path);\r\n    return LambdaUtils.eval(new CompletableFuture<>(), () -> open(path, Optional.of(parameters)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "create",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FSDataOutputStream create(final Path f, final FsPermission permission, final boolean overwrite, final int bufferSize, final short replication, final long blockSize, final Progressable progress) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}\", f, permission, overwrite, blockSize);\r\n    statIncrement(CALL_CREATE);\r\n    trailingPeriodCheck(f);\r\n    Path qualifiedPath = makeQualified(f);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.CREATE, overwrite, tracingHeaderFormat, listener);\r\n        OutputStream outputStream = abfsStore.createFile(qualifiedPath, statistics, overwrite, permission == null ? FsPermission.getFileDefault() : permission, FsPermission.getUMask(getConf()), tracingContext);\r\n        statIncrement(FILES_CREATED);\r\n        return new FSDataOutputStream(outputStream, statistics);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(f, ex);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(final Path f, final FsPermission permission, final boolean overwrite, final int bufferSize, final short replication, final long blockSize, final Progressable progress) throws IOException\n{\r\n    statIncrement(CALL_CREATE_NON_RECURSIVE);\r\n    final Path parent = f.getParent();\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.CREATE_NON_RECURSIVE, tracingHeaderFormat, listener);\r\n    final FileStatus parentFileStatus = tryGetFileStatus(parent, tracingContext);\r\n    if (parentFileStatus == null) {\r\n        throw new FileNotFoundException(\"Cannot create file \" + f.getName() + \" because parent folder does not exist.\");\r\n    }\r\n    return create(f, permission, overwrite, bufferSize, replication, blockSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(final Path f, final FsPermission permission, final EnumSet<CreateFlag> flags, final int bufferSize, final short replication, final long blockSize, final Progressable progress) throws IOException\n{\r\n    final EnumSet<CreateFlag> createflags = EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE);\r\n    final boolean overwrite = flags.containsAll(createflags);\r\n    return this.createNonRecursive(f, permission, overwrite, bufferSize, replication, blockSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(final Path f, final boolean overwrite, final int bufferSize, final short replication, final long blockSize, final Progressable progress) throws IOException\n{\r\n    return this.createNonRecursive(f, FsPermission.getFileDefault(), overwrite, bufferSize, replication, blockSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "append",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FSDataOutputStream append(final Path f, final int bufferSize, final Progressable progress) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.append path: {} bufferSize: {}\", f.toString(), bufferSize);\r\n    statIncrement(CALL_APPEND);\r\n    Path qualifiedPath = makeQualified(f);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.APPEND, tracingHeaderFormat, listener);\r\n        OutputStream outputStream = abfsStore.openFileForWrite(qualifiedPath, statistics, false, tracingContext);\r\n        return new FSDataOutputStream(outputStream, statistics);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(f, ex);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "rename",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "boolean rename(final Path src, final Path dst) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.rename src: {} dst: {}\", src, dst);\r\n    statIncrement(CALL_RENAME);\r\n    trailingPeriodCheck(dst);\r\n    Path parentFolder = src.getParent();\r\n    if (parentFolder == null) {\r\n        return false;\r\n    }\r\n    Path qualifiedSrcPath = makeQualified(src);\r\n    Path qualifiedDstPath = makeQualified(dst);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.RENAME, true, tracingHeaderFormat, listener);\r\n    if (makeQualified(parentFolder).equals(qualifiedDstPath)) {\r\n        return tryGetFileStatus(qualifiedSrcPath, tracingContext) != null;\r\n    }\r\n    FileStatus dstFileStatus = null;\r\n    if (qualifiedSrcPath.equals(qualifiedDstPath)) {\r\n        dstFileStatus = tryGetFileStatus(qualifiedDstPath, tracingContext);\r\n        if (dstFileStatus == null) {\r\n            return false;\r\n        }\r\n        return dstFileStatus.isDirectory() ? false : true;\r\n    }\r\n    if (!abfsStore.getIsNamespaceEnabled(tracingContext) && dstFileStatus == null) {\r\n        dstFileStatus = tryGetFileStatus(qualifiedDstPath, tracingContext);\r\n    }\r\n    try {\r\n        String sourceFileName = src.getName();\r\n        Path adjustedDst = dst;\r\n        if (dstFileStatus != null) {\r\n            if (!dstFileStatus.isDirectory()) {\r\n                return qualifiedSrcPath.equals(qualifiedDstPath);\r\n            }\r\n            adjustedDst = new Path(dst, sourceFileName);\r\n        }\r\n        qualifiedDstPath = makeQualified(adjustedDst);\r\n        abfsStore.rename(qualifiedSrcPath, qualifiedDstPath, tracingContext, null);\r\n        return true;\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        LOG.debug(\"Rename operation failed. \", ex);\r\n        checkException(src, ex, AzureServiceErrorCode.PATH_ALREADY_EXISTS, AzureServiceErrorCode.INVALID_RENAME_SOURCE_PATH, AzureServiceErrorCode.SOURCE_PATH_NOT_FOUND, AzureServiceErrorCode.INVALID_SOURCE_OR_DESTINATION_RESOURCE_TYPE, AzureServiceErrorCode.RENAME_DESTINATION_PARENT_PATH_NOT_FOUND, AzureServiceErrorCode.INTERNAL_OPERATION_ABORT);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createResilientCommitSupport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ResilientCommitByRename createResilientCommitSupport(final Path path) throws IOException\n{\r\n    if (!hasPathCapability(path, CommonPathCapabilities.ETAGS_PRESERVED_IN_RENAME)) {\r\n        throw new UnsupportedOperationException(\"Resilient commit support not available for \" + path);\r\n    }\r\n    return new ResilientCommitByRenameImpl();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "delete",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean delete(final Path f, final boolean recursive) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.delete path: {} recursive: {}\", f.toString(), recursive);\r\n    statIncrement(CALL_DELETE);\r\n    Path qualifiedPath = makeQualified(f);\r\n    if (f.isRoot()) {\r\n        if (!recursive) {\r\n            return false;\r\n        }\r\n        return deleteRoot();\r\n    }\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.DELETE, tracingHeaderFormat, listener);\r\n        abfsStore.delete(qualifiedPath, recursive, tracingContext);\r\n        return true;\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(f, ex, AzureServiceErrorCode.PATH_NOT_FOUND);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "listStatus",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileStatus[] listStatus(final Path f) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.listStatus path: {}\", f.toString());\r\n    statIncrement(CALL_LIST_STATUS);\r\n    Path qualifiedPath = makeQualified(f);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.LISTSTATUS, true, tracingHeaderFormat, listener);\r\n        FileStatus[] result = abfsStore.listStatus(qualifiedPath, tracingContext);\r\n        return result;\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(f, ex);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "statIncrement",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void statIncrement(AbfsStatistic statistic)\n{\r\n    incrementStatistic(statistic);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "incrementStatistic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementStatistic(AbfsStatistic statistic)\n{\r\n    if (abfsCounters != null) {\r\n        abfsCounters.incrementCounter(statistic, 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "trailingPeriodCheck",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void trailingPeriodCheck(Path path) throws IllegalArgumentException\n{\r\n    while (!path.isRoot()) {\r\n        String pathToString = path.toString();\r\n        if (pathToString.length() != 0) {\r\n            if (pathToString.charAt(pathToString.length() - 1) == '.') {\r\n                throw new IllegalArgumentException(\"ABFS does not allow files or directories to end with a dot.\");\r\n            }\r\n            path = path.getParent();\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "mkdirs",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean mkdirs(final Path f, final FsPermission permission) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.mkdirs path: {} permissions: {}\", f, permission);\r\n    statIncrement(CALL_MKDIRS);\r\n    trailingPeriodCheck(f);\r\n    final Path parentFolder = f.getParent();\r\n    if (parentFolder == null) {\r\n        return true;\r\n    }\r\n    Path qualifiedPath = makeQualified(f);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.MKDIR, false, tracingHeaderFormat, listener);\r\n        abfsStore.createDirectory(qualifiedPath, permission == null ? FsPermission.getDirDefault() : permission, FsPermission.getUMask(getConf()), tracingContext);\r\n        statIncrement(DIRECTORIES_CREATED);\r\n        return true;\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(f, ex);\r\n        return true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (isClosed) {\r\n        return;\r\n    }\r\n    super.close();\r\n    LOG.debug(\"AzureBlobFileSystem.close\");\r\n    if (getConf() != null) {\r\n        String iostatisticsLoggingLevel = getConf().getTrimmed(IOSTATISTICS_LOGGING_LEVEL, IOSTATISTICS_LOGGING_LEVEL_DEFAULT);\r\n        logIOStatisticsAtLevel(LOG, iostatisticsLoggingLevel, getIOStatistics());\r\n    }\r\n    IOUtils.cleanupWithLogger(LOG, abfsStore, delegationTokenManager);\r\n    this.isClosed = true;\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Closing Abfs: {}\", toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus getFileStatus(final Path f) throws IOException\n{\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.GET_FILESTATUS, tracingHeaderFormat, listener);\r\n    return getFileStatus(f, tracingContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileStatus",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileStatus getFileStatus(final Path path, TracingContext tracingContext) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.getFileStatus path: {}\", path);\r\n    statIncrement(CALL_GET_FILE_STATUS);\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        return abfsStore.getFileStatus(qualifiedPath, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "breakLease",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void breakLease(final Path f) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.breakLease path: {}\", f);\r\n    Path qualifiedPath = makeQualified(f);\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Break lease for %s\", qualifiedPath)) {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.BREAK_LEASE, tracingHeaderFormat, listener);\r\n        abfsStore.breakLease(qualifiedPath, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(f, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "makeQualified",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path makeQualified(Path path)\n{\r\n    if (path != null) {\r\n        String uriPath = path.toUri().getPath();\r\n        path = uriPath.isEmpty() ? path : new Path(uriPath);\r\n    }\r\n    return super.makeQualified(path);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    return this.workingDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWorkingDirectory(final Path newDir)\n{\r\n    if (newDir.isAbsolute()) {\r\n        this.workingDir = newDir;\r\n    } else {\r\n        this.workingDir = new Path(workingDir, newDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return FileSystemUriSchemes.ABFS_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getHomeDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getHomeDirectory()\n{\r\n    return makeQualified(new Path(FileSystemConfigurations.USER_HOME_DIRECTORY_PREFIX + \"/\" + abfsStore.getUser()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len)\n{\r\n    if (file == null) {\r\n        return null;\r\n    }\r\n    if ((start < 0) || (len < 0)) {\r\n        throw new IllegalArgumentException(\"Invalid start or len parameter\");\r\n    }\r\n    if (file.getLen() < start) {\r\n        return new BlockLocation[0];\r\n    }\r\n    final String blobLocationHost = abfsStore.getAbfsConfiguration().getAzureBlockLocationHost();\r\n    final String[] name = { blobLocationHost };\r\n    final String[] host = { blobLocationHost };\r\n    long blockSize = file.getBlockSize();\r\n    if (blockSize <= 0) {\r\n        throw new IllegalArgumentException(\"The block size for the given file is not a positive number: \" + blockSize);\r\n    }\r\n    int numberOfLocations = (int) (len / blockSize) + ((len % blockSize == 0) ? 0 : 1);\r\n    BlockLocation[] locations = new BlockLocation[numberOfLocations];\r\n    for (int i = 0; i < locations.length; i++) {\r\n        long currentOffset = start + (i * blockSize);\r\n        long currentLength = Math.min(blockSize, start + len - currentOffset);\r\n        locations[i] = new BlockLocation(name, host, currentOffset, currentLength);\r\n    }\r\n    return locations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "finalize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finalize() throws Throwable\n{\r\n    LOG.debug(\"finalize() called.\");\r\n    close();\r\n    super.finalize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getOwnerUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getOwnerUser()\n{\r\n    return abfsStore.getUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getOwnerUserPrimaryGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getOwnerUserPrimaryGroup()\n{\r\n    return abfsStore.getPrimaryGroup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "deleteRoot",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean deleteRoot() throws IOException\n{\r\n    LOG.debug(\"Deleting root content\");\r\n    final ExecutorService executorService = Executors.newFixedThreadPool(10);\r\n    try {\r\n        final FileStatus[] ls = listStatus(makeQualified(new Path(File.separator)));\r\n        final ArrayList<Future> deleteTasks = new ArrayList<>();\r\n        for (final FileStatus fs : ls) {\r\n            final Future deleteTask = executorService.submit(new Callable<Void>() {\r\n\r\n                @Override\r\n                public Void call() throws Exception {\r\n                    delete(fs.getPath(), fs.isDirectory());\r\n                    if (fs.isDirectory()) {\r\n                        statIncrement(DIRECTORIES_DELETED);\r\n                    } else {\r\n                        statIncrement(FILES_DELETED);\r\n                    }\r\n                    return null;\r\n                }\r\n            });\r\n            deleteTasks.add(deleteTask);\r\n        }\r\n        for (final Future deleteTask : deleteTasks) {\r\n            execute(\"deleteRoot\", new Callable<Void>() {\r\n\r\n                @Override\r\n                public Void call() throws Exception {\r\n                    deleteTask.get();\r\n                    return null;\r\n                }\r\n            });\r\n        }\r\n    } finally {\r\n        executorService.shutdownNow();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setOwner",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setOwner(final Path path, final String owner, final String group) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.setOwner path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.SET_OWNER, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        super.setOwner(path, owner, group);\r\n        return;\r\n    }\r\n    if ((owner == null || owner.isEmpty()) && (group == null || group.isEmpty())) {\r\n        throw new IllegalArgumentException(\"A valid owner or group must be specified.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        abfsStore.setOwner(qualifiedPath, owner, group, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setXAttr",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setXAttr(final Path path, final String name, final byte[] value, final EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.setXAttr path: {}\", path);\r\n    if (name == null || name.isEmpty() || value == null) {\r\n        throw new IllegalArgumentException(\"A valid name and value must be specified.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.SET_ATTR, true, tracingHeaderFormat, listener);\r\n        Hashtable<String, String> properties = abfsStore.getPathStatus(qualifiedPath, tracingContext);\r\n        String xAttrName = ensureValidAttributeName(name);\r\n        boolean xAttrExists = properties.containsKey(xAttrName);\r\n        XAttrSetFlag.validate(name, xAttrExists, flag);\r\n        String xAttrValue = abfsStore.decodeAttribute(value);\r\n        properties.put(xAttrName, xAttrValue);\r\n        abfsStore.setPathProperties(qualifiedPath, properties, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getXAttr",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "byte[] getXAttr(final Path path, final String name) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.getXAttr path: {}\", path);\r\n    if (name == null || name.isEmpty()) {\r\n        throw new IllegalArgumentException(\"A valid name must be specified.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    byte[] value = null;\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.GET_ATTR, true, tracingHeaderFormat, listener);\r\n        Hashtable<String, String> properties = abfsStore.getPathStatus(qualifiedPath, tracingContext);\r\n        String xAttrName = ensureValidAttributeName(name);\r\n        if (properties.containsKey(xAttrName)) {\r\n            String xAttrValue = properties.get(xAttrName);\r\n            value = abfsStore.encodeAttribute(xAttrValue);\r\n        }\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureValidAttributeName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String ensureValidAttributeName(String attribute)\n{\r\n    return attribute.replace('.', '_');\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setPermission",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setPermission(final Path path, final FsPermission permission) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.setPermission path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.SET_PERMISSION, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        super.setPermission(path, permission);\r\n        return;\r\n    }\r\n    if (permission == null) {\r\n        throw new IllegalArgumentException(\"The permission can't be null\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        abfsStore.setPermission(qualifiedPath, permission, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "modifyAclEntries",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void modifyAclEntries(final Path path, final List<AclEntry> aclSpec) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.modifyAclEntries path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.MODIFY_ACL, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"modifyAclEntries is only supported by storage accounts with the \" + \"hierarchical namespace enabled.\");\r\n    }\r\n    if (aclSpec == null || aclSpec.isEmpty()) {\r\n        throw new IllegalArgumentException(\"The value of the aclSpec parameter is invalid.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        abfsStore.modifyAclEntries(qualifiedPath, aclSpec, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "removeAclEntries",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void removeAclEntries(final Path path, final List<AclEntry> aclSpec) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.removeAclEntries path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.REMOVE_ACL_ENTRIES, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"removeAclEntries is only supported by storage accounts with the \" + \"hierarchical namespace enabled.\");\r\n    }\r\n    if (aclSpec == null || aclSpec.isEmpty()) {\r\n        throw new IllegalArgumentException(\"The aclSpec argument is invalid.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        abfsStore.removeAclEntries(qualifiedPath, aclSpec, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "removeDefaultAcl",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeDefaultAcl(final Path path) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.removeDefaultAcl path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.REMOVE_DEFAULT_ACL, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"removeDefaultAcl is only supported by storage accounts with the \" + \"hierarchical namespace enabled.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        abfsStore.removeDefaultAcl(qualifiedPath, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "removeAcl",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeAcl(final Path path) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.removeAcl path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.REMOVE_ACL, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"removeAcl is only supported by storage accounts with the \" + \"hierarchical namespace enabled.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        abfsStore.removeAcl(qualifiedPath, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setAcl",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setAcl(final Path path, final List<AclEntry> aclSpec) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.setAcl path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.SET_ACL, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"setAcl is only supported by storage accounts with the hierarchical \" + \"namespace enabled.\");\r\n    }\r\n    if (aclSpec == null || aclSpec.size() == 0) {\r\n        throw new IllegalArgumentException(\"The aclSpec argument is invalid.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        abfsStore.setAcl(qualifiedPath, aclSpec, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAclStatus",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AclStatus getAclStatus(final Path path) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.getAclStatus path: {}\", path);\r\n    TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.GET_ACL_STATUS, true, tracingHeaderFormat, listener);\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"getAclStatus is only supported by storage account with the \" + \"hierarchical namespace enabled.\");\r\n    }\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        return abfsStore.getAclStatus(qualifiedPath, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(path, ex);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "access",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void access(final Path path, final FsAction mode) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.access path : {}, mode : {}\", path, mode);\r\n    Path qualifiedPath = makeQualified(path);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.ACCESS, tracingHeaderFormat, listener);\r\n        this.abfsStore.access(qualifiedPath, mode, tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkCheckAccessException(path, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "exists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean exists(Path f) throws IOException\n{\r\n    statIncrement(CALL_EXIST);\r\n    return super.exists(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(Path path) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\r\n    if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.LISTSTATUS, true, tracingHeaderFormat, listener);\r\n        AbfsListStatusRemoteIterator abfsLsItr = new AbfsListStatusRemoteIterator(path, abfsStore, tracingContext);\r\n        return RemoteIterators.typeCastingRemoteIterator(abfsLsItr);\r\n    } else {\r\n        return super.listStatusIterator(path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "listLocatedStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path path, final PathFilter filter) throws FileNotFoundException, IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\r\n    final RemoteIterator<FileStatus> sourceEntries = filteringRemoteIterator(listStatusIterator(path), (st) -> filter.accept(st.getPath()));\r\n    return mappingRemoteIterator(sourceEntries, st -> new AbfsLocatedFileStatus(st, st.isFile() ? getFileBlockLocations(st, 0, st.getLen()) : null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "tryGetFileStatus",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus tryGetFileStatus(final Path f, TracingContext tracingContext)\n{\r\n    try {\r\n        return getFileStatus(f, tracingContext);\r\n    } catch (IOException ex) {\r\n        LOG.debug(\"File not found {}\", f);\r\n        statIncrement(ERROR_IGNORED);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "fileSystemExists",
  "errType" : [ "AzureBlobFileSystemException", "FileNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean fileSystemExists() throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.fileSystemExists uri: {}\", uri);\r\n    try {\r\n        TracingContext tracingContext = new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.TEST_OP, tracingHeaderFormat, listener);\r\n        abfsStore.getFilesystemProperties(tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        try {\r\n            checkException(null, ex);\r\n        } catch (FileNotFoundException e) {\r\n            statIncrement(ERROR_IGNORED);\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFileSystem",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createFileSystem(TracingContext tracingContext) throws IOException\n{\r\n    LOG.debug(\"AzureBlobFileSystem.createFileSystem uri: {}\", uri);\r\n    try {\r\n        abfsStore.createFilesystem(tracingContext);\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        checkException(null, ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "ensureAuthority",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "URI ensureAuthority(URI uri, final Configuration conf)\n{\r\n    Preconditions.checkNotNull(uri, \"uri\");\r\n    if (uri.getAuthority() == null) {\r\n        final URI defaultUri = FileSystem.getDefaultUri(conf);\r\n        if (defaultUri != null && isAbfsScheme(defaultUri.getScheme())) {\r\n            try {\r\n                uri = new URI(uri.getScheme(), defaultUri.getAuthority(), uri.getPath(), uri.getQuery(), uri.getFragment());\r\n            } catch (URISyntaxException e) {\r\n                throw new IllegalArgumentException(new InvalidUriException(uri.toString()));\r\n            }\r\n        }\r\n    }\r\n    if (uri.getAuthority() == null) {\r\n        throw new IllegalArgumentException(new InvalidUriAuthorityException(uri.toString()));\r\n    }\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isAbfsScheme",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isAbfsScheme(final String scheme)\n{\r\n    if (scheme == null) {\r\n        return false;\r\n    }\r\n    if (scheme.equals(FileSystemUriSchemes.ABFS_SCHEME) || scheme.equals(FileSystemUriSchemes.ABFS_SECURE_SCHEME)) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystemOperation<T> execute(final String scopeDescription, final Callable<T> callableFileOperation) throws IOException\n{\r\n    return execute(scopeDescription, callableFileOperation, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "execute",
  "errType" : [ "AbfsRestOperationException", "AzureBlobFileSystemException", "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSystemOperation<T> execute(final String scopeDescription, final Callable<T> callableFileOperation, T defaultResultValue) throws IOException\n{\r\n    try {\r\n        final T executionResult = callableFileOperation.call();\r\n        return new FileSystemOperation<>(executionResult, null);\r\n    } catch (AbfsRestOperationException abfsRestOperationException) {\r\n        return new FileSystemOperation<>(defaultResultValue, abfsRestOperationException);\r\n    } catch (AzureBlobFileSystemException azureBlobFileSystemException) {\r\n        throw new IOException(azureBlobFileSystemException);\r\n    } catch (Exception exception) {\r\n        if (exception instanceof ExecutionException) {\r\n            exception = (Exception) getRootCause(exception);\r\n        }\r\n        final FileSystemOperationUnhandledException fileSystemOperationUnhandledException = new FileSystemOperationUnhandledException(exception);\r\n        throw new IOException(fileSystemOperationUnhandledException);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkCheckAccessException",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkCheckAccessException(final Path path, final AzureBlobFileSystemException exception) throws IOException\n{\r\n    if (exception instanceof AbfsRestOperationException) {\r\n        AbfsRestOperationException ere = (AbfsRestOperationException) exception;\r\n        if (ere.getStatusCode() == HttpURLConnection.HTTP_FORBIDDEN) {\r\n            throw (IOException) new AccessControlException(ere.getMessage()).initCause(exception);\r\n        }\r\n    }\r\n    checkException(path, exception);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "checkException",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkException(final Path path, final AzureBlobFileSystemException exception, final AzureServiceErrorCode... allowedErrorCodesList) throws IOException\n{\r\n    if (exception instanceof AbfsRestOperationException) {\r\n        AbfsRestOperationException ere = (AbfsRestOperationException) exception;\r\n        if (ArrayUtils.contains(allowedErrorCodesList, ere.getErrorCode())) {\r\n            return;\r\n        }\r\n        String message = ere.getMessage();\r\n        switch(ere.getStatusCode()) {\r\n            case HttpURLConnection.HTTP_NOT_FOUND:\r\n                throw (IOException) new FileNotFoundException(message).initCause(exception);\r\n            case HttpURLConnection.HTTP_CONFLICT:\r\n                throw (IOException) new FileAlreadyExistsException(message).initCause(exception);\r\n            case HttpURLConnection.HTTP_FORBIDDEN:\r\n            case HttpURLConnection.HTTP_UNAUTHORIZED:\r\n                throw (IOException) new AccessDeniedException(message).initCause(exception);\r\n            default:\r\n                throw ere;\r\n        }\r\n    } else if (exception instanceof SASTokenProviderException) {\r\n        throw exception;\r\n    } else {\r\n        if (path == null) {\r\n            throw exception;\r\n        }\r\n        throw new PathIOException(path.toString(), exception);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRootCause",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Throwable getRootCause(Throwable throwable)\n{\r\n    if (throwable == null) {\r\n        throw new IllegalArgumentException(\"throwable can not be null\");\r\n    }\r\n    Throwable result = throwable;\r\n    while (result.getCause() != null) {\r\n        result = result.getCause();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Token<?> getDelegationToken(final String renewer) throws IOException\n{\r\n    statIncrement(CALL_GET_DELEGATION_TOKEN);\r\n    return this.delegationTokenEnabled ? this.delegationTokenManager.getDelegationToken(renewer) : super.getDelegationToken(renewer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    String name = null;\r\n    if (delegationTokenManager != null) {\r\n        name = delegationTokenManager.getCanonicalServiceName();\r\n    }\r\n    return name != null ? name : super.getCanonicalServiceName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFsStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem.Statistics getFsStatistics()\n{\r\n    return this.statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setListenerOperation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setListenerOperation(FSOperationType operation)\n{\r\n    listener.setOperation(operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfsStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureBlobFileSystemStore getAbfsStore()\n{\r\n    return abfsStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfsClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsClient getAbfsClient()\n{\r\n    return abfsStore.getClient();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getDelegationTokenManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsDelegationTokenManager getDelegationTokenManager()\n{\r\n    return delegationTokenManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getIsNamespaceEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getIsNamespaceEnabled(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    return abfsStore.getIsNamespaceEnabled(tracingContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getInstrumentationMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Long> getInstrumentationMap()\n{\r\n    return abfsCounters.toMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileSystemId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFileSystemId()\n{\r\n    return fileSystemId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getClientCorrelationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClientCorrelationId()\n{\r\n    return clientCorrelationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean hasPathCapability(final Path path, final String capability) throws IOException\n{\r\n    final Path p = makeQualified(path);\r\n    switch(validatePathCapabilityArgs(p, capability)) {\r\n        case CommonPathCapabilities.FS_PERMISSIONS:\r\n        case CommonPathCapabilities.FS_APPEND:\r\n        case CommonPathCapabilities.ETAGS_AVAILABLE:\r\n            return true;\r\n        case CommonPathCapabilities.ETAGS_PRESERVED_IN_RENAME:\r\n        case CommonPathCapabilities.FS_ACLS:\r\n            return getIsNamespaceEnabled(new TracingContext(clientCorrelationId, fileSystemId, FSOperationType.HAS_PATH_CAPABILITY, tracingHeaderFormat, listener));\r\n        default:\r\n            return super.hasPathCapability(p, capability);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return abfsCounters != null ? abfsCounters.getIOStatistics() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "initializeSingleton",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initializeSingleton(boolean enableAutoThrottling)\n{\r\n    if (!enableAutoThrottling) {\r\n        return;\r\n    }\r\n    if (singleton == null) {\r\n        singleton = new AbfsClientThrottlingIntercept();\r\n        isAutoThrottlingEnabled = true;\r\n        LOG.debug(\"Client-side throttling is enabled for the ABFS file system.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "updateMetrics",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void updateMetrics(AbfsRestOperationType operationType, AbfsHttpOperation abfsHttpOperation)\n{\r\n    if (!isAutoThrottlingEnabled || abfsHttpOperation == null) {\r\n        return;\r\n    }\r\n    int status = abfsHttpOperation.getStatusCode();\r\n    long contentLength = 0;\r\n    boolean isFailedOperation = (status < HttpURLConnection.HTTP_OK || status >= HttpURLConnection.HTTP_INTERNAL_ERROR);\r\n    switch(operationType) {\r\n        case Append:\r\n            contentLength = abfsHttpOperation.getBytesSent();\r\n            if (contentLength > 0) {\r\n                singleton.writeThrottler.addBytesTransferred(contentLength, isFailedOperation);\r\n            }\r\n            break;\r\n        case ReadFile:\r\n            String range = abfsHttpOperation.getConnection().getRequestProperty(HttpHeaderConfigurations.RANGE);\r\n            contentLength = getContentLengthIfKnown(range);\r\n            if (contentLength > 0) {\r\n                singleton.readThrottler.addBytesTransferred(contentLength, isFailedOperation);\r\n            }\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "sendingRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void sendingRequest(AbfsRestOperationType operationType, AbfsCounters abfsCounters)\n{\r\n    if (!isAutoThrottlingEnabled) {\r\n        return;\r\n    }\r\n    switch(operationType) {\r\n        case ReadFile:\r\n            if (singleton.readThrottler.suspendIfNecessary() && abfsCounters != null) {\r\n                abfsCounters.incrementCounter(AbfsStatistic.READ_THROTTLES, 1);\r\n            }\r\n            break;\r\n        case Append:\r\n            if (singleton.writeThrottler.suspendIfNecessary() && abfsCounters != null) {\r\n                abfsCounters.incrementCounter(AbfsStatistic.WRITE_THROTTLES, 1);\r\n            }\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getContentLengthIfKnown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getContentLengthIfKnown(String range)\n{\r\n    long contentLength = 0;\r\n    if (range != null && range.startsWith(RANGE_PREFIX)) {\r\n        String[] offsets = range.substring(RANGE_PREFIX.length()).split(\"-\");\r\n        if (offsets.length == 2) {\r\n            contentLength = Long.parseLong(offsets[1]) - Long.parseLong(offsets[0]) + 1;\r\n        }\r\n    }\r\n    return contentLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Boolean validate(final String configValue) throws InvalidConfigurationValueException\n{\r\n    Boolean result = super.validate(configValue);\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    if (configValue.equalsIgnoreCase(TRUE) || configValue.equalsIgnoreCase(FALSE)) {\r\n        return Boolean.valueOf(configValue);\r\n    }\r\n    throw new InvalidConfigurationValueException(getConfigKey());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "transformIdentityForGetRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String transformIdentityForGetRequest(String originalIdentity, boolean isUserName, String localIdentity) throws IOException\n{\r\n    String localIdentityForOrig = isUserName ? localToAadIdentityLookup.lookupForLocalUserIdentity(originalIdentity) : localToAadIdentityLookup.lookupForLocalGroupIdentity(originalIdentity);\r\n    if (localIdentityForOrig == null || localIdentityForOrig.isEmpty()) {\r\n        return super.transformIdentityForGetRequest(originalIdentity, isUserName, localIdentity);\r\n    }\r\n    return localIdentityForOrig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "addBytesTransferred",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addBytesTransferred(long count, boolean isFailedOperation)\n{\r\n    AbfsOperationMetrics metrics = blobMetrics.get();\r\n    if (isFailedOperation) {\r\n        metrics.bytesFailed.addAndGet(count);\r\n        metrics.operationsFailed.incrementAndGet();\r\n    } else {\r\n        metrics.bytesSuccessful.addAndGet(count);\r\n        metrics.operationsSuccessful.incrementAndGet();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "suspendIfNecessary",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean suspendIfNecessary()\n{\r\n    int duration = sleepDuration;\r\n    if (duration > 0) {\r\n        try {\r\n            Thread.sleep(duration);\r\n            return true;\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSleepDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSleepDuration()\n{\r\n    return sleepDuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "analyzeMetricsAndUpdateSleepDuration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int analyzeMetricsAndUpdateSleepDuration(AbfsOperationMetrics metrics, int sleepDuration)\n{\r\n    final double percentageConversionFactor = 100;\r\n    double bytesFailed = metrics.bytesFailed.get();\r\n    double bytesSuccessful = metrics.bytesSuccessful.get();\r\n    double operationsFailed = metrics.operationsFailed.get();\r\n    double operationsSuccessful = metrics.operationsSuccessful.get();\r\n    double errorPercentage = (bytesFailed <= 0) ? 0 : (percentageConversionFactor * bytesFailed / (bytesFailed + bytesSuccessful));\r\n    long periodMs = metrics.endTime - metrics.startTime;\r\n    double newSleepDuration;\r\n    if (errorPercentage < MIN_ACCEPTABLE_ERROR_PERCENTAGE) {\r\n        ++consecutiveNoErrorCount;\r\n        double reductionFactor = (consecutiveNoErrorCount * analysisPeriodMs >= RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) ? RAPID_SLEEP_DECREASE_FACTOR : SLEEP_DECREASE_FACTOR;\r\n        newSleepDuration = sleepDuration * reductionFactor;\r\n    } else if (errorPercentage < MAX_EQUILIBRIUM_ERROR_PERCENTAGE) {\r\n        newSleepDuration = sleepDuration;\r\n    } else {\r\n        consecutiveNoErrorCount = 0;\r\n        double additionalDelayNeeded = 5 * analysisPeriodMs;\r\n        if (bytesSuccessful > 0) {\r\n            additionalDelayNeeded = (bytesSuccessful + bytesFailed) * periodMs / bytesSuccessful - periodMs;\r\n        }\r\n        newSleepDuration = additionalDelayNeeded / (operationsFailed + operationsSuccessful);\r\n        final double maxSleepDuration = analysisPeriodMs;\r\n        final double minSleepDuration = sleepDuration * SLEEP_INCREASE_FACTOR;\r\n        newSleepDuration = Math.max(newSleepDuration, minSleepDuration) + 1;\r\n        newSleepDuration = Math.min(newSleepDuration, maxSleepDuration);\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(String.format(\"%5.5s, %10d, %10d, %10d, %10d, %6.2f, %5d, %5d, %5d\", name, (int) bytesFailed, (int) bytesSuccessful, (int) operationsFailed, (int) operationsSuccessful, errorPercentage, periodMs, (int) sleepDuration, (int) newSleepDuration));\r\n    }\r\n    return (int) newSleepDuration;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "handleKind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean handleKind(Text kind)\n{\r\n    return AbfsDelegationTokenIdentifier.TOKEN_KIND.equals(kind);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "isManaged",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isManaged(Token<?> token) throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "renew",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long renew(final Token<?> token, Configuration conf) throws IOException, InterruptedException\n{\r\n    LOG.debug(\"Renewing the delegation token\");\r\n    return getInstance(conf).renewDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "cancel",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cancel(final Token<?> token, Configuration conf) throws IOException, InterruptedException\n{\r\n    LOG.debug(\"Cancelling the delegation token\");\r\n    getInstance(conf).cancelDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsDelegationTokenManager getInstance(Configuration conf) throws IOException\n{\r\n    return new AbfsDelegationTokenManager(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "dumpHeadersToDebugLog",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void dumpHeadersToDebugLog(final String origin, final Map<String, List<String>> headers)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"{}\", origin);\r\n        for (Map.Entry<String, List<String>> entry : headers.entrySet()) {\r\n            String key = entry.getKey();\r\n            if (key == null) {\r\n                key = \"HTTP Response\";\r\n            }\r\n            String values = StringUtils.join(\";\", entry.getValue());\r\n            if (key.contains(\"Cookie\")) {\r\n                values = \"*cookie info*\";\r\n            }\r\n            if (key.equals(\"sig\")) {\r\n                values = \"XXXX\";\r\n            }\r\n            LOG.debug(\"  {}={}\", key, values);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(String renewer) throws IOException\n{\r\n    URIBuilder uriBuilder = new URIBuilder().setPath(DEFAULT_DELEGATION_TOKEN_MANAGER_ENDPOINT).addParameter(OP_PARAM_KEY_NAME, GET_DELEGATION_TOKEN_OP).addParameter(RENEWER_PARAM_KEY_NAME, renewer).addParameter(SERVICE_PARAM_KEY_NAME, WASB_DT_SERVICE_NAME.toString());\r\n    String responseBody = remoteCallHelper.makeRemoteRequest(dtServiceUrls, uriBuilder.getPath(), uriBuilder.getQueryParams(), HttpGet.METHOD_NAME);\r\n    return TokenUtils.toDelegationToken(JsonUtils.parse(responseBody));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long renewDelegationToken(Token<?> token) throws IOException\n{\r\n    URIBuilder uriBuilder = new URIBuilder().setPath(DEFAULT_DELEGATION_TOKEN_MANAGER_ENDPOINT).addParameter(OP_PARAM_KEY_NAME, RENEW_DELEGATION_TOKEN_OP).addParameter(TOKEN_PARAM_KEY_NAME, token.encodeToUrlString());\r\n    String responseBody = remoteCallHelper.makeRemoteRequest(dtServiceUrls, uriBuilder.getPath(), uriBuilder.getQueryParams(), HttpPut.METHOD_NAME);\r\n    Map<?, ?> parsedResp = JsonUtils.parse(responseBody);\r\n    return ((Number) parsedResp.get(\"long\")).longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cancelDelegationToken(Token<?> token) throws IOException\n{\r\n    URIBuilder uriBuilder = new URIBuilder().setPath(DEFAULT_DELEGATION_TOKEN_MANAGER_ENDPOINT).addParameter(OP_PARAM_KEY_NAME, CANCEL_DELEGATION_TOKEN_OP).addParameter(TOKEN_PARAM_KEY_NAME, token.encodeToUrlString());\r\n    remoteCallHelper.makeRemoteRequest(dtServiceUrls, uriBuilder.getPath(), uriBuilder.getQueryParams(), HttpPut.METHOD_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return \"wasb\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    if (returnUriAsCanonicalServiceName) {\r\n        return getUri().toString();\r\n    }\r\n    return super.getCanonicalServiceName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "suppressRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void suppressRetryPolicy()\n{\r\n    suppressRetryPolicy = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "resumeRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resumeRetryPolicy()\n{\r\n    suppressRetryPolicy = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "newMetricsSourceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String newMetricsSourceName()\n{\r\n    int number = metricsSourceNameCounter.incrementAndGet();\r\n    final String baseName = \"AzureFileSystemMetrics\";\r\n    if (number == 1) {\r\n        return baseName;\r\n    } else {\r\n        return baseName + number;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isWasbScheme",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean isWasbScheme(String scheme)\n{\r\n    return scheme != null && (scheme.equalsIgnoreCase(\"asv\") || scheme.equalsIgnoreCase(\"asvs\") || scheme.equalsIgnoreCase(\"wasb\") || scheme.equalsIgnoreCase(\"wasbs\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "reconstructAuthorityIfNeeded",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "URI reconstructAuthorityIfNeeded(URI uri, Configuration conf)\n{\r\n    if (null == uri.getAuthority()) {\r\n        URI defaultUri = FileSystem.getDefaultUri(conf);\r\n        if (defaultUri != null && isWasbScheme(defaultUri.getScheme())) {\r\n            try {\r\n                return new URI(uri.getScheme(), defaultUri.getAuthority(), uri.getPath(), uri.getQuery(), uri.getFragment());\r\n            } catch (URISyntaxException e) {\r\n                throw new Error(\"Bad URI construction\", e);\r\n            }\r\n        }\r\n    }\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "checkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkPath(Path path)\n{\r\n    super.checkPath(new Path(reconstructAuthorityIfNeeded(path.toUri(), getConf())));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void initialize(URI uri, Configuration conf) throws IOException, IllegalArgumentException\n{\r\n    uri = reconstructAuthorityIfNeeded(uri, conf);\r\n    if (null == uri.getAuthority()) {\r\n        final String errMsg = String.format(\"Cannot initialize WASB file system, URI authority not recognized.\");\r\n        throw new IllegalArgumentException(errMsg);\r\n    }\r\n    super.initialize(uri, conf);\r\n    if (store == null) {\r\n        store = createDefaultStore(conf);\r\n    }\r\n    instrumentation = new AzureFileSystemInstrumentation(conf);\r\n    if (!conf.getBoolean(SKIP_AZURE_METRICS_PROPERTY_NAME, false)) {\r\n        AzureFileSystemMetricsSystem.fileSystemStarted();\r\n        metricsSourceName = newMetricsSourceName();\r\n        String sourceDesc = \"Azure Storage Volume File System metrics\";\r\n        AzureFileSystemMetricsSystem.registerSource(metricsSourceName, sourceDesc, instrumentation);\r\n    }\r\n    store.initialize(uri, conf, instrumentation);\r\n    setConf(conf);\r\n    this.ugi = UserGroupInformation.getCurrentUser();\r\n    this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\r\n    this.workingDir = new Path(\"/user\", UserGroupInformation.getCurrentUser().getShortUserName()).makeQualified(getUri(), getWorkingDirectory());\r\n    this.appendSupportEnabled = conf.getBoolean(APPEND_SUPPORT_ENABLE_PROPERTY_NAME, false);\r\n    LOG.debug(\"NativeAzureFileSystem. Initializing.\");\r\n    LOG.debug(\"  blockSize  = {}\", store.getHadoopBlockSize());\r\n    deleteThreadCount = conf.getInt(AZURE_DELETE_THREADS, DEFAULT_AZURE_DELETE_THREADS);\r\n    renameThreadCount = conf.getInt(AZURE_RENAME_THREADS, DEFAULT_AZURE_RENAME_THREADS);\r\n    boolean useSecureMode = conf.getBoolean(AzureNativeFileSystemStore.KEY_USE_SECURE_MODE, AzureNativeFileSystemStore.DEFAULT_USE_SECURE_MODE);\r\n    this.azureAuthorization = useSecureMode && conf.getBoolean(KEY_AZURE_AUTHORIZATION, DEFAULT_AZURE_AUTHORIZATION);\r\n    this.kerberosSupportEnabled = conf.getBoolean(Constants.AZURE_KERBEROS_SUPPORT_PROPERTY_NAME, false);\r\n    if (this.azureAuthorization) {\r\n        this.authorizer = new RemoteWasbAuthorizerImpl();\r\n        authorizer.init(conf);\r\n        this.chmodAllowedUsers = Arrays.asList(conf.getTrimmedStrings(AZURE_CHMOD_USERLIST_PROPERTY_NAME, AZURE_CHMOD_USERLIST_PROPERTY_DEFAULT_VALUE));\r\n        this.chownAllowedUsers = Arrays.asList(conf.getTrimmedStrings(AZURE_CHOWN_USERLIST_PROPERTY_NAME, AZURE_CHOWN_USERLIST_PROPERTY_DEFAULT_VALUE));\r\n        this.daemonUsers = Arrays.asList(conf.getTrimmedStrings(AZURE_DAEMON_USERLIST_PROPERTY_NAME, AZURE_DAEMON_USERLIST_PROPERTY_DEFAULT_VALUE));\r\n    }\r\n    if (UserGroupInformation.isSecurityEnabled() && kerberosSupportEnabled) {\r\n        this.wasbDelegationTokenManager = new RemoteWasbDelegationTokenManager(conf);\r\n    }\r\n    this.returnUriAsCanonicalServiceName = conf.getBoolean(RETURN_URI_AS_CANONICAL_SERVICE_NAME_PROPERTY_NAME, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getHomeDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getHomeDirectory()\n{\r\n    return makeQualified(new Path(USER_HOME_DIR_PREFIX_DEFAULT + \"/\" + this.ugi.getShortUserName()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateWasbAuthorizer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateWasbAuthorizer(WasbAuthorizerInterface authorizer)\n{\r\n    this.authorizer = authorizer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createDefaultStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NativeFileSystemStore createDefaultStore(Configuration conf)\n{\r\n    actualStore = new AzureNativeFileSystemStore();\r\n    if (suppressRetryPolicy) {\r\n        actualStore.suppressRetryPolicy();\r\n    }\r\n    return actualStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "encodeTrailingPeriod",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String encodeTrailingPeriod(String toEncode)\n{\r\n    Matcher matcher = TRAILING_PERIOD_PATTERN.matcher(toEncode);\r\n    return matcher.replaceAll(TRAILING_PERIOD_PLACEHOLDER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "decodeTrailingPeriod",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String decodeTrailingPeriod(String toDecode)\n{\r\n    Matcher matcher = TRAILING_PERIOD_PLACEHOLDER_PATTERN.matcher(toDecode);\r\n    return matcher.replaceAll(\".\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "pathToKey",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String pathToKey(Path path)\n{\r\n    URI tmpUri = path.toUri();\r\n    String pathUri = tmpUri.getPath();\r\n    Path newPath = path;\r\n    if (\"\".equals(pathUri)) {\r\n        newPath = new Path(tmpUri.toString() + Path.SEPARATOR);\r\n    }\r\n    if (!newPath.isAbsolute()) {\r\n        throw new IllegalArgumentException(\"Path must be absolute: \" + path);\r\n    }\r\n    String key = null;\r\n    key = newPath.toUri().getPath();\r\n    key = removeTrailingSlash(key);\r\n    key = encodeTrailingPeriod(key);\r\n    if (key.length() == 1) {\r\n        return key;\r\n    } else {\r\n        return key.substring(1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "removeTrailingSlash",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String removeTrailingSlash(String key)\n{\r\n    if (key.length() == 0 || key.length() == 1) {\r\n        return key;\r\n    }\r\n    if (key.charAt(key.length() - 1) == '/') {\r\n        return key.substring(0, key.length() - 1);\r\n    } else {\r\n        return key;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "keyToPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path keyToPath(String key)\n{\r\n    if (key.equals(\"/\")) {\r\n        return new Path(\"/\");\r\n    }\r\n    return new Path(\"/\" + decodeTrailingPeriod(key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "makeAbsolute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path makeAbsolute(Path path)\n{\r\n    if (path.isAbsolute()) {\r\n        return path;\r\n    }\r\n    return new Path(workingDir, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureNativeFileSystemStore getStore()\n{\r\n    return actualStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStoreInterface",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NativeFileSystemStore getStoreInterface()\n{\r\n    return store;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "performAuthCheck",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void performAuthCheck(Path requestingAccessForPath, WasbAuthorizationOperations accessType, String operation, Path originalPath) throws WasbAuthorizationException, IOException\n{\r\n    if (azureAuthorization && this.authorizer != null) {\r\n        requestingAccessForPath = requestingAccessForPath.makeQualified(getUri(), getWorkingDirectory());\r\n        originalPath = originalPath.makeQualified(getUri(), getWorkingDirectory());\r\n        String owner = getOwnerForPath(requestingAccessForPath);\r\n        if (!this.authorizer.authorize(requestingAccessForPath.toString(), accessType.toString(), owner)) {\r\n            throw new WasbAuthorizationException(operation + \" operation for Path : \" + originalPath.toString() + \" not allowed\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getInstrumentation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureFileSystemInstrumentation getInstrumentation()\n{\r\n    return instrumentation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "append",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "FSDataOutputStream append(Path f, int bufferSize, Progressable progress) throws IOException\n{\r\n    if (!appendSupportEnabled) {\r\n        throw new UnsupportedOperationException(\"Append Support not enabled\");\r\n    }\r\n    LOG.debug(\"Opening file: {} for append\", f);\r\n    Path absolutePath = makeAbsolute(f);\r\n    performAuthCheck(absolutePath, WasbAuthorizationOperations.WRITE, \"append\", absolutePath);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata meta = null;\r\n    try {\r\n        meta = store.retrieveMetadata(key);\r\n    } catch (Exception ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"%s is not found\", key));\r\n        } else {\r\n            throw ex;\r\n        }\r\n    }\r\n    if (meta == null) {\r\n        throw new FileNotFoundException(f.toString());\r\n    }\r\n    if (meta.isDirectory()) {\r\n        throw new FileNotFoundException(f.toString() + \" is a directory not a file.\");\r\n    }\r\n    if (store.isPageBlobKey(key)) {\r\n        throw new IOException(\"Append not supported for Page Blobs\");\r\n    }\r\n    DataOutputStream appendStream = null;\r\n    try {\r\n        appendStream = store.retrieveAppendStream(key, bufferSize);\r\n    } catch (Exception ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"%s is not found\", key));\r\n        } else {\r\n            throw ex;\r\n        }\r\n    }\r\n    return new FSDataOutputStream(appendStream, statistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    return create(f, permission, overwrite, true, bufferSize, replication, blockSize, progress, (SelfRenewingLease) null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "acquireLease",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SelfRenewingLease acquireLease(Path path) throws AzureException\n{\r\n    String fullKey = pathToKey(makeAbsolute(path));\r\n    return getStore().acquireLease(fullKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createNonRecursive",
  "errType" : [ "AzureException", "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    Path parent = f.getParent();\r\n    SelfRenewingLease lease = null;\r\n    if (store.isAtomicRenameKey(pathToKey(f))) {\r\n        try {\r\n            lease = acquireLease(parent);\r\n        } catch (AzureException e) {\r\n            String errorCode = \"\";\r\n            try {\r\n                StorageException e2 = (StorageException) e.getCause();\r\n                errorCode = e2.getErrorCode();\r\n            } catch (Exception e3) {\r\n            }\r\n            if (errorCode.equals(\"BlobNotFound\")) {\r\n                throw new FileNotFoundException(\"Cannot create file \" + f.getName() + \" because parent folder does not exist.\");\r\n            }\r\n            LOG.warn(\"Got unexpected exception trying to get lease on {} . {}\", pathToKey(parent), e.getMessage());\r\n            throw e;\r\n        }\r\n    }\r\n    if (!exists(parent)) {\r\n        try {\r\n            lease.free();\r\n        } catch (Exception e) {\r\n            LOG.warn(\"Unable to free lease because: {}\", e.getMessage());\r\n        }\r\n        throw new FileNotFoundException(\"Cannot create file \" + f.getName() + \" because parent folder does not exist.\");\r\n    }\r\n    FSDataOutputStream out = null;\r\n    try {\r\n        out = create(f, permission, overwrite, false, bufferSize, replication, blockSize, progress, lease);\r\n    } finally {\r\n        try {\r\n            if (lease != null) {\r\n                lease.free();\r\n            }\r\n        } catch (Exception e) {\r\n            NativeAzureFileSystemHelper.cleanup(LOG, out);\r\n            String msg = \"Unable to free lease on \" + parent.toUri();\r\n            LOG.error(msg);\r\n            throw new IOException(msg, e);\r\n        }\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(Path f, FsPermission permission, EnumSet<CreateFlag> flags, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    final EnumSet<CreateFlag> createflags = EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE);\r\n    boolean overwrite = flags.containsAll(createflags);\r\n    return this.createNonRecursive(f, permission, overwrite, bufferSize, replication, blockSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createNonRecursive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException\n{\r\n    return this.createNonRecursive(f, FsPermission.getFileDefault(), overwrite, bufferSize, replication, blockSize, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, boolean createParent, int bufferSize, short replication, long blockSize, Progressable progress, SelfRenewingLease parentFolderLease) throws FileAlreadyExistsException, IOException\n{\r\n    LOG.debug(\"Creating file: {}\", f.toString());\r\n    if (containsColon(f)) {\r\n        throw new IOException(\"Cannot create file \" + f + \" through WASB that has colons in the name\");\r\n    }\r\n    Path absolutePath = makeAbsolute(f);\r\n    Path ancestor = getAncestor(absolutePath);\r\n    performAuthCheck(ancestor, WasbAuthorizationOperations.WRITE, \"create\", absolutePath);\r\n    return createInternal(f, permission, overwrite, parentFolderLease);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createInternal",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "FSDataOutputStream createInternal(Path f, FsPermission permission, boolean overwrite, SelfRenewingLease parentFolderLease) throws FileAlreadyExistsException, IOException\n{\r\n    Path absolutePath = makeAbsolute(f);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata existingMetadata = store.retrieveMetadata(key);\r\n    if (existingMetadata != null) {\r\n        if (existingMetadata.isDirectory()) {\r\n            throw new FileAlreadyExistsException(\"Cannot create file \" + f + \"; already exists as a directory.\");\r\n        }\r\n        if (!overwrite) {\r\n            throw new FileAlreadyExistsException(\"File already exists:\" + f);\r\n        } else {\r\n            performAuthCheck(absolutePath, WasbAuthorizationOperations.WRITE, \"create\", absolutePath);\r\n        }\r\n    }\r\n    Path parentFolder = absolutePath.getParent();\r\n    if (parentFolder != null && parentFolder.getParent() != null) {\r\n        String parentKey = pathToKey(parentFolder);\r\n        FileMetadata parentMetadata = store.retrieveMetadata(parentKey);\r\n        if (parentMetadata != null && parentMetadata.isDirectory() && parentMetadata.getBlobMaterialization() == BlobMaterialization.Explicit) {\r\n            if (parentFolderLease != null) {\r\n                store.updateFolderLastModifiedTime(parentKey, parentFolderLease);\r\n            } else {\r\n                updateParentFolderLastModifiedTime(key);\r\n            }\r\n        } else {\r\n            Path firstExisting = parentFolder.getParent();\r\n            FileMetadata metadata = store.retrieveMetadata(pathToKey(firstExisting));\r\n            while (metadata == null) {\r\n                firstExisting = firstExisting.getParent();\r\n                metadata = store.retrieveMetadata(pathToKey(firstExisting));\r\n            }\r\n            mkdirs(parentFolder, metadata.getPermission(), true);\r\n        }\r\n    }\r\n    FsPermission masked = applyUMask(permission, UMaskApplyMode.NewFile);\r\n    PermissionStatus permissionStatus = createPermissionStatus(masked);\r\n    OutputStream bufOutStream;\r\n    if (store.isPageBlobKey(key)) {\r\n        bufOutStream = store.storefile(key, permissionStatus, key);\r\n    } else {\r\n        String keyEncoded = encodeKey(key);\r\n        store.storeEmptyLinkFile(key, keyEncoded, permissionStatus);\r\n        bufOutStream = new NativeAzureFsOutputStream(store.storefile(keyEncoded, permissionStatus, key), key, keyEncoded);\r\n    }\r\n    FSDataOutputStream fsOut = new FSDataOutputStream(bufOutStream, statistics);\r\n    instrumentation.fileCreated();\r\n    return fsOut;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean delete(Path path) throws IOException\n{\r\n    return delete(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive) throws IOException\n{\r\n    return delete(f, recursive, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "deleteWithAuthEnabled",
  "errType" : [ "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 55,
  "sourceCodeText" : "boolean deleteWithAuthEnabled(Path f, boolean recursive, boolean skipParentFolderLastModifiedTimeUpdate) throws IOException\n{\r\n    LOG.debug(\"Deleting file: {}\", f);\r\n    Path absolutePath = makeAbsolute(f);\r\n    Path parentPath = absolutePath.getParent();\r\n    if (parentPath != null) {\r\n        performAuthCheck(parentPath, WasbAuthorizationOperations.WRITE, \"delete\", absolutePath);\r\n    } else {\r\n        performAuthCheck(absolutePath, WasbAuthorizationOperations.WRITE, \"delete\", absolutePath);\r\n    }\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata metaFile = null;\r\n    try {\r\n        metaFile = store.retrieveMetadata(key);\r\n    } catch (IOException e) {\r\n        Throwable innerException = checkForAzureStorageException(e);\r\n        if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {\r\n            return false;\r\n        }\r\n        throw e;\r\n    }\r\n    if (null == metaFile) {\r\n        return false;\r\n    }\r\n    FileMetadata parentMetadata = null;\r\n    String parentKey = null;\r\n    if (parentPath != null) {\r\n        parentKey = pathToKey(parentPath);\r\n        try {\r\n            parentMetadata = store.retrieveMetadata(parentKey);\r\n        } catch (IOException e) {\r\n            Throwable innerException = checkForAzureStorageException(e);\r\n            if (innerException instanceof StorageException) {\r\n                if (isFileNotFoundException((StorageException) innerException)) {\r\n                    throw new IOException(\"File \" + f + \" has a parent directory \" + parentPath + \" whose metadata cannot be retrieved. Can't resolve\");\r\n                }\r\n            }\r\n            throw e;\r\n        }\r\n        if (parentMetadata == null) {\r\n            throw new IOException(\"File \" + f + \" has a parent directory \" + parentPath + \" whose metadata cannot be retrieved. Can't resolve\");\r\n        }\r\n        if (!parentMetadata.isDirectory()) {\r\n            throw new AzureException(\"File \" + f + \" has a parent directory \" + parentPath + \" which is also a file. Can't resolve.\");\r\n        }\r\n    }\r\n    if (!metaFile.isDirectory()) {\r\n        if (parentPath != null && parentPath.getParent() != null) {\r\n            if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\r\n                LOG.debug(\"Found an implicit parent directory while trying to\" + \" delete the file {}. Creating the directory blob for\" + \" it in {}.\", f, parentKey);\r\n                store.storeEmptyFolder(parentKey, createPermissionStatus(FsPermission.getDefault()));\r\n            } else {\r\n                if (!skipParentFolderLastModifiedTimeUpdate) {\r\n                    updateParentFolderLastModifiedTime(key);\r\n                }\r\n            }\r\n        }\r\n        if (isStickyBitCheckViolated(metaFile, parentMetadata)) {\r\n            throw new WasbAuthorizationException(String.format(\"%s has sticky bit set. \" + \"File %s cannot be deleted.\", parentPath, f));\r\n        }\r\n        try {\r\n            if (store.delete(key)) {\r\n                instrumentation.fileDeleted();\r\n            } else {\r\n                return false;\r\n            }\r\n        } catch (IOException e) {\r\n            Throwable innerException = checkForAzureStorageException(e);\r\n            if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {\r\n                return false;\r\n            }\r\n            throw e;\r\n        }\r\n    } else {\r\n        LOG.debug(\"Directory Delete encountered: {}\", f);\r\n        if (parentPath != null && parentPath.getParent() != null) {\r\n            if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\r\n                LOG.debug(\"Found an implicit parent directory while trying to\" + \" delete the directory {}. Creating the directory blob for\" + \" it in {}. \", f, parentKey);\r\n                store.storeEmptyFolder(parentKey, createPermissionStatus(FsPermission.getDefault()));\r\n            }\r\n        }\r\n        if (!metaFile.getKey().equals(\"/\") && isStickyBitCheckViolated(metaFile, parentMetadata)) {\r\n            throw new WasbAuthorizationException(String.format(\"%s has sticky bit set. \" + \"File %s cannot be deleted.\", parentPath, f));\r\n        }\r\n        ArrayList<FileMetadata> fileMetadataList = new ArrayList<>();\r\n        boolean isPartialDelete = false;\r\n        long start = Time.monotonicNow();\r\n        try {\r\n            isPartialDelete = getFolderContentsToDelete(metaFile, fileMetadataList);\r\n        } catch (IOException e) {\r\n            Throwable innerException = checkForAzureStorageException(e);\r\n            if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {\r\n                return false;\r\n            }\r\n            throw e;\r\n        }\r\n        long end = Time.monotonicNow();\r\n        LOG.debug(\"Time taken to list {} blobs for delete operation: {} ms\", fileMetadataList.size(), (end - start));\r\n        final FileMetadata[] contents = fileMetadataList.toArray(new FileMetadata[fileMetadataList.size()]);\r\n        if (contents.length > 0 && !recursive) {\r\n            throw new IOException(\"Non-recursive delete of non-empty directory \" + f);\r\n        }\r\n        AzureFileSystemThreadTask task = new AzureFileSystemThreadTask() {\r\n\r\n            @Override\r\n            public boolean execute(FileMetadata file) throws IOException {\r\n                if (!deleteFile(file.getKey(), file.isDirectory())) {\r\n                    LOG.warn(\"Attempt to delete non-existent {} {}\", file.isDirectory() ? \"directory\" : \"file\", file.getKey());\r\n                }\r\n                return true;\r\n            }\r\n        };\r\n        AzureFileSystemThreadPoolExecutor executor = getThreadPoolExecutor(this.deleteThreadCount, \"AzureBlobDeleteThread\", \"Delete\", key, AZURE_DELETE_THREADS);\r\n        if (!executor.executeParallel(contents, task)) {\r\n            LOG.error(\"Failed to delete files / subfolders in blob {}\", key);\r\n            return false;\r\n        }\r\n        if (metaFile.getKey().equals(\"/\")) {\r\n            LOG.error(\"Cannot delete root directory {}\", f);\r\n            return false;\r\n        }\r\n        if (isPartialDelete || (store.retrieveMetadata(metaFile.getKey()) != null && !deleteFile(metaFile.getKey(), metaFile.isDirectory()))) {\r\n            LOG.error(\"Failed delete directory : {}\", f);\r\n            return false;\r\n        }\r\n        Path parent = absolutePath.getParent();\r\n        if (parent != null && parent.getParent() != null) {\r\n            if (!skipParentFolderLastModifiedTimeUpdate) {\r\n                updateParentFolderLastModifiedTime(key);\r\n            }\r\n        }\r\n    }\r\n    LOG.debug(\"Delete Successful for : {}\", f);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "deleteWithoutAuth",
  "errType" : [ "IOException", "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 49,
  "sourceCodeText" : "boolean deleteWithoutAuth(Path f, boolean recursive, boolean skipParentFolderLastModifiedTimeUpdate) throws IOException\n{\r\n    LOG.debug(\"Deleting file: {}\", f);\r\n    Path absolutePath = makeAbsolute(f);\r\n    Path parentPath = absolutePath.getParent();\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata metaFile = null;\r\n    try {\r\n        metaFile = store.retrieveMetadata(key);\r\n    } catch (IOException e) {\r\n        Throwable innerException = checkForAzureStorageException(e);\r\n        if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {\r\n            return false;\r\n        }\r\n        throw e;\r\n    }\r\n    if (null == metaFile) {\r\n        return false;\r\n    }\r\n    if (!metaFile.isDirectory()) {\r\n        if (parentPath.getParent() != null) {\r\n            String parentKey = pathToKey(parentPath);\r\n            FileMetadata parentMetadata = null;\r\n            try {\r\n                parentMetadata = store.retrieveMetadata(parentKey);\r\n            } catch (IOException e) {\r\n                Throwable innerException = checkForAzureStorageException(e);\r\n                if (innerException instanceof StorageException) {\r\n                    if (isFileNotFoundException((StorageException) innerException)) {\r\n                        throw new IOException(\"File \" + f + \" has a parent directory \" + parentPath + \" whose metadata cannot be retrieved. Can't resolve\");\r\n                    }\r\n                }\r\n                throw e;\r\n            }\r\n            if (parentMetadata == null) {\r\n                throw new IOException(\"File \" + f + \" has a parent directory \" + parentPath + \" whose metadata cannot be retrieved. Can't resolve\");\r\n            }\r\n            if (!parentMetadata.isDirectory()) {\r\n                throw new AzureException(\"File \" + f + \" has a parent directory \" + parentPath + \" which is also a file. Can't resolve.\");\r\n            }\r\n            if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\r\n                LOG.debug(\"Found an implicit parent directory while trying to\" + \" delete the file {}. Creating the directory blob for\" + \" it in {}.\", f, parentKey);\r\n                store.storeEmptyFolder(parentKey, createPermissionStatus(FsPermission.getDefault()));\r\n            } else {\r\n                if (!skipParentFolderLastModifiedTimeUpdate) {\r\n                    updateParentFolderLastModifiedTime(key);\r\n                }\r\n            }\r\n        }\r\n        try {\r\n            if (store.delete(key)) {\r\n                instrumentation.fileDeleted();\r\n            } else {\r\n                return false;\r\n            }\r\n        } catch (IOException e) {\r\n            Throwable innerException = checkForAzureStorageException(e);\r\n            if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {\r\n                return false;\r\n            }\r\n            throw e;\r\n        }\r\n    } else {\r\n        LOG.debug(\"Directory Delete encountered: {}\", f);\r\n        if (parentPath.getParent() != null) {\r\n            String parentKey = pathToKey(parentPath);\r\n            FileMetadata parentMetadata = null;\r\n            try {\r\n                parentMetadata = store.retrieveMetadata(parentKey);\r\n            } catch (IOException e) {\r\n                Throwable innerException = checkForAzureStorageException(e);\r\n                if (innerException instanceof StorageException) {\r\n                    if (isFileNotFoundException((StorageException) innerException)) {\r\n                        throw new IOException(\"File \" + f + \" has a parent directory \" + parentPath + \" whose metadata cannot be retrieved. Can't resolve\");\r\n                    }\r\n                }\r\n                throw e;\r\n            }\r\n            if (parentMetadata == null) {\r\n                throw new IOException(\"File \" + f + \" has a parent directory \" + parentPath + \" whose metadata cannot be retrieved. Can't resolve\");\r\n            }\r\n            if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\r\n                LOG.debug(\"Found an implicit parent directory while trying to\" + \" delete the directory {}. Creating the directory blob for\" + \" it in {}. \", f, parentKey);\r\n                store.storeEmptyFolder(parentKey, createPermissionStatus(FsPermission.getDefault()));\r\n            }\r\n        }\r\n        long start = Time.monotonicNow();\r\n        final FileMetadata[] contents;\r\n        try {\r\n            contents = store.list(key, AZURE_LIST_ALL, AZURE_UNBOUNDED_DEPTH);\r\n        } catch (IOException e) {\r\n            Throwable innerException = checkForAzureStorageException(e);\r\n            if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {\r\n                return false;\r\n            }\r\n            throw e;\r\n        }\r\n        long end = Time.monotonicNow();\r\n        LOG.debug(\"Time taken to list {} blobs for delete operation: {} ms\", contents.length, (end - start));\r\n        if (contents.length > 0) {\r\n            if (!recursive) {\r\n                throw new IOException(\"Non-recursive delete of non-empty directory \" + f);\r\n            }\r\n        }\r\n        AzureFileSystemThreadTask task = new AzureFileSystemThreadTask() {\r\n\r\n            @Override\r\n            public boolean execute(FileMetadata file) throws IOException {\r\n                if (!deleteFile(file.getKey(), file.isDirectory())) {\r\n                    LOG.warn(\"Attempt to delete non-existent {} {}\", file.isDirectory() ? \"directory\" : \"file\", file.getKey());\r\n                }\r\n                return true;\r\n            }\r\n        };\r\n        AzureFileSystemThreadPoolExecutor executor = getThreadPoolExecutor(this.deleteThreadCount, \"AzureBlobDeleteThread\", \"Delete\", key, AZURE_DELETE_THREADS);\r\n        if (!executor.executeParallel(contents, task)) {\r\n            LOG.error(\"Failed to delete files / subfolders in blob {}\", key);\r\n            return false;\r\n        }\r\n        if (store.retrieveMetadata(metaFile.getKey()) != null && !deleteFile(metaFile.getKey(), metaFile.isDirectory())) {\r\n            LOG.error(\"Failed delete directory : {}\", f);\r\n            return false;\r\n        }\r\n        Path parent = absolutePath.getParent();\r\n        if (parent != null && parent.getParent() != null) {\r\n            if (!skipParentFolderLastModifiedTimeUpdate) {\r\n                updateParentFolderLastModifiedTime(key);\r\n            }\r\n        }\r\n    }\r\n    LOG.debug(\"Delete Successful for : {}\", f);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean delete(Path f, boolean recursive, boolean skipParentFolderLastModifiedTimeUpdate) throws IOException\n{\r\n    if (this.azureAuthorization) {\r\n        return deleteWithAuthEnabled(f, recursive, skipParentFolderLastModifiedTimeUpdate);\r\n    } else {\r\n        return deleteWithoutAuth(f, recursive, skipParentFolderLastModifiedTimeUpdate);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getThreadPoolExecutor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureFileSystemThreadPoolExecutor getThreadPoolExecutor(int threadCount, String threadNamePrefix, String operation, String key, String config)\n{\r\n    return new AzureFileSystemThreadPoolExecutor(threadCount, threadNamePrefix, operation, key, config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getFolderContentsToDelete",
  "errType" : [ "WasbAuthorizationException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "boolean getFolderContentsToDelete(FileMetadata folderToDelete, ArrayList<FileMetadata> finalList) throws IOException\n{\r\n    final int maxListingDepth = 1;\r\n    Stack<FileMetadata> foldersToProcess = new Stack<FileMetadata>();\r\n    HashMap<String, FileMetadata> folderContentsMap = new HashMap<String, FileMetadata>();\r\n    boolean isPartialDelete = false;\r\n    Path pathToDelete = makeAbsolute(folderToDelete.getPath());\r\n    foldersToProcess.push(folderToDelete);\r\n    while (!foldersToProcess.empty()) {\r\n        FileMetadata currentFolder = foldersToProcess.pop();\r\n        Path currentPath = makeAbsolute(currentFolder.getPath());\r\n        boolean canDeleteChildren = true;\r\n        try {\r\n            performAuthCheck(currentPath, WasbAuthorizationOperations.WRITE, \"delete\", pathToDelete);\r\n        } catch (WasbAuthorizationException we) {\r\n            LOG.debug(\"Authorization check failed for {}\", currentPath);\r\n            canDeleteChildren = false;\r\n        }\r\n        if (canDeleteChildren) {\r\n            FileMetadata[] fileMetadataList = store.list(currentFolder.getKey(), AZURE_LIST_ALL, maxListingDepth);\r\n            for (FileMetadata childItem : fileMetadataList) {\r\n                if (isStickyBitCheckViolated(childItem, currentFolder, false)) {\r\n                    canDeleteChildren = false;\r\n                    Path filePath = makeAbsolute(childItem.getPath());\r\n                    LOG.error(\"User does not have permissions to delete {}. \" + \"Parent directory has sticky bit set.\", filePath);\r\n                } else {\r\n                    if (childItem.isDirectory()) {\r\n                        foldersToProcess.push(childItem);\r\n                    }\r\n                    folderContentsMap.put(childItem.getKey(), childItem);\r\n                }\r\n            }\r\n        } else {\r\n            LOG.error(\"Authorization check failed. Files or folders under {} \" + \"will not be processed for deletion.\", currentPath);\r\n        }\r\n        if (!canDeleteChildren) {\r\n            String pathToRemove = currentFolder.getKey();\r\n            while (!pathToRemove.equals(folderToDelete.getKey())) {\r\n                if (folderContentsMap.containsKey(pathToRemove)) {\r\n                    LOG.debug(\"Cannot delete {} since some of its contents \" + \"cannot be deleted\", pathToRemove);\r\n                    folderContentsMap.remove(pathToRemove);\r\n                }\r\n                Path parentPath = keyToPath(pathToRemove).getParent();\r\n                pathToRemove = pathToKey(parentPath);\r\n            }\r\n            isPartialDelete = true;\r\n        }\r\n    }\r\n    for (HashMap.Entry<String, FileMetadata> entry : folderContentsMap.entrySet()) {\r\n        finalList.add(entry.getValue());\r\n    }\r\n    return isPartialDelete;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isStickyBitCheckViolated",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isStickyBitCheckViolated(FileMetadata metaData, FileMetadata parentMetadata, boolean throwOnException) throws IOException\n{\r\n    try {\r\n        return isStickyBitCheckViolated(metaData, parentMetadata);\r\n    } catch (FileNotFoundException ex) {\r\n        if (throwOnException) {\r\n            throw ex;\r\n        } else {\r\n            LOG.debug(\"Encountered FileNotFoundException while performing \" + \"stickybit check operation for {}\", metaData.getKey());\r\n            return true;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isStickyBitCheckViolated",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean isStickyBitCheckViolated(FileMetadata metaData, FileMetadata parentMetadata) throws IOException\n{\r\n    if (!this.azureAuthorization) {\r\n        return false;\r\n    }\r\n    if (parentMetadata == null) {\r\n        throw new FileNotFoundException(String.format(\"Parent metadata for '%s' not found!\", metaData.getKey()));\r\n    }\r\n    if (!parentMetadata.getPermission().getStickyBit()) {\r\n        return false;\r\n    }\r\n    String currentUser = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    String parentDirectoryOwner = parentMetadata.getOwner();\r\n    String currentFileOwner = metaData.getOwner();\r\n    if ((parentDirectoryOwner.equalsIgnoreCase(currentUser)) || currentFileOwner.equalsIgnoreCase(currentUser)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "deleteFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean deleteFile(String path, boolean isDir) throws IOException\n{\r\n    if (!store.delete(path)) {\r\n        return false;\r\n    }\r\n    if (isDir) {\r\n        instrumentation.directoryDeleted();\r\n    } else {\r\n        instrumentation.fileDeleted();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileStatus(Path f) throws FileNotFoundException, IOException\n{\r\n    LOG.debug(\"Getting the file status for {}\", f.toString());\r\n    return getFileStatusInternal(f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "existsInternal",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean existsInternal(Path f) throws IOException\n{\r\n    try {\r\n        this.getFileStatusInternal(f);\r\n        return true;\r\n    } catch (FileNotFoundException fnfe) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getFileStatusInternal",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "FileStatus getFileStatusInternal(Path f) throws FileNotFoundException, IOException\n{\r\n    Path absolutePath = makeAbsolute(f);\r\n    String key = pathToKey(absolutePath);\r\n    if (key.length() == 0) {\r\n        return new FileStatus(0, true, 1, store.getHadoopBlockSize(), 0, 0, FsPermission.getDefault(), \"\", \"\", absolutePath.makeQualified(getUri(), getWorkingDirectory()));\r\n    }\r\n    FileMetadata meta = null;\r\n    try {\r\n        meta = store.retrieveMetadata(key);\r\n    } catch (Exception ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"%s is not found\", key));\r\n        }\r\n        throw ex;\r\n    }\r\n    if (meta != null) {\r\n        if (meta.isDirectory()) {\r\n            LOG.debug(\"Path {} is a folder.\", f.toString());\r\n            if (conditionalRedoFolderRename(f)) {\r\n                throw new FileNotFoundException(absolutePath + \": No such file or directory.\");\r\n            }\r\n            return updateFileStatusPath(meta, absolutePath);\r\n        }\r\n        LOG.debug(\"Found the path: {} as a file.\", f.toString());\r\n        return updateFileStatusPath(meta, absolutePath);\r\n    }\r\n    throw new FileNotFoundException(absolutePath + \": No such file or directory.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "conditionalRedoFolderRename",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean conditionalRedoFolderRename(Path f) throws IOException\n{\r\n    if (f.getName().equals(\"\")) {\r\n        return false;\r\n    }\r\n    Path absoluteRenamePendingFile = renamePendingFilePath(f);\r\n    if (existsInternal(absoluteRenamePendingFile)) {\r\n        FolderRenamePending pending = new FolderRenamePending(absoluteRenamePendingFile, this);\r\n        pending.redo();\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "renamePendingFilePath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path renamePendingFilePath(Path f)\n{\r\n    Path absPath = makeAbsolute(f);\r\n    String key = pathToKey(absPath);\r\n    key += \"-RenamePending.json\";\r\n    return keyToPath(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getUri",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URI getUri()\n{\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "listStatus",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "FileStatus[] listStatus(Path f) throws FileNotFoundException, IOException\n{\r\n    LOG.debug(\"Listing status for {}\", f.toString());\r\n    Path absolutePath = makeAbsolute(f);\r\n    performAuthCheck(absolutePath, WasbAuthorizationOperations.READ, \"liststatus\", absolutePath);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata meta = null;\r\n    try {\r\n        meta = store.retrieveMetadata(key);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"%s is not found\", f));\r\n        }\r\n        throw ex;\r\n    }\r\n    if (meta == null) {\r\n        LOG.debug(\"Did not find any metadata for path: {}\", key);\r\n        throw new FileNotFoundException(f + \" is not found\");\r\n    }\r\n    if (!meta.isDirectory()) {\r\n        LOG.debug(\"Found path as a file\");\r\n        return new FileStatus[] { updateFileStatusPath(meta, absolutePath) };\r\n    }\r\n    FileMetadata[] listing;\r\n    listing = listWithErrorHandling(key, AZURE_LIST_ALL, 1);\r\n    boolean renamed = conditionalRedoFolderRenames(listing);\r\n    if (renamed) {\r\n        listing = listWithErrorHandling(key, AZURE_LIST_ALL, 1);\r\n    }\r\n    FileMetadata[] result = null;\r\n    if (key.equals(\"/\")) {\r\n        ArrayList<FileMetadata> status = new ArrayList<>(listing.length);\r\n        for (FileMetadata fileMetadata : listing) {\r\n            if (fileMetadata.isDirectory()) {\r\n                if (fileMetadata.getKey().equals(AZURE_TEMP_FOLDER)) {\r\n                    continue;\r\n                }\r\n                status.add(updateFileStatusPath(fileMetadata, fileMetadata.getPath()));\r\n            } else {\r\n                status.add(updateFileStatusPath(fileMetadata, fileMetadata.getPath()));\r\n            }\r\n        }\r\n        result = status.toArray(new FileMetadata[0]);\r\n    } else {\r\n        for (int i = 0; i < listing.length; i++) {\r\n            FileMetadata fileMetadata = listing[i];\r\n            listing[i] = updateFileStatusPath(fileMetadata, fileMetadata.getPath());\r\n        }\r\n        result = listing;\r\n    }\r\n    LOG.debug(\"Found path as a directory with {}\" + \" files in it.\", result.length);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "listWithErrorHandling",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileMetadata[] listWithErrorHandling(String prefix, final int maxListingCount, final int maxListingDepth) throws IOException\n{\r\n    try {\r\n        return store.list(prefix, maxListingCount, maxListingDepth);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"%s is not found\", prefix));\r\n        }\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "conditionalRedoFolderRenames",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean conditionalRedoFolderRenames(FileMetadata[] listing) throws IllegalArgumentException, IOException\n{\r\n    boolean renamed = false;\r\n    for (FileMetadata fileMetadata : listing) {\r\n        Path subpath = fileMetadata.getPath();\r\n        if (isRenamePendingFile(subpath)) {\r\n            FolderRenamePending pending = new FolderRenamePending(subpath, this);\r\n            pending.redo();\r\n            renamed = true;\r\n        }\r\n    }\r\n    return renamed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isRenamePendingFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRenamePendingFile(Path path)\n{\r\n    return path.toString().endsWith(FolderRenamePending.SUFFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateFileStatusPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileMetadata updateFileStatusPath(FileMetadata meta, Path path)\n{\r\n    meta.setPath(path.makeQualified(getUri(), getWorkingDirectory()));\r\n    meta.removeKey();\r\n    return meta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "applyUMask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsPermission applyUMask(final FsPermission permission, final UMaskApplyMode applyMode)\n{\r\n    FsPermission newPermission = new FsPermission(permission);\r\n    if (applyMode == UMaskApplyMode.NewFile || applyMode == UMaskApplyMode.NewDirectory) {\r\n        newPermission = newPermission.applyUMask(FsPermission.getUMask(getConf()));\r\n    }\r\n    return newPermission;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createPermissionStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "PermissionStatus createPermissionStatus(FsPermission permission) throws IOException\n{\r\n    return new PermissionStatus(UserGroupInformation.getCurrentUser().getShortUserName(), getConf().get(AZURE_DEFAULT_GROUP_PROPERTY_NAME, AZURE_DEFAULT_GROUP_DEFAULT), permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getAncestor",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Path getAncestor(Path f) throws IOException\n{\r\n    for (Path current = f, parent = current.getParent(); parent != null; current = parent, parent = current.getParent()) {\r\n        String currentKey = pathToKey(current);\r\n        FileMetadata currentMetadata = store.retrieveMetadata(currentKey);\r\n        if (currentMetadata != null && currentMetadata.isDirectory()) {\r\n            Path ancestor = currentMetadata.getPath();\r\n            LOG.debug(\"Found ancestor {}, for path: {}\", ancestor.toString(), f.toString());\r\n            return ancestor;\r\n        }\r\n    }\r\n    return new Path(\"/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission) throws IOException\n{\r\n    return mkdirs(f, permission, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "boolean mkdirs(Path f, FsPermission permission, boolean noUmask) throws IOException\n{\r\n    LOG.debug(\"Creating directory: {}\", f.toString());\r\n    if (containsColon(f)) {\r\n        throw new IOException(\"Cannot create directory \" + f + \" through WASB that has colons in the name\");\r\n    }\r\n    Path absolutePath = makeAbsolute(f);\r\n    Path ancestor = getAncestor(absolutePath);\r\n    if (absolutePath.equals(ancestor)) {\r\n        return true;\r\n    }\r\n    performAuthCheck(ancestor, WasbAuthorizationOperations.WRITE, \"mkdirs\", absolutePath);\r\n    PermissionStatus permissionStatus = null;\r\n    if (noUmask) {\r\n        permissionStatus = createPermissionStatus(applyUMask(FsPermission.createImmutable((short) (permission.toShort() | USER_WX_PERMISION)), UMaskApplyMode.NewDirectoryNoUmask));\r\n    } else {\r\n        permissionStatus = createPermissionStatus(applyUMask(permission, UMaskApplyMode.NewDirectory));\r\n    }\r\n    ArrayList<String> keysToCreateAsFolder = new ArrayList<String>();\r\n    for (Path current = absolutePath, parent = current.getParent(); parent != null; current = parent, parent = current.getParent()) {\r\n        String currentKey = pathToKey(current);\r\n        FileMetadata currentMetadata = store.retrieveMetadata(currentKey);\r\n        if (currentMetadata != null && !currentMetadata.isDirectory()) {\r\n            throw new FileAlreadyExistsException(\"Cannot create directory \" + f + \" because \" + current + \" is an existing file.\");\r\n        } else if (currentMetadata == null) {\r\n            keysToCreateAsFolder.add(currentKey);\r\n        }\r\n    }\r\n    for (String currentKey : keysToCreateAsFolder) {\r\n        store.storeEmptyFolder(currentKey, permissionStatus);\r\n    }\r\n    instrumentation.directoryCreated();\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataInputStream open(Path f, int bufferSize) throws FileNotFoundException, IOException\n{\r\n    return open(f, bufferSize, Optional.empty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "open",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "FSDataInputStream open(Path f, int bufferSize, Optional<Configuration> options) throws FileNotFoundException, IOException\n{\r\n    LOG.debug(\"Opening file: {}\", f.toString());\r\n    Path absolutePath = makeAbsolute(f);\r\n    performAuthCheck(absolutePath, WasbAuthorizationOperations.READ, \"read\", absolutePath);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata meta = null;\r\n    try {\r\n        meta = store.retrieveMetadata(key);\r\n    } catch (Exception ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"%s is not found\", key));\r\n        }\r\n        throw ex;\r\n    }\r\n    if (meta == null) {\r\n        throw new FileNotFoundException(f.toString());\r\n    }\r\n    if (meta.isDirectory()) {\r\n        throw new FileNotFoundException(f.toString() + \" is a directory not a file.\");\r\n    }\r\n    InputStream inputStream;\r\n    try {\r\n        inputStream = store.retrieve(key, 0, options);\r\n    } catch (Exception ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"%s is not found\", key));\r\n        }\r\n        throw ex;\r\n    }\r\n    return new FSDataInputStream(new BufferedFSInputStream(new NativeAzureFsInputStream(inputStream, key, meta.getLen()), bufferSize));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "openFileWithOptions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CompletableFuture<FSDataInputStream> openFileWithOptions(Path path, OpenFileParameters parameters) throws IOException\n{\r\n    AbstractFSBuilderImpl.rejectUnknownMandatoryKeys(parameters.getMandatoryKeys(), Collections.emptySet(), \"for \" + path);\r\n    return LambdaUtils.eval(new CompletableFuture<>(), () -> open(path, parameters.getBufferSize(), Optional.of(parameters.getOptions())));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "rename",
  "errType" : [ "FileNotFoundException", "IOException", "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 51,
  "sourceCodeText" : "boolean rename(Path src, Path dst) throws FileNotFoundException, IOException\n{\r\n    FolderRenamePending renamePending = null;\r\n    LOG.debug(\"Moving {} to {}\", src, dst);\r\n    if (containsColon(dst)) {\r\n        throw new IOException(\"Cannot rename to file \" + dst + \" through WASB that has colons in the name\");\r\n    }\r\n    Path absoluteSrcPath = makeAbsolute(src);\r\n    Path srcParentFolder = absoluteSrcPath.getParent();\r\n    if (srcParentFolder == null) {\r\n        return false;\r\n    }\r\n    String srcKey = pathToKey(absoluteSrcPath);\r\n    if (srcKey.length() == 0) {\r\n        return false;\r\n    }\r\n    performAuthCheck(srcParentFolder, WasbAuthorizationOperations.WRITE, \"rename\", absoluteSrcPath);\r\n    if (this.azureAuthorization) {\r\n        try {\r\n            performStickyBitCheckForRenameOperation(absoluteSrcPath, srcParentFolder);\r\n        } catch (FileNotFoundException ex) {\r\n            return false;\r\n        } catch (IOException ex) {\r\n            Throwable innerException = checkForAzureStorageException(ex);\r\n            if (innerException instanceof StorageException && isFileNotFoundException((StorageException) innerException)) {\r\n                LOG.debug(\"Encountered FileNotFound Exception when performing sticky bit check \" + \"on {}. Failing rename\", srcKey);\r\n                return false;\r\n            }\r\n            throw ex;\r\n        }\r\n    }\r\n    Path absoluteDstPath = makeAbsolute(dst);\r\n    Path dstParentFolder = absoluteDstPath.getParent();\r\n    String dstKey = pathToKey(absoluteDstPath);\r\n    FileMetadata dstMetadata = null;\r\n    try {\r\n        dstMetadata = store.retrieveMetadata(dstKey);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException) {\r\n            if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n                LOG.debug(\"BlobNotFound exception encountered for Destination key : {}. \" + \"Swallowing the exception to handle race condition gracefully\", dstKey);\r\n            }\r\n        } else {\r\n            throw ex;\r\n        }\r\n    }\r\n    if (dstMetadata != null && dstMetadata.isDirectory()) {\r\n        performAuthCheck(absoluteDstPath, WasbAuthorizationOperations.WRITE, \"rename\", absoluteDstPath);\r\n        dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));\r\n        LOG.debug(\"Destination {} \" + \" is a directory, adjusted the destination to be {}\", dst, dstKey);\r\n    } else if (dstMetadata != null) {\r\n        LOG.debug(\"Destination {}\" + \" is an already existing file, failing the rename.\", dst);\r\n        return false;\r\n    } else {\r\n        FileMetadata parentOfDestMetadata = null;\r\n        try {\r\n            parentOfDestMetadata = store.retrieveMetadata(pathToKey(absoluteDstPath.getParent()));\r\n        } catch (IOException ex) {\r\n            Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n            if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n                LOG.debug(\"Parent of destination {} doesn't exists. Failing rename\", dst);\r\n                return false;\r\n            }\r\n            throw ex;\r\n        }\r\n        if (parentOfDestMetadata == null) {\r\n            LOG.debug(\"Parent of the destination {}\" + \" doesn't exist, failing the rename.\", dst);\r\n            return false;\r\n        } else if (!parentOfDestMetadata.isDirectory()) {\r\n            LOG.debug(\"Parent of the destination {}\" + \" is a file, failing the rename.\", dst);\r\n            return false;\r\n        } else {\r\n            performAuthCheck(dstParentFolder, WasbAuthorizationOperations.WRITE, \"rename\", absoluteDstPath);\r\n        }\r\n    }\r\n    FileMetadata srcMetadata = null;\r\n    try {\r\n        srcMetadata = store.retrieveMetadata(srcKey);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            LOG.debug(\"Source {} doesn't exists. Failing rename\", src);\r\n            return false;\r\n        }\r\n        throw ex;\r\n    }\r\n    if (srcMetadata == null) {\r\n        LOG.debug(\"Source {} doesn't exist, failing the rename.\", src);\r\n        return false;\r\n    } else if (!srcMetadata.isDirectory()) {\r\n        LOG.debug(\"Source {} found as a file, renaming.\", src);\r\n        try {\r\n            store.rename(srcKey, dstKey, false, null, false);\r\n        } catch (IOException ex) {\r\n            Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n            if (innerException instanceof StorageException) {\r\n                if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n                    LOG.debug(\"BlobNotFoundException encountered. Failing rename\", src);\r\n                    return false;\r\n                }\r\n                if (NativeAzureFileSystemHelper.isBlobAlreadyExistsConflict((StorageException) innerException)) {\r\n                    LOG.debug(\"Destination BlobAlreadyExists. Failing rename\", src);\r\n                    return false;\r\n                }\r\n            }\r\n            throw ex;\r\n        }\r\n    } else {\r\n        renamePending = prepareAtomicFolderRename(srcKey, dstKey);\r\n        renamePending.execute();\r\n        LOG.debug(\"Renamed {} to {} successfully.\", src, dst);\r\n        renamePending.cleanup();\r\n        return true;\r\n    }\r\n    updateParentFolderLastModifiedTime(srcKey);\r\n    updateParentFolderLastModifiedTime(dstKey);\r\n    LOG.debug(\"Renamed {} to {} successfully.\", src, dst);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateParentFolderLastModifiedTime",
  "errType" : [ "AzureException", "Exception", "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void updateParentFolderLastModifiedTime(String key) throws IOException\n{\r\n    Path parent = makeAbsolute(keyToPath(key)).getParent();\r\n    if (parent != null && parent.getParent() != null) {\r\n        String parentKey = pathToKey(parent);\r\n        FileMetadata parentMetadata = store.retrieveMetadata(parentKey);\r\n        if (parentMetadata != null) {\r\n            if (parentMetadata.isDirectory() && parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\r\n                store.storeEmptyFolder(parentKey, createPermissionStatus(FsPermission.getDefault()));\r\n            }\r\n            if (store.isAtomicRenameKey(parentKey)) {\r\n                SelfRenewingLease lease = null;\r\n                try {\r\n                    lease = leaseSourceFolder(parentKey);\r\n                    store.updateFolderLastModifiedTime(parentKey, lease);\r\n                } catch (AzureException e) {\r\n                    String errorCode = \"\";\r\n                    try {\r\n                        StorageException e2 = (StorageException) e.getCause();\r\n                        errorCode = e2.getErrorCode();\r\n                    } catch (Exception e3) {\r\n                    }\r\n                    if (errorCode.equals(\"BlobNotFound\")) {\r\n                        throw new FileNotFoundException(\"Folder does not exist: \" + parentKey);\r\n                    }\r\n                    LOG.warn(\"Got unexpected exception trying to get lease on {}. {}\", parentKey, e.getMessage());\r\n                    throw e;\r\n                } finally {\r\n                    try {\r\n                        if (lease != null) {\r\n                            lease.free();\r\n                        }\r\n                    } catch (Exception e) {\r\n                        LOG.error(\"Unable to free lease on {}\", parentKey, e);\r\n                    }\r\n                }\r\n            } else {\r\n                store.updateFolderLastModifiedTime(parentKey, null);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "prepareAtomicFolderRename",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FolderRenamePending prepareAtomicFolderRename(String srcKey, String dstKey) throws IOException\n{\r\n    if (store.isAtomicRenameKey(srcKey)) {\r\n        SelfRenewingLease lease = leaseSourceFolder(srcKey);\r\n        FolderRenamePending renamePending = new FolderRenamePending(srcKey, dstKey, lease, this);\r\n        renamePending.writeFile(this);\r\n        return renamePending;\r\n    } else {\r\n        FolderRenamePending renamePending = new FolderRenamePending(srcKey, dstKey, null, this);\r\n        return renamePending;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "leaseSourceFolder",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SelfRenewingLease leaseSourceFolder(String srcKey) throws AzureException\n{\r\n    return store.acquireLease(srcKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "performStickyBitCheckForRenameOperation",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void performStickyBitCheckForRenameOperation(Path srcPath, Path srcParentPath) throws FileNotFoundException, WasbAuthorizationException, IOException\n{\r\n    String srcKey = pathToKey(srcPath);\r\n    FileMetadata srcMetadata = null;\r\n    srcMetadata = store.retrieveMetadata(srcKey);\r\n    if (srcMetadata == null) {\r\n        LOG.debug(\"Source {} doesn't exist. Failing rename.\", srcPath);\r\n        throw new FileNotFoundException(String.format(\"%s does not exist.\", srcPath));\r\n    }\r\n    String parentkey = pathToKey(srcParentPath);\r\n    FileMetadata parentMetadata = store.retrieveMetadata(parentkey);\r\n    if (parentMetadata == null) {\r\n        LOG.debug(\"Path {} doesn't exist, failing rename.\", srcParentPath);\r\n        throw new FileNotFoundException(String.format(\"%s does not exist.\", parentkey));\r\n    }\r\n    if (isStickyBitCheckViolated(srcMetadata, parentMetadata)) {\r\n        throw new WasbAuthorizationException(String.format(\"Rename operation for %s is not permitted.\" + \" Details : Stickybit check failed.\", srcPath));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWorkingDirectory(Path newDir)\n{\r\n    workingDir = makeAbsolute(newDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    return workingDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setPermission",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void setPermission(Path p, FsPermission permission) throws FileNotFoundException, IOException\n{\r\n    Path absolutePath = makeAbsolute(p);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata metadata = null;\r\n    try {\r\n        metadata = store.retrieveMetadata(key);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"File %s doesn't exists.\", p));\r\n        }\r\n        throw ex;\r\n    }\r\n    if (metadata == null) {\r\n        throw new FileNotFoundException(\"File doesn't exist: \" + p);\r\n    }\r\n    if (azureAuthorization) {\r\n        UserGroupInformation currentUgi = UserGroupInformation.getCurrentUser();\r\n        if (!isAllowedUser(currentUgi.getShortUserName(), chmodAllowedUsers) && !isAllowedUser(currentUgi.getShortUserName(), daemonUsers)) {\r\n            String owner = metadata.getOwner();\r\n            if (!currentUgi.getShortUserName().equals(owner)) {\r\n                throw new WasbAuthorizationException(String.format(\"user '%s' does not have the privilege to \" + \"change the permission of files/folders.\", currentUgi.getShortUserName()));\r\n            }\r\n        }\r\n    }\r\n    permission = applyUMask(permission, metadata.isDirectory() ? UMaskApplyMode.ChangeExistingDirectory : UMaskApplyMode.ChangeExistingFile);\r\n    if (metadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\r\n        store.storeEmptyFolder(key, createPermissionStatus(permission));\r\n    } else if (!metadata.getPermission().equals(permission)) {\r\n        store.changePermissionStatus(key, new PermissionStatus(metadata.getOwner(), metadata.getGroup(), permission));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setOwner",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void setOwner(Path p, String username, String groupname) throws IOException\n{\r\n    Path absolutePath = makeAbsolute(p);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata metadata = null;\r\n    try {\r\n        metadata = store.retrieveMetadata(key);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(String.format(\"File %s doesn't exists.\", p));\r\n        }\r\n        throw ex;\r\n    }\r\n    if (metadata == null) {\r\n        throw new FileNotFoundException(\"File doesn't exist: \" + p);\r\n    }\r\n    if (this.azureAuthorization && username != null) {\r\n        UserGroupInformation currentUgi = UserGroupInformation.getCurrentUser();\r\n        if (!isAllowedUser(currentUgi.getShortUserName(), chownAllowedUsers)) {\r\n            throw new WasbAuthorizationException(String.format(\"user '%s' does not have the privilege to change \" + \"the ownership of files/folders.\", currentUgi.getShortUserName()));\r\n        }\r\n    }\r\n    PermissionStatus newPermissionStatus = new PermissionStatus(username == null ? metadata.getOwner() : username, groupname == null ? metadata.getGroup() : groupname, metadata.getPermission());\r\n    if (metadata.getBlobMaterialization() == BlobMaterialization.Implicit) {\r\n        store.storeEmptyFolder(key, newPermissionStatus);\r\n    } else {\r\n        store.changePermissionStatus(key, newPermissionStatus);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setXAttr",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setXAttr(Path path, String xAttrName, byte[] value, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    Path absolutePath = makeAbsolute(path);\r\n    performAuthCheck(absolutePath, WasbAuthorizationOperations.WRITE, \"setXAttr\", absolutePath);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata metadata;\r\n    try {\r\n        metadata = store.retrieveMetadata(key);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(\"File \" + path + \" doesn't exists.\");\r\n        }\r\n        throw ex;\r\n    }\r\n    if (metadata == null) {\r\n        throw new FileNotFoundException(\"File doesn't exist: \" + path);\r\n    }\r\n    boolean xAttrExists = store.retrieveAttribute(key, xAttrName) != null;\r\n    XAttrSetFlag.validate(xAttrName, xAttrExists, flag);\r\n    store.storeAttribute(key, xAttrName, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getXAttr",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "byte[] getXAttr(Path path, String xAttrName) throws IOException\n{\r\n    Path absolutePath = makeAbsolute(path);\r\n    performAuthCheck(absolutePath, WasbAuthorizationOperations.READ, \"getXAttr\", absolutePath);\r\n    String key = pathToKey(absolutePath);\r\n    FileMetadata metadata;\r\n    try {\r\n        metadata = store.retrieveMetadata(key);\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException)) {\r\n            throw new FileNotFoundException(\"File \" + path + \" doesn't exists.\");\r\n        }\r\n        throw ex;\r\n    }\r\n    if (metadata == null) {\r\n        throw new FileNotFoundException(\"File doesn't exist: \" + path);\r\n    }\r\n    return store.retrieveAttribute(key, xAttrName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isAllowedUser",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean isAllowedUser(String username, List<String> userList)\n{\r\n    if (null == userList || userList.isEmpty()) {\r\n        return false;\r\n    }\r\n    boolean shouldSkipUserCheck = userList.size() == 1 && userList.get(0).equals(\"*\");\r\n    if (!shouldSkipUserCheck) {\r\n        Preconditions.checkArgument(!userList.contains(\"*\"), \"User list must contain either '*' or a list of user names,\" + \" but not both.\");\r\n        return userList.contains(username);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (isClosed) {\r\n        return;\r\n    }\r\n    super.close();\r\n    store.close();\r\n    long startTime = System.currentTimeMillis();\r\n    if (!getConf().getBoolean(SKIP_AZURE_METRICS_PROPERTY_NAME, false)) {\r\n        AzureFileSystemMetricsSystem.unregisterSource(metricsSourceName);\r\n        AzureFileSystemMetricsSystem.fileSystemClosed();\r\n    }\r\n    LOG.debug(\"Submitting metrics when file system closed took {} ms.\", (System.currentTimeMillis() - startTime));\r\n    isClosed = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<?> getDelegationToken(final String renewer) throws IOException\n{\r\n    if (kerberosSupportEnabled) {\r\n        return wasbDelegationTokenManager.getDelegationToken(renewer);\r\n    } else {\r\n        return super.getDelegationToken(renewer);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "access",
  "errType" : [ "WasbAuthorizationException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void access(Path path, FsAction mode) throws IOException\n{\r\n    if (azureAuthorization && authorizer != null) {\r\n        try {\r\n            getFileStatus(path);\r\n            switch(mode) {\r\n                case READ:\r\n                case READ_EXECUTE:\r\n                    performAuthCheck(path, WasbAuthorizationOperations.READ, \"access\", path);\r\n                    break;\r\n                case WRITE:\r\n                case WRITE_EXECUTE:\r\n                    performAuthCheck(path, WasbAuthorizationOperations.WRITE, \"access\", path);\r\n                    break;\r\n                case READ_WRITE:\r\n                case ALL:\r\n                    performAuthCheck(path, WasbAuthorizationOperations.READ, \"access\", path);\r\n                    performAuthCheck(path, WasbAuthorizationOperations.WRITE, \"access\", path);\r\n                    break;\r\n                case EXECUTE:\r\n                case NONE:\r\n                default:\r\n                    break;\r\n            }\r\n        } catch (WasbAuthorizationException wae) {\r\n            throw new AccessControlException(wae);\r\n        }\r\n    } else {\r\n        super.access(path, mode);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "containsColon",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean containsColon(Path p)\n{\r\n    return p.toUri().getPath().toString().contains(\":\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "handleFilesWithDanglingTempData",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void handleFilesWithDanglingTempData(Path root, DanglingFileHandler handler) throws IOException\n{\r\n    long cutoffForDangling = new Date().getTime() - getConf().getInt(AZURE_TEMP_EXPIRY_PROPERTY_NAME, AZURE_TEMP_EXPIRY_DEFAULT) * 1000;\r\n    FileMetadata[] listing = store.list(pathToKey(root), AZURE_LIST_ALL, AZURE_UNBOUNDED_DEPTH);\r\n    for (FileMetadata file : listing) {\r\n        if (!file.isDirectory()) {\r\n            String link = store.getLinkInFileMetadata(file.getKey());\r\n            if (link != null) {\r\n                FileMetadata linkMetadata = store.retrieveMetadata(link);\r\n                if (linkMetadata != null && linkMetadata.getModificationTime() >= cutoffForDangling) {\r\n                    handler.handleFile(file, linkMetadata);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "recoverFilesWithDanglingTempData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void recoverFilesWithDanglingTempData(Path root, Path destination) throws IOException\n{\r\n    LOG.debug(\"Recovering files with dangling temp data in {}\", root);\r\n    handleFilesWithDanglingTempData(root, new DanglingFileRecoverer(destination));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "deleteFilesWithDanglingTempData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteFilesWithDanglingTempData(Path root) throws IOException\n{\r\n    LOG.debug(\"Deleting files with dangling temp data in {}\", root);\r\n    handleFilesWithDanglingTempData(root, new DanglingFileDeleter());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "finalize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finalize() throws Throwable\n{\r\n    LOG.debug(\"finalize() called.\");\r\n    close();\r\n    super.finalize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "encodeKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String encodeKey(String aKey)\n{\r\n    String fileName = aKey.substring(aKey.lastIndexOf(Path.SEPARATOR) + 1, aKey.length());\r\n    String filePrefix = AZURE_TEMP_FOLDER + Path.SEPARATOR + UUID.randomUUID().toString();\r\n    String randomizedKey = filePrefix + fileName;\r\n    return randomizedKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getOwnerForPath",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String getOwnerForPath(Path absolutePath) throws IOException\n{\r\n    String owner = \"\";\r\n    FileMetadata meta = null;\r\n    String key = pathToKey(absolutePath);\r\n    try {\r\n        meta = store.retrieveMetadata(key);\r\n        if (meta != null) {\r\n            owner = meta.getOwner();\r\n            LOG.debug(\"Retrieved '{}' as owner for path - {}\", owner, absolutePath);\r\n        } else {\r\n            LOG.debug(\"Cannot find file/folder - '{}'. Returning owner as empty string\", absolutePath);\r\n        }\r\n    } catch (IOException ex) {\r\n        Throwable innerException = NativeAzureFileSystemHelper.checkForAzureStorageException(ex);\r\n        boolean isfileNotFoundException = innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException);\r\n        if (!isfileNotFoundException) {\r\n            String errorMsg = \"Could not retrieve owner information for path - \" + absolutePath;\r\n            LOG.error(errorMsg);\r\n            throw new IOException(errorMsg, ex);\r\n        }\r\n    }\r\n    return owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateChownAllowedUsers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateChownAllowedUsers(List<String> chownAllowedUsers)\n{\r\n    this.chownAllowedUsers = chownAllowedUsers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateChmodAllowedUsers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateChmodAllowedUsers(List<String> chmodAllowedUsers)\n{\r\n    this.chmodAllowedUsers = chmodAllowedUsers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateDaemonUsers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateDaemonUsers(List<String> daemonUsers)\n{\r\n    this.daemonUsers = daemonUsers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hasPathCapability",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasPathCapability(final Path path, final String capability) throws IOException\n{\r\n    switch(validatePathCapabilityArgs(path, capability)) {\r\n        case CommonPathCapabilities.FS_PERMISSIONS:\r\n            return true;\r\n        case CommonPathCapabilities.FS_APPEND:\r\n            return appendSupportEnabled;\r\n        default:\r\n            return super.hasPathCapability(path, capability);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateHttpClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateHttpClient(HttpClient client)\n{\r\n    this.client = client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "makeRemoteRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String makeRemoteRequest(String[] urls, String path, List<NameValuePair> queryParams, String httpMethod) throws IOException\n{\r\n    return retryableRequest(urls, path, queryParams, httpMethod);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retryableRequest",
  "errType" : [ "URISyntaxException", "IOException", "IOException", "NumberFormatException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "String retryableRequest(String[] urls, String path, List<NameValuePair> queryParams, String httpMethod) throws IOException\n{\r\n    HttpResponse response = null;\r\n    HttpUriRequest httpRequest = null;\r\n    int indexOfLocalUrl = -1;\r\n    for (int i = 0; i < urls.length; i++) {\r\n        if (urls[i].toLowerCase().startsWith(\"https://localhost:\") || urls[i].toLowerCase().startsWith(\"http://localhost:\")) {\r\n            indexOfLocalUrl = i;\r\n        }\r\n    }\r\n    boolean requiresNewAuth = false;\r\n    for (int retry = 0, index = (indexOfLocalUrl != -1) ? indexOfLocalUrl : random.nextInt(urls.length); ; retry++, index++) {\r\n        if (index >= urls.length) {\r\n            index = index % urls.length;\r\n        }\r\n        if (indexOfLocalUrl != -1 && retry == 1) {\r\n            index = (index + random.nextInt(urls.length)) % urls.length;\r\n            if (index == indexOfLocalUrl) {\r\n                index = (index + 1) % urls.length;\r\n            }\r\n        }\r\n        try {\r\n            httpRequest = getHttpRequest(urls, path, queryParams, index, httpMethod, requiresNewAuth);\r\n            httpRequest.setHeader(\"Accept\", APPLICATION_JSON);\r\n            response = client.execute(httpRequest);\r\n            StatusLine statusLine = response.getStatusLine();\r\n            if (statusLine == null || statusLine.getStatusCode() != HttpStatus.SC_OK) {\r\n                requiresNewAuth = (statusLine == null) || (statusLine.getStatusCode() == HttpStatus.SC_UNAUTHORIZED);\r\n                throw new WasbRemoteCallException(httpRequest.getURI().toString() + \":\" + ((statusLine != null) ? statusLine.toString() : \"NULL\"));\r\n            } else {\r\n                requiresNewAuth = false;\r\n            }\r\n            Header contentTypeHeader = response.getFirstHeader(\"Content-Type\");\r\n            if (contentTypeHeader == null || !APPLICATION_JSON.equals(contentTypeHeader.getValue())) {\r\n                throw new WasbRemoteCallException(httpRequest.getURI().toString() + \":\" + \"Content-Type mismatch: expected: \" + APPLICATION_JSON + \", got \" + ((contentTypeHeader != null) ? contentTypeHeader.getValue() : \"NULL\"));\r\n            }\r\n            Header contentLengthHeader = response.getFirstHeader(\"Content-Length\");\r\n            if (contentLengthHeader == null) {\r\n                throw new WasbRemoteCallException(httpRequest.getURI().toString() + \":\" + \"Content-Length header missing\");\r\n            }\r\n            try {\r\n                if (Integer.parseInt(contentLengthHeader.getValue()) > MAX_CONTENT_LENGTH) {\r\n                    throw new WasbRemoteCallException(httpRequest.getURI().toString() + \":\" + \"Content-Length:\" + contentLengthHeader.getValue() + \"exceeded max:\" + MAX_CONTENT_LENGTH);\r\n                }\r\n            } catch (NumberFormatException nfe) {\r\n                throw new WasbRemoteCallException(httpRequest.getURI().toString() + \":\" + \"Invalid Content-Length value :\" + contentLengthHeader.getValue());\r\n            }\r\n            BufferedReader rd = null;\r\n            StringBuilder responseBody = new StringBuilder();\r\n            try {\r\n                rd = new BufferedReader(new InputStreamReader(response.getEntity().getContent(), StandardCharsets.UTF_8));\r\n                String responseLine = \"\";\r\n                while ((responseLine = rd.readLine()) != null) {\r\n                    responseBody.append(responseLine);\r\n                }\r\n            } finally {\r\n                rd.close();\r\n            }\r\n            return responseBody.toString();\r\n        } catch (URISyntaxException uriSyntaxEx) {\r\n            throw new WasbRemoteCallException(\"Encountered URISyntaxException \" + \"while building the HttpGetRequest to remote service\", uriSyntaxEx);\r\n        } catch (IOException e) {\r\n            LOG.debug(e.getMessage(), e);\r\n            try {\r\n                shouldRetry(e, retry, (httpRequest != null) ? httpRequest.getURI().toString() : urls[index]);\r\n            } catch (IOException ioex) {\r\n                String message = \"Encountered error while making remote call to \" + String.join(\",\", urls) + \" retried \" + retry + \" time(s).\";\r\n                LOG.error(message, ioex);\r\n                throw new WasbRemoteCallException(message, ioex);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getHttpRequest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "HttpUriRequest getHttpRequest(String[] urls, String path, List<NameValuePair> queryParams, int urlIndex, String httpMethod, boolean requiresNewAuth) throws URISyntaxException, IOException\n{\r\n    URIBuilder uriBuilder = null;\r\n    uriBuilder = new URIBuilder(urls[urlIndex]).setPath(path).setParameters(queryParams);\r\n    if (uriBuilder.getHost().equals(\"localhost\")) {\r\n        uriBuilder.setHost(InetAddress.getLocalHost().getCanonicalHostName());\r\n    }\r\n    HttpUriRequest httpUriRequest = null;\r\n    switch(httpMethod) {\r\n        case HttpPut.METHOD_NAME:\r\n            httpUriRequest = new HttpPut(uriBuilder.build());\r\n            break;\r\n        case HttpPost.METHOD_NAME:\r\n            httpUriRequest = new HttpPost(uriBuilder.build());\r\n            break;\r\n        default:\r\n            httpUriRequest = new HttpGet(uriBuilder.build());\r\n            break;\r\n    }\r\n    return httpUriRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "shouldRetry",
  "errType" : [ "InterruptedIOException", "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void shouldRetry(final IOException ioe, final int retry, final String url) throws IOException\n{\r\n    CharSequence authenticationExceptionMessage = Constants.AUTHENTICATION_FAILED_ERROR_MESSAGE;\r\n    if (ioe instanceof WasbRemoteCallException && ioe.getMessage().equals(authenticationExceptionMessage)) {\r\n        throw ioe;\r\n    }\r\n    try {\r\n        final RetryPolicy.RetryAction a = (retryPolicy != null) ? retryPolicy.shouldRetry(ioe, retry, 0, true) : RetryPolicy.RetryAction.FAIL;\r\n        boolean isRetry = a.action == RetryPolicy.RetryAction.RetryDecision.RETRY;\r\n        boolean isFailoverAndRetry = a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;\r\n        if (isRetry || isFailoverAndRetry) {\r\n            LOG.debug(\"Retrying connect to Remote service:{}. Already tried {}\" + \" time(s); retry policy is {}, \" + \"delay {}ms.\", url, retry, retryPolicy, a.delayMillis);\r\n            Thread.sleep(a.delayMillis);\r\n            return;\r\n        }\r\n    } catch (InterruptedIOException e) {\r\n        LOG.warn(e.getMessage(), e);\r\n        Thread.currentThread().interrupt();\r\n        return;\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Original exception is \", ioe);\r\n        throw new WasbRemoteCallException(e.getMessage(), e);\r\n    }\r\n    LOG.debug(\"Not retrying anymore, already retried the urls {} time(s)\", retry);\r\n    throw new WasbRemoteCallException(url + \":\" + \"Encountered IOException while making remote call\", ioe);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "paths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ListResultEntrySchema> paths()\n{\r\n    return this.paths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultSchema withPaths(final List<ListResultEntrySchema> paths)\n{\r\n    this.paths = paths;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "compute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long compute(byte[] input)\n{\r\n    init();\r\n    for (int i = 0; i < input.length; i++) {\r\n        value = TABLE[(input[i] ^ (int) value) & 0xFF] ^ (value >>> 8);\r\n    }\r\n    return ~value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init()\n{\r\n    value = -1;\r\n    for (int n = 0; n < TABLE_LENGTH; ++n) {\r\n        long crc = n;\r\n        for (int i = 0; i < 8; ++i) {\r\n            if ((crc & 1) == 1) {\r\n                crc = (crc >>> 1) ^ POLY;\r\n            } else {\r\n                crc >>>= 1;\r\n            }\r\n        }\r\n        TABLE[n] = crc;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setMockFileSystemForTesting",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMockFileSystemForTesting(FileSystem fileSystem)\n{\r\n    this.mockFileSystemForTesting = fileSystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (doPrintUsage(Arrays.asList(args))) {\r\n        printUsage();\r\n        return -1;\r\n    }\r\n    Path pathToCheck = null;\r\n    boolean doRecover = false;\r\n    boolean doDelete = false;\r\n    for (String arg : args) {\r\n        if (!arg.startsWith(\"-\")) {\r\n            if (pathToCheck != null) {\r\n                System.err.println(\"Can't specify multiple paths to check on the command-line\");\r\n                return 1;\r\n            }\r\n            pathToCheck = new Path(arg);\r\n        } else if (arg.equals(\"-move\")) {\r\n            doRecover = true;\r\n        } else if (arg.equals(\"-delete\")) {\r\n            doDelete = true;\r\n        }\r\n    }\r\n    if (doRecover && doDelete) {\r\n        System.err.println(\"Conflicting options: can't specify both -move and -delete.\");\r\n        return 1;\r\n    }\r\n    if (pathToCheck == null) {\r\n        pathToCheck = new Path(\"/\");\r\n    }\r\n    FileSystem fs;\r\n    if (mockFileSystemForTesting == null) {\r\n        fs = FileSystem.get(pathToCheck.toUri(), getConf());\r\n    } else {\r\n        fs = mockFileSystemForTesting;\r\n    }\r\n    if (!recursiveCheckChildPathName(fs, fs.makeQualified(pathToCheck))) {\r\n        pathNameWarning = true;\r\n    }\r\n    if (!(fs instanceof NativeAzureFileSystem)) {\r\n        System.err.println(\"Can only check WASB file system. Instead I'm asked to\" + \" check: \" + fs.getUri());\r\n        return 2;\r\n    }\r\n    NativeAzureFileSystem wasbFs = (NativeAzureFileSystem) fs;\r\n    if (doRecover) {\r\n        System.out.println(\"Recovering files with dangling data under: \" + pathToCheck);\r\n        wasbFs.recoverFilesWithDanglingTempData(pathToCheck, new Path(LOST_AND_FOUND_PATH));\r\n    } else if (doDelete) {\r\n        System.out.println(\"Deleting temp files with dangling data under: \" + pathToCheck);\r\n        wasbFs.deleteFilesWithDanglingTempData(pathToCheck);\r\n    } else {\r\n        System.out.println(\"Please specify -move or -delete\");\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPathNameWarning",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getPathNameWarning()\n{\r\n    return pathNameWarning;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "recursiveCheckChildPathName",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean recursiveCheckChildPathName(FileSystem fs, Path p) throws IOException\n{\r\n    if (p == null) {\r\n        return true;\r\n    }\r\n    FileStatus status;\r\n    try {\r\n        status = fs.getFileStatus(p);\r\n    } catch (FileNotFoundException e) {\r\n        System.out.println(\"Path \" + p + \" does not exist!\");\r\n        return true;\r\n    }\r\n    if (status.isFile()) {\r\n        if (containsColon(p)) {\r\n            System.out.println(\"Warning: file \" + p + \" has a colon in its name.\");\r\n            return false;\r\n        } else {\r\n            return true;\r\n        }\r\n    } else {\r\n        boolean flag;\r\n        if (containsColon(p)) {\r\n            System.out.println(\"Warning: directory \" + p + \" has a colon in its name.\");\r\n            flag = false;\r\n        } else {\r\n            flag = true;\r\n        }\r\n        FileStatus[] listed = fs.listStatus(p);\r\n        for (FileStatus l : listed) {\r\n            if (!recursiveCheckChildPathName(fs, l.getPath())) {\r\n                flag = false;\r\n            }\r\n        }\r\n        return flag;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "containsColon",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean containsColon(Path p)\n{\r\n    return p.toUri().getPath().toString().contains(\":\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void printUsage()\n{\r\n    System.out.println(\"Usage: WasbFSck [<path>] [-move | -delete]\");\r\n    System.out.println(\"\\t<path>\\tstart checking from this path\");\r\n    System.out.println(\"\\t-move\\tmove any files whose upload was interrupted\" + \" mid-stream to \" + LOST_AND_FOUND_PATH);\r\n    System.out.println(\"\\t-delete\\tdelete any files whose upload was interrupted\" + \" mid-stream\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "doPrintUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean doPrintUsage(List<String> args)\n{\r\n    return args.contains(\"-H\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new WasbFsck(new Configuration()), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureBlobFileSystem getFileSystem()\n{\r\n    return (AzureBlobFileSystem) super.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "bindToFileSystem",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void bindToFileSystem(FileSystem filesystem, Path path) throws IOException\n{\r\n    if (!(filesystem instanceof AzureBlobFileSystem)) {\r\n        throw new PathIOException(path.toString(), \"Not an abfs filesystem: \" + filesystem.getClass());\r\n    }\r\n    super.bindToFileSystem(filesystem, path);\r\n    try {\r\n        resilientCommitByRename = getFileSystem().createResilientCommitSupport(path);\r\n        LOG.debug(\"Bonded to filesystem with resilient commits under path {}\", path);\r\n    } catch (UnsupportedOperationException e) {\r\n        LOG.debug(\"No resilient commit support under path {}\", path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "storePreservesEtagsThroughRenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean storePreservesEtagsThroughRenames(final Path path)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "storeSupportsResilientCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean storeSupportsResilientCommit()\n{\r\n    return resilientCommitByRename != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "commitFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CommitFileResult commitFile(final FileEntry entry) throws IOException\n{\r\n    if (resilientCommitByRename != null) {\r\n        final Pair<Boolean, Duration> result = resilientCommitByRename.commitSingleFileByRename(entry.getSourcePath(), entry.getDestPath(), entry.getEtag());\r\n        return CommitFileResult.fromResilientCommit(result.getLeft(), result.getRight());\r\n    } else {\r\n        return super.commitFile(entry);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "trackInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void trackInfo(AbfsPerfInfo perfInfo)\n{\r\n    if (!enabled) {\r\n        return;\r\n    }\r\n    if (isValidInstant(perfInfo.getAggregateStart()) && perfInfo.getAggregateCount() > 0) {\r\n        recordClientLatency(perfInfo.getTrackingStart(), perfInfo.getTrackingEnd(), perfInfo.getCallerName(), perfInfo.getCalleeName(), perfInfo.getSuccess(), perfInfo.getAggregateStart(), perfInfo.getAggregateCount(), perfInfo.getResult());\r\n    } else {\r\n        recordClientLatency(perfInfo.getTrackingStart(), perfInfo.getTrackingEnd(), perfInfo.getCallerName(), perfInfo.getCalleeName(), perfInfo.getSuccess(), perfInfo.getResult());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLatencyInstant",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Instant getLatencyInstant()\n{\r\n    if (!enabled) {\r\n        return null;\r\n    }\r\n    return Instant.now();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "recordClientLatency",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void recordClientLatency(Instant operationStart, Instant operationStop, String callerName, String calleeName, boolean success, AbfsPerfLoggable res)\n{\r\n    Instant trackerStart = Instant.now();\r\n    long latency = isValidInstant(operationStart) && isValidInstant(operationStop) ? Duration.between(operationStart, operationStop).toMillis() : -1;\r\n    String latencyDetails = String.format(singletonLatencyReportingFormat, Instant.now(), callerName, calleeName, success ? \"Succeeded\" : \"Failed\", latency, res == null ? \"\" : (\" \" + res.getLogString()));\r\n    this.offerToQueue(trackerStart, latencyDetails);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "recordClientLatency",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void recordClientLatency(Instant operationStart, Instant operationStop, String callerName, String calleeName, boolean success, Instant aggregateStart, long aggregateCount, AbfsPerfLoggable res)\n{\r\n    Instant trackerStart = Instant.now();\r\n    long latency = isValidInstant(operationStart) && isValidInstant(operationStop) ? Duration.between(operationStart, operationStop).toMillis() : -1;\r\n    long aggregateLatency = isValidInstant(aggregateStart) && isValidInstant(operationStop) ? Duration.between(aggregateStart, operationStop).toMillis() : -1;\r\n    String latencyDetails = String.format(aggregateLatencyReportingFormat, Instant.now(), callerName, calleeName, success ? \"Succeeded\" : \"Failed\", latency, aggregateLatency, aggregateCount, res == null ? \"\" : (\" \" + res.getLogString()));\r\n    offerToQueue(trackerStart, latencyDetails);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getClientLatency",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getClientLatency()\n{\r\n    if (!enabled) {\r\n        return null;\r\n    }\r\n    Instant trackerStart = Instant.now();\r\n    String latencyDetails = queue.poll();\r\n    if (LOG.isDebugEnabled()) {\r\n        Instant stop = Instant.now();\r\n        long elapsed = Duration.between(trackerStart, stop).toMillis();\r\n        LOG.debug(\"Dequeued latency info [{} ms]: {}\", elapsed, latencyDetails);\r\n    }\r\n    return latencyDetails;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "offerToQueue",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void offerToQueue(Instant trackerStart, String latencyDetails)\n{\r\n    queue.offer(latencyDetails);\r\n    if (LOG.isDebugEnabled()) {\r\n        Instant trackerStop = Instant.now();\r\n        long elapsed = Duration.between(trackerStart, trackerStop).toMillis();\r\n        LOG.debug(\"Queued latency info [{} ms]: {}\", elapsed, latencyDetails);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isValidInstant",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isValidInstant(Instant testInstant)\n{\r\n    return testInstant != null && testInstant != Instant.MIN && testInstant != Instant.MAX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "fromShort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] fromShort(short s)\n{\r\n    return ByteBuffer.allocate(2).putShort(s).array();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toShort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "short toShort(byte firstByte, byte secondByte)\n{\r\n    return ByteBuffer.wrap(new byte[] { firstByte, secondByte }).getShort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "withMD5Checking",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlobRequestOptions withMD5Checking()\n{\r\n    BlobRequestOptions options = new BlobRequestOptions();\r\n    options.setUseTransactionalContentMD5(true);\r\n    return options;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\exceptions",
  "methodName" : "getStatusCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getStatusCode()\n{\r\n    return this.statusCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\exceptions",
  "methodName" : "getErrorCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AzureServiceErrorCode getErrorCode()\n{\r\n    return this.errorCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\exceptions",
  "methodName" : "getErrorMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getErrorMessage()\n{\r\n    return this.errorMessage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\exceptions",
  "methodName" : "formatMessage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String formatMessage(final AbfsHttpOperation abfsHttpOperation)\n{\r\n    if (abfsHttpOperation.getMethod().equals(\"HEAD\")) {\r\n        return String.format(\"Operation failed: \\\"%1$s\\\", %2$s, HEAD, %3$s\", abfsHttpOperation.getStatusDescription(), abfsHttpOperation.getStatusCode(), abfsHttpOperation.getMaskedUrl());\r\n    }\r\n    return String.format(\"Operation failed: \\\"%1$s\\\", %2$s, %3$s, %4$s, %5$s, \\\"%6$s\\\"\", abfsHttpOperation.getStatusDescription(), abfsHttpOperation.getStatusCode(), abfsHttpOperation.getMethod(), abfsHttpOperation.getMaskedUrl(), abfsHttpOperation.getStorageErrorCode(), abfsHttpOperation.getStorageErrorMessage().replaceAll(\"\\\\n\", \" \"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getUriDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getUriDefaultPort()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "hook",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hook(OperationContext operationContext, AzureFileSystemInstrumentation instrumentation, BandwidthGaugeUpdater blockUploadGaugeUpdater)\n{\r\n    ResponseReceivedMetricUpdater listener = new ResponseReceivedMetricUpdater(operationContext, instrumentation, blockUploadGaugeUpdater);\r\n    operationContext.getResponseReceivedEventHandler().addListener(listener);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getRequestContentLength",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getRequestContentLength(HttpURLConnection connection)\n{\r\n    String lengthString = connection.getRequestProperty(HeaderConstants.CONTENT_LENGTH);\r\n    if (lengthString != null) {\r\n        return Long.parseLong(lengthString);\r\n    } else {\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getResponseContentLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getResponseContentLength(HttpURLConnection connection)\n{\r\n    return connection.getContentLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "eventOccurred",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void eventOccurred(ResponseReceivedEvent eventArg)\n{\r\n    instrumentation.webResponse();\r\n    if (!(eventArg.getConnectionObject() instanceof HttpURLConnection)) {\r\n        return;\r\n    }\r\n    HttpURLConnection connection = (HttpURLConnection) eventArg.getConnectionObject();\r\n    RequestResult currentResult = eventArg.getRequestResult();\r\n    if (currentResult == null) {\r\n        return;\r\n    }\r\n    long requestLatency = currentResult.getStopDate().getTime() - currentResult.getStartDate().getTime();\r\n    if (currentResult.getStatusCode() == HttpURLConnection.HTTP_CREATED && connection.getRequestMethod().equalsIgnoreCase(\"PUT\")) {\r\n        long length = getRequestContentLength(connection);\r\n        if (length > 0) {\r\n            blockUploadGaugeUpdater.blockUploaded(currentResult.getStartDate(), currentResult.getStopDate(), length);\r\n            instrumentation.rawBytesUploaded(length);\r\n            instrumentation.blockUploaded(requestLatency);\r\n        }\r\n    } else if (currentResult.getStatusCode() == HttpURLConnection.HTTP_PARTIAL && connection.getRequestMethod().equalsIgnoreCase(\"GET\")) {\r\n        long length = getResponseContentLength(connection);\r\n        if (length > 0) {\r\n            blockUploadGaugeUpdater.blockDownloaded(currentResult.getStartDate(), currentResult.getStopDate(), length);\r\n            instrumentation.rawBytesDownloaded(length);\r\n            instrumentation.blockDownloaded(requestLatency);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "validate",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Long validate(final String configValue) throws InvalidConfigurationValueException\n{\r\n    Long result = super.validate(configValue);\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    try {\r\n        result = Long.parseLong(configValue);\r\n        if (getThrowIfInvalid() && (result < this.min || result > this.max)) {\r\n            throw new InvalidConfigurationValueException(getConfigKey());\r\n        }\r\n        if (result < this.min) {\r\n            return this.min;\r\n        } else if (result > this.max) {\r\n            return this.max;\r\n        }\r\n    } catch (NumberFormatException ex) {\r\n        throw new InvalidConfigurationValueException(getConfigKey(), ex);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStorageAccountKey",
  "errType" : [ "IllegalAccessException|InvalidConfigurationValueException", "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getStorageAccountKey(String accountName, Configuration rawConfig) throws KeyProviderException\n{\r\n    String key = null;\r\n    try {\r\n        AbfsConfiguration abfsConfig = new AbfsConfiguration(rawConfig, accountName);\r\n        key = abfsConfig.getPasswordString(ConfigurationKeys.FS_AZURE_ACCOUNT_KEY_PROPERTY_NAME);\r\n        validateStorageAccountKey(key);\r\n    } catch (IllegalAccessException | InvalidConfigurationValueException e) {\r\n        LOG.debug(\"Failure to retrieve storage account key for {}\", accountName, e);\r\n        throw new KeyProviderException(\"Failure to initialize configuration for \" + accountName + \" key =\\\"\" + key + \"\\\"\" + \": \" + e, e);\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"Unable to get key for {} from credential providers. {}\", accountName, ioe, ioe);\r\n    }\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "validateStorageAccountKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateStorageAccountKey(String key) throws InvalidConfigurationValueException\n{\r\n    Base64StringConfigurationBasicValidator validator = new Base64StringConfigurationBasicValidator(ConfigurationKeys.FS_AZURE_ACCOUNT_KEY_PROPERTY_NAME, \"\", true);\r\n    validator.validate(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContentLengthIfKnown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getContentLengthIfKnown(String range)\n{\r\n    long contentLength = 0;\r\n    if (range != null && range.startsWith(\"bytes=\")) {\r\n        String[] offsets = range.substring(\"bytes=\".length()).split(\"-\");\r\n        if (offsets.length == 2) {\r\n            contentLength = Long.parseLong(offsets[1]) - Long.parseLong(offsets[0]) + 1;\r\n        }\r\n    }\r\n    return contentLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContentLengthIfKnown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getContentLengthIfKnown(HttpURLConnection conn, OperationType operationType)\n{\r\n    long contentLength = 0;\r\n    switch(operationType) {\r\n        case AppendBlock:\r\n        case PutBlock:\r\n            String lengthString = conn.getRequestProperty(HeaderConstants.CONTENT_LENGTH);\r\n            contentLength = (lengthString != null) ? Long.parseLong(lengthString) : 0;\r\n            break;\r\n        case PutPage:\r\n        case GetBlob:\r\n            contentLength = BlobOperationDescriptor.getContentLengthIfKnown(conn.getRequestProperty(\"x-ms-range\"));\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n    return contentLength;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getOperationType",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "OperationType getOperationType(HttpURLConnection conn)\n{\r\n    OperationType operationType = OperationType.Unknown;\r\n    String method = conn.getRequestMethod();\r\n    String compValue = getQueryParameter(conn.getURL(), \"comp\");\r\n    if (method.equalsIgnoreCase(\"PUT\")) {\r\n        if (compValue != null) {\r\n            switch(compValue) {\r\n                case \"metadata\":\r\n                    operationType = OperationType.SetMetadata;\r\n                    break;\r\n                case \"properties\":\r\n                    operationType = OperationType.SetProperties;\r\n                    break;\r\n                case \"block\":\r\n                    operationType = OperationType.PutBlock;\r\n                    break;\r\n                case \"page\":\r\n                    String pageWrite = conn.getRequestProperty(\"x-ms-page-write\");\r\n                    if (pageWrite != null && pageWrite.equalsIgnoreCase(\"UPDATE\")) {\r\n                        operationType = OperationType.PutPage;\r\n                    }\r\n                    break;\r\n                case \"appendblock\":\r\n                    operationType = OperationType.AppendBlock;\r\n                    break;\r\n                case \"blocklist\":\r\n                    operationType = OperationType.PutBlockList;\r\n                    break;\r\n                default:\r\n                    break;\r\n            }\r\n        } else {\r\n            String blobType = conn.getRequestProperty(\"x-ms-blob-type\");\r\n            if (blobType != null && (blobType.equalsIgnoreCase(\"PageBlob\") || blobType.equalsIgnoreCase(\"BlockBlob\") || blobType.equalsIgnoreCase(\"AppendBlob\"))) {\r\n                operationType = OperationType.CreateBlob;\r\n            } else if (blobType == null) {\r\n                String resType = getQueryParameter(conn.getURL(), \"restype\");\r\n                if (resType != null && resType.equalsIgnoreCase(\"container\")) {\r\n                    operationType = operationType.CreateContainer;\r\n                }\r\n            }\r\n        }\r\n    } else if (method.equalsIgnoreCase(\"GET\")) {\r\n        if (compValue != null) {\r\n            switch(compValue) {\r\n                case \"list\":\r\n                    operationType = OperationType.ListBlobs;\r\n                    break;\r\n                case \"metadata\":\r\n                    operationType = OperationType.GetMetadata;\r\n                    break;\r\n                case \"blocklist\":\r\n                    operationType = OperationType.GetBlockList;\r\n                    break;\r\n                case \"pagelist\":\r\n                    operationType = OperationType.GetPageList;\r\n                    break;\r\n                default:\r\n                    break;\r\n            }\r\n        } else if (conn.getRequestProperty(\"x-ms-range\") != null) {\r\n            operationType = OperationType.GetBlob;\r\n        }\r\n    } else if (method.equalsIgnoreCase(\"HEAD\")) {\r\n        operationType = OperationType.GetProperties;\r\n    } else if (method.equalsIgnoreCase(\"DELETE\")) {\r\n        String resType = getQueryParameter(conn.getURL(), \"restype\");\r\n        if (resType != null && resType.equalsIgnoreCase(\"container\")) {\r\n            operationType = operationType.DeleteContainer;\r\n        } else {\r\n            operationType = OperationType.DeleteBlob;\r\n        }\r\n    }\r\n    return operationType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getQueryParameter",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getQueryParameter(URL url, String queryParameterName)\n{\r\n    String query = (url != null) ? url.getQuery() : null;\r\n    if (query == null) {\r\n        return null;\r\n    }\r\n    String searchValue = queryParameterName + \"=\";\r\n    int offset = query.indexOf(searchValue);\r\n    String value = null;\r\n    if (offset != -1) {\r\n        int beginIndex = offset + searchValue.length();\r\n        int endIndex = query.indexOf('&', beginIndex);\r\n        value = (endIndex == -1) ? query.substring(beginIndex) : query.substring(beginIndex, endIndex);\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "parseLastModifiedTime",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long parseLastModifiedTime(final String lastModifiedTime)\n{\r\n    long parsedTime = 0;\r\n    try {\r\n        Date utcDate = new SimpleDateFormat(DATE_TIME_PATTERN, Locale.US).parse(lastModifiedTime);\r\n        parsedTime = utcDate.getTime();\r\n    } catch (ParseException e) {\r\n        LOG.error(\"Failed to parse the date {}\", lastModifiedTime);\r\n    } finally {\r\n        return parsedTime;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "isRecentlyModified",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isRecentlyModified(final String lastModifiedTime, final Instant expectedLMTUpdateTime)\n{\r\n    long lmtEpochTime = DateTimeUtils.parseLastModifiedTime(lastModifiedTime);\r\n    long currentEpochTime = expectedLMTUpdateTime.toEpochMilli();\r\n    return ((lmtEpochTime > currentEpochTime) || ((currentEpochTime - lmtEpochTime) <= DEFAULT_CLOCK_SKEW_WITH_SERVER_IN_MS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getThreadPool",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ThreadPoolExecutor getThreadPool(int threadCount) throws Exception\n{\r\n    return new ThreadPoolExecutor(threadCount, threadCount, 2, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new AzureFileSystemThreadFactory(this.threadNamePrefix));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "executeParallel",
  "errType" : [ "Exception", "RejectedExecutionException", "InterruptedException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "boolean executeParallel(FileMetadata[] contents, AzureFileSystemThreadTask threadOperation) throws IOException\n{\r\n    boolean operationStatus = false;\r\n    boolean threadsEnabled = false;\r\n    int threadCount = this.threadCount;\r\n    ThreadPoolExecutor ioThreadPool = null;\r\n    long start = Time.monotonicNow();\r\n    threadCount = Math.min(contents.length, threadCount);\r\n    if (threadCount > 1) {\r\n        try {\r\n            ioThreadPool = getThreadPool(threadCount);\r\n            threadsEnabled = true;\r\n        } catch (Exception e) {\r\n            LOG.warn(\"Failed to create thread pool with threads {} for operation {} on blob {}.\" + \" Use config {} to set less number of threads. Setting config value to <= 1 will disable threads.\", threadCount, operation, key, config);\r\n        }\r\n    } else {\r\n        LOG.warn(\"Disabling threads for {} operation as thread count {} is <= 1\", operation, threadCount);\r\n    }\r\n    if (threadsEnabled) {\r\n        LOG.debug(\"Using thread pool for {} operation with threads {}\", operation, threadCount);\r\n        boolean started = false;\r\n        AzureFileSystemThreadRunnable runnable = new AzureFileSystemThreadRunnable(contents, threadOperation, operation);\r\n        for (int i = 0; i < threadCount && runnable.lastException == null && runnable.operationStatus; i++) {\r\n            try {\r\n                ioThreadPool.execute(runnable);\r\n                started = true;\r\n            } catch (RejectedExecutionException ex) {\r\n                LOG.error(\"Rejected execution of thread for {} operation on blob {}.\" + \" Continuing with existing threads. Use config {} to set less number of threads\" + \" to avoid this error\", operation, key, config);\r\n            }\r\n        }\r\n        ioThreadPool.shutdown();\r\n        try {\r\n            ioThreadPool.awaitTermination(Long.MAX_VALUE, TimeUnit.DAYS);\r\n        } catch (InterruptedException intrEx) {\r\n            ioThreadPool.shutdownNow();\r\n            Thread.currentThread().interrupt();\r\n            LOG.error(\"Threads got interrupted {} blob operation for {} \", operation, key);\r\n        }\r\n        int threadsNotUsed = threadCount - runnable.threadsUsed.get();\r\n        if (threadsNotUsed > 0) {\r\n            LOG.warn(\"{} threads not used for {} operation on blob {}\", threadsNotUsed, operation, key);\r\n        }\r\n        if (!started) {\r\n            threadsEnabled = false;\r\n            LOG.info(\"Not able to schedule threads to {} blob {}. Fall back to {} blob serially.\", operation, key, operation);\r\n        } else {\r\n            IOException lastException = runnable.lastException;\r\n            if (lastException == null && runnable.operationStatus && runnable.filesProcessed.get() < contents.length) {\r\n                LOG.error(\"{} failed as operation on subfolders and files failed.\", operation);\r\n                lastException = new IOException(operation + \" failed as operation on subfolders and files failed.\");\r\n            }\r\n            if (lastException != null) {\r\n                throw lastException;\r\n            }\r\n            operationStatus = runnable.operationStatus;\r\n        }\r\n    }\r\n    if (!threadsEnabled) {\r\n        LOG.debug(\"Serializing the {} operation\", operation);\r\n        for (int i = 0; i < contents.length; i++) {\r\n            if (!threadOperation.execute(contents[i])) {\r\n                LOG.warn(\"Failed to {} file {}\", operation, contents[i]);\r\n                return false;\r\n            }\r\n        }\r\n        operationStatus = true;\r\n    }\r\n    long end = Time.monotonicNow();\r\n    LOG.info(\"Time taken for {} operation is: {} ms with threads: {}\", operation, (end - start), threadCount);\r\n    return operationStatus;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "name",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String name()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withName(String name)\n{\r\n    this.name = name;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "isDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Boolean isDirectory()\n{\r\n    return isDirectory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withIsDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withIsDirectory(final Boolean isDirectory)\n{\r\n    this.isDirectory = isDirectory;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "lastModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String lastModified()\n{\r\n    return lastModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withLastModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withLastModified(String lastModified)\n{\r\n    this.lastModified = lastModified;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "eTag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String eTag()\n{\r\n    return eTag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withETag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withETag(final String eTag)\n{\r\n    this.eTag = eTag;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "contentLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Long contentLength()\n{\r\n    return contentLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withContentLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withContentLength(final Long contentLength)\n{\r\n    this.contentLength = contentLength;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "owner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String owner()\n{\r\n    return owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withOwner(final String owner)\n{\r\n    this.owner = owner;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "group",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String group()\n{\r\n    return group;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withGroup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withGroup(final String group)\n{\r\n    this.group = group;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "permissions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String permissions()\n{\r\n    return permissions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "withPermissions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultEntrySchema withPermissions(final String permissions)\n{\r\n    this.permissions = permissions;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "handleKind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean handleKind(Text kind)\n{\r\n    return WasbDelegationTokenIdentifier.TOKEN_KIND.equals(kind);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "isManaged",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isManaged(Token<?> token) throws IOException\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "renew",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long renew(final Token<?> token, Configuration conf) throws IOException, InterruptedException\n{\r\n    LOG.debug(\"Renewing the delegation token\");\r\n    return getInstance(conf).renewDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "cancel",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cancel(final Token<?> token, Configuration conf) throws IOException, InterruptedException\n{\r\n    LOG.debug(\"Cancelling the delegation token\");\r\n    getInstance(conf).cancelDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WasbDelegationTokenManager getInstance(Configuration conf) throws IOException\n{\r\n    return new RemoteWasbDelegationTokenManager(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "decode",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "byte[] decode(final String data)\n{\r\n    if (data == null) {\r\n        throw new IllegalArgumentException(\"The data parameter is not a valid base64-encoded string.\");\r\n    }\r\n    int byteArrayLength = 3 * data.length() / 4;\r\n    if (data.endsWith(\"==\")) {\r\n        byteArrayLength -= 2;\r\n    } else if (data.endsWith(\"=\")) {\r\n        byteArrayLength -= 1;\r\n    }\r\n    final byte[] retArray = new byte[byteArrayLength];\r\n    int byteDex = 0;\r\n    int charDex = 0;\r\n    for (; charDex < data.length(); charDex += 4) {\r\n        final int char1 = DECODE_64[(byte) data.charAt(charDex)];\r\n        final int char2 = DECODE_64[(byte) data.charAt(charDex + 1)];\r\n        final int char3 = DECODE_64[(byte) data.charAt(charDex + 2)];\r\n        final int char4 = DECODE_64[(byte) data.charAt(charDex + 3)];\r\n        if (char1 < 0 || char2 < 0 || char3 == -1 || char4 == -1) {\r\n            throw new IllegalArgumentException(\"The data parameter is not a valid base64-encoded string.\");\r\n        }\r\n        int tVal = char1 << 18;\r\n        tVal += char2 << 12;\r\n        tVal += (char3 & 0xff) << 6;\r\n        tVal += char4 & 0xff;\r\n        if (char3 == -2) {\r\n            tVal &= 0x00FFF000;\r\n            retArray[byteDex++] = (byte) (tVal >> 16 & 0xFF);\r\n        } else if (char4 == -2) {\r\n            tVal &= 0x00FFFFC0;\r\n            retArray[byteDex++] = (byte) (tVal >> 16 & 0xFF);\r\n            retArray[byteDex++] = (byte) (tVal >> 8 & 0xFF);\r\n        } else {\r\n            retArray[byteDex++] = (byte) (tVal >> 16 & 0xFF);\r\n            retArray[byteDex++] = (byte) (tVal >> 8 & 0xFF);\r\n            retArray[byteDex++] = (byte) (tVal & 0xFF);\r\n        }\r\n    }\r\n    return retArray;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "decodeAsByteObjectArray",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Byte[] decodeAsByteObjectArray(final String data)\n{\r\n    int byteArrayLength = 3 * data.length() / 4;\r\n    if (data.endsWith(\"==\")) {\r\n        byteArrayLength -= 2;\r\n    } else if (data.endsWith(\"=\")) {\r\n        byteArrayLength -= 1;\r\n    }\r\n    final Byte[] retArray = new Byte[byteArrayLength];\r\n    int byteDex = 0;\r\n    int charDex = 0;\r\n    for (; charDex < data.length(); charDex += 4) {\r\n        final int char1 = DECODE_64[(byte) data.charAt(charDex)];\r\n        final int char2 = DECODE_64[(byte) data.charAt(charDex + 1)];\r\n        final int char3 = DECODE_64[(byte) data.charAt(charDex + 2)];\r\n        final int char4 = DECODE_64[(byte) data.charAt(charDex + 3)];\r\n        if (char1 < 0 || char2 < 0 || char3 == -1 || char4 == -1) {\r\n            throw new IllegalArgumentException(\"The data parameter is not a valid base64-encoded string.\");\r\n        }\r\n        int tVal = char1 << 18;\r\n        tVal += char2 << 12;\r\n        tVal += (char3 & 0xff) << 6;\r\n        tVal += char4 & 0xff;\r\n        if (char3 == -2) {\r\n            tVal &= 0x00FFF000;\r\n            retArray[byteDex++] = (byte) (tVal >> 16 & 0xFF);\r\n        } else if (char4 == -2) {\r\n            tVal &= 0x00FFFFC0;\r\n            retArray[byteDex++] = (byte) (tVal >> 16 & 0xFF);\r\n            retArray[byteDex++] = (byte) (tVal >> 8 & 0xFF);\r\n        } else {\r\n            retArray[byteDex++] = (byte) (tVal >> 16 & 0xFF);\r\n            retArray[byteDex++] = (byte) (tVal >> 8 & 0xFF);\r\n            retArray[byteDex++] = (byte) (tVal & 0xFF);\r\n        }\r\n    }\r\n    return retArray;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "encode",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String encode(final byte[] data)\n{\r\n    final StringBuilder builder = new StringBuilder();\r\n    final int dataRemainder = data.length % 3;\r\n    int j = 0;\r\n    int n = 0;\r\n    for (; j < data.length; j += 3) {\r\n        if (j < data.length - dataRemainder) {\r\n            n = ((data[j] & 0xFF) << 16) + ((data[j + 1] & 0xFF) << 8) + (data[j + 2] & 0xFF);\r\n        } else {\r\n            if (dataRemainder == 1) {\r\n                n = (data[j] & 0xFF) << 16;\r\n            } else if (dataRemainder == 2) {\r\n                n = ((data[j] & 0xFF) << 16) + ((data[j + 1] & 0xFF) << 8);\r\n            }\r\n        }\r\n        builder.append(BASE_64_CHARS.charAt((byte) ((n >>> 18) & 0x3F)));\r\n        builder.append(BASE_64_CHARS.charAt((byte) ((n >>> 12) & 0x3F)));\r\n        builder.append(BASE_64_CHARS.charAt((byte) ((n >>> 6) & 0x3F)));\r\n        builder.append(BASE_64_CHARS.charAt((byte) (n & 0x3F)));\r\n    }\r\n    final int bLength = builder.length();\r\n    if (data.length % 3 == 1) {\r\n        builder.replace(bLength - 2, bLength, \"==\");\r\n    } else if (data.length % 3 == 2) {\r\n        builder.replace(bLength - 1, bLength, \"=\");\r\n    }\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "encode",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String encode(final Byte[] data)\n{\r\n    final StringBuilder builder = new StringBuilder();\r\n    final int dataRemainder = data.length % 3;\r\n    int j = 0;\r\n    int n = 0;\r\n    for (; j < data.length; j += 3) {\r\n        if (j < data.length - dataRemainder) {\r\n            n = ((data[j] & 0xFF) << 16) + ((data[j + 1] & 0xFF) << 8) + (data[j + 2] & 0xFF);\r\n        } else {\r\n            if (dataRemainder == 1) {\r\n                n = (data[j] & 0xFF) << 16;\r\n            } else if (dataRemainder == 2) {\r\n                n = ((data[j] & 0xFF) << 16) + ((data[j + 1] & 0xFF) << 8);\r\n            }\r\n        }\r\n        builder.append(BASE_64_CHARS.charAt((byte) ((n >>> 18) & 0x3F)));\r\n        builder.append(BASE_64_CHARS.charAt((byte) ((n >>> 12) & 0x3F)));\r\n        builder.append(BASE_64_CHARS.charAt((byte) ((n >>> 6) & 0x3F)));\r\n        builder.append(BASE_64_CHARS.charAt((byte) (n & 0x3F)));\r\n    }\r\n    final int bLength = builder.length();\r\n    if (data.length % 3 == 1) {\r\n        builder.replace(bLength - 2, bLength, \"==\");\r\n    } else if (data.length % 3 == 2) {\r\n        builder.replace(bLength - 1, bLength, \"=\");\r\n    }\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "validateIsBase64String",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean validateIsBase64String(final String data)\n{\r\n    if (data == null || data.length() % 4 != 0) {\r\n        return false;\r\n    }\r\n    for (int m = 0; m < data.length(); m++) {\r\n        final byte charByte = (byte) data.charAt(m);\r\n        if (DECODE_64[charByte] == -2) {\r\n            if (m < data.length() - 2) {\r\n                return false;\r\n            } else if (m == data.length() - 2 && DECODE_64[(byte) data.charAt(m + 1)] != -2) {\r\n                return false;\r\n            }\r\n        }\r\n        if (charByte < 0 || DECODE_64[charByte] == -1) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addBytesTransferred",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addBytesTransferred(long count, boolean isFailedOperation)\n{\r\n    BlobOperationMetrics metrics = blobMetrics.get();\r\n    if (isFailedOperation) {\r\n        metrics.bytesFailed.addAndGet(count);\r\n        metrics.operationsFailed.incrementAndGet();\r\n    } else {\r\n        metrics.bytesSuccessful.addAndGet(count);\r\n        metrics.operationsSuccessful.incrementAndGet();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "suspendIfNecessary",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void suspendIfNecessary()\n{\r\n    int duration = sleepDuration;\r\n    if (duration > 0) {\r\n        try {\r\n            Thread.sleep(duration);\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getSleepDuration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSleepDuration()\n{\r\n    return sleepDuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "analyzeMetricsAndUpdateSleepDuration",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int analyzeMetricsAndUpdateSleepDuration(BlobOperationMetrics metrics, int sleepDuration)\n{\r\n    final double percentageConversionFactor = 100;\r\n    double bytesFailed = metrics.bytesFailed.get();\r\n    double bytesSuccessful = metrics.bytesSuccessful.get();\r\n    double operationsFailed = metrics.operationsFailed.get();\r\n    double operationsSuccessful = metrics.operationsSuccessful.get();\r\n    double errorPercentage = (bytesFailed <= 0) ? 0 : percentageConversionFactor * bytesFailed / (bytesFailed + bytesSuccessful);\r\n    long periodMs = metrics.endTime - metrics.startTime;\r\n    double newSleepDuration;\r\n    if (errorPercentage < MIN_ACCEPTABLE_ERROR_PERCENTAGE) {\r\n        ++consecutiveNoErrorCount;\r\n        double reductionFactor = (consecutiveNoErrorCount * analysisPeriodMs >= RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) ? RAPID_SLEEP_DECREASE_FACTOR : SLEEP_DECREASE_FACTOR;\r\n        newSleepDuration = sleepDuration * reductionFactor;\r\n    } else if (errorPercentage < MAX_EQUILIBRIUM_ERROR_PERCENTAGE) {\r\n        newSleepDuration = sleepDuration;\r\n    } else {\r\n        consecutiveNoErrorCount = 0;\r\n        double additionalDelayNeeded = 5 * analysisPeriodMs;\r\n        if (bytesSuccessful > 0) {\r\n            additionalDelayNeeded = (bytesSuccessful + bytesFailed) * periodMs / bytesSuccessful - periodMs;\r\n        }\r\n        newSleepDuration = additionalDelayNeeded / (operationsFailed + operationsSuccessful);\r\n        final double maxSleepDuration = analysisPeriodMs;\r\n        final double minSleepDuration = sleepDuration * SLEEP_INCREASE_FACTOR;\r\n        newSleepDuration = Math.max(newSleepDuration, minSleepDuration) + 1;\r\n        newSleepDuration = Math.min(newSleepDuration, maxSleepDuration);\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(String.format(\"%5.5s, %10d, %10d, %10d, %10d, %6.2f, %5d, %5d, %5d\", name, (int) bytesFailed, (int) bytesSuccessful, (int) operationsFailed, (int) operationsSuccessful, errorPercentage, periodMs, (int) sleepDuration, (int) newSleepDuration));\r\n    }\r\n    return (int) newSleepDuration;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean hasNext() throws IOException\n{\r\n    if (currIterator.hasNext()) {\r\n        return true;\r\n    }\r\n    currIterator = getNextIterator();\r\n    return currIterator.hasNext();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus next() throws IOException\n{\r\n    if (!this.hasNext()) {\r\n        throw new NoSuchElementException();\r\n    }\r\n    return currIterator.next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getNextIterator",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Iterator<FileStatus> getNextIterator() throws IOException\n{\r\n    fetchBatchesAsync();\r\n    try {\r\n        AbfsListResult listResult = null;\r\n        while (listResult == null && (!isIterationComplete || !listResultQueue.isEmpty())) {\r\n            listResult = listResultQueue.poll(POLL_WAIT_TIME_IN_MS, TimeUnit.MILLISECONDS);\r\n        }\r\n        if (listResult == null) {\r\n            return Collections.emptyIterator();\r\n        } else if (listResult.isFailedListing()) {\r\n            throw listResult.getListingException();\r\n        } else {\r\n            return listResult.getFileStatusIterator();\r\n        }\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n        LOG.error(\"Thread got interrupted: {}\", e);\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "fetchBatchesAsync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fetchBatchesAsync()\n{\r\n    if (isAsyncInProgress || isIterationComplete) {\r\n        return;\r\n    }\r\n    synchronized (this) {\r\n        if (isAsyncInProgress || isIterationComplete) {\r\n            return;\r\n        }\r\n        isAsyncInProgress = true;\r\n    }\r\n    CompletableFuture.runAsync(() -> asyncOp());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "asyncOp",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void asyncOp()\n{\r\n    try {\r\n        while (!isIterationComplete && listResultQueue.size() <= MAX_QUEUE_SIZE) {\r\n            addNextBatchIteratorToQueue();\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.error(\"Fetching filestatuses failed\", ioe);\r\n        try {\r\n            listResultQueue.put(new AbfsListResult(ioe));\r\n        } catch (InterruptedException interruptedException) {\r\n            Thread.currentThread().interrupt();\r\n            LOG.error(\"Thread got interrupted: {}\", interruptedException);\r\n        }\r\n    } finally {\r\n        synchronized (this) {\r\n            isAsyncInProgress = false;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "addNextBatchIteratorToQueue",
  "errType" : [ "InterruptedException", "AbfsRestOperationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addNextBatchIteratorToQueue() throws IOException\n{\r\n    List<FileStatus> fileStatuses = new ArrayList<>();\r\n    try {\r\n        try {\r\n            continuation = listingSupport.listStatus(path, null, fileStatuses, FETCH_ALL_FALSE, continuation, tracingContext);\r\n        } catch (AbfsRestOperationException ex) {\r\n            AzureBlobFileSystem.checkException(path, ex);\r\n        }\r\n        if (!fileStatuses.isEmpty()) {\r\n            listResultQueue.put(new AbfsListResult(fileStatuses.iterator()));\r\n        }\r\n    } catch (InterruptedException ie) {\r\n        Thread.currentThread().interrupt();\r\n        LOG.error(\"Thread interrupted\", ie);\r\n    }\r\n    if (continuation == null || continuation.isEmpty()) {\r\n        isIterationComplete = true;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getId()\n{\r\n    return this.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "run",
  "errType" : [ "InterruptedException", "InterruptedException", "IOException", "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void run()\n{\r\n    try {\r\n        UNLEASH_WORKERS.await();\r\n    } catch (InterruptedException ex) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n    ReadBufferManager bufferManager = ReadBufferManager.getBufferManager();\r\n    ReadBuffer buffer;\r\n    while (true) {\r\n        try {\r\n            buffer = bufferManager.getNextBlockToRead();\r\n        } catch (InterruptedException ex) {\r\n            Thread.currentThread().interrupt();\r\n            return;\r\n        }\r\n        if (buffer != null) {\r\n            try {\r\n                int bytesRead = buffer.getStream().readRemote(buffer.getOffset(), buffer.getBuffer(), 0, Math.min(buffer.getRequestedLength(), buffer.getBuffer().length), buffer.getTracingContext());\r\n                bufferManager.doneReading(buffer, ReadBufferStatus.AVAILABLE, bytesRead);\r\n            } catch (IOException ex) {\r\n                buffer.setErrException(ex);\r\n                bufferManager.doneReading(buffer, ReadBufferStatus.READ_FAILED, 0);\r\n            } catch (Exception ex) {\r\n                buffer.setErrException(new PathIOException(buffer.getStream().getPath(), ex));\r\n                bufferManager.doneReading(buffer, ReadBufferStatus.READ_FAILED, 0);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "refreshToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureADToken refreshToken() throws IOException\n{\r\n    LOG.debug(\"AADToken: refreshing refresh-token based token\");\r\n    return AzureADAuthenticator.getTokenUsingRefreshToken(authEndpoint, clientId, refreshToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateWasbRemoteCallHelper",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void updateWasbRemoteCallHelper(WasbRemoteCallHelper helper)\n{\r\n    this.remoteCallHelper = helper;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void init(Configuration conf) throws IOException\n{\r\n    LOG.debug(\"Initializing RemoteWasbAuthorizerImpl instance\");\r\n    this.isKerberosSupportEnabled = conf.getBoolean(Constants.AZURE_KERBEROS_SUPPORT_PROPERTY_NAME, false);\r\n    this.isSpnegoTokenCacheEnabled = conf.getBoolean(Constants.AZURE_ENABLE_SPNEGO_TOKEN_CACHE, true);\r\n    this.commaSeparatedUrls = conf.getTrimmedStrings(KEY_REMOTE_AUTH_SERVICE_URLS);\r\n    if (this.commaSeparatedUrls == null || this.commaSeparatedUrls.length <= 0) {\r\n        throw new IOException(KEY_REMOTE_AUTH_SERVICE_URLS + \" config not set\" + \" in configuration.\");\r\n    }\r\n    this.retryPolicy = RetryUtils.getMultipleLinearRandomRetry(conf, AUTHORIZER_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY, true, AUTHORIZER_HTTP_CLIENT_RETRY_POLICY_SPEC_SPEC, AUTHORIZER_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT);\r\n    if (isKerberosSupportEnabled && UserGroupInformation.isSecurityEnabled()) {\r\n        this.remoteCallHelper = new SecureWasbRemoteCallHelper(retryPolicy, false, isSpnegoTokenCacheEnabled);\r\n    } else {\r\n        this.remoteCallHelper = new WasbRemoteCallHelper(retryPolicy);\r\n    }\r\n    this.cache = new CachingAuthorizer<>(conf.getTimeDuration(AUTHORIZATION_CACHEENTRY_EXPIRY_PERIOD, 5L, TimeUnit.MINUTES), \"AUTHORIZATION\");\r\n    this.cache.init(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "authorize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean authorize(String wasbAbsolutePath, String accessType, String resourceOwner) throws IOException\n{\r\n    if (wasbAbsolutePath.endsWith(NativeAzureFileSystem.FolderRenamePending.SUFFIX)) {\r\n        return true;\r\n    }\r\n    CachedAuthorizerEntry cacheKey = new CachedAuthorizerEntry(wasbAbsolutePath, accessType, resourceOwner);\r\n    Boolean cacheresult = cache.get(cacheKey);\r\n    if (cacheresult != null) {\r\n        return cacheresult;\r\n    }\r\n    boolean authorizeresult = authorizeInternal(wasbAbsolutePath, accessType, resourceOwner);\r\n    cache.put(cacheKey, authorizeresult);\r\n    return authorizeresult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "authorizeInternal",
  "errType" : [ "WasbRemoteCallException|JsonParseException|JsonMappingException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "boolean authorizeInternal(String wasbAbsolutePath, String accessType, String resourceOwner) throws IOException\n{\r\n    try {\r\n        final URIBuilder uriBuilder = new URIBuilder();\r\n        uriBuilder.setPath(\"/\" + CHECK_AUTHORIZATION_OP);\r\n        uriBuilder.addParameter(WASB_ABSOLUTE_PATH_QUERY_PARAM_NAME, wasbAbsolutePath);\r\n        uriBuilder.addParameter(ACCESS_OPERATION_QUERY_PARAM_NAME, accessType);\r\n        if (resourceOwner != null && StringUtils.isNotEmpty(resourceOwner)) {\r\n            uriBuilder.addParameter(WASB_RESOURCE_OWNER_QUERY_PARAM_NAME, resourceOwner);\r\n        }\r\n        String responseBody = remoteCallHelper.makeRemoteRequest(commaSeparatedUrls, uriBuilder.getPath(), uriBuilder.getQueryParams(), HttpGet.METHOD_NAME);\r\n        RemoteWasbAuthorizerResponse authorizerResponse = RESPONSE_READER.readValue(responseBody);\r\n        if (authorizerResponse == null) {\r\n            throw new WasbAuthorizationException(\"RemoteWasbAuthorizerResponse object null from remote call\");\r\n        } else if (authorizerResponse.getResponseCode() == REMOTE_CALL_SUCCESS_CODE) {\r\n            return authorizerResponse.getAuthorizationResult();\r\n        } else {\r\n            throw new WasbAuthorizationException(\"Remote authorization\" + \" service encountered an error \" + authorizerResponse.getResponseMessage());\r\n        }\r\n    } catch (WasbRemoteCallException | JsonParseException | JsonMappingException ex) {\r\n        throw new WasbAuthorizationException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getResponseCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getResponseCode()\n{\r\n    return responseCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setResponseCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResponseCode(int responseCode)\n{\r\n    this.responseCode = responseCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getAuthorizationResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getAuthorizationResult()\n{\r\n    return authorizationResult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setAuthorizationResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAuthorizationResult(boolean authorizationResult)\n{\r\n    this.authorizationResult = authorizationResult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getResponseMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getResponseMessage()\n{\r\n    return responseMessage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setResponseMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResponseMessage(String message)\n{\r\n    this.responseMessage = message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void init(Configuration conf)\n{\r\n    isEnabled = conf.getBoolean(KEY_AUTH_SERVICE_CACHING_ENABLE, KEY_AUTH_SERVICE_CACHING_ENABLE_DEFAULT);\r\n    if (isEnabled) {\r\n        LOG.debug(\"{} : Initializing CachingAuthorizer instance\", label);\r\n        cache = CacheBuilder.newBuilder().maximumSize(conf.getInt(KEY_AUTH_SERVICE_CACHING_MAX_ENTRIES, KEY_AUTH_SERVICE_CACHING_MAX_ENTRIES_DEFAULT)).expireAfterWrite(cacheEntryExpiryPeriodInMinutes, TimeUnit.MINUTES).build();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "V get(K key)\n{\r\n    if (!isEnabled) {\r\n        return null;\r\n    }\r\n    V result = cache.getIfPresent(key);\r\n    if (result == null) {\r\n        LOG.debug(\"{}: CACHE MISS: {}\", label, key.toString());\r\n    } else {\r\n        LOG.debug(\"{}: CACHE HIT: {}, {}\", label, key.toString(), result.toString());\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void put(K key, V value)\n{\r\n    if (isEnabled) {\r\n        LOG.debug(\"{}: CACHE PUT: {}, {}\", label, key.toString(), value.toString());\r\n        cache.put(key, value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clear()\n{\r\n    if (isEnabled) {\r\n        cache.invalidateAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getAccessType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAccessType()\n{\r\n    return accessType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOwner()\n{\r\n    return owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == this) {\r\n        return true;\r\n    }\r\n    if (o == null) {\r\n        return false;\r\n    }\r\n    if (!(o instanceof CachedAuthorizerEntry)) {\r\n        return false;\r\n    }\r\n    CachedAuthorizerEntry c = (CachedAuthorizerEntry) o;\r\n    return this.getPath().equals(c.getPath()) && this.getAccessType().equals(c.getAccessType()) && this.getOwner().equals(c.getOwner());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return this.toString().hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return path + \":\" + accessType + \":\" + owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStorageAccount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStorageAccount()\n{\r\n    return storageAccount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getContainer()\n{\r\n    return container;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == this) {\r\n        return true;\r\n    }\r\n    if (o == null) {\r\n        return false;\r\n    }\r\n    if (!(o instanceof CachedSASKeyEntry)) {\r\n        return false;\r\n    }\r\n    CachedSASKeyEntry c = (CachedSASKeyEntry) o;\r\n    return this.getStorageAccount().equals(c.getStorageAccount()) && this.getContainer().equals(c.getContainer()) && this.getPath().equals(c.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return this.toString().hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return storageAccount + \":\" + container + \":\" + path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getFileSystemInstanceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UUID getFileSystemInstanceId()\n{\r\n    return fileSystemInstanceId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getMetricsRegistryInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MetricsInfo getMetricsRegistryInfo()\n{\r\n    return registry.info();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "setAccountName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAccountName(String accountName)\n{\r\n    registry.tag(\"accountName\", \"Name of the Azure Storage account that these metrics are going against\", accountName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "setContainerName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setContainerName(String containerName)\n{\r\n    registry.tag(\"containerName\", \"Name of the Azure Storage container that these metrics are going against\", containerName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "webResponse",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void webResponse()\n{\r\n    numberOfWebResponses.incr();\r\n    inMemoryNumberOfWebResponses.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentWebResponses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentWebResponses()\n{\r\n    return inMemoryNumberOfWebResponses.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "fileCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fileCreated()\n{\r\n    numberOfFilesCreated.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "fileDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fileDeleted()\n{\r\n    numberOfFilesDeleted.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "directoryCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void directoryCreated()\n{\r\n    numberOfDirectoriesCreated.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "directoryDeleted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void directoryDeleted()\n{\r\n    numberOfDirectoriesDeleted.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "updateBytesWrittenInLastSecond",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateBytesWrittenInLastSecond(long currentBytesWritten)\n{\r\n    bytesWrittenInLastSecond.set(currentBytesWritten);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "updateBytesReadInLastSecond",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateBytesReadInLastSecond(long currentBytesRead)\n{\r\n    bytesReadInLastSecond.set(currentBytesRead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "currentUploadBytesPerSecond",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void currentUploadBytesPerSecond(long bytesPerSecond)\n{\r\n    if (bytesPerSecond > currentMaximumUploadBytesPerSecond) {\r\n        currentMaximumUploadBytesPerSecond = bytesPerSecond;\r\n        maximumUploadBytesPerSecond.set(bytesPerSecond);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "currentDownloadBytesPerSecond",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void currentDownloadBytesPerSecond(long bytesPerSecond)\n{\r\n    if (bytesPerSecond > currentMaximumDownloadBytesPerSecond) {\r\n        currentMaximumDownloadBytesPerSecond = bytesPerSecond;\r\n        maximumDownloadBytesPerSecond.set(bytesPerSecond);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "rawBytesUploaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void rawBytesUploaded(long numberOfBytes)\n{\r\n    rawBytesUploaded.incr(numberOfBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "rawBytesDownloaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void rawBytesDownloaded(long numberOfBytes)\n{\r\n    rawBytesDownloaded.incr(numberOfBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "blockUploaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void blockUploaded(long latency)\n{\r\n    currentBlockUploadLatency.addPoint(latency);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "blockDownloaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void blockDownloaded(long latency)\n{\r\n    currentBlockDownloadLatency.addPoint(latency);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "clientErrorEncountered",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clientErrorEncountered()\n{\r\n    clientErrors.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "serverErrorEncountered",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serverErrorEncountered()\n{\r\n    serverErrors.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getBlockUploadLatency",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBlockUploadLatency()\n{\r\n    return currentBlockUploadLatency.getCurrentAverage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getBlockDownloadLatency",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBlockDownloadLatency()\n{\r\n    return currentBlockDownloadLatency.getCurrentAverage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentMaximumUploadBandwidth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCurrentMaximumUploadBandwidth()\n{\r\n    return currentMaximumUploadBytesPerSecond;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getCurrentMaximumDownloadBandwidth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCurrentMaximumDownloadBandwidth()\n{\r\n    return currentMaximumDownloadBytesPerSecond;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void getMetrics(MetricsCollector builder, boolean all)\n{\r\n    averageBlockDownloadLatencyMs.set(currentBlockDownloadLatency.getCurrentAverage());\r\n    averageBlockUploadLatencyMs.set(currentBlockUploadLatency.getCurrentAverage());\r\n    registry.snapshot(builder.addRecord(registry.info().name()), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init(AbfsConfiguration abfsConfiguration)\n{\r\n    tokenFetchRetryPolicy = abfsConfiguration.getOauthTokenFetchRetryPolicy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getTokenUsingClientCreds",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AzureADToken getTokenUsingClientCreds(String authEndpoint, String clientId, String clientSecret) throws IOException\n{\r\n    Preconditions.checkNotNull(authEndpoint, \"authEndpoint\");\r\n    Preconditions.checkNotNull(clientId, \"clientId\");\r\n    Preconditions.checkNotNull(clientSecret, \"clientSecret\");\r\n    boolean isVersion2AuthenticationEndpoint = authEndpoint.contains(\"/oauth2/v2.0/\");\r\n    QueryParams qp = new QueryParams();\r\n    if (isVersion2AuthenticationEndpoint) {\r\n        qp.add(\"scope\", SCOPE);\r\n    } else {\r\n        qp.add(\"resource\", RESOURCE_NAME);\r\n    }\r\n    qp.add(\"grant_type\", \"client_credentials\");\r\n    qp.add(\"client_id\", clientId);\r\n    qp.add(\"client_secret\", clientSecret);\r\n    LOG.debug(\"AADToken: starting to fetch token using client creds for client ID \" + clientId);\r\n    return getTokenCall(authEndpoint, qp.serialize(), null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getTokenFromMsi",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "AzureADToken getTokenFromMsi(final String authEndpoint, final String tenantGuid, final String clientId, String authority, boolean bypassCache) throws IOException\n{\r\n    QueryParams qp = new QueryParams();\r\n    qp.add(\"api-version\", \"2018-02-01\");\r\n    qp.add(\"resource\", RESOURCE_NAME);\r\n    if (tenantGuid != null && tenantGuid.length() > 0) {\r\n        authority = authority + tenantGuid;\r\n        LOG.debug(\"MSI authority : {}\", authority);\r\n        qp.add(\"authority\", authority);\r\n    }\r\n    if (clientId != null && clientId.length() > 0) {\r\n        qp.add(\"client_id\", clientId);\r\n    }\r\n    if (bypassCache) {\r\n        qp.add(\"bypass_cache\", \"true\");\r\n    }\r\n    Hashtable<String, String> headers = new Hashtable<>();\r\n    headers.put(\"Metadata\", \"true\");\r\n    LOG.debug(\"AADToken: starting to fetch token using MSI\");\r\n    return getTokenCall(authEndpoint, qp.serialize(), headers, \"GET\", true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getTokenUsingRefreshToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "AzureADToken getTokenUsingRefreshToken(final String authEndpoint, final String clientId, final String refreshToken) throws IOException\n{\r\n    QueryParams qp = new QueryParams();\r\n    qp.add(\"grant_type\", \"refresh_token\");\r\n    qp.add(\"refresh_token\", refreshToken);\r\n    if (clientId != null) {\r\n        qp.add(\"client_id\", clientId);\r\n    }\r\n    LOG.debug(\"AADToken: starting to fetch token using refresh token for client ID \" + clientId);\r\n    return getTokenCall(authEndpoint, qp.serialize(), null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getTokenCall",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureADToken getTokenCall(String authEndpoint, String body, Hashtable<String, String> headers, String httpMethod) throws IOException\n{\r\n    return getTokenCall(authEndpoint, body, headers, httpMethod, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getTokenCall",
  "errType" : [ "HttpException", "IOException", "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "AzureADToken getTokenCall(String authEndpoint, String body, Hashtable<String, String> headers, String httpMethod, boolean isMsi) throws IOException\n{\r\n    AzureADToken token = null;\r\n    int httperror = 0;\r\n    IOException ex = null;\r\n    boolean succeeded = false;\r\n    boolean isRecoverableFailure = true;\r\n    int retryCount = 0;\r\n    boolean shouldRetry;\r\n    LOG.trace(\"First execution of REST operation getTokenSingleCall\");\r\n    do {\r\n        httperror = 0;\r\n        ex = null;\r\n        try {\r\n            token = getTokenSingleCall(authEndpoint, body, headers, httpMethod, isMsi);\r\n        } catch (HttpException e) {\r\n            httperror = e.httpErrorCode;\r\n            ex = e;\r\n        } catch (IOException e) {\r\n            httperror = -1;\r\n            isRecoverableFailure = isRecoverableFailure(e);\r\n            ex = new HttpException(httperror, \"\", String.format(\"AzureADAuthenticator.getTokenCall threw %s : %s\", e.getClass().getTypeName(), e.getMessage()), authEndpoint, \"\", \"\");\r\n        }\r\n        succeeded = ((httperror == 0) && (ex == null));\r\n        shouldRetry = !succeeded && isRecoverableFailure && tokenFetchRetryPolicy.shouldRetry(retryCount, httperror);\r\n        retryCount++;\r\n        if (shouldRetry) {\r\n            LOG.debug(\"Retrying getTokenSingleCall. RetryCount = {}\", retryCount);\r\n            try {\r\n                Thread.sleep(tokenFetchRetryPolicy.getRetryInterval(retryCount));\r\n            } catch (InterruptedException e) {\r\n                Thread.currentThread().interrupt();\r\n            }\r\n        }\r\n    } while (shouldRetry);\r\n    if (!succeeded) {\r\n        throw ex;\r\n    }\r\n    return token;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "isRecoverableFailure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRecoverableFailure(IOException e)\n{\r\n    return !(e instanceof MalformedURLException || e instanceof FileNotFoundException);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getTokenSingleCall",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "AzureADToken getTokenSingleCall(String authEndpoint, String payload, Hashtable<String, String> headers, String httpMethod, boolean isMsi) throws IOException\n{\r\n    AzureADToken token = null;\r\n    HttpURLConnection conn = null;\r\n    String urlString = authEndpoint;\r\n    httpMethod = (httpMethod == null) ? \"POST\" : httpMethod;\r\n    if (httpMethod.equals(\"GET\")) {\r\n        urlString = urlString + \"?\" + payload;\r\n    }\r\n    try {\r\n        LOG.debug(\"Requesting an OAuth token by {} to {}\", httpMethod, authEndpoint);\r\n        URL url = new URL(urlString);\r\n        conn = (HttpURLConnection) url.openConnection();\r\n        conn.setRequestMethod(httpMethod);\r\n        conn.setReadTimeout(READ_TIMEOUT);\r\n        conn.setConnectTimeout(CONNECT_TIMEOUT);\r\n        if (headers != null && headers.size() > 0) {\r\n            for (Map.Entry<String, String> entry : headers.entrySet()) {\r\n                conn.setRequestProperty(entry.getKey(), entry.getValue());\r\n            }\r\n        }\r\n        conn.setRequestProperty(\"Connection\", \"close\");\r\n        AbfsIoUtils.dumpHeadersToDebugLog(\"Request Headers\", conn.getRequestProperties());\r\n        if (httpMethod.equals(\"POST\")) {\r\n            conn.setDoOutput(true);\r\n            conn.getOutputStream().write(payload.getBytes(\"UTF-8\"));\r\n        }\r\n        int httpResponseCode = conn.getResponseCode();\r\n        LOG.debug(\"Response {}\", httpResponseCode);\r\n        AbfsIoUtils.dumpHeadersToDebugLog(\"Response Headers\", conn.getHeaderFields());\r\n        String requestId = conn.getHeaderField(\"x-ms-request-id\");\r\n        String responseContentType = conn.getHeaderField(\"Content-Type\");\r\n        long responseContentLength = conn.getHeaderFieldLong(\"Content-Length\", 0);\r\n        requestId = requestId == null ? \"\" : requestId;\r\n        if (httpResponseCode == HttpURLConnection.HTTP_OK && responseContentType.startsWith(\"application/json\") && responseContentLength > 0) {\r\n            InputStream httpResponseStream = conn.getInputStream();\r\n            token = parseTokenFromStream(httpResponseStream, isMsi);\r\n        } else {\r\n            InputStream stream = conn.getErrorStream();\r\n            if (stream == null) {\r\n                stream = conn.getInputStream();\r\n            }\r\n            String responseBody = consumeInputStream(stream, 1024);\r\n            String proxies = \"none\";\r\n            String httpProxy = System.getProperty(\"http.proxy\");\r\n            String httpsProxy = System.getProperty(\"https.proxy\");\r\n            if (httpProxy != null || httpsProxy != null) {\r\n                proxies = \"http:\" + httpProxy + \"; https:\" + httpsProxy;\r\n            }\r\n            String operation = \"AADToken: HTTP connection to \" + authEndpoint + \" failed for getting token from AzureAD.\";\r\n            String logMessage = operation + \" HTTP response: \" + httpResponseCode + \" \" + conn.getResponseMessage() + \" Proxies: \" + proxies + (responseBody.isEmpty() ? \"\" : (\"\\nFirst 1K of Body: \" + responseBody));\r\n            LOG.debug(logMessage);\r\n            if (httpResponseCode == HttpURLConnection.HTTP_OK) {\r\n                throw new UnexpectedResponseException(httpResponseCode, requestId, operation + \" Unexpected response.\" + \" Check configuration, URLs and proxy settings.\" + \" proxies=\" + proxies, authEndpoint, responseContentType, responseBody);\r\n            } else {\r\n                throw new HttpException(httpResponseCode, requestId, operation, authEndpoint, responseContentType, responseBody);\r\n            }\r\n        }\r\n    } finally {\r\n        if (conn != null) {\r\n            conn.disconnect();\r\n        }\r\n    }\r\n    return token;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "parseTokenFromStream",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "AzureADToken parseTokenFromStream(InputStream httpResponseStream, boolean isMsi) throws IOException\n{\r\n    AzureADToken token = new AzureADToken();\r\n    try {\r\n        int expiryPeriodInSecs = 0;\r\n        long expiresOnInSecs = -1;\r\n        JsonFactory jf = new JsonFactory();\r\n        JsonParser jp = jf.createParser(httpResponseStream);\r\n        String fieldName, fieldValue;\r\n        jp.nextToken();\r\n        while (jp.hasCurrentToken()) {\r\n            if (jp.getCurrentToken() == JsonToken.FIELD_NAME) {\r\n                fieldName = jp.getCurrentName();\r\n                jp.nextToken();\r\n                fieldValue = jp.getText();\r\n                if (fieldName.equals(\"access_token\")) {\r\n                    token.setAccessToken(fieldValue);\r\n                }\r\n                if (fieldName.equals(\"expires_in\")) {\r\n                    expiryPeriodInSecs = Integer.parseInt(fieldValue);\r\n                }\r\n                if (fieldName.equals(\"expires_on\")) {\r\n                    expiresOnInSecs = Long.parseLong(fieldValue);\r\n                }\r\n            }\r\n            jp.nextToken();\r\n        }\r\n        jp.close();\r\n        if (expiresOnInSecs > 0) {\r\n            LOG.debug(\"Expiry based on expires_on: {}\", expiresOnInSecs);\r\n            token.setExpiry(new Date(expiresOnInSecs * 1000));\r\n        } else {\r\n            if (isMsi) {\r\n                throw new UnsupportedOperationException(\"MSI Responded with invalid expires_on\");\r\n            }\r\n            LOG.debug(\"Expiry based on expires_in: {}\", expiryPeriodInSecs);\r\n            long expiry = System.currentTimeMillis();\r\n            expiry = expiry + expiryPeriodInSecs * 1000L;\r\n            token.setExpiry(new Date(expiry));\r\n        }\r\n        LOG.debug(\"AADToken: fetched token with expiry {}, expiresOn passed: {}\", token.getExpiry().toString(), expiresOnInSecs);\r\n    } catch (Exception ex) {\r\n        LOG.debug(\"AADToken: got exception when parsing json token \" + ex.toString());\r\n        throw ex;\r\n    } finally {\r\n        httpResponseStream.close();\r\n    }\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "consumeInputStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String consumeInputStream(InputStream inStream, int length) throws IOException\n{\r\n    if (inStream == null) {\r\n        return \"\";\r\n    }\r\n    byte[] b = new byte[length];\r\n    int totalBytesRead = 0;\r\n    int bytesRead = 0;\r\n    do {\r\n        bytesRead = inStream.read(b, totalBytesRead, length - totalBytesRead);\r\n        if (bytesRead > 0) {\r\n            totalBytesRead += bytesRead;\r\n        }\r\n    } while (bytesRead >= 0 && totalBytesRead < length);\r\n    return new String(b, 0, totalBytesRead, StandardCharsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withWriteBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withWriteBufferSize(final int writeBufferSize)\n{\r\n    this.writeBufferSize = writeBufferSize;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "enableFlush",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext enableFlush(final boolean enableFlush)\n{\r\n    this.enableFlush = enableFlush;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "enableSmallWriteOptimization",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext enableSmallWriteOptimization(final boolean enableSmallWriteOptimization)\n{\r\n    this.enableSmallWriteOptimization = enableSmallWriteOptimization;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "disableOutputStreamFlush",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext disableOutputStreamFlush(final boolean disableOutputStreamFlush)\n{\r\n    this.disableOutputStreamFlush = disableOutputStreamFlush;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withStreamStatistics(final AbfsOutputStreamStatistics streamStatistics)\n{\r\n    this.streamStatistics = streamStatistics;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withAppendBlob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withAppendBlob(final boolean isAppendBlob)\n{\r\n    this.isAppendBlob = isAppendBlob;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withBlockFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withBlockFactory(final DataBlocks.BlockFactory blockFactory)\n{\r\n    this.blockFactory = blockFactory;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withBlockOutputActiveBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withBlockOutputActiveBlocks(final int blockOutputActiveBlocks)\n{\r\n    this.blockOutputActiveBlocks = blockOutputActiveBlocks;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withClient(final AbfsClient client)\n{\r\n    this.client = client;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withPosition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withPosition(final long position)\n{\r\n    this.position = position;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withFsStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withFsStatistics(final FileSystem.Statistics statistics)\n{\r\n    this.statistics = statistics;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withPath(final String path)\n{\r\n    this.path = path;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withExecutorService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withExecutorService(final ExecutorService executorService)\n{\r\n    this.executorService = executorService;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withTracingContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withTracingContext(final TracingContext tracingContext)\n{\r\n    this.tracingContext = tracingContext;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext build()\n{\r\n    if (streamStatistics == null) {\r\n        streamStatistics = new AbfsOutputStreamStatisticsImpl();\r\n    }\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withWriteMaxConcurrentRequestCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withWriteMaxConcurrentRequestCount(final int writeMaxConcurrentRequestCount)\n{\r\n    this.writeMaxConcurrentRequestCount = writeMaxConcurrentRequestCount;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withMaxWriteRequestsToQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withMaxWriteRequestsToQueue(final int maxWriteRequestsToQueue)\n{\r\n    this.maxWriteRequestsToQueue = maxWriteRequestsToQueue;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withLease",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamContext withLease(final AbfsLease lease)\n{\r\n    this.lease = lease;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getWriteBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getWriteBufferSize()\n{\r\n    return writeBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isEnableFlush",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnableFlush()\n{\r\n    return enableFlush;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isDisableOutputStreamFlush",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDisableOutputStreamFlush()\n{\r\n    return disableOutputStreamFlush;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsOutputStreamStatistics getStreamStatistics()\n{\r\n    return streamStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isAppendBlob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isAppendBlob()\n{\r\n    return isAppendBlob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getWriteMaxConcurrentRequestCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getWriteMaxConcurrentRequestCount()\n{\r\n    return this.writeMaxConcurrentRequestCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMaxWriteRequestsToQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxWriteRequestsToQueue()\n{\r\n    return this.maxWriteRequestsToQueue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isEnableSmallWriteOptimization",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnableSmallWriteOptimization()\n{\r\n    return this.enableSmallWriteOptimization;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLease",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsLease getLease()\n{\r\n    return this.lease;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLeaseId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getLeaseId()\n{\r\n    if (this.lease == null) {\r\n        return null;\r\n    }\r\n    return this.lease.getLeaseID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBlockFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataBlocks.BlockFactory getBlockFactory()\n{\r\n    return blockFactory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBlockOutputActiveBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBlockOutputActiveBlocks()\n{\r\n    return blockOutputActiveBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsClient getClient()\n{\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem.Statistics getStatistics()\n{\r\n    return statistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getPosition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPosition()\n{\r\n    return position;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getExecutorService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExecutorService getExecutorService()\n{\r\n    return executorService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTracingContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TracingContext getTracingContext()\n{\r\n    return tracingContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String validate(final String configValue) throws InvalidConfigurationValueException\n{\r\n    String result = super.validate((configValue));\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    if (!Base64.validateIsBase64String(configValue)) {\r\n        throw new InvalidConfigurationValueException(getConfigKey());\r\n    }\r\n    return configValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "checkStreamState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkStreamState() throws IOException\n{\r\n    if (lastError != null) {\r\n        throw lastError;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    switch(capability.toLowerCase(Locale.ENGLISH)) {\r\n        case StreamCapabilities.HSYNC:\r\n        case StreamCapabilities.HFLUSH:\r\n            return true;\r\n        default:\r\n            return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (closed) {\r\n        return;\r\n    }\r\n    LOG.debug(\"Closing page blob output stream.\");\r\n    flush();\r\n    checkStreamState();\r\n    ioThreadPool.shutdown();\r\n    try {\r\n        LOG.debug(ioThreadPool.toString());\r\n        if (!ioThreadPool.awaitTermination(10, TimeUnit.MINUTES)) {\r\n            LOG.debug(\"Timed out after 10 minutes waiting for IO requests to finish\");\r\n            NativeAzureFileSystemHelper.logAllLiveStackTraces();\r\n            LOG.debug(ioThreadPool.toString());\r\n            throw new IOException(\"Timed out waiting for IO requests to finish\");\r\n        }\r\n    } catch (InterruptedException e) {\r\n        LOG.debug(\"Caught InterruptedException\");\r\n        Thread.currentThread().interrupt();\r\n    }\r\n    closed = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "flushIOBuffers",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void flushIOBuffers()\n{\r\n    if (outBuffer.size() == 0) {\r\n        return;\r\n    }\r\n    lastQueuedTask = new WriteRequest(outBuffer.toByteArray());\r\n    ioThreadPool.execute(lastQueuedTask);\r\n    outBuffer = new ByteArrayOutputStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "waitForLastFlushCompletion",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitForLastFlushCompletion() throws IOException\n{\r\n    try {\r\n        if (lastQueuedTask != null) {\r\n            lastQueuedTask.waitTillDone();\r\n        }\r\n    } catch (InterruptedException e1) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "conditionalExtendFile",
  "errType" : [ "StorageException", "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void conditionalExtendFile()\n{\r\n    final long MAX_PAGE_BLOB_SIZE = 1024L * 1024L * 1024L * 1024L;\r\n    if (currentBlobSize == MAX_PAGE_BLOB_SIZE) {\r\n        return;\r\n    }\r\n    if (currentBlobSize - currentBlobOffset <= MAX_RAW_BYTES_PER_REQUEST) {\r\n        CloudPageBlob cloudPageBlob = (CloudPageBlob) blob.getBlob();\r\n        long newSize = currentBlobSize + configuredPageBlobExtensionSize;\r\n        if (newSize > MAX_PAGE_BLOB_SIZE) {\r\n            newSize = MAX_PAGE_BLOB_SIZE;\r\n        }\r\n        final int MAX_RETRIES = 3;\r\n        int retries = 1;\r\n        boolean resizeDone = false;\r\n        while (!resizeDone && retries <= MAX_RETRIES) {\r\n            try {\r\n                cloudPageBlob.resize(newSize);\r\n                resizeDone = true;\r\n                currentBlobSize = newSize;\r\n            } catch (StorageException e) {\r\n                LOG.warn(\"Failed to extend size of \" + cloudPageBlob.getUri());\r\n                try {\r\n                    Thread.sleep(2000 * retries * retries);\r\n                } catch (InterruptedException e1) {\r\n                    Thread.currentThread().interrupt();\r\n                }\r\n            } finally {\r\n                retries++;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "flush",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void flush() throws IOException\n{\r\n    checkStreamState();\r\n    flushIOBuffers();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(final byte[] data) throws IOException\n{\r\n    write(data, 0, data.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(final byte[] data, final int offset, final int length) throws IOException\n{\r\n    if (offset < 0 || length < 0 || length > data.length - offset) {\r\n        throw new IndexOutOfBoundsException();\r\n    }\r\n    writeInternal(data, offset, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(final int byteVal) throws IOException\n{\r\n    write(new byte[] { (byte) (byteVal & 0xFF) });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "writeInternal",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void writeInternal(final byte[] data, int offset, int length) throws IOException\n{\r\n    while (length > 0) {\r\n        checkStreamState();\r\n        final int availableBufferBytes = MAX_DATA_BYTES_PER_REQUEST - this.outBuffer.size();\r\n        final int nextWrite = Math.min(availableBufferBytes, length);\r\n        outBuffer.write(data, offset, nextWrite);\r\n        offset += nextWrite;\r\n        length -= nextWrite;\r\n        if (outBuffer.size() > MAX_DATA_BYTES_PER_REQUEST) {\r\n            throw new RuntimeException(\"Internal error: maximum write size \" + Integer.toString(MAX_DATA_BYTES_PER_REQUEST) + \"exceeded.\");\r\n        }\r\n        if (outBuffer.size() == MAX_DATA_BYTES_PER_REQUEST) {\r\n            flushIOBuffers();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hsync",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void hsync() throws IOException\n{\r\n    LOG.debug(\"Entering PageBlobOutputStream#hsync().\");\r\n    long start = System.currentTimeMillis();\r\n    flush();\r\n    LOG.debug(ioThreadPool.toString());\r\n    try {\r\n        if (lastQueuedTask != null) {\r\n            lastQueuedTask.waitTillDone();\r\n        }\r\n    } catch (InterruptedException e1) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n    checkStreamState();\r\n    LOG.debug(\"Leaving PageBlobOutputStream#hsync(). Total hsync duration = \" + (System.currentTimeMillis() - start) + \" msec.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hflush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hflush() throws IOException\n{\r\n    hsync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "sync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sync() throws IOException\n{\r\n    hflush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "killIoThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killIoThreads()\n{\r\n    ioThreadPool.shutdownNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "validate",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Integer validate(final String configValue) throws InvalidConfigurationValueException\n{\r\n    Integer result = super.validate(configValue);\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    try {\r\n        result = Integer.parseInt(configValue);\r\n        if (getThrowIfInvalid() && (result != outlier) && (result < this.min || result > this.max)) {\r\n            throw new InvalidConfigurationValueException(getConfigKey());\r\n        }\r\n        if (result == outlier) {\r\n            return result;\r\n        }\r\n        if (result < this.min) {\r\n            return this.min;\r\n        }\r\n        if (result > this.max) {\r\n            return this.max;\r\n        }\r\n    } catch (NumberFormatException ex) {\r\n        throw new InvalidConfigurationValueException(getConfigKey(), ex);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getUriDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getUriDefaultPort()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "createInputStreamId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createInputStreamId()\n{\r\n    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int read(long position, byte[] buffer, int offset, int length) throws IOException\n{\r\n    synchronized (this) {\r\n        if (closed) {\r\n            throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n        }\r\n    }\r\n    LOG.debug(\"pread requested offset = {} len = {} bufferedPreadDisabled = {}\", offset, length, bufferedPreadDisabled);\r\n    if (!bufferedPreadDisabled) {\r\n        return super.read(position, buffer, offset, length);\r\n    }\r\n    validatePositionedReadArgs(position, buffer, offset, length);\r\n    if (length == 0) {\r\n        return 0;\r\n    }\r\n    if (streamStatistics != null) {\r\n        streamStatistics.readOperationStarted();\r\n    }\r\n    int bytesRead = readRemote(position, buffer, offset, length, tracingContext);\r\n    if (statistics != null) {\r\n        statistics.incrementBytesRead(bytesRead);\r\n    }\r\n    if (streamStatistics != null) {\r\n        streamStatistics.bytesRead(bytesRead);\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    byte[] b = new byte[1];\r\n    int numberOfBytesRead = read(b, 0, 1);\r\n    if (numberOfBytesRead < 0) {\r\n        return -1;\r\n    } else {\r\n        return (b[0] & 0xFF);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "int read(final byte[] b, final int off, final int len) throws IOException\n{\r\n    if (b != null) {\r\n        LOG.debug(\"read requested b.length = {} offset = {} len = {}\", b.length, off, len);\r\n    } else {\r\n        LOG.debug(\"read requested b = null offset = {} len = {}\", off, len);\r\n    }\r\n    int currentOff = off;\r\n    int currentLen = len;\r\n    int lastReadBytes;\r\n    int totalReadBytes = 0;\r\n    if (streamStatistics != null) {\r\n        streamStatistics.readOperationStarted();\r\n    }\r\n    incrementReadOps();\r\n    do {\r\n        long filePosAtStartOfBuffer = fCursor - limit;\r\n        if (nextReadPos >= filePosAtStartOfBuffer && nextReadPos <= fCursor) {\r\n            bCursor = (int) (nextReadPos - filePosAtStartOfBuffer);\r\n            if (bCursor != limit && streamStatistics != null) {\r\n                streamStatistics.seekInBuffer();\r\n            }\r\n        } else {\r\n            fCursor = nextReadPos;\r\n            limit = 0;\r\n            bCursor = 0;\r\n        }\r\n        if (shouldReadFully()) {\r\n            lastReadBytes = readFileCompletely(b, currentOff, currentLen);\r\n        } else if (shouldReadLastBlock()) {\r\n            lastReadBytes = readLastBlock(b, currentOff, currentLen);\r\n        } else {\r\n            lastReadBytes = readOneBlock(b, currentOff, currentLen);\r\n        }\r\n        if (lastReadBytes > 0) {\r\n            currentOff += lastReadBytes;\r\n            currentLen -= lastReadBytes;\r\n            totalReadBytes += lastReadBytes;\r\n        }\r\n        if (currentLen <= 0 || currentLen > b.length - currentOff) {\r\n            break;\r\n        }\r\n    } while (lastReadBytes > 0);\r\n    return totalReadBytes > 0 ? totalReadBytes : lastReadBytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "shouldReadFully",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldReadFully()\n{\r\n    return this.firstRead && this.context.readSmallFilesCompletely() && this.contentLength <= this.bufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "shouldReadLastBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean shouldReadLastBlock()\n{\r\n    long footerStart = max(0, this.contentLength - FOOTER_SIZE);\r\n    return this.firstRead && this.context.optimizeFooterRead() && this.fCursor >= footerStart;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readOneBlock",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int readOneBlock(final byte[] b, final int off, final int len) throws IOException\n{\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    if (!validate(b, off, len)) {\r\n        return -1;\r\n    }\r\n    if (bCursor == limit) {\r\n        if (fCursor >= contentLength) {\r\n            return -1;\r\n        }\r\n        long bytesRead = 0;\r\n        bCursor = 0;\r\n        limit = 0;\r\n        if (buffer == null) {\r\n            LOG.debug(\"created new buffer size {}\", bufferSize);\r\n            buffer = new byte[bufferSize];\r\n        }\r\n        if (alwaysReadBufferSize) {\r\n            bytesRead = readInternal(fCursor, buffer, 0, bufferSize, false);\r\n        } else {\r\n            if (-1 == fCursorAfterLastRead || fCursorAfterLastRead == fCursor || b.length >= bufferSize) {\r\n                LOG.debug(\"Sequential read with read ahead size of {}\", bufferSize);\r\n                bytesRead = readInternal(fCursor, buffer, 0, bufferSize, false);\r\n            } else {\r\n                int lengthWithReadAhead = Math.min(b.length + readAheadRange, bufferSize);\r\n                LOG.debug(\"Random read with read ahead size of {}\", lengthWithReadAhead);\r\n                bytesRead = readInternal(fCursor, buffer, 0, lengthWithReadAhead, true);\r\n            }\r\n        }\r\n        if (firstRead) {\r\n            firstRead = false;\r\n        }\r\n        if (bytesRead == -1) {\r\n            return -1;\r\n        }\r\n        limit += bytesRead;\r\n        fCursor += bytesRead;\r\n        fCursorAfterLastRead = fCursor;\r\n    }\r\n    return copyToUserBuffer(b, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readFileCompletely",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int readFileCompletely(final byte[] b, final int off, final int len) throws IOException\n{\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    if (!validate(b, off, len)) {\r\n        return -1;\r\n    }\r\n    savePointerState();\r\n    bCursor = (int) fCursor;\r\n    return optimisedRead(b, off, len, 0, contentLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readLastBlock",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int readLastBlock(final byte[] b, final int off, final int len) throws IOException\n{\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    if (!validate(b, off, len)) {\r\n        return -1;\r\n    }\r\n    savePointerState();\r\n    long lastBlockStart = max(0, contentLength - bufferSize);\r\n    bCursor = (int) (fCursor - lastBlockStart);\r\n    long actualLenToRead = min(bufferSize, contentLength);\r\n    return optimisedRead(b, off, len, lastBlockStart, actualLenToRead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "optimisedRead",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int optimisedRead(final byte[] b, final int off, final int len, final long readFrom, final long actualLen) throws IOException\n{\r\n    fCursor = readFrom;\r\n    int totalBytesRead = 0;\r\n    int lastBytesRead = 0;\r\n    try {\r\n        buffer = new byte[bufferSize];\r\n        for (int i = 0; i < MAX_OPTIMIZED_READ_ATTEMPTS && fCursor < contentLength; i++) {\r\n            lastBytesRead = readInternal(fCursor, buffer, limit, (int) actualLen - limit, true);\r\n            if (lastBytesRead > 0) {\r\n                totalBytesRead += lastBytesRead;\r\n                limit += lastBytesRead;\r\n                fCursor += lastBytesRead;\r\n                fCursorAfterLastRead = fCursor;\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Optimized read failed. Defaulting to readOneBlock {}\", e);\r\n        restorePointerState();\r\n        return readOneBlock(b, off, len);\r\n    } finally {\r\n        firstRead = false;\r\n    }\r\n    if (totalBytesRead < 1) {\r\n        restorePointerState();\r\n        return -1;\r\n    }\r\n    if (fCursor < contentLength && bCursor > limit) {\r\n        restorePointerState();\r\n        return readOneBlock(b, off, len);\r\n    }\r\n    return copyToUserBuffer(b, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "savePointerState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void savePointerState()\n{\r\n    this.limitBkp = this.limit;\r\n    this.fCursorBkp = this.fCursor;\r\n    this.fCursorAfterLastReadBkp = this.fCursorAfterLastRead;\r\n    this.bCursorBkp = this.bCursor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "restorePointerState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void restorePointerState()\n{\r\n    this.limit = this.limitBkp;\r\n    this.fCursor = this.fCursorBkp;\r\n    this.fCursorAfterLastRead = this.fCursorAfterLastReadBkp;\r\n    this.bCursor = this.bCursorBkp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean validate(final byte[] b, final int off, final int len) throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n    Preconditions.checkNotNull(b);\r\n    LOG.debug(\"read one block requested b.length = {} off {} len {}\", b.length, off, len);\r\n    if (this.available() == 0) {\r\n        return false;\r\n    }\r\n    if (off < 0 || len < 0 || len > b.length - off) {\r\n        throw new IndexOutOfBoundsException();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "copyToUserBuffer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int copyToUserBuffer(byte[] b, int off, int len)\n{\r\n    int bytesRemaining = limit - bCursor;\r\n    int bytesToRead = min(len, bytesRemaining);\r\n    System.arraycopy(buffer, bCursor, b, off, bytesToRead);\r\n    bCursor += bytesToRead;\r\n    nextReadPos += bytesToRead;\r\n    if (statistics != null) {\r\n        statistics.incrementBytesRead(bytesToRead);\r\n    }\r\n    if (streamStatistics != null) {\r\n        streamStatistics.bytesReadFromBuffer(bytesToRead);\r\n        streamStatistics.bytesRead(bytesToRead);\r\n    }\r\n    return bytesToRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readInternal",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int readInternal(final long position, final byte[] b, final int offset, final int length, final boolean bypassReadAhead) throws IOException\n{\r\n    if (readAheadEnabled && !bypassReadAhead) {\r\n        if (offset != 0) {\r\n            throw new IllegalArgumentException(\"readahead buffers cannot have non-zero buffer offsets\");\r\n        }\r\n        int receivedBytes;\r\n        int numReadAheads = this.readAheadQueueDepth;\r\n        long nextOffset = position;\r\n        long nextSize = min((long) bufferSize, contentLength - nextOffset);\r\n        LOG.debug(\"read ahead enabled issuing readheads num = {}\", numReadAheads);\r\n        TracingContext readAheadTracingContext = new TracingContext(tracingContext);\r\n        readAheadTracingContext.setPrimaryRequestID();\r\n        while (numReadAheads > 0 && nextOffset < contentLength) {\r\n            LOG.debug(\"issuing read ahead requestedOffset = {} requested size {}\", nextOffset, nextSize);\r\n            ReadBufferManager.getBufferManager().queueReadAhead(this, nextOffset, (int) nextSize, new TracingContext(readAheadTracingContext));\r\n            nextOffset = nextOffset + nextSize;\r\n            numReadAheads--;\r\n            nextSize = min((long) readAheadBlockSize, contentLength - nextOffset);\r\n        }\r\n        receivedBytes = ReadBufferManager.getBufferManager().getBlock(this, position, length, b);\r\n        bytesFromReadAhead += receivedBytes;\r\n        if (receivedBytes > 0) {\r\n            incrementReadOps();\r\n            LOG.debug(\"Received data from read ahead, not doing remote read\");\r\n            if (streamStatistics != null) {\r\n                streamStatistics.readAheadBytesRead(receivedBytes);\r\n            }\r\n            return receivedBytes;\r\n        }\r\n        receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\r\n        return receivedBytes;\r\n    } else {\r\n        LOG.debug(\"read ahead disabled, reading remote\");\r\n        return readRemote(position, b, offset, length, new TracingContext(tracingContext));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readRemote",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "int readRemote(long position, byte[] b, int offset, int length, TracingContext tracingContext) throws IOException\n{\r\n    if (position < 0) {\r\n        throw new IllegalArgumentException(\"attempting to read from negative offset\");\r\n    }\r\n    if (position >= contentLength) {\r\n        return -1;\r\n    }\r\n    if (b == null) {\r\n        throw new IllegalArgumentException(\"null byte array passed in to read() method\");\r\n    }\r\n    if (offset >= b.length) {\r\n        throw new IllegalArgumentException(\"offset greater than length of array\");\r\n    }\r\n    if (length < 0) {\r\n        throw new IllegalArgumentException(\"requested read length is less than zero\");\r\n    }\r\n    if (length > (b.length - offset)) {\r\n        throw new IllegalArgumentException(\"requested read length is more than will fit after requested offset in buffer\");\r\n    }\r\n    final AbfsRestOperation op;\r\n    AbfsPerfTracker tracker = client.getAbfsPerfTracker();\r\n    try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"readRemote\", \"read\")) {\r\n        if (streamStatistics != null) {\r\n            streamStatistics.remoteReadOperation();\r\n        }\r\n        LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);\r\n        op = client.read(path, position, b, offset, length, tolerateOobAppends ? \"*\" : eTag, cachedSasToken.get(), tracingContext);\r\n        cachedSasToken.update(op.getSasToken());\r\n        LOG.debug(\"issuing HTTP GET request params position = {} b.length = {} \" + \"offset = {} length = {}\", position, b.length, offset, length);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n        incrementReadOps();\r\n    } catch (AzureBlobFileSystemException ex) {\r\n        if (ex instanceof AbfsRestOperationException) {\r\n            AbfsRestOperationException ere = (AbfsRestOperationException) ex;\r\n            if (ere.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\r\n                throw new FileNotFoundException(ere.getMessage());\r\n            }\r\n        }\r\n        throw new IOException(ex);\r\n    }\r\n    long bytesRead = op.getResult().getBytesReceived();\r\n    if (streamStatistics != null) {\r\n        streamStatistics.remoteBytesRead(bytesRead);\r\n    }\r\n    if (bytesRead > Integer.MAX_VALUE) {\r\n        throw new IOException(\"Unexpected Content-Length\");\r\n    }\r\n    LOG.debug(\"HTTP request read bytes = {}\", bytesRead);\r\n    bytesFromRemoteRead += bytesRead;\r\n    return (int) bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "incrementReadOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementReadOps()\n{\r\n    if (statistics != null) {\r\n        statistics.incrementReadOps(1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void seek(long n) throws IOException\n{\r\n    LOG.debug(\"requested seek to position {}\", n);\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n    if (n < 0) {\r\n        throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);\r\n    }\r\n    if (n > contentLength) {\r\n        throw new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF);\r\n    }\r\n    if (streamStatistics != null) {\r\n        streamStatistics.seek(n, fCursor);\r\n    }\r\n    nextReadPos = n;\r\n    LOG.debug(\"set nextReadPos to {}\", nextReadPos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n    long currentPos = getPos();\r\n    if (currentPos == contentLength) {\r\n        if (n > 0) {\r\n            throw new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF);\r\n        }\r\n    }\r\n    long newPos = currentPos + n;\r\n    if (newPos < 0) {\r\n        newPos = 0;\r\n        n = newPos - currentPos;\r\n    }\r\n    if (newPos > contentLength) {\r\n        newPos = contentLength;\r\n        n = newPos - currentPos;\r\n    }\r\n    seek(newPos);\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int available() throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n    final long remaining = this.contentLength - this.getPos();\r\n    return remaining <= Integer.MAX_VALUE ? (int) remaining : Integer.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "length",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long length() throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n    return contentLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n    return nextReadPos < 0 ? 0 : nextReadPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTracingContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TracingContext getTracingContext()\n{\r\n    return tracingContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean seekToNewSource(long l) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    LOG.debug(\"Closing {}\", this);\r\n    closed = true;\r\n    buffer = null;\r\n    ReadBufferManager.getBufferManager().purgeBuffersForStream(this);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "mark",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void mark(int readlimit)\n{\r\n    throw new UnsupportedOperationException(\"mark()/reset() not supported on this stream\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset() throws IOException\n{\r\n    throw new UnsupportedOperationException(\"mark()/reset() not supported on this stream\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "markSupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean markSupported()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "unbuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void unbuffer()\n{\r\n    buffer = null;\r\n    fCursor = fCursor - limit + bCursor;\r\n    fCursorAfterLastRead = -1;\r\n    bCursor = 0;\r\n    limit = 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    return StreamCapabilities.UNBUFFER.equals(toLowerCase(capability));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getBuffer()\n{\r\n    return buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadRange()\n{\r\n    return readAheadRange;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setCachedSasToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCachedSasToken(final CachedSASToken cachedSasToken)\n{\r\n    this.cachedSasToken = cachedSasToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStreamID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStreamID()\n{\r\n    return inputStreamId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamStatistics getStreamStatistics()\n{\r\n    return streamStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "registerListener",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerListener(Listener listener1)\n{\r\n    listener = listener1;\r\n    tracingContext.setListener(listener);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesFromReadAhead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesFromReadAhead()\n{\r\n    return bytesFromReadAhead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesFromRemoteRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesFromRemoteRead()\n{\r\n    return bytesFromRemoteRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBufferSize()\n{\r\n    return bufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadQueueDepth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadQueueDepth()\n{\r\n    return readAheadQueueDepth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "shouldAlwaysReadBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldAlwaysReadBufferSize()\n{\r\n    return alwaysReadBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(super.toString());\r\n    if (streamStatistics != null) {\r\n        sb.append(\"AbfsInputStream@(\").append(this.hashCode()).append(\"){\");\r\n        sb.append(streamStatistics.toString());\r\n        sb.append(\"}\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBCursor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBCursor()\n{\r\n    return this.bCursor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFCursor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFCursor()\n{\r\n    return this.fCursor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFCursorAfterLastRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFCursorAfterLastRead()\n{\r\n    return this.fCursorAfterLastRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLimit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLimit()\n{\r\n    return this.limit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSasTokenRenewPeriodForStreamsInSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSasTokenRenewPeriodForStreamsInSeconds()\n{\r\n    return sasTokenRenewPeriodForStreamsInSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekBackwards",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void seekBackwards(long negativeOffset)\n{\r\n    seekOps.incrementAndGet();\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.STREAM_READ_SEEK_BACKWARD_OPERATIONS);\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.STREAM_READ_SEEK_BYTES_BACKWARDS, negativeOffset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekForwards",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void seekForwards(long skipped)\n{\r\n    seekOps.incrementAndGet();\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.STREAM_READ_SEEK_FORWARD_OPERATIONS);\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.STREAM_READ_SEEK_BYTES_SKIPPED, skipped);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void seek(long seekTo, long currentPos)\n{\r\n    if (seekTo >= currentPos) {\r\n        this.seekForwards(seekTo - currentPos);\r\n    } else {\r\n        this.seekBackwards(currentPos - seekTo);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "bytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bytesRead(long bytes)\n{\r\n    bytesRead.addAndGet(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "bytesReadFromBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bytesReadFromBuffer(long bytes)\n{\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.BYTES_READ_BUFFER, bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "seekInBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void seekInBuffer()\n{\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.SEEK_IN_BUFFER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readOperationStarted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readOperationStarted()\n{\r\n    readOps.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readAheadBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readAheadBytesRead(long bytes)\n{\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.READ_AHEAD_BYTES_READ, bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "remoteBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void remoteBytesRead(long bytes)\n{\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.REMOTE_BYTES_READ, bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "remoteReadOperation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void remoteReadOperation()\n{\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.REMOTE_READ_OP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return ioStatisticsStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSeekOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSeekOperations()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.STREAM_READ_SEEK_OPERATIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getForwardSeekOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getForwardSeekOperations()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.STREAM_READ_SEEK_FORWARD_OPERATIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBackwardSeekOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBackwardSeekOperations()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.STREAM_READ_SEEK_BACKWARD_OPERATIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesRead()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.STREAM_READ_BYTES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesSkippedOnSeek",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesSkippedOnSeek()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.STREAM_READ_SEEK_BYTES_SKIPPED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesBackwardsOnSeek",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesBackwardsOnSeek()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.STREAM_READ_SEEK_BYTES_BACKWARDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSeekInBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSeekInBuffer()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.SEEK_IN_BUFFER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getReadOperations()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.STREAM_READ_OPERATIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesReadFromBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesReadFromBuffer()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.BYTES_READ_BUFFER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRemoteReadOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRemoteReadOperations()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.REMOTE_READ_OP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getReadAheadBytesRead()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.READ_AHEAD_BYTES_READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRemoteBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRemoteBytesRead()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.REMOTE_BYTES_READ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getActionHttpGetRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getActionHttpGetRequest()\n{\r\n    return ioStatisticsStore.meanStatistics().get(ACTION_HTTP_GET_REQUEST + SUFFIX_MEAN).mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"StreamStatistics{\");\r\n    sb.append(ioStatisticsStore.toString());\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "bytesToUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bytesToUpload(long bytes)\n{\r\n    bytesUpload.addAndGet(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "uploadSuccessful",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void uploadSuccessful(long bytes)\n{\r\n    bytesUploadedSuccessfully.addAndGet(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "uploadFailed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void uploadFailed(long bytes)\n{\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.BYTES_UPLOAD_FAILED, bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "timeSpentTaskWait",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DurationTracker timeSpentTaskWait()\n{\r\n    return ioStatisticsStore.trackDuration(StreamStatisticNames.TIME_SPENT_ON_TASK_WAIT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "queueShrunk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void queueShrunk()\n{\r\n    ioStatisticsStore.incrementCounter(StreamStatisticNames.QUEUE_SHRUNK_OPS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "writeCurrentBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeCurrentBuffer()\n{\r\n    writeCurrentBufferOps.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "blockAllocated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void blockAllocated()\n{\r\n    blocksAllocated.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "blockReleased",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void blockReleased()\n{\r\n    blocksReleased.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return ioStatisticsStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesToUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesToUpload()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.BYTES_TO_UPLOAD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesUploadSuccessful",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesUploadSuccessful()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.BYTES_UPLOAD_SUCCESSFUL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesUploadFailed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesUploadFailed()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.BYTES_UPLOAD_FAILED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTimeSpentOnTaskWait",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTimeSpentOnTaskWait()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.TIME_SPENT_ON_TASK_WAIT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getQueueShrunkOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getQueueShrunkOps()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.QUEUE_SHRUNK_OPS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getWriteCurrentBufferOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getWriteCurrentBufferOperations()\n{\r\n    return ioStatisticsStore.counters().get(StreamStatisticNames.WRITE_CURRENT_BUFFER_OPERATIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTimeSpentOnPutRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getTimeSpentOnPutRequest()\n{\r\n    return ioStatisticsStore.meanStatistics().get(StreamStatisticNames.TIME_SPENT_ON_PUT_REQUEST + StoreStatisticNames.SUFFIX_MEAN).mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder outputStreamStats = new StringBuilder(\"OutputStream Statistics{\");\r\n    outputStreamStats.append(ioStatisticsStore.toString());\r\n    outputStreamStats.append(\"}\");\r\n    return outputStreamStats.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "registerSuccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfInfo registerSuccess(boolean success)\n{\r\n    this.success = success;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "registerResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfInfo registerResult(AbfsPerfLoggable res)\n{\r\n    this.res = res;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "registerAggregates",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfInfo registerAggregates(Instant aggregateStart, long aggregateCount)\n{\r\n    this.aggregateStart = aggregateStart;\r\n    this.aggregateCount = aggregateCount;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "finishTracking",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsPerfInfo finishTracking()\n{\r\n    if (this.trackingEnd == null) {\r\n        this.trackingEnd = abfsPerfTracker.getLatencyInstant();\r\n    }\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "registerCallee",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfInfo registerCallee(String calleeName)\n{\r\n    this.calleeName = calleeName;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    abfsPerfTracker.trackInfo(this.finishTracking());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getCallerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCallerName()\n{\r\n    return callerName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getCalleeName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCalleeName()\n{\r\n    return calleeName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTrackingStart",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Instant getTrackingStart()\n{\r\n    return trackingStart;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getTrackingEnd",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Instant getTrackingEnd()\n{\r\n    return trackingEnd;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSuccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getSuccess()\n{\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAggregateStart",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Instant getAggregateStart()\n{\r\n    return aggregateStart;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAggregateCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAggregateCount()\n{\r\n    return aggregateCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfLoggable getResult()\n{\r\n    return res;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String validate(final String configValue) throws InvalidConfigurationValueException\n{\r\n    String result = super.validate((configValue));\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    return configValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEtag()\n{\r\n    return etag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"AbfsLocatedFileStatus{\" + \"etag='\" + etag + '\\'' + \"} \" + super.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    return super.equals(o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return super.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Text getServiceName()\n{\r\n    return new Text(getScheme());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return FileSystemUriSchemes.ABFS_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "isTokenRequired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isTokenRequired()\n{\r\n    return UserGroupInformation.isSecurityEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "addDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Token<?> addDelegationTokens(Configuration conf, Credentials creds, String renewer, String url) throws Exception\n{\r\n    if (!url.startsWith(getServiceName().toString())) {\r\n        url = getServiceName().toString() + \"://\" + url;\r\n    }\r\n    FileSystem fs = FileSystem.get(URI.create(url), conf);\r\n    Token<?> token = fs.getDelegationToken(renewer);\r\n    if (token == null) {\r\n        throw new IOException(FETCH_FAILED + \": \" + url);\r\n    }\r\n    creds.addToken(token.getService(), token);\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "lookupForLocalUserIdentity",
  "errType" : [ "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String lookupForLocalUserIdentity(String originalIdentity) throws IOException\n{\r\n    if (Strings.isNullOrEmpty(originalIdentity)) {\r\n        return EMPTY_STRING;\r\n    }\r\n    if (userMap.size() == 0) {\r\n        loadMap(userMap, userMappingFileLocation, NO_OF_FIELDS_USER_MAPPING, ARRAY_INDEX_FOR_AAD_SP_OBJECT_ID);\r\n    }\r\n    try {\r\n        String username = !Strings.isNullOrEmpty(userMap.get(originalIdentity)) ? userMap.get(originalIdentity).split(COLON)[ARRAY_INDEX_FOR_LOCAL_USER_NAME] : EMPTY_STRING;\r\n        return username;\r\n    } catch (ArrayIndexOutOfBoundsException e) {\r\n        LOG.error(\"Error while parsing the line, returning empty string\", e);\r\n        return EMPTY_STRING;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "lookupForLocalGroupIdentity",
  "errType" : [ "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String lookupForLocalGroupIdentity(String originalIdentity) throws IOException\n{\r\n    if (Strings.isNullOrEmpty(originalIdentity)) {\r\n        return EMPTY_STRING;\r\n    }\r\n    if (groupMap.size() == 0) {\r\n        loadMap(groupMap, groupMappingFileLocation, NO_OF_FIELDS_GROUP_MAPPING, ARRAY_INDEX_FOR_AAD_SG_OBJECT_ID);\r\n    }\r\n    try {\r\n        String groupname = !Strings.isNullOrEmpty(groupMap.get(originalIdentity)) ? groupMap.get(originalIdentity).split(COLON)[ARRAY_INDEX_FOR_LOCAL_GROUP_NAME] : EMPTY_STRING;\r\n        return groupname;\r\n    } catch (ArrayIndexOutOfBoundsException e) {\r\n        LOG.error(\"Error while parsing the line, returning empty string\", e);\r\n        return EMPTY_STRING;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "loadMap",
  "errType" : [ "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void loadMap(HashMap<String, String> cache, String fileLocation, int noOfFields, int keyIndex) throws IOException\n{\r\n    LOG.debug(\"Loading identity map from file {}\", fileLocation);\r\n    int errorRecord = 0;\r\n    File file = new File(fileLocation);\r\n    LineIterator it = null;\r\n    try {\r\n        it = FileUtils.lineIterator(file, \"UTF-8\");\r\n        while (it.hasNext()) {\r\n            String line = it.nextLine();\r\n            if (!Strings.isNullOrEmpty(line.trim()) && !line.startsWith(HASH)) {\r\n                if (line.split(COLON).length != noOfFields) {\r\n                    errorRecord += 1;\r\n                    continue;\r\n                }\r\n                cache.put(line.split(COLON)[keyIndex], line);\r\n            }\r\n        }\r\n        LOG.debug(\"Loaded map stats - File: {}, Loaded: {}, Error: {} \", fileLocation, cache.size(), errorRecord);\r\n    } catch (ArrayIndexOutOfBoundsException e) {\r\n        LOG.error(\"Error while parsing mapping file\", e);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, it);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "deserializeAclSpec",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<String, String> deserializeAclSpec(final String aclSpecString) throws AzureBlobFileSystemException\n{\r\n    final Map<String, String> aclEntries = new HashMap<>();\r\n    final String[] aceArray = aclSpecString.split(AbfsHttpConstants.COMMA);\r\n    for (String ace : aceArray) {\r\n        int idx = ace.lastIndexOf(AbfsHttpConstants.COLON);\r\n        final String key = ace.substring(0, idx);\r\n        final String val = ace.substring(idx + 1);\r\n        if (aclEntries.containsKey(key)) {\r\n            throw new InvalidAclOperationException(\"Duplicate acl entries are not allowed.\");\r\n        }\r\n        aclEntries.put(key, val);\r\n    }\r\n    return aclEntries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "serializeAclSpec",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String serializeAclSpec(final Map<String, String> aclEntries)\n{\r\n    final StringBuilder sb = new StringBuilder();\r\n    for (Map.Entry<String, String> aclEntry : aclEntries.entrySet()) {\r\n        sb.append(aclEntry.getKey() + AbfsHttpConstants.COLON + aclEntry.getValue() + AbfsHttpConstants.COMMA);\r\n    }\r\n    if (sb.length() > 0) {\r\n        sb.setLength(sb.length() - 1);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "processAclString",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String processAclString(final String aclSpecString)\n{\r\n    final List<String> aclEntries = Arrays.asList(aclSpecString.split(AbfsHttpConstants.COMMA));\r\n    final StringBuilder sb = new StringBuilder();\r\n    boolean containsMask = false;\r\n    for (int i = aclEntries.size() - 1; i >= 0; i--) {\r\n        String ace = aclEntries.get(i);\r\n        if (ace.startsWith(AbfsHttpConstants.ACCESS_OTHER) || ace.startsWith(AbfsHttpConstants.ACCESS_USER + AbfsHttpConstants.COLON)) {\r\n        } else if (ace.startsWith(AbfsHttpConstants.ACCESS_MASK)) {\r\n            containsMask = true;\r\n        } else if (ace.startsWith(AbfsHttpConstants.ACCESS_GROUP + AbfsHttpConstants.COLON) && !containsMask) {\r\n        } else {\r\n            sb.insert(0, ace + AbfsHttpConstants.COMMA);\r\n        }\r\n    }\r\n    return sb.length() == 0 ? AbfsHttpConstants.EMPTY_STRING : sb.substring(0, sb.length() - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "removeAclEntriesInternal",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void removeAclEntriesInternal(Map<String, String> aclEntries, Map<String, String> toRemoveEntries) throws AzureBlobFileSystemException\n{\r\n    boolean accessAclTouched = false;\r\n    boolean defaultAclTouched = false;\r\n    final Set<String> removeIndicationSet = new HashSet<>();\r\n    for (String entryKey : toRemoveEntries.keySet()) {\r\n        final boolean isDefaultAcl = isDefaultAce(entryKey);\r\n        if (removeNamedAceAndUpdateSet(entryKey, isDefaultAcl, removeIndicationSet, aclEntries)) {\r\n            if (isDefaultAcl) {\r\n                defaultAclTouched = true;\r\n            } else {\r\n                accessAclTouched = true;\r\n            }\r\n        }\r\n    }\r\n    if (removeIndicationSet.contains(AbfsHttpConstants.ACCESS_MASK) && containsNamedAce(aclEntries, false)) {\r\n        throw new InvalidAclOperationException(\"Access mask is required when a named access acl is present.\");\r\n    }\r\n    if (accessAclTouched) {\r\n        if (removeIndicationSet.contains(AbfsHttpConstants.ACCESS_MASK)) {\r\n            aclEntries.remove(AbfsHttpConstants.ACCESS_MASK);\r\n        }\r\n        recalculateMask(aclEntries, false);\r\n    }\r\n    if (removeIndicationSet.contains(AbfsHttpConstants.DEFAULT_MASK) && containsNamedAce(aclEntries, true)) {\r\n        throw new InvalidAclOperationException(\"Default mask is required when a named default acl is present.\");\r\n    }\r\n    if (defaultAclTouched) {\r\n        if (removeIndicationSet.contains(AbfsHttpConstants.DEFAULT_MASK)) {\r\n            aclEntries.remove(AbfsHttpConstants.DEFAULT_MASK);\r\n        }\r\n        if (removeIndicationSet.contains(AbfsHttpConstants.DEFAULT_USER)) {\r\n            aclEntries.put(AbfsHttpConstants.DEFAULT_USER, aclEntries.get(AbfsHttpConstants.ACCESS_USER));\r\n        }\r\n        if (removeIndicationSet.contains(AbfsHttpConstants.DEFAULT_GROUP)) {\r\n            aclEntries.put(AbfsHttpConstants.DEFAULT_GROUP, aclEntries.get(AbfsHttpConstants.ACCESS_GROUP));\r\n        }\r\n        if (removeIndicationSet.contains(AbfsHttpConstants.DEFAULT_OTHER)) {\r\n            aclEntries.put(AbfsHttpConstants.DEFAULT_OTHER, aclEntries.get(AbfsHttpConstants.ACCESS_OTHER));\r\n        }\r\n        recalculateMask(aclEntries, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "modifyAclEntriesInternal",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void modifyAclEntriesInternal(Map<String, String> aclEntries, Map<String, String> toModifyEntries) throws AzureBlobFileSystemException\n{\r\n    boolean namedAccessAclTouched = false;\r\n    boolean namedDefaultAclTouched = false;\r\n    for (Map.Entry<String, String> toModifyEntry : toModifyEntries.entrySet()) {\r\n        aclEntries.put(toModifyEntry.getKey(), toModifyEntry.getValue());\r\n        if (isNamedAce(toModifyEntry.getKey())) {\r\n            if (isDefaultAce(toModifyEntry.getKey())) {\r\n                namedDefaultAclTouched = true;\r\n            } else {\r\n                namedAccessAclTouched = true;\r\n            }\r\n        }\r\n    }\r\n    if (!toModifyEntries.containsKey(AbfsHttpConstants.ACCESS_MASK) && namedAccessAclTouched) {\r\n        aclEntries.remove(AbfsHttpConstants.ACCESS_MASK);\r\n    }\r\n    if (!toModifyEntries.containsKey(AbfsHttpConstants.DEFAULT_MASK) && namedDefaultAclTouched) {\r\n        aclEntries.remove(AbfsHttpConstants.DEFAULT_MASK);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setAclEntriesInternal",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setAclEntriesInternal(Map<String, String> aclEntries, Map<String, String> getAclEntries) throws AzureBlobFileSystemException\n{\r\n    boolean defaultAclTouched = false;\r\n    for (String entryKey : aclEntries.keySet()) {\r\n        if (isDefaultAce(entryKey)) {\r\n            defaultAclTouched = true;\r\n            break;\r\n        }\r\n    }\r\n    for (Map.Entry<String, String> ace : getAclEntries.entrySet()) {\r\n        if (AbfsAclHelper.isDefaultAce(ace.getKey()) && (ace.getKey() != AbfsHttpConstants.DEFAULT_MASK || !defaultAclTouched) && !aclEntries.containsKey(ace.getKey())) {\r\n            aclEntries.put(ace.getKey(), ace.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isUpnFormatAclEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isUpnFormatAclEntries(Map<String, String> aclEntries)\n{\r\n    for (Map.Entry<String, String> entry : aclEntries.entrySet()) {\r\n        if (entry.getKey().contains(AbfsHttpConstants.AT)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "removeNamedAceAndUpdateSet",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean removeNamedAceAndUpdateSet(String entry, boolean isDefaultAcl, Set<String> removeIndicationSet, Map<String, String> aclEntries) throws AzureBlobFileSystemException\n{\r\n    final int startIndex = isDefaultAcl ? 1 : 0;\r\n    final String[] entryParts = entry.split(AbfsHttpConstants.COLON);\r\n    final String tag = isDefaultAcl ? AbfsHttpConstants.DEFAULT_SCOPE + entryParts[startIndex] + AbfsHttpConstants.COLON : entryParts[startIndex] + AbfsHttpConstants.COLON;\r\n    if ((entry.equals(AbfsHttpConstants.ACCESS_USER) || entry.equals(AbfsHttpConstants.ACCESS_GROUP) || entry.equals(AbfsHttpConstants.ACCESS_OTHER))) {\r\n        throw new InvalidAclOperationException(\"Cannot remove user, group or other entry from access ACL.\");\r\n    }\r\n    boolean touched = false;\r\n    if (!isNamedAce(entry)) {\r\n        removeIndicationSet.add(tag);\r\n        touched = true;\r\n    } else {\r\n        if (aclEntries.remove(entry) != null) {\r\n            touched = true;\r\n        }\r\n    }\r\n    return touched;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "recalculateMask",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void recalculateMask(Map<String, String> aclEntries, boolean isDefaultMask)\n{\r\n    FsAction mask = FsAction.NONE;\r\n    if (!isExtendAcl(aclEntries, isDefaultMask)) {\r\n        return;\r\n    }\r\n    for (Map.Entry<String, String> aclEntry : aclEntries.entrySet()) {\r\n        if (isDefaultMask) {\r\n            if ((isDefaultAce(aclEntry.getKey()) && isNamedAce(aclEntry.getKey())) || aclEntry.getKey().equals(AbfsHttpConstants.DEFAULT_GROUP)) {\r\n                mask = mask.or(FsAction.getFsAction(aclEntry.getValue()));\r\n            }\r\n        } else {\r\n            if ((!isDefaultAce(aclEntry.getKey()) && isNamedAce(aclEntry.getKey())) || aclEntry.getKey().equals(AbfsHttpConstants.ACCESS_GROUP)) {\r\n                mask = mask.or(FsAction.getFsAction(aclEntry.getValue()));\r\n            }\r\n        }\r\n    }\r\n    aclEntries.put(isDefaultMask ? AbfsHttpConstants.DEFAULT_MASK : AbfsHttpConstants.ACCESS_MASK, mask.SYMBOL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isExtendAcl",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean isExtendAcl(Map<String, String> aclEntries, boolean checkDefault)\n{\r\n    for (String entryKey : aclEntries.keySet()) {\r\n        if (checkDefault && !(entryKey.equals(AbfsHttpConstants.DEFAULT_USER) || entryKey.equals(AbfsHttpConstants.DEFAULT_GROUP) || entryKey.equals(AbfsHttpConstants.DEFAULT_OTHER) || !isDefaultAce(entryKey))) {\r\n            return true;\r\n        }\r\n        if (!checkDefault && !(entryKey.equals(AbfsHttpConstants.ACCESS_USER) || entryKey.equals(AbfsHttpConstants.ACCESS_GROUP) || entryKey.equals(AbfsHttpConstants.ACCESS_OTHER) || isDefaultAce(entryKey))) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "containsNamedAce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean containsNamedAce(Map<String, String> aclEntries, boolean checkDefault)\n{\r\n    for (String entryKey : aclEntries.keySet()) {\r\n        if (isNamedAce(entryKey) && (checkDefault == isDefaultAce(entryKey))) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isDefaultAce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isDefaultAce(String entry)\n{\r\n    return entry.startsWith(AbfsHttpConstants.DEFAULT_SCOPE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isNamedAce",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isNamedAce(String entry)\n{\r\n    return entry.charAt(entry.length() - 1) != AbfsHttpConstants.COLON.charAt(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateRetryPolicy()\n{\r\n    if (serviceClient != null && retryPolicyFactory != null) {\r\n        serviceClient.getDefaultRequestOptions().setRetryPolicyFactory(retryPolicyFactory);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateTimeoutInMs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateTimeoutInMs()\n{\r\n    if (serviceClient != null && timeoutIntervalInMs > 0) {\r\n        serviceClient.getDefaultRequestOptions().setTimeoutIntervalInMs(timeoutIntervalInMs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setRetryPolicyFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRetryPolicyFactory(final RetryPolicyFactory retryPolicyFactory)\n{\r\n    this.retryPolicyFactory = retryPolicyFactory;\r\n    updateRetryPolicy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setTimeoutInMs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTimeoutInMs(int timeoutInMs)\n{\r\n    timeoutIntervalInMs = timeoutInMs;\r\n    updateTimeoutInMs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createBlobClient(CloudStorageAccount account)\n{\r\n    serviceClient = account.createCloudBlobClient();\r\n    updateRetryPolicy();\r\n    updateTimeoutInMs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createBlobClient(URI baseUri)\n{\r\n    createBlobClient(baseUri, (StorageCredentials) null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createBlobClient(URI baseUri, StorageCredentials credentials)\n{\r\n    serviceClient = new CloudBlobClient(baseUri, credentials);\r\n    updateRetryPolicy();\r\n    updateTimeoutInMs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StorageCredentials getCredentials()\n{\r\n    return serviceClient.getCredentials();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerReference",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CloudBlobContainerWrapper getContainerReference(String uri) throws URISyntaxException, StorageException\n{\r\n    return new CloudBlobContainerWrapperImpl(serviceClient.getContainerReference(uri));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bind(Object extension, URI uri, Configuration conf) throws IOException\n{\r\n    if (extension instanceof BoundDTExtension) {\r\n        ((BoundDTExtension) extension).bind(uri, conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close(Object extension)\n{\r\n    ifBoundDTExtension(extension, v -> {\r\n        IOUtils.closeStreams(v);\r\n        return null;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getUserAgentSuffix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUserAgentSuffix(Object extension, String def)\n{\r\n    return ifBoundDTExtension(extension, BoundDTExtension::getUserAgentSuffix).orElse(def);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCanonicalServiceName(Object extension, String def)\n{\r\n    return ifBoundDTExtension(extension, BoundDTExtension::getCanonicalServiceName).orElse(def);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\extensions",
  "methodName" : "ifBoundDTExtension",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Optional<V> ifBoundDTExtension(Object extension, Function<? super BoundDTExtension, ? extends V> fn)\n{\r\n    if (extension instanceof BoundDTExtension) {\r\n        return Optional.of((BoundDTExtension) extension).map(fn);\r\n    } else {\r\n        return Optional.empty();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "refreshToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AzureADToken refreshToken() throws IOException\n{\r\n    LOG.debug(\"AADToken: refreshing token from MSI\");\r\n    AzureADToken token = AzureADAuthenticator.getTokenFromMsi(authEndpoint, tenantGuid, clientId, authority, false);\r\n    tokenFetchTime = System.currentTimeMillis();\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "isTokenAboutToExpire",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isTokenAboutToExpire()\n{\r\n    if (tokenFetchTime == -1 || super.isTokenAboutToExpire()) {\r\n        return true;\r\n    }\r\n    boolean expiring = false;\r\n    long elapsedTimeSinceLastTokenRefreshInMillis = System.currentTimeMillis() - tokenFetchTime;\r\n    expiring = elapsedTimeSinceLastTokenRefreshInMillis >= ONE_HOUR || elapsedTimeSinceLastTokenRefreshInMillis < 0;\r\n    if (expiring) {\r\n        LOG.debug(\"MSIToken: token renewing. Time elapsed since last token fetch:\" + \" {} milli seconds\", elapsedTimeSinceLastTokenRefreshInMillis);\r\n    }\r\n    return expiring;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "getPosition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPosition()\n{\r\n    return this.position;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "getoffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getoffset()\n{\r\n    return this.offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLength()\n{\r\n    return this.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "getMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Mode getMode()\n{\r\n    return this.mode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "isAppendBlob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isAppendBlob()\n{\r\n    return this.isAppendBlob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\services",
  "methodName" : "getLeaseId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLeaseId()\n{\r\n    return this.leaseId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\contracts\\exceptions",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    if (this.getMessage() == null && this.getCause() == null) {\r\n        return \"AzureBlobFileSystemException\";\r\n    }\r\n    if (this.getCause() == null) {\r\n        return this.getMessage();\r\n    }\r\n    if (this.getMessage() == null) {\r\n        return this.getCause().toString();\r\n    }\r\n    return this.getMessage() + this.getCause().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getSasKeyExpiryPeriod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSasKeyExpiryPeriod()\n{\r\n    return sasKeyExpiryPeriod;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "signRequest",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void signRequest(HttpURLConnection connection, final long contentLength) throws UnsupportedEncodingException\n{\r\n    String gmtTime = getGMTTime();\r\n    connection.setRequestProperty(HttpHeaderConfigurations.X_MS_DATE, gmtTime);\r\n    final String stringToSign = canonicalize(connection, accountName, contentLength);\r\n    final String computedBase64Signature = computeHmac256(stringToSign);\r\n    String signature = String.format(\"%s %s:%s\", \"SharedKey\", accountName, computedBase64Signature);\r\n    connection.setRequestProperty(HttpHeaderConfigurations.AUTHORIZATION, signature);\r\n    LOG.debug(\"Signing request with timestamp of {} and signature {}\", gmtTime, signature);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "computeHmac256",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String computeHmac256(final String stringToSign)\n{\r\n    byte[] utf8Bytes;\r\n    try {\r\n        utf8Bytes = stringToSign.getBytes(AbfsHttpConstants.UTF_8);\r\n    } catch (final UnsupportedEncodingException e) {\r\n        throw new IllegalArgumentException(e);\r\n    }\r\n    byte[] hmac;\r\n    synchronized (this) {\r\n        hmac = hmacSha256.doFinal(utf8Bytes);\r\n    }\r\n    return Base64.encode(hmac);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "addCanonicalizedHeaders",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void addCanonicalizedHeaders(final HttpURLConnection conn, final StringBuilder canonicalizedString)\n{\r\n    final Map<String, List<String>> headers = conn.getRequestProperties();\r\n    final ArrayList<String> httpStorageHeaderNameArray = new ArrayList<String>();\r\n    for (final String key : headers.keySet()) {\r\n        if (key.toLowerCase(Locale.ROOT).startsWith(AbfsHttpConstants.HTTP_HEADER_PREFIX)) {\r\n            httpStorageHeaderNameArray.add(key.toLowerCase(Locale.ROOT));\r\n        }\r\n    }\r\n    Collections.sort(httpStorageHeaderNameArray);\r\n    for (final String key : httpStorageHeaderNameArray) {\r\n        final StringBuilder canonicalizedElement = new StringBuilder(key);\r\n        String delimiter = \":\";\r\n        final ArrayList<String> values = getHeaderValues(headers, key);\r\n        boolean appendCanonicalizedElement = false;\r\n        for (final String value : values) {\r\n            if (value != null) {\r\n                appendCanonicalizedElement = true;\r\n            }\r\n            final String unfoldedValue = CRLF.matcher(value).replaceAll(Matcher.quoteReplacement(\"\"));\r\n            canonicalizedElement.append(delimiter);\r\n            canonicalizedElement.append(unfoldedValue);\r\n            delimiter = \",\";\r\n        }\r\n        if (appendCanonicalizedElement) {\r\n            appendCanonicalizedElement(canonicalizedString, canonicalizedElement.toString());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "initializeMac",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initializeMac()\n{\r\n    try {\r\n        hmacSha256 = Mac.getInstance(HMAC_SHA256);\r\n        hmacSha256.init(new SecretKeySpec(accountKey, HMAC_SHA256));\r\n    } catch (final Exception e) {\r\n        throw new IllegalArgumentException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "appendCanonicalizedElement",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void appendCanonicalizedElement(final StringBuilder builder, final String element)\n{\r\n    builder.append(\"\\n\");\r\n    builder.append(element);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "canonicalizeHttpRequest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "String canonicalizeHttpRequest(final URL address, final String accountName, final String method, final String contentType, final long contentLength, final String date, final HttpURLConnection conn) throws UnsupportedEncodingException\n{\r\n    final StringBuilder canonicalizedString = new StringBuilder(EXPECTED_BLOB_QUEUE_CANONICALIZED_STRING_LENGTH);\r\n    canonicalizedString.append(conn.getRequestMethod());\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.CONTENT_ENCODING, AbfsHttpConstants.EMPTY_STRING));\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.CONTENT_LANGUAGE, AbfsHttpConstants.EMPTY_STRING));\r\n    appendCanonicalizedElement(canonicalizedString, contentLength <= 0 ? \"\" : String.valueOf(contentLength));\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.CONTENT_MD5, AbfsHttpConstants.EMPTY_STRING));\r\n    appendCanonicalizedElement(canonicalizedString, contentType != null ? contentType : AbfsHttpConstants.EMPTY_STRING);\r\n    final String dateString = getHeaderValue(conn, HttpHeaderConfigurations.X_MS_DATE, AbfsHttpConstants.EMPTY_STRING);\r\n    appendCanonicalizedElement(canonicalizedString, dateString.equals(AbfsHttpConstants.EMPTY_STRING) ? date : \"\");\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.IF_MODIFIED_SINCE, AbfsHttpConstants.EMPTY_STRING));\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.IF_MATCH, AbfsHttpConstants.EMPTY_STRING));\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.IF_NONE_MATCH, AbfsHttpConstants.EMPTY_STRING));\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.IF_UNMODIFIED_SINCE, AbfsHttpConstants.EMPTY_STRING));\r\n    appendCanonicalizedElement(canonicalizedString, getHeaderValue(conn, HttpHeaderConfigurations.RANGE, AbfsHttpConstants.EMPTY_STRING));\r\n    addCanonicalizedHeaders(conn, canonicalizedString);\r\n    appendCanonicalizedElement(canonicalizedString, getCanonicalizedResource(address, accountName));\r\n    return canonicalizedString.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getCanonicalizedResource",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "String getCanonicalizedResource(final URL address, final String accountName) throws UnsupportedEncodingException\n{\r\n    final StringBuilder resourcepath = new StringBuilder(AbfsHttpConstants.FORWARD_SLASH);\r\n    resourcepath.append(accountName);\r\n    resourcepath.append(address.getPath());\r\n    final StringBuilder canonicalizedResource = new StringBuilder(resourcepath.toString());\r\n    if (address.getQuery() == null || !address.getQuery().contains(AbfsHttpConstants.EQUAL)) {\r\n        return canonicalizedResource.toString();\r\n    }\r\n    final Map<String, String[]> queryVariables = parseQueryString(address.getQuery());\r\n    final Map<String, String> lowercasedKeyNameValue = new HashMap<>();\r\n    for (final Entry<String, String[]> entry : queryVariables.entrySet()) {\r\n        final List<String> sortedValues = Arrays.asList(entry.getValue());\r\n        Collections.sort(sortedValues);\r\n        final StringBuilder stringValue = new StringBuilder();\r\n        for (final String value : sortedValues) {\r\n            if (stringValue.length() > 0) {\r\n                stringValue.append(AbfsHttpConstants.COMMA);\r\n            }\r\n            stringValue.append(value);\r\n        }\r\n        lowercasedKeyNameValue.put((entry.getKey()) == null ? null : entry.getKey().toLowerCase(Locale.ROOT), stringValue.toString());\r\n    }\r\n    final ArrayList<String> sortedKeys = new ArrayList<String>(lowercasedKeyNameValue.keySet());\r\n    Collections.sort(sortedKeys);\r\n    for (final String key : sortedKeys) {\r\n        final StringBuilder queryParamString = new StringBuilder();\r\n        queryParamString.append(key);\r\n        queryParamString.append(\":\");\r\n        queryParamString.append(lowercasedKeyNameValue.get(key));\r\n        appendCanonicalizedElement(canonicalizedResource, queryParamString.toString());\r\n    }\r\n    return canonicalizedResource.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getHeaderValues",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ArrayList<String> getHeaderValues(final Map<String, List<String>> headers, final String headerName)\n{\r\n    final ArrayList<String> arrayOfValues = new ArrayList<String>();\r\n    List<String> values = null;\r\n    for (final Entry<String, List<String>> entry : headers.entrySet()) {\r\n        if (entry.getKey().toLowerCase(Locale.ROOT).equals(headerName)) {\r\n            values = entry.getValue();\r\n            break;\r\n        }\r\n    }\r\n    if (values != null) {\r\n        for (final String value : values) {\r\n            arrayOfValues.add(trimStart(value));\r\n        }\r\n    }\r\n    return arrayOfValues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "parseQueryString",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "HashMap<String, String[]> parseQueryString(String parseString) throws UnsupportedEncodingException\n{\r\n    final HashMap<String, String[]> retVals = new HashMap<>();\r\n    if (parseString == null || parseString.isEmpty()) {\r\n        return retVals;\r\n    }\r\n    final int queryDex = parseString.indexOf(AbfsHttpConstants.QUESTION_MARK);\r\n    if (queryDex >= 0 && parseString.length() > 0) {\r\n        parseString = parseString.substring(queryDex + 1);\r\n    }\r\n    final String[] valuePairs = parseString.contains(AbfsHttpConstants.AND_MARK) ? parseString.split(AbfsHttpConstants.AND_MARK) : parseString.split(AbfsHttpConstants.SEMICOLON);\r\n    for (int m = 0; m < valuePairs.length; m++) {\r\n        final int equalDex = valuePairs[m].indexOf(AbfsHttpConstants.EQUAL);\r\n        if (equalDex < 0 || equalDex == valuePairs[m].length() - 1) {\r\n            continue;\r\n        }\r\n        String key = valuePairs[m].substring(0, equalDex);\r\n        String value = valuePairs[m].substring(equalDex + 1);\r\n        key = safeDecode(key);\r\n        value = safeDecode(value);\r\n        String[] values = retVals.get(key);\r\n        if (values == null) {\r\n            values = new String[] { value };\r\n            if (!value.equals(\"\")) {\r\n                retVals.put(key, values);\r\n            }\r\n        }\r\n    }\r\n    return retVals;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "safeDecode",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String safeDecode(final String stringToDecode) throws UnsupportedEncodingException\n{\r\n    if (stringToDecode == null) {\r\n        return null;\r\n    }\r\n    if (stringToDecode.length() == 0) {\r\n        return \"\";\r\n    }\r\n    if (stringToDecode.contains(AbfsHttpConstants.PLUS)) {\r\n        final StringBuilder outBuilder = new StringBuilder();\r\n        int startDex = 0;\r\n        for (int m = 0; m < stringToDecode.length(); m++) {\r\n            if (stringToDecode.charAt(m) == '+') {\r\n                if (m > startDex) {\r\n                    outBuilder.append(URLDecoder.decode(stringToDecode.substring(startDex, m), AbfsHttpConstants.UTF_8));\r\n                }\r\n                outBuilder.append(AbfsHttpConstants.PLUS);\r\n                startDex = m + 1;\r\n            }\r\n        }\r\n        if (startDex != stringToDecode.length()) {\r\n            outBuilder.append(URLDecoder.decode(stringToDecode.substring(startDex, stringToDecode.length()), AbfsHttpConstants.UTF_8));\r\n        }\r\n        return outBuilder.toString();\r\n    } else {\r\n        return URLDecoder.decode(stringToDecode, AbfsHttpConstants.UTF_8);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "trimStart",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String trimStart(final String value)\n{\r\n    int spaceDex = 0;\r\n    while (spaceDex < value.length() && value.charAt(spaceDex) == ' ') {\r\n        spaceDex++;\r\n    }\r\n    return value.substring(spaceDex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getHeaderValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHeaderValue(final HttpURLConnection conn, final String headerName, final String defaultValue)\n{\r\n    final String headerValue = conn.getRequestProperty(headerName);\r\n    return headerValue == null ? defaultValue : headerValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "canonicalize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String canonicalize(final HttpURLConnection conn, final String accountName, final Long contentLength) throws UnsupportedEncodingException\n{\r\n    if (contentLength < -1) {\r\n        throw new IllegalArgumentException(\"The Content-Length header must be greater than or equal to -1.\");\r\n    }\r\n    String contentType = getHeaderValue(conn, HttpHeaderConfigurations.CONTENT_TYPE, \"\");\r\n    return canonicalizeHttpRequest(conn.getURL(), accountName, conn.getRequestMethod(), contentType, contentLength, null, conn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getGMTTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getGMTTime()\n{\r\n    return getGMTTime(new Date());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getGMTTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getGMTTime(final Date date)\n{\r\n    return rfc1123GmtDateTimeFormatter.get().format(date);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withReadBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withReadBufferSize(final int readBufferSize)\n{\r\n    this.readBufferSize = readBufferSize;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withReadAheadQueueDepth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsInputStreamContext withReadAheadQueueDepth(final int readAheadQueueDepth)\n{\r\n    this.readAheadQueueDepth = (readAheadQueueDepth >= 0) ? readAheadQueueDepth : Runtime.getRuntime().availableProcessors();\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withTolerateOobAppends",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withTolerateOobAppends(final boolean tolerateOobAppends)\n{\r\n    this.tolerateOobAppends = tolerateOobAppends;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withReadAheadRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withReadAheadRange(final int readAheadRange)\n{\r\n    this.readAheadRange = readAheadRange;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withStreamStatistics(final AbfsInputStreamStatistics streamStatistics)\n{\r\n    this.streamStatistics = streamStatistics;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withReadSmallFilesCompletely",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withReadSmallFilesCompletely(final boolean readSmallFilesCompletely)\n{\r\n    this.readSmallFilesCompletely = readSmallFilesCompletely;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withOptimizeFooterRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withOptimizeFooterRead(final boolean optimizeFooterRead)\n{\r\n    this.optimizeFooterRead = optimizeFooterRead;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withShouldReadBufferSizeAlways",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withShouldReadBufferSizeAlways(final boolean alwaysReadBufferSize)\n{\r\n    this.alwaysReadBufferSize = alwaysReadBufferSize;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withReadAheadBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withReadAheadBlockSize(final int readAheadBlockSize)\n{\r\n    this.readAheadBlockSize = readAheadBlockSize;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "withBufferedPreadDisabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamContext withBufferedPreadDisabled(final boolean bufferedPreadDisabled)\n{\r\n    this.bufferedPreadDisabled = bufferedPreadDisabled;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbfsInputStreamContext build()\n{\r\n    if (readBufferSize > readAheadBlockSize) {\r\n        LOG.debug(\"fs.azure.read.request.size[={}] is configured for higher size than \" + \"fs.azure.read.readahead.blocksize[={}]. Auto-align \" + \"readAhead block size to be same as readRequestSize.\", readBufferSize, readAheadBlockSize);\r\n        readAheadBlockSize = readBufferSize;\r\n    }\r\n    Preconditions.checkArgument(readAheadRange > 0, \"Read ahead range should be greater than 0\");\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadBufferSize()\n{\r\n    return readBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadQueueDepth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadQueueDepth()\n{\r\n    return readAheadQueueDepth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isTolerateOobAppends",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isTolerateOobAppends()\n{\r\n    return tolerateOobAppends;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadRange()\n{\r\n    return readAheadRange;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStreamStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsInputStreamStatistics getStreamStatistics()\n{\r\n    return streamStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "readSmallFilesCompletely",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean readSmallFilesCompletely()\n{\r\n    return this.readSmallFilesCompletely;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "optimizeFooterRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean optimizeFooterRead()\n{\r\n    return this.optimizeFooterRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "shouldReadBufferSizeAlways",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldReadBufferSizeAlways()\n{\r\n    return alwaysReadBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadBlockSize()\n{\r\n    return readAheadBlockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isBufferedPreadDisabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isBufferedPreadDisabled()\n{\r\n    return bufferedPreadDisabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStorageAccountKey",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getStorageAccountKey(String accountName, Configuration conf) throws KeyProviderException\n{\r\n    String key = null;\r\n    try {\r\n        Configuration c = ProviderUtils.excludeIncompatibleCredentialProviders(conf, NativeAzureFileSystem.class);\r\n        char[] keyChars = c.getPassword(getStorageAccountKeyName(accountName));\r\n        if (keyChars != null) {\r\n            key = new String(keyChars);\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"Unable to get key from credential providers.\", ioe);\r\n    }\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStorageAccountKeyName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStorageAccountKeyName(String accountName)\n{\r\n    return KEY_ACCOUNT_KEY_PREFIX + accountName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setTimeoutInMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimeoutInMs(int timeoutInMs)\n{\r\n    timeoutIntervalInMs = timeoutInMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setRetryPolicyFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRetryPolicyFactory(RetryPolicyFactory retryPolicyFactory)\n{\r\n    retryPolicy = retryPolicyFactory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createBlobClient(CloudStorageAccount account)\n{\r\n    String errorMsg = \"createBlobClient is an invalid operation in\" + \" SAS Key Mode\";\r\n    LOG.error(errorMsg);\r\n    throw new UnsupportedOperationException(errorMsg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createBlobClient(URI baseUri)\n{\r\n    String errorMsg = \"createBlobClient is an invalid operation in \" + \"SAS Key Mode\";\r\n    LOG.error(errorMsg);\r\n    throw new UnsupportedOperationException(errorMsg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createBlobClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createBlobClient(URI baseUri, StorageCredentials credentials)\n{\r\n    String errorMsg = \"createBlobClient is an invalid operation in SAS \" + \"Key Mode\";\r\n    LOG.error(errorMsg);\r\n    throw new UnsupportedOperationException(errorMsg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StorageCredentials getCredentials()\n{\r\n    String errorMsg = \"getCredentials is an invalid operation in SAS \" + \"Key Mode\";\r\n    LOG.error(errorMsg);\r\n    throw new UnsupportedOperationException(errorMsg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerReference",
  "errType" : [ "SASKeyGenerationException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CloudBlobContainerWrapper getContainerReference(String name) throws URISyntaxException, StorageException\n{\r\n    try {\r\n        CloudBlobContainer container = new CloudBlobContainer(sasKeyGenerator.getContainerSASUri(storageAccount, name));\r\n        if (retryPolicy != null) {\r\n            container.getServiceClient().getDefaultRequestOptions().setRetryPolicyFactory(retryPolicy);\r\n        }\r\n        if (timeoutIntervalInMs > 0) {\r\n            container.getServiceClient().getDefaultRequestOptions().setTimeoutIntervalInMs(timeoutIntervalInMs);\r\n        }\r\n        return (useContainerSasKeyForAllAccess) ? new SASCloudBlobContainerWrapperImpl(storageAccount, container, null) : new SASCloudBlobContainerWrapperImpl(storageAccount, container, sasKeyGenerator);\r\n    } catch (SASKeyGenerationException sasEx) {\r\n        String errorMsg = \"Encountered SASKeyGeneration exception while \" + \"generating SAS Key for container : \" + name + \" inside Storage account : \" + storageAccount;\r\n        LOG.error(errorMsg);\r\n        throw new StorageException(SAS_ERROR_CODE, errorMsg, sasEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setStorageAccountName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStorageAccountName(String storageAccount)\n{\r\n    this.storageAccount = storageAccount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getUriDefaultPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getUriDefaultPort()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "toDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> toDelegationToken(final Map<?, ?> inputMap) throws IOException\n{\r\n    final Map<?, ?> m = (Map<?, ?>) inputMap.get(Token.class.getSimpleName());\r\n    return (Token<DelegationTokenIdentifier>) toToken(m);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "toToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Token<? extends TokenIdentifier> toToken(final Map<?, ?> m) throws IOException\n{\r\n    if (m == null) {\r\n        return null;\r\n    }\r\n    String urlString = (String) m.get(URL_STRING);\r\n    if (urlString != null) {\r\n        final Token<DelegationTokenIdentifier> token = new Token<>();\r\n        LOG.debug(\"Read url string param - {}\", urlString);\r\n        token.decodeFromUrlString(urlString);\r\n        return token;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hasResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasResult()\n{\r\n    return result != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsHttpOperation getResult()\n{\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "hardSetResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hardSetResult(int httpStatus)\n{\r\n    result = AbfsHttpOperation.getAbfsHttpOperationWithFixedResult(this.url, this.method, httpStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URL getUrl()\n{\r\n    return url;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRequestHeaders",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<AbfsHttpHeader> getRequestHeaders()\n{\r\n    return requestHeaders;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isARetriedRequest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isARetriedRequest()\n{\r\n    return (retryCount > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getSasToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSasToken()\n{\r\n    return sasToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "execute",
  "errType" : [ "AzureBlobFileSystemException", "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void execute(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try {\r\n        IOStatisticsBinding.trackDurationOfInvocation(abfsCounters, AbfsStatistic.getStatNameFromHttpCall(method), () -> completeExecute(tracingContext));\r\n    } catch (AzureBlobFileSystemException aze) {\r\n        throw aze;\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(\"Error while tracking Duration of an \" + \"AbfsRestOperation call\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "completeExecute",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void completeExecute(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    String latencyHeader = this.client.getAbfsPerfTracker().getClientLatency();\r\n    if (latencyHeader != null && !latencyHeader.isEmpty()) {\r\n        AbfsHttpHeader httpHeader = new AbfsHttpHeader(HttpHeaderConfigurations.X_MS_ABFS_CLIENT_LATENCY, latencyHeader);\r\n        requestHeaders.add(httpHeader);\r\n    }\r\n    retryCount = 0;\r\n    LOG.debug(\"First execution of REST operation - {}\", operationType);\r\n    while (!executeHttpOperation(retryCount, tracingContext)) {\r\n        try {\r\n            ++retryCount;\r\n            tracingContext.setRetryCount(retryCount);\r\n            LOG.debug(\"Retrying REST operation {}. RetryCount = {}\", operationType, retryCount);\r\n            Thread.sleep(client.getRetryPolicy().getRetryInterval(retryCount));\r\n        } catch (InterruptedException ex) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n    }\r\n    if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {\r\n        throw new AbfsRestOperationException(result.getStatusCode(), result.getStorageErrorCode(), result.getStorageErrorMessage(), null, result);\r\n    }\r\n    LOG.trace(\"{} REST operation complete\", operationType);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "executeHttpOperation",
  "errType" : [ "IOException", "UnknownHostException", "IOException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "boolean executeHttpOperation(final int retryCount, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    AbfsHttpOperation httpOperation = null;\r\n    try {\r\n        httpOperation = new AbfsHttpOperation(url, method, requestHeaders);\r\n        incrementCounter(AbfsStatistic.CONNECTIONS_MADE, 1);\r\n        tracingContext.constructHeader(httpOperation);\r\n        switch(client.getAuthType()) {\r\n            case Custom:\r\n            case OAuth:\r\n                LOG.debug(\"Authenticating request with OAuth2 access token\");\r\n                httpOperation.getConnection().setRequestProperty(HttpHeaderConfigurations.AUTHORIZATION, client.getAccessToken());\r\n                break;\r\n            case SAS:\r\n                httpOperation.setMaskForSAS();\r\n                break;\r\n            case SharedKey:\r\n                LOG.debug(\"Signing request with shared key\");\r\n                client.getSharedKeyCredentials().signRequest(httpOperation.getConnection(), hasRequestBody ? bufferLength : 0);\r\n                break;\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Auth failure: {}, {}\", method, url);\r\n        throw new AbfsRestOperationException(-1, null, \"Auth failure: \" + e.getMessage(), e);\r\n    }\r\n    try {\r\n        AbfsIoUtils.dumpHeadersToDebugLog(\"Request Headers\", httpOperation.getConnection().getRequestProperties());\r\n        AbfsClientThrottlingIntercept.sendingRequest(operationType, abfsCounters);\r\n        if (hasRequestBody) {\r\n            httpOperation.sendRequest(buffer, bufferOffset, bufferLength);\r\n            incrementCounter(AbfsStatistic.SEND_REQUESTS, 1);\r\n            incrementCounter(AbfsStatistic.BYTES_SENT, bufferLength);\r\n        }\r\n        httpOperation.processResponse(buffer, bufferOffset, bufferLength);\r\n        incrementCounter(AbfsStatistic.GET_RESPONSES, 1);\r\n        if (httpOperation.getStatusCode() >= HttpURLConnection.HTTP_OK && httpOperation.getStatusCode() <= HttpURLConnection.HTTP_PARTIAL) {\r\n            incrementCounter(AbfsStatistic.BYTES_RECEIVED, httpOperation.getBytesReceived());\r\n        } else if (httpOperation.getStatusCode() == HttpURLConnection.HTTP_UNAVAILABLE) {\r\n            incrementCounter(AbfsStatistic.SERVER_UNAVAILABLE, 1);\r\n        }\r\n    } catch (UnknownHostException ex) {\r\n        String hostname = null;\r\n        hostname = httpOperation.getHost();\r\n        LOG.warn(\"Unknown host name: {}. Retrying to resolve the host name...\", hostname);\r\n        if (!client.getRetryPolicy().shouldRetry(retryCount, -1)) {\r\n            throw new InvalidAbfsRestOperationException(ex);\r\n        }\r\n        return false;\r\n    } catch (IOException ex) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"HttpRequestFailure: {}, {}\", httpOperation, ex);\r\n        }\r\n        if (!client.getRetryPolicy().shouldRetry(retryCount, -1)) {\r\n            throw new InvalidAbfsRestOperationException(ex);\r\n        }\r\n        return false;\r\n    } finally {\r\n        AbfsClientThrottlingIntercept.updateMetrics(operationType, httpOperation);\r\n    }\r\n    LOG.debug(\"HttpRequest: {}: {}\", operationType, httpOperation);\r\n    if (client.getRetryPolicy().shouldRetry(retryCount, httpOperation.getStatusCode())) {\r\n        return false;\r\n    }\r\n    result = httpOperation;\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementCounter(AbfsStatistic statistic, long value)\n{\r\n    if (abfsCounters != null) {\r\n        abfsCounters.incrementCounter(statistic, value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "validateClientCorrelationID",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String validateClientCorrelationID(String clientCorrelationID)\n{\r\n    if ((clientCorrelationID.length() > MAX_CLIENT_CORRELATION_ID_LENGTH) || (!clientCorrelationID.matches(CLIENT_CORRELATION_ID_PATTERN))) {\r\n        LOG.debug(\"Invalid config provided; correlation id not included in header.\");\r\n        return EMPTY_STRING;\r\n    }\r\n    return clientCorrelationID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setPrimaryRequestID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setPrimaryRequestID()\n{\r\n    primaryRequestId = UUID.randomUUID().toString();\r\n    if (listener != null) {\r\n        listener.updatePrimaryRequestID(primaryRequestId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setStreamID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStreamID(String stream)\n{\r\n    streamID = stream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setOperation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOperation(FSOperationType operation)\n{\r\n    this.opType = operation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setRetryCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRetryCount(int retryCount)\n{\r\n    this.retryCount = retryCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "setListener",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setListener(Listener listener)\n{\r\n    this.listener = listener;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "constructHeader",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void constructHeader(AbfsHttpOperation httpOperation)\n{\r\n    clientRequestId = UUID.randomUUID().toString();\r\n    switch(format) {\r\n        case ALL_ID_FORMAT:\r\n            header = clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\" + primaryRequestId + \":\" + streamID + \":\" + opType + \":\" + retryCount;\r\n            break;\r\n        case TWO_ID_FORMAT:\r\n            header = clientCorrelationID + \":\" + clientRequestId;\r\n            break;\r\n        default:\r\n            header = clientRequestId;\r\n    }\r\n    if (listener != null) {\r\n        listener.callTracingHeaderValidator(header, format);\r\n    }\r\n    httpOperation.setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID, header);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\utils",
  "methodName" : "getHeader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHeader()\n{\r\n    return header;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStorageAccountKey",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getStorageAccountKey(String accountName, Configuration conf) throws KeyProviderException\n{\r\n    String envelope = super.getStorageAccountKey(accountName, conf);\r\n    final String command = conf.get(KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT);\r\n    if (command == null) {\r\n        throw new KeyProviderException(\"Script path is not specified via fs.azure.shellkeyprovider.script\");\r\n    }\r\n    String[] cmd = command.split(\" \");\r\n    String[] cmdWithEnvelope = Arrays.copyOf(cmd, cmd.length + 1);\r\n    cmdWithEnvelope[cmdWithEnvelope.length - 1] = envelope;\r\n    String decryptedKey = null;\r\n    try {\r\n        decryptedKey = Shell.execCommand(cmdWithEnvelope);\r\n    } catch (IOException ex) {\r\n        throw new KeyProviderException(ex);\r\n    }\r\n    return decryptedKey.trim();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "parse",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<?, ?> parse(final String jsonString) throws IOException\n{\r\n    try {\r\n        return JsonSerialization.mapReader().readValue(jsonString);\r\n    } catch (Exception e) {\r\n        LOG.debug(\"JSON Parsing exception: {} while parsing {}\", e.getMessage(), jsonString);\r\n        if (jsonString.toLowerCase(Locale.ENGLISH).contains(\"server error\")) {\r\n            LOG.error(\"Internal Server Error was encountered while making a request\");\r\n        }\r\n        throw new IOException(\"JSON Parsing Error: \" + e.getMessage(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "shouldRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldRetry(final int retryCount, final int statusCode)\n{\r\n    return retryCount < this.retryCount && (statusCode == -1 || statusCode == HttpURLConnection.HTTP_CLIENT_TIMEOUT || (statusCode >= HttpURLConnection.HTTP_INTERNAL_ERROR && statusCode != HttpURLConnection.HTTP_NOT_IMPLEMENTED && statusCode != HttpURLConnection.HTTP_VERSION));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRetryInterval",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getRetryInterval(final int retryCount)\n{\r\n    final long boundedRandDelta = (int) (this.deltaBackoff * MIN_RANDOM_RATIO) + this.randRef.nextInt((int) (this.deltaBackoff * MAX_RANDOM_RATIO) - (int) (this.deltaBackoff * MIN_RANDOM_RATIO));\r\n    final double incrementDelta = (Math.pow(2, retryCount - 1)) * boundedRandDelta;\r\n    final long retryInterval = (int) Math.round(Math.min(this.minBackoff + incrementDelta, maxBackoff));\r\n    return retryInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRetryCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRetryCount()\n{\r\n    return this.retryCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMinBackoff",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMinBackoff()\n{\r\n    return this.minBackoff;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMaxBackoff",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxBackoff()\n{\r\n    return maxBackoff;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getDeltaBackoff",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDeltaBackoff()\n{\r\n    return this.deltaBackoff;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "transformIdentityForGetRequest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String transformIdentityForGetRequest(String originalIdentity, boolean isUserName, String localIdentity) throws IOException\n{\r\n    if (originalIdentity == null) {\r\n        originalIdentity = localIdentity;\r\n    }\r\n    if (!skipSuperUserReplacement && SUPER_USER.equals(originalIdentity)) {\r\n        return localIdentity;\r\n    }\r\n    if (skipUserIdentityReplacement) {\r\n        return originalIdentity;\r\n    }\r\n    if (originalIdentity.equals(servicePrincipalId) && isInSubstitutionList(localIdentity)) {\r\n        return localIdentity;\r\n    }\r\n    if (isUserName && shouldUseShortUserName(originalIdentity)) {\r\n        return getShortName(originalIdentity);\r\n    }\r\n    return originalIdentity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "transformUserOrGroupForSetRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String transformUserOrGroupForSetRequest(String userOrGroup)\n{\r\n    if (userOrGroup == null || userOrGroup.isEmpty() || skipUserIdentityReplacement) {\r\n        return userOrGroup;\r\n    }\r\n    if (isInSubstitutionList(userOrGroup)) {\r\n        return servicePrincipalId;\r\n    }\r\n    if (shouldUseFullyQualifiedUserName(userOrGroup)) {\r\n        return getFullyQualifiedName(userOrGroup);\r\n    }\r\n    return userOrGroup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "transformAclEntriesForSetRequest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void transformAclEntriesForSetRequest(final List<AclEntry> aclEntries)\n{\r\n    if (skipUserIdentityReplacement) {\r\n        return;\r\n    }\r\n    for (int i = 0; i < aclEntries.size(); i++) {\r\n        AclEntry aclEntry = aclEntries.get(i);\r\n        String name = aclEntry.getName();\r\n        String transformedName = name;\r\n        if (name == null || name.isEmpty() || aclEntry.getType().equals(AclEntryType.OTHER) || aclEntry.getType().equals(AclEntryType.MASK)) {\r\n            continue;\r\n        }\r\n        if (isInSubstitutionList(name)) {\r\n            transformedName = servicePrincipalId;\r\n        } else if (aclEntry.getType().equals(AclEntryType.USER) && shouldUseFullyQualifiedUserName(name)) {\r\n            transformedName = getFullyQualifiedName(name);\r\n        }\r\n        if (transformedName.equals(name)) {\r\n            continue;\r\n        }\r\n        AclEntry.Builder aclEntryBuilder = new AclEntry.Builder();\r\n        aclEntryBuilder.setType(aclEntry.getType());\r\n        aclEntryBuilder.setName(transformedName);\r\n        aclEntryBuilder.setScope(aclEntry.getScope());\r\n        aclEntryBuilder.setPermission(aclEntry.getPermission());\r\n        aclEntries.set(i, aclEntryBuilder.build());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "transformAclEntriesForGetRequest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void transformAclEntriesForGetRequest(final List<AclEntry> aclEntries, String localUser, String localGroup) throws IOException\n{\r\n    if (skipUserIdentityReplacement) {\r\n        return;\r\n    }\r\n    for (int i = 0; i < aclEntries.size(); i++) {\r\n        AclEntry aclEntry = aclEntries.get(i);\r\n        String name = aclEntry.getName();\r\n        String transformedName = name;\r\n        if (name == null || name.isEmpty() || aclEntry.getType().equals(AclEntryType.OTHER) || aclEntry.getType().equals(AclEntryType.MASK)) {\r\n            continue;\r\n        }\r\n        if (aclEntry.getType().equals(AclEntryType.USER)) {\r\n            transformedName = transformIdentityForGetRequest(name, true, localUser);\r\n        } else if (aclEntry.getType().equals(AclEntryType.GROUP)) {\r\n            transformedName = transformIdentityForGetRequest(name, false, localGroup);\r\n        }\r\n        if (transformedName.equals(name)) {\r\n            continue;\r\n        }\r\n        AclEntry.Builder aclEntryBuilder = new AclEntry.Builder();\r\n        aclEntryBuilder.setType(aclEntry.getType());\r\n        aclEntryBuilder.setName(transformedName);\r\n        aclEntryBuilder.setScope(aclEntry.getScope());\r\n        aclEntryBuilder.setPermission(aclEntry.getPermission());\r\n        aclEntries.set(i, aclEntryBuilder.build());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "isShortUserName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isShortUserName(String owner)\n{\r\n    return (owner != null) && !owner.contains(AT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "shouldUseShortUserName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldUseShortUserName(String owner)\n{\r\n    return enableShortName && !isShortUserName(owner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getShortName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getShortName(String userName)\n{\r\n    if (userName == null) {\r\n        return null;\r\n    }\r\n    if (isShortUserName(userName)) {\r\n        return userName;\r\n    }\r\n    String userNameBeforeAt = userName.substring(0, userName.indexOf(AT));\r\n    if (isSecure) {\r\n        return userNameBeforeAt.toLowerCase(Locale.ENGLISH);\r\n    }\r\n    return userNameBeforeAt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getFullyQualifiedName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFullyQualifiedName(String name)\n{\r\n    if (domainIsSet && (name != null) && !name.contains(AT)) {\r\n        return name + AT + domainName;\r\n    }\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "shouldUseFullyQualifiedUserName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean shouldUseFullyQualifiedUserName(String owner)\n{\r\n    return domainIsSet && !SUPER_USER.equals(owner) && !isUuid(owner) && enableShortName && isShortUserName(owner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "isInSubstitutionList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isInSubstitutionList(String localUserName)\n{\r\n    return serviceWhiteList.contains(STAR) || serviceWhiteList.contains(localUserName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "isUuid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isUuid(String input)\n{\r\n    if (input == null)\r\n        return false;\r\n    return input.matches(UUID_PATTERN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getOutStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputStream getOutStream()\n{\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hasCapability",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasCapability(String capability)\n{\r\n    return StoreImplementationUtils.hasCapability(out, capability);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hflush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hflush() throws IOException\n{\r\n    if (out instanceof Syncable) {\r\n        ((Syncable) out).hflush();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "hsync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void hsync() throws IOException\n{\r\n    if (out instanceof Syncable) {\r\n        ((Syncable) out).hsync();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    IOException ioeFromFlush = null;\r\n    try {\r\n        flush();\r\n    } catch (IOException e) {\r\n        ioeFromFlush = e;\r\n        throw e;\r\n    } finally {\r\n        try {\r\n            this.out.close();\r\n        } catch (IOException e) {\r\n            if (ioeFromFlush == e) {\r\n                LOG.debug(\"flush() and close() throwing back same Exception. Just swallowing the latter\", e);\r\n            } else {\r\n                throw e;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "suppressRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void suppressRetryPolicy()\n{\r\n    suppressRetryPolicy = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "addTestHookToOperationContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addTestHookToOperationContext(TestHookOperationContext testHook)\n{\r\n    this.testHookOperationContext = testHook;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "suppressRetryPolicyInClientIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void suppressRetryPolicyInClientIfNeeded()\n{\r\n    if (suppressRetryPolicy) {\r\n        storageInteractionLayer.setRetryPolicyFactory(new RetryNoRetry());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createPermissionJsonSerializer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JSON createPermissionJsonSerializer()\n{\r\n    org.eclipse.jetty.util.log.Log.getProperties().setProperty(\"org.eclipse.jetty.util.log.announce\", \"false\");\r\n    JSON serializer = new JSON();\r\n    serializer.addConvertor(PermissionStatus.class, new PermissionStatusJsonSerializer());\r\n    return serializer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setAzureStorageInteractionLayer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAzureStorageInteractionLayer(StorageInterface storageInteractionLayer)\n{\r\n    this.storageInteractionLayer = storageInteractionLayer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBandwidthGaugeUpdater",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BandwidthGaugeUpdater getBandwidthGaugeUpdater()\n{\r\n    return bandwidthGaugeUpdater;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isConcurrentOOBAppendAllowed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isConcurrentOOBAppendAllowed()\n{\r\n    return tolerateOobAppends;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "initialize",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void initialize(URI uri, Configuration conf, AzureFileSystemInstrumentation instrumentation) throws IllegalArgumentException, AzureException, IOException\n{\r\n    if (null == instrumentation) {\r\n        throw new IllegalArgumentException(\"Null instrumentation\");\r\n    }\r\n    this.instrumentation = instrumentation;\r\n    if (null == uri) {\r\n        throw new IllegalArgumentException(\"Cannot initialize WASB file system, URI is null\");\r\n    }\r\n    if (null == conf) {\r\n        throw new IllegalArgumentException(\"Cannot initialize WASB file system, conf is null\");\r\n    }\r\n    if (!conf.getBoolean(NativeAzureFileSystem.SKIP_AZURE_METRICS_PROPERTY_NAME, false)) {\r\n        this.bandwidthGaugeUpdater = new BandwidthGaugeUpdater(instrumentation);\r\n    }\r\n    sessionUri = uri;\r\n    sessionConfiguration = conf;\r\n    useSecureMode = conf.getBoolean(KEY_USE_SECURE_MODE, DEFAULT_USE_SECURE_MODE);\r\n    useLocalSasKeyMode = conf.getBoolean(KEY_USE_LOCAL_SAS_KEY_MODE, DEFAULT_USE_LOCAL_SAS_KEY_MODE);\r\n    if (null == this.storageInteractionLayer) {\r\n        if (!useSecureMode) {\r\n            this.storageInteractionLayer = new StorageInterfaceImpl();\r\n        } else {\r\n            this.storageInteractionLayer = new SecureStorageInterfaceImpl(useLocalSasKeyMode, conf);\r\n        }\r\n    }\r\n    configureAzureStorageSession();\r\n    createAzureStorageSession();\r\n    pageBlobDirs = getDirectorySet(KEY_PAGE_BLOB_DIRECTORIES);\r\n    LOG.debug(\"Page blob directories:  {}\", setToString(pageBlobDirs));\r\n    userAgentId = conf.get(USER_AGENT_ID_KEY, USER_AGENT_ID_DEFAULT);\r\n    blockBlobWithCompationDirs = getDirectorySet(KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES);\r\n    LOG.debug(\"Block blobs with compaction directories:  {}\", setToString(blockBlobWithCompationDirs));\r\n    atomicRenameDirs = getDirectorySet(KEY_ATOMIC_RENAME_DIRECTORIES);\r\n    String hbaseRoot;\r\n    try {\r\n        hbaseRoot = verifyAndConvertToStandardFormat(sessionConfiguration.get(\"hbase.rootdir\", \"hbase\"));\r\n        if (hbaseRoot != null) {\r\n            atomicRenameDirs.add(hbaseRoot);\r\n        }\r\n    } catch (URISyntaxException e) {\r\n        LOG.warn(\"Unable to initialize HBase root as an atomic rename directory.\");\r\n    }\r\n    LOG.debug(\"Atomic rename directories: {} \", setToString(atomicRenameDirs));\r\n    metadataKeyCaseSensitive = conf.getBoolean(KEY_BLOB_METADATA_KEY_CASE_SENSITIVE, true);\r\n    if (!metadataKeyCaseSensitive) {\r\n        LOG.info(\"{} configured as false. Blob metadata will be treated case insensitive.\", KEY_BLOB_METADATA_KEY_CASE_SENSITIVE);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setToString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String setToString(Set<String> set)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    int i = 1;\r\n    for (String s : set) {\r\n        sb.append(\"/\" + s);\r\n        if (i != set.size()) {\r\n            sb.append(\", \");\r\n        }\r\n        i++;\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getAccountFromAuthority",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getAccountFromAuthority(URI uri) throws URISyntaxException\n{\r\n    String authority = uri.getRawAuthority();\r\n    if (null == authority) {\r\n        throw new URISyntaxException(uri.toString(), \"Expected URI with a valid authority\");\r\n    }\r\n    if (!authority.contains(WASB_AUTHORITY_DELIMITER)) {\r\n        return authority;\r\n    }\r\n    String[] authorityParts = authority.split(WASB_AUTHORITY_DELIMITER, 2);\r\n    if (authorityParts.length < 2 || \"\".equals(authorityParts[0])) {\r\n        final String errMsg = String.format(\"URI '%s' has a malformed WASB authority, expected container name. \" + \"Authority takes the form wasb://[<container name>@]<account name>\", uri.toString());\r\n        throw new IllegalArgumentException(errMsg);\r\n    }\r\n    return authorityParts[1];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerFromAuthority",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getContainerFromAuthority(URI uri) throws URISyntaxException\n{\r\n    String authority = uri.getRawAuthority();\r\n    if (null == authority) {\r\n        throw new URISyntaxException(uri.toString(), \"Expected URI with a valid authority\");\r\n    }\r\n    if (!authority.contains(WASB_AUTHORITY_DELIMITER)) {\r\n        return AZURE_ROOT_CONTAINER;\r\n    }\r\n    String[] authorityParts = authority.split(WASB_AUTHORITY_DELIMITER, 2);\r\n    if (authorityParts.length < 2 || \"\".equals(authorityParts[0])) {\r\n        final String errMsg = String.format(\"URI '%s' has a malformed WASB authority, expected container name.\" + \"Authority takes the form wasb://[<container name>@]<account name>\", uri.toString());\r\n        throw new IllegalArgumentException(errMsg);\r\n    }\r\n    return authorityParts[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getHTTPScheme",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getHTTPScheme()\n{\r\n    String sessionScheme = sessionUri.getScheme();\r\n    if (sessionScheme != null && (sessionScheme.equalsIgnoreCase(\"asvs\") || sessionScheme.equalsIgnoreCase(\"wasbs\"))) {\r\n        return HTTPS_SCHEME;\r\n    } else {\r\n        return HTTP_SCHEME;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "configureAzureStorageSession",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void configureAzureStorageSession() throws AzureException\n{\r\n    if (sessionUri == null) {\r\n        throw new AssertionError(\"Expected a non-null session URI when configuring storage session\");\r\n    }\r\n    if (storageInteractionLayer == null) {\r\n        throw new AssertionError(String.format(\"Cannot configure storage session for URI '%s' \" + \"if storage session has not been established.\", sessionUri.toString()));\r\n    }\r\n    tolerateOobAppends = sessionConfiguration.getBoolean(KEY_READ_TOLERATE_CONCURRENT_APPEND, DEFAULT_READ_TOLERATE_CONCURRENT_APPEND);\r\n    this.downloadBlockSizeBytes = sessionConfiguration.getInt(KEY_STREAM_MIN_READ_SIZE, DEFAULT_DOWNLOAD_BLOCK_SIZE);\r\n    this.uploadBlockSizeBytes = sessionConfiguration.getInt(KEY_WRITE_BLOCK_SIZE, DEFAULT_UPLOAD_BLOCK_SIZE);\r\n    this.hadoopBlockSize = sessionConfiguration.getLong(HADOOP_BLOCK_SIZE_PROPERTY_NAME, DEFAULT_HADOOP_BLOCK_SIZE);\r\n    this.inputStreamVersion = sessionConfiguration.getInt(KEY_INPUT_STREAM_VERSION, DEFAULT_INPUT_STREAM_VERSION);\r\n    int storageConnectionTimeout = sessionConfiguration.getInt(KEY_STORAGE_CONNECTION_TIMEOUT, 0);\r\n    if (0 < storageConnectionTimeout) {\r\n        storageInteractionLayer.setTimeoutInMs(storageConnectionTimeout * 1000);\r\n    }\r\n    int cpuCores = 2 * Runtime.getRuntime().availableProcessors();\r\n    concurrentWrites = sessionConfiguration.getInt(KEY_CONCURRENT_CONNECTION_VALUE_OUT, Math.min(cpuCores, DEFAULT_CONCURRENT_WRITES));\r\n    minBackoff = sessionConfiguration.getInt(KEY_MIN_BACKOFF_INTERVAL, DEFAULT_MIN_BACKOFF_INTERVAL);\r\n    maxBackoff = sessionConfiguration.getInt(KEY_MAX_BACKOFF_INTERVAL, DEFAULT_MAX_BACKOFF_INTERVAL);\r\n    deltaBackoff = sessionConfiguration.getInt(KEY_BACKOFF_INTERVAL, DEFAULT_BACKOFF_INTERVAL);\r\n    maxRetries = sessionConfiguration.getInt(KEY_MAX_IO_RETRIES, DEFAULT_MAX_RETRY_ATTEMPTS);\r\n    storageInteractionLayer.setRetryPolicyFactory(new RetryExponentialRetry(minBackoff, deltaBackoff, maxBackoff, maxRetries));\r\n    selfThrottlingEnabled = sessionConfiguration.getBoolean(KEY_SELF_THROTTLE_ENABLE, DEFAULT_SELF_THROTTLE_ENABLE);\r\n    selfThrottlingReadFactor = sessionConfiguration.getFloat(KEY_SELF_THROTTLE_READ_FACTOR, DEFAULT_SELF_THROTTLE_READ_FACTOR);\r\n    selfThrottlingWriteFactor = sessionConfiguration.getFloat(KEY_SELF_THROTTLE_WRITE_FACTOR, DEFAULT_SELF_THROTTLE_WRITE_FACTOR);\r\n    if (!selfThrottlingEnabled) {\r\n        autoThrottlingEnabled = sessionConfiguration.getBoolean(KEY_AUTO_THROTTLE_ENABLE, DEFAULT_AUTO_THROTTLE_ENABLE);\r\n        if (autoThrottlingEnabled) {\r\n            ClientThrottlingIntercept.initializeSingleton();\r\n        }\r\n    } else {\r\n        autoThrottlingEnabled = false;\r\n    }\r\n    OperationContext.setLoggingEnabledByDefault(sessionConfiguration.getBoolean(KEY_ENABLE_STORAGE_CLIENT_LOGGING, false));\r\n    LOG.debug(\"AzureNativeFileSystemStore init. Settings={},{},{},{{},{},{},{}},{{},{},{}}\", concurrentWrites, tolerateOobAppends, ((storageConnectionTimeout > 0) ? storageConnectionTimeout : STORAGE_CONNECTION_TIMEOUT_DEFAULT), minBackoff, deltaBackoff, maxBackoff, maxRetries, selfThrottlingEnabled, selfThrottlingReadFactor, selfThrottlingWriteFactor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "connectUsingAnonymousCredentials",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void connectUsingAnonymousCredentials(final URI uri) throws StorageException, IOException, URISyntaxException\n{\r\n    String accountName = getAccountFromAuthority(uri);\r\n    URI storageUri = new URI(getHTTPScheme() + \":\" + PATH_DELIMITER + PATH_DELIMITER + accountName);\r\n    String containerName = getContainerFromAuthority(uri);\r\n    storageInteractionLayer.createBlobClient(storageUri);\r\n    suppressRetryPolicyInClientIfNeeded();\r\n    container = storageInteractionLayer.getContainerReference(containerName);\r\n    rootDirectory = container.getDirectoryReference(\"\");\r\n    boolean canAccess;\r\n    try {\r\n        canAccess = container.exists(getInstrumentedContext());\r\n    } catch (StorageException ex) {\r\n        LOG.error(\"Service returned StorageException when checking existence \" + \"of container {} in account {}\", containerName, accountName, ex);\r\n        canAccess = false;\r\n    }\r\n    if (!canAccess) {\r\n        throw new AzureException(String.format(NO_ACCESS_TO_CONTAINER_MSG, accountName, containerName));\r\n    }\r\n    isAnonymousCredentials = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "connectUsingCredentials",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void connectUsingCredentials(String accountName, StorageCredentials credentials, String containerName) throws URISyntaxException, StorageException, AzureException\n{\r\n    URI blobEndPoint;\r\n    if (isStorageEmulatorAccount(accountName)) {\r\n        isStorageEmulator = true;\r\n        CloudStorageAccount account = CloudStorageAccount.getDevelopmentStorageAccount();\r\n        storageInteractionLayer.createBlobClient(account);\r\n    } else {\r\n        blobEndPoint = new URI(getHTTPScheme() + \"://\" + accountName);\r\n        storageInteractionLayer.createBlobClient(blobEndPoint, credentials);\r\n    }\r\n    suppressRetryPolicyInClientIfNeeded();\r\n    container = storageInteractionLayer.getContainerReference(containerName);\r\n    rootDirectory = container.getDirectoryReference(\"\");\r\n    canCreateOrModifyContainer = credentials instanceof StorageCredentialsAccountAndKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "connectToAzureStorageInSecureMode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void connectToAzureStorageInSecureMode(String accountName, String containerName, URI sessionUri) throws AzureException, StorageException, URISyntaxException\n{\r\n    LOG.debug(\"Connecting to Azure storage in Secure Mode\");\r\n    if (!(this.storageInteractionLayer instanceof SecureStorageInterfaceImpl)) {\r\n        throw new AssertionError(\"connectToAzureStorageInSecureMode() should be called only\" + \" for SecureStorageInterfaceImpl instances\");\r\n    }\r\n    ((SecureStorageInterfaceImpl) this.storageInteractionLayer).setStorageAccountName(accountName);\r\n    connectingUsingSAS = true;\r\n    container = storageInteractionLayer.getContainerReference(containerName);\r\n    rootDirectory = container.getDirectoryReference(\"\");\r\n    canCreateOrModifyContainer = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "connectUsingConnectionStringCredentials",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void connectUsingConnectionStringCredentials(final String accountName, final String containerName, final String accountKey) throws InvalidKeyException, StorageException, IOException, URISyntaxException\n{\r\n    String rawAccountName = accountName.split(\"\\\\.\")[0];\r\n    StorageCredentials credentials = new StorageCredentialsAccountAndKey(rawAccountName, accountKey);\r\n    connectUsingCredentials(accountName, credentials, containerName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "connectUsingSASCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void connectUsingSASCredentials(final String accountName, final String containerName, final String sas) throws InvalidKeyException, StorageException, IOException, URISyntaxException\n{\r\n    StorageCredentials credentials = new StorageCredentialsSharedAccessSignature(sas);\r\n    connectingUsingSAS = true;\r\n    connectUsingCredentials(accountName, credentials, containerName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isStorageEmulatorAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isStorageEmulatorAccount(final String accountName)\n{\r\n    return accountName.equalsIgnoreCase(sessionConfiguration.get(STORAGE_EMULATOR_ACCOUNT_NAME_PROPERTY_NAME, DEFAULT_STORAGE_EMULATOR_ACCOUNT_NAME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getAccountKeyFromConfiguration",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getAccountKeyFromConfiguration(String accountName, Configuration conf) throws KeyProviderException\n{\r\n    String key = null;\r\n    String keyProviderClass = conf.get(KEY_ACCOUNT_KEYPROVIDER_PREFIX + accountName);\r\n    KeyProvider keyProvider = null;\r\n    if (keyProviderClass == null) {\r\n        keyProvider = new SimpleKeyProvider();\r\n    } else {\r\n        Object keyProviderObject = null;\r\n        try {\r\n            Class<?> clazz = conf.getClassByName(keyProviderClass);\r\n            keyProviderObject = clazz.newInstance();\r\n        } catch (Exception e) {\r\n            throw new KeyProviderException(\"Unable to load key provider class.\", e);\r\n        }\r\n        if (!(keyProviderObject instanceof KeyProvider)) {\r\n            throw new KeyProviderException(keyProviderClass + \" specified in config is not a valid KeyProvider class.\");\r\n        }\r\n        keyProvider = (KeyProvider) keyProviderObject;\r\n    }\r\n    key = keyProvider.getStorageAccountKey(accountName, conf);\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "createAzureStorageSession",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void createAzureStorageSession() throws AzureException, IOException\n{\r\n    if (null == sessionUri || null == sessionConfiguration) {\r\n        throw new AzureException(\"Filesystem object not initialized properly.\" + \"Unable to start session with Azure Storage server.\");\r\n    }\r\n    try {\r\n        if (getContainerFromAuthority(sessionUri) == null) {\r\n            throw new AssertionError(String.format(\"Non-null container expected from session URI: %s.\", sessionUri.toString()));\r\n        }\r\n        String accountName = getAccountFromAuthority(sessionUri);\r\n        if (null == accountName) {\r\n            final String errMsg = String.format(\"Cannot load WASB file system account name not\" + \" specified in URI: %s.\", sessionUri.toString());\r\n            throw new AzureException(errMsg);\r\n        }\r\n        instrumentation.setAccountName(accountName);\r\n        String containerName = getContainerFromAuthority(sessionUri);\r\n        instrumentation.setContainerName(containerName);\r\n        if (isStorageEmulatorAccount(accountName)) {\r\n            connectUsingCredentials(accountName, null, containerName);\r\n            return;\r\n        }\r\n        if (useSecureMode) {\r\n            connectToAzureStorageInSecureMode(accountName, containerName, sessionUri);\r\n            return;\r\n        }\r\n        String propertyValue = sessionConfiguration.get(KEY_ACCOUNT_SAS_PREFIX + containerName + \".\" + accountName);\r\n        if (propertyValue != null) {\r\n            connectUsingSASCredentials(accountName, containerName, propertyValue);\r\n            return;\r\n        }\r\n        propertyValue = getAccountKeyFromConfiguration(accountName, sessionConfiguration);\r\n        if (StringUtils.isNotEmpty(propertyValue)) {\r\n            connectUsingConnectionStringCredentials(getAccountFromAuthority(sessionUri), getContainerFromAuthority(sessionUri), propertyValue);\r\n        } else {\r\n            LOG.debug(\"The account access key is not configured for {}. \" + \"Now try anonymous access.\", sessionUri);\r\n            connectUsingAnonymousCredentials(sessionUri);\r\n        }\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "trim",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String trim(String s, String toTrim)\n{\r\n    return StringUtils.removeEnd(StringUtils.removeStart(s, toTrim), toTrim);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "verifyAndConvertToStandardFormat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String verifyAndConvertToStandardFormat(String rawDir) throws URISyntaxException\n{\r\n    URI asUri = new URI(rawDir);\r\n    if (asUri.getAuthority() == null || asUri.getAuthority().toLowerCase(Locale.ENGLISH).equalsIgnoreCase(sessionUri.getAuthority().toLowerCase(Locale.ENGLISH))) {\r\n        return trim(asUri.getPath(), \"/\");\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getDirectorySet",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Set<String> getDirectorySet(final String configVar) throws AzureException\n{\r\n    String[] rawDirs = sessionConfiguration.getStrings(configVar, new String[0]);\r\n    Set<String> directorySet = new HashSet<String>();\r\n    for (String currentDir : rawDirs) {\r\n        String myDir;\r\n        try {\r\n            myDir = verifyAndConvertToStandardFormat(currentDir.trim());\r\n        } catch (URISyntaxException ex) {\r\n            throw new AzureException(String.format(\"The directory %s specified in the configuration entry %s is not\" + \" a valid URI.\", currentDir, configVar));\r\n        }\r\n        if (myDir != null) {\r\n            directorySet.add(myDir);\r\n        }\r\n    }\r\n    return directorySet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isPageBlobKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isPageBlobKey(String key)\n{\r\n    return isKeyForDirectorySet(key, pageBlobDirs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isBlockBlobWithCompactionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isBlockBlobWithCompactionKey(String key)\n{\r\n    return isKeyForDirectorySet(key, blockBlobWithCompationDirs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isAtomicRenameKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isAtomicRenameKey(String key)\n{\r\n    return isKeyForDirectorySet(key, atomicRenameDirs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isKeyForDirectorySet",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean isKeyForDirectorySet(String key, Set<String> dirSet)\n{\r\n    String defaultFS = FileSystem.getDefaultUri(sessionConfiguration).toString();\r\n    for (String dir : dirSet) {\r\n        if (dir.isEmpty()) {\r\n            return true;\r\n        }\r\n        if (matchAsteriskPattern(key, dir)) {\r\n            return true;\r\n        }\r\n        try {\r\n            URI uriPageBlobDir = new URI(dir);\r\n            if (null == uriPageBlobDir.getAuthority()) {\r\n                String dirWithPrefix = trim(defaultFS, \"/\") + \"/\" + dir;\r\n                if (matchAsteriskPattern(key, dirWithPrefix)) {\r\n                    return true;\r\n                }\r\n            }\r\n        } catch (URISyntaxException e) {\r\n            LOG.info(\"URI syntax error creating URI for {}\", dir);\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "matchAsteriskPattern",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean matchAsteriskPattern(String pathName, String pattern)\n{\r\n    if (pathName == null || pathName.length() == 0) {\r\n        return false;\r\n    }\r\n    int pathIndex = 0;\r\n    int patternIndex = 0;\r\n    while (pathIndex < pathName.length() && patternIndex < pattern.length()) {\r\n        char charToMatch = pattern.charAt(patternIndex);\r\n        if (charToMatch != ASTERISK_SYMBOL) {\r\n            if (charToMatch != pathName.charAt(pathIndex)) {\r\n                return false;\r\n            }\r\n            pathIndex++;\r\n            patternIndex++;\r\n            continue;\r\n        }\r\n        if (patternIndex > 0 && pattern.charAt(patternIndex - 1) != Path.SEPARATOR_CHAR || patternIndex + 1 < pattern.length() && pattern.charAt(patternIndex + 1) != Path.SEPARATOR_CHAR) {\r\n            if (ASTERISK_SYMBOL != pathName.charAt(pathIndex)) {\r\n                return false;\r\n            }\r\n            pathIndex++;\r\n            patternIndex++;\r\n            continue;\r\n        }\r\n        patternIndex++;\r\n        while (pathIndex < pathName.length() && pathName.charAt(pathIndex) != Path.SEPARATOR_CHAR) {\r\n            pathIndex++;\r\n        }\r\n    }\r\n    return patternIndex == pattern.length() && (pathIndex == pathName.length() || pathName.charAt(pathIndex) == Path.SEPARATOR_CHAR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getHadoopBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHadoopBlockSize()\n{\r\n    return hadoopBlockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "checkContainer",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "ContainerState checkContainer(ContainerAccessType accessType) throws StorageException, AzureException\n{\r\n    synchronized (containerStateLock) {\r\n        if (isOkContainerState(accessType)) {\r\n            return currentKnownContainerState;\r\n        }\r\n        if (currentKnownContainerState == ContainerState.ExistsAtWrongVersion) {\r\n            String containerVersion = retrieveVersionAttribute(container);\r\n            throw wrongVersionException(containerVersion);\r\n        }\r\n        if (currentKnownContainerState == ContainerState.ExistsAtRightVersion) {\r\n            throw new AssertionError(\"Unexpected state: \" + currentKnownContainerState);\r\n        }\r\n        try {\r\n            container.downloadAttributes(getInstrumentedContext());\r\n            currentKnownContainerState = ContainerState.Unknown;\r\n        } catch (StorageException ex) {\r\n            if (StorageErrorCodeStrings.CONTAINER_NOT_FOUND.toString().equals(ex.getErrorCode())) {\r\n                currentKnownContainerState = ContainerState.DoesntExist;\r\n            } else {\r\n                throw ex;\r\n            }\r\n        }\r\n        if (currentKnownContainerState == ContainerState.DoesntExist) {\r\n            if (needToCreateContainer(accessType)) {\r\n                storeVersionAttribute(container);\r\n                container.create(getInstrumentedContext());\r\n                currentKnownContainerState = ContainerState.ExistsAtRightVersion;\r\n            }\r\n        } else {\r\n            String containerVersion = retrieveVersionAttribute(container);\r\n            if (containerVersion != null) {\r\n                if (containerVersion.equals(FIRST_WASB_VERSION)) {\r\n                    if (needToStampVersion(accessType)) {\r\n                        storeVersionAttribute(container);\r\n                        container.uploadMetadata(getInstrumentedContext());\r\n                    }\r\n                } else if (!containerVersion.equals(CURRENT_WASB_VERSION)) {\r\n                    currentKnownContainerState = ContainerState.ExistsAtWrongVersion;\r\n                    throw wrongVersionException(containerVersion);\r\n                } else {\r\n                    currentKnownContainerState = ContainerState.ExistsAtRightVersion;\r\n                }\r\n            } else {\r\n                currentKnownContainerState = ContainerState.ExistsNoVersion;\r\n                if (needToStampVersion(accessType)) {\r\n                    storeVersionAttribute(container);\r\n                    container.uploadMetadata(getInstrumentedContext());\r\n                    currentKnownContainerState = ContainerState.ExistsAtRightVersion;\r\n                }\r\n            }\r\n        }\r\n        return currentKnownContainerState;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "wrongVersionException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AzureException wrongVersionException(String containerVersion)\n{\r\n    return new AzureException(\"The container \" + container.getName() + \" is at an unsupported version: \" + containerVersion + \". Current supported version: \" + FIRST_WASB_VERSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "needToStampVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needToStampVersion(ContainerAccessType accessType)\n{\r\n    return accessType != ContainerAccessType.PureRead && canCreateOrModifyContainer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "needToCreateContainer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needToCreateContainer(ContainerAccessType accessType)\n{\r\n    return accessType == ContainerAccessType.PureWrite;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isOkContainerState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isOkContainerState(ContainerAccessType accessType)\n{\r\n    switch(currentKnownContainerState) {\r\n        case Unknown:\r\n            return connectingUsingSAS;\r\n        case DoesntExist:\r\n            return false;\r\n        case ExistsAtRightVersion:\r\n            return true;\r\n        case ExistsAtWrongVersion:\r\n            return false;\r\n        case ExistsNoVersion:\r\n            return !needToStampVersion(accessType);\r\n        default:\r\n            throw new AssertionError(\"Unknown access type: \" + accessType);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getUseTransactionalContentMD5",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getUseTransactionalContentMD5()\n{\r\n    return sessionConfiguration.getBoolean(KEY_CHECK_BLOCK_MD5, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getUploadOptions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BlobRequestOptions getUploadOptions()\n{\r\n    BlobRequestOptions options = new BlobRequestOptions();\r\n    options.setStoreBlobContentMD5(sessionConfiguration.getBoolean(KEY_STORE_BLOB_MD5, false));\r\n    options.setUseTransactionalContentMD5(getUseTransactionalContentMD5());\r\n    options.setConcurrentRequestCount(concurrentWrites);\r\n    options.setRetryPolicyFactory(new RetryExponentialRetry(minBackoff, deltaBackoff, maxBackoff, maxRetries));\r\n    return options;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getDownloadOptions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlobRequestOptions getDownloadOptions()\n{\r\n    BlobRequestOptions options = new BlobRequestOptions();\r\n    options.setRetryPolicyFactory(new RetryExponentialRetry(minBackoff, deltaBackoff, maxBackoff, maxRetries));\r\n    options.setUseTransactionalContentMD5(getUseTransactionalContentMD5());\r\n    return options;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storefile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "DataOutputStream storefile(String keyEncoded, PermissionStatus permissionStatus, String key) throws AzureException\n{\r\n    try {\r\n        if (null == storageInteractionLayer) {\r\n            final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n            throw new AzureException(errMsg);\r\n        }\r\n        if (!isAuthenticatedAccess()) {\r\n            throw new AzureException(new IOException(\"Uploads to public accounts using anonymous \" + \"access is prohibited.\"));\r\n        }\r\n        checkContainer(ContainerAccessType.PureWrite);\r\n        if (AZURE_ROOT_CONTAINER.equals(getContainerFromAuthority(sessionUri))) {\r\n            final String errMsg = String.format(\"Writes to '%s' container for URI '%s' are prohibited, \" + \"only updates on non-root containers permitted.\", AZURE_ROOT_CONTAINER, sessionUri.toString());\r\n            throw new AzureException(errMsg);\r\n        }\r\n        CloudBlobWrapper blob = getBlobReference(keyEncoded);\r\n        storePermissionStatus(blob, permissionStatus);\r\n        OutputStream outputStream;\r\n        if (isBlockBlobWithCompactionKey(key)) {\r\n            BlockBlobAppendStream blockBlobOutputStream = new BlockBlobAppendStream((CloudBlockBlobWrapper) blob, keyEncoded, this.uploadBlockSizeBytes, true, getInstrumentedContext());\r\n            outputStream = blockBlobOutputStream;\r\n        } else {\r\n            outputStream = openOutputStream(blob);\r\n        }\r\n        DataOutputStream dataOutStream = new SyncableDataOutputStream(outputStream);\r\n        return dataOutStream;\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "openOutputStream",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "OutputStream openOutputStream(final CloudBlobWrapper blob) throws StorageException\n{\r\n    if (blob instanceof CloudPageBlobWrapper) {\r\n        return new PageBlobOutputStream((CloudPageBlobWrapper) blob, getInstrumentedContext(), sessionConfiguration);\r\n    } else {\r\n        return ((CloudBlockBlobWrapper) blob).openOutputStream(getUploadOptions(), getInstrumentedContext());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "openInputStream",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "InputStream openInputStream(CloudBlobWrapper blob, Optional<Configuration> options) throws StorageException, IOException\n{\r\n    if (blob instanceof CloudBlockBlobWrapper) {\r\n        LOG.debug(\"Using stream seek algorithm {}\", inputStreamVersion);\r\n        switch(inputStreamVersion) {\r\n            case 1:\r\n                return blob.openInputStream(getDownloadOptions(), getInstrumentedContext(isConcurrentOOBAppendAllowed()));\r\n            case 2:\r\n                boolean bufferedPreadDisabled = options.map(c -> c.getBoolean(FS_AZURE_BLOCK_BLOB_BUFFERED_PREAD_DISABLE, false)).orElse(false);\r\n                return new BlockBlobInputStream((CloudBlockBlobWrapper) blob, getDownloadOptions(), getInstrumentedContext(isConcurrentOOBAppendAllowed()), bufferedPreadDisabled);\r\n            default:\r\n                throw new IOException(\"Unknown seek algorithm: \" + inputStreamVersion);\r\n        }\r\n    } else {\r\n        return new PageBlobInputStream((CloudPageBlobWrapper) blob, getInstrumentedContext(isConcurrentOOBAppendAllowed()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "defaultPermissionNoBlobMetadata",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PermissionStatus defaultPermissionNoBlobMetadata()\n{\r\n    return new PermissionStatus(\"\", \"\", FsPermission.getDefault());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storeMetadataAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void storeMetadataAttribute(CloudBlobWrapper blob, String key, String value)\n{\r\n    HashMap<String, String> metadata = blob.getMetadata();\r\n    if (null == metadata) {\r\n        metadata = new HashMap<String, String>();\r\n    }\r\n    metadata.put(key, value);\r\n    blob.setMetadata(metadata);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getMetadataAttribute",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getMetadataAttribute(HashMap<String, String> metadata, String... keyAlternatives)\n{\r\n    if (null == metadata) {\r\n        return null;\r\n    }\r\n    for (String key : keyAlternatives) {\r\n        if (metadataKeyCaseSensitive) {\r\n            if (metadata.containsKey(key)) {\r\n                return metadata.get(key);\r\n            }\r\n        } else {\r\n            for (Entry<String, String> entry : metadata.entrySet()) {\r\n                if (key.equalsIgnoreCase(entry.getKey())) {\r\n                    return entry.getValue();\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "removeMetadataAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeMetadataAttribute(CloudBlobWrapper blob, String key)\n{\r\n    HashMap<String, String> metadata = blob.getMetadata();\r\n    if (metadata != null) {\r\n        metadata.remove(key);\r\n        blob.setMetadata(metadata);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storePermissionStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void storePermissionStatus(CloudBlobWrapper blob, PermissionStatus permissionStatus)\n{\r\n    storeMetadataAttribute(blob, PERMISSION_METADATA_KEY, PERMISSION_JSON_SERIALIZER.toJSON(permissionStatus));\r\n    removeMetadataAttribute(blob, OLD_PERMISSION_METADATA_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPermissionStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "PermissionStatus getPermissionStatus(CloudBlobWrapper blob)\n{\r\n    String permissionMetadataValue = getMetadataAttribute(blob.getMetadata(), PERMISSION_METADATA_KEY, OLD_PERMISSION_METADATA_KEY);\r\n    if (permissionMetadataValue != null) {\r\n        return PermissionStatusJsonSerializer.fromJSONString(permissionMetadataValue);\r\n    } else {\r\n        return defaultPermissionNoBlobMetadata();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storeFolderAttribute",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void storeFolderAttribute(CloudBlobWrapper blob)\n{\r\n    storeMetadataAttribute(blob, IS_FOLDER_METADATA_KEY, \"true\");\r\n    removeMetadataAttribute(blob, OLD_IS_FOLDER_METADATA_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "encodeMetadataAttribute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String encodeMetadataAttribute(String value) throws UnsupportedEncodingException\n{\r\n    return value == null ? null : URLEncoder.encode(value, METADATA_ENCODING.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "decodeMetadataAttribute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String decodeMetadataAttribute(String encoded) throws UnsupportedEncodingException\n{\r\n    return encoded == null ? null : URLDecoder.decode(encoded, METADATA_ENCODING.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "ensureValidAttributeName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String ensureValidAttributeName(String attribute)\n{\r\n    return attribute.replace('.', '_');\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storeLinkAttribute",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void storeLinkAttribute(CloudBlobWrapper blob, String linkTarget) throws UnsupportedEncodingException\n{\r\n    String encodedLinkTarget = encodeMetadataAttribute(linkTarget);\r\n    storeMetadataAttribute(blob, LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY, encodedLinkTarget);\r\n    removeMetadataAttribute(blob, OLD_LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getLinkAttributeValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getLinkAttributeValue(CloudBlobWrapper blob) throws UnsupportedEncodingException\n{\r\n    String encodedLinkTarget = getMetadataAttribute(blob.getMetadata(), LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY, OLD_LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY);\r\n    return decodeMetadataAttribute(encodedLinkTarget);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieveFolderAttribute",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean retrieveFolderAttribute(CloudBlobWrapper blob)\n{\r\n    HashMap<String, String> metadata = blob.getMetadata();\r\n    if (null != metadata) {\r\n        if (metadataKeyCaseSensitive) {\r\n            return metadata.containsKey(IS_FOLDER_METADATA_KEY) || metadata.containsKey(OLD_IS_FOLDER_METADATA_KEY);\r\n        } else {\r\n            for (String key : metadata.keySet()) {\r\n                if (key.equalsIgnoreCase(IS_FOLDER_METADATA_KEY) || key.equalsIgnoreCase(OLD_IS_FOLDER_METADATA_KEY)) {\r\n                    return true;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storeVersionAttribute",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void storeVersionAttribute(CloudBlobContainerWrapper container)\n{\r\n    HashMap<String, String> metadata = container.getMetadata();\r\n    if (null == metadata) {\r\n        metadata = new HashMap<String, String>();\r\n    }\r\n    metadata.put(VERSION_METADATA_KEY, CURRENT_WASB_VERSION);\r\n    if (metadata.containsKey(OLD_VERSION_METADATA_KEY)) {\r\n        metadata.remove(OLD_VERSION_METADATA_KEY);\r\n    }\r\n    container.setMetadata(metadata);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieveVersionAttribute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String retrieveVersionAttribute(CloudBlobContainerWrapper container)\n{\r\n    return getMetadataAttribute(container.getMetadata(), VERSION_METADATA_KEY, OLD_VERSION_METADATA_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storeEmptyFolder",
  "errType" : [ "StorageException", "URISyntaxException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void storeEmptyFolder(String key, PermissionStatus permissionStatus) throws AzureException\n{\r\n    if (null == storageInteractionLayer) {\r\n        final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n        throw new AssertionError(errMsg);\r\n    }\r\n    if (!isAuthenticatedAccess()) {\r\n        throw new AzureException(\"Uploads to to public accounts using anonymous access is prohibited.\");\r\n    }\r\n    try {\r\n        checkContainer(ContainerAccessType.PureWrite);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        storePermissionStatus(blob, permissionStatus);\r\n        storeFolderAttribute(blob);\r\n        openOutputStream(blob).close();\r\n    } catch (StorageException e) {\r\n        throw new AzureException(e);\r\n    } catch (URISyntaxException e) {\r\n        throw new AzureException(e);\r\n    } catch (IOException e) {\r\n        Throwable t = e.getCause();\r\n        if (t instanceof StorageException) {\r\n            StorageException se = (StorageException) t;\r\n            if (!\"LeaseIdMissing\".equals(se.getErrorCode())) {\r\n                throw new AzureException(e);\r\n            }\r\n        } else {\r\n            throw new AzureException(e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storeEmptyLinkFile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void storeEmptyLinkFile(String key, String tempBlobKey, PermissionStatus permissionStatus) throws AzureException\n{\r\n    if (null == storageInteractionLayer) {\r\n        final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n        throw new AssertionError(errMsg);\r\n    }\r\n    if (!isAuthenticatedAccess()) {\r\n        throw new AzureException(\"Uploads to to public accounts using anonymous access is prohibited.\");\r\n    }\r\n    try {\r\n        checkContainer(ContainerAccessType.PureWrite);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        storePermissionStatus(blob, permissionStatus);\r\n        storeLinkAttribute(blob, tempBlobKey);\r\n        openOutputStream(blob).close();\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getLinkInFileMetadata",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getLinkInFileMetadata(String key) throws AzureException\n{\r\n    if (null == storageInteractionLayer) {\r\n        final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n        throw new AssertionError(errMsg);\r\n    }\r\n    try {\r\n        checkContainer(ContainerAccessType.PureRead);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        blob.downloadAttributes(getInstrumentedContext());\r\n        return getLinkAttributeValue(blob);\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isAuthenticatedAccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isAuthenticatedAccess() throws AzureException\n{\r\n    if (isAnonymousCredentials) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "listRootBlobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<ListBlobItem> listRootBlobs(boolean includeMetadata, boolean useFlatBlobListing) throws StorageException, URISyntaxException\n{\r\n    return rootDirectory.listBlobs(null, useFlatBlobListing, includeMetadata ? EnumSet.of(BlobListingDetails.METADATA) : EnumSet.noneOf(BlobListingDetails.class), null, getInstrumentedContext());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "listRootBlobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<ListBlobItem> listRootBlobs(String aPrefix, boolean includeMetadata, boolean useFlatBlobListing) throws StorageException, URISyntaxException\n{\r\n    Iterable<ListBlobItem> list = rootDirectory.listBlobs(aPrefix, useFlatBlobListing, includeMetadata ? EnumSet.of(BlobListingDetails.METADATA) : EnumSet.noneOf(BlobListingDetails.class), null, getInstrumentedContext());\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "listRootBlobs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Iterable<ListBlobItem> listRootBlobs(String aPrefix, boolean useFlatBlobListing, EnumSet<BlobListingDetails> listingDetails, BlobRequestOptions options, OperationContext opContext) throws StorageException, URISyntaxException\n{\r\n    CloudBlobDirectoryWrapper directory = this.container.getDirectoryReference(aPrefix);\r\n    return directory.listBlobs(null, useFlatBlobListing, listingDetails, options, opContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBlobReference",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "CloudBlobWrapper getBlobReference(String aKey) throws StorageException, URISyntaxException\n{\r\n    CloudBlobWrapper blob = null;\r\n    if (isPageBlobKey(aKey)) {\r\n        blob = this.container.getPageBlobReference(aKey);\r\n    } else {\r\n        blob = this.container.getBlockBlobReference(aKey);\r\n        blob.setStreamMinimumReadSizeInBytes(downloadBlockSizeBytes);\r\n        blob.setWriteBlockSizeInBytes(uploadBlockSizeBytes);\r\n    }\r\n    return blob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "normalizeKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String normalizeKey(URI keyUri)\n{\r\n    String normKey;\r\n    int parts = isStorageEmulator ? 4 : 3;\r\n    normKey = keyUri.getPath().split(\"/\", parts)[(parts - 1)];\r\n    return normKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "normalizeKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String normalizeKey(CloudBlobWrapper blob)\n{\r\n    return normalizeKey(blob.getUri());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "normalizeKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String normalizeKey(CloudBlobDirectoryWrapper directory)\n{\r\n    String dirKey = normalizeKey(directory.getUri());\r\n    if (dirKey.endsWith(PATH_DELIMITER)) {\r\n        dirKey = dirKey.substring(0, dirKey.length() - 1);\r\n    }\r\n    return dirKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getInstrumentedContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OperationContext getInstrumentedContext()\n{\r\n    return getInstrumentedContext(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getInstrumentedContext",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "OperationContext getInstrumentedContext(boolean bindConcurrentOOBIo)\n{\r\n    OperationContext operationContext = new OperationContext();\r\n    operationContext.getSendingRequestEventHandler().addListener(new StorageEvent<SendingRequestEvent>() {\r\n\r\n        @Override\r\n        public void eventOccurred(SendingRequestEvent eventArg) {\r\n            HttpURLConnection connection = (HttpURLConnection) eventArg.getConnectionObject();\r\n            String userAgentInfo = String.format(Utility.LOCALE_US, \"WASB/%s (%s) %s\", VersionInfo.getVersion(), userAgentId, BaseRequest.getUserAgent());\r\n            connection.setRequestProperty(Constants.HeaderConstants.USER_AGENT, userAgentInfo);\r\n        }\r\n    });\r\n    if (selfThrottlingEnabled) {\r\n        SelfThrottlingIntercept.hook(operationContext, selfThrottlingReadFactor, selfThrottlingWriteFactor);\r\n    } else if (autoThrottlingEnabled) {\r\n        ClientThrottlingIntercept.hook(operationContext);\r\n    }\r\n    if (bandwidthGaugeUpdater != null) {\r\n        ResponseReceivedMetricUpdater.hook(operationContext, instrumentation, bandwidthGaugeUpdater);\r\n    }\r\n    if (bindConcurrentOOBIo) {\r\n        SendRequestIntercept.bind(operationContext);\r\n    }\r\n    if (testHookOperationContext != null) {\r\n        operationContext = testHookOperationContext.modifyOperationContext(operationContext);\r\n    }\r\n    ErrorMetricUpdater.hook(operationContext, instrumentation);\r\n    return operationContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieveMetadata",
  "errType" : [ "Exception", "StorageException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "FileMetadata retrieveMetadata(String key) throws IOException\n{\r\n    if (null == storageInteractionLayer) {\r\n        final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n        throw new AssertionError(errMsg);\r\n    }\r\n    LOG.debug(\"Retrieving metadata for {}\", key);\r\n    try {\r\n        if (checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist) {\r\n            return null;\r\n        }\r\n        if (key.equals(\"/\")) {\r\n            return new FileMetadata(key, 0, defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit, hadoopBlockSize);\r\n        }\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        if (null != blob && blob.exists(getInstrumentedContext())) {\r\n            LOG.debug(\"Found {} as an explicit blob. Checking if it's a file or folder.\", key);\r\n            try {\r\n                blob.downloadAttributes(getInstrumentedContext());\r\n                BlobProperties properties = blob.getProperties();\r\n                if (retrieveFolderAttribute(blob)) {\r\n                    LOG.debug(\"{} is a folder blob.\", key);\r\n                    return new FileMetadata(key, properties.getLastModified().getTime(), getPermissionStatus(blob), BlobMaterialization.Explicit, hadoopBlockSize);\r\n                } else {\r\n                    LOG.debug(\"{} is a normal blob.\", key);\r\n                    return new FileMetadata(key, getDataLength(blob, properties), properties.getLastModified().getTime(), getPermissionStatus(blob), hadoopBlockSize);\r\n                }\r\n            } catch (StorageException e) {\r\n                if (!NativeAzureFileSystemHelper.isFileNotFoundException(e)) {\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n        Iterable<ListBlobItem> objects = listRootBlobs(key, true, EnumSet.of(BlobListingDetails.METADATA), null, getInstrumentedContext());\r\n        for (ListBlobItem blobItem : objects) {\r\n            if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\r\n                LOG.debug(\"Found blob as a directory-using this file under it to infer its properties {}\", blobItem.getUri());\r\n                blob = (CloudBlobWrapper) blobItem;\r\n                BlobProperties properties = blob.getProperties();\r\n                return new FileMetadata(key, properties.getLastModified().getTime(), getPermissionStatus(blob), BlobMaterialization.Implicit, hadoopBlockSize);\r\n            }\r\n        }\r\n        return null;\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieveAttribute",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "byte[] retrieveAttribute(String key, String attribute) throws IOException\n{\r\n    try {\r\n        checkContainer(ContainerAccessType.PureRead);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        blob.downloadAttributes(getInstrumentedContext());\r\n        String value = getMetadataAttribute(blob.getMetadata(), ensureValidAttributeName(attribute));\r\n        value = decodeMetadataAttribute(value);\r\n        return value == null ? null : value.getBytes(METADATA_ENCODING);\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "storeAttribute",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void storeAttribute(String key, String attribute, byte[] value) throws IOException\n{\r\n    try {\r\n        checkContainer(ContainerAccessType.ReadThenWrite);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        blob.downloadAttributes(getInstrumentedContext());\r\n        String encodedValue = encodeMetadataAttribute(new String(value, METADATA_ENCODING));\r\n        storeMetadataAttribute(blob, ensureValidAttributeName(attribute), encodedValue);\r\n        blob.uploadMetadata(getInstrumentedContext());\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InputStream retrieve(String key) throws AzureException, IOException\n{\r\n    return retrieve(key, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InputStream retrieve(String key, long startByteOffset) throws AzureException, IOException\n{\r\n    return retrieve(key, startByteOffset, Optional.empty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieve",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "InputStream retrieve(String key, long startByteOffset, Optional<Configuration> options) throws AzureException, IOException\n{\r\n    try {\r\n        if (null == storageInteractionLayer) {\r\n            final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n            throw new AssertionError(errMsg);\r\n        }\r\n        checkContainer(ContainerAccessType.PureRead);\r\n        InputStream inputStream = openInputStream(getBlobReference(key), options);\r\n        if (startByteOffset > 0) {\r\n            inputStream.skip(startByteOffset);\r\n        }\r\n        return inputStream;\r\n    } catch (IOException e) {\r\n        throw e;\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "list",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileMetadata[] list(String prefix, final int maxListingCount, final int maxListingDepth) throws IOException\n{\r\n    return listInternal(prefix, maxListingCount, maxListingDepth);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "listInternal",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "FileMetadata[] listInternal(String prefix, final int maxListingCount, final int maxListingDepth) throws IOException\n{\r\n    try {\r\n        checkContainer(ContainerAccessType.PureRead);\r\n        if (0 < prefix.length() && !prefix.endsWith(PATH_DELIMITER)) {\r\n            prefix += PATH_DELIMITER;\r\n        }\r\n        boolean enableFlatListing = false;\r\n        if (maxListingDepth < 0 && sessionConfiguration.getBoolean(KEY_ENABLE_FLAT_LISTING, DEFAULT_ENABLE_FLAT_LISTING)) {\r\n            enableFlatListing = true;\r\n        }\r\n        Iterable<ListBlobItem> objects;\r\n        if (prefix.equals(\"/\")) {\r\n            objects = listRootBlobs(true, enableFlatListing);\r\n        } else {\r\n            objects = listRootBlobs(prefix, true, enableFlatListing);\r\n        }\r\n        HashMap<String, FileMetadata> fileMetadata = new HashMap<>(256);\r\n        for (ListBlobItem blobItem : objects) {\r\n            if (0 < maxListingCount && fileMetadata.size() >= maxListingCount) {\r\n                break;\r\n            }\r\n            if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\r\n                String blobKey = null;\r\n                CloudBlobWrapper blob = (CloudBlobWrapper) blobItem;\r\n                BlobProperties properties = blob.getProperties();\r\n                blobKey = normalizeKey(blob);\r\n                FileMetadata metadata;\r\n                if (retrieveFolderAttribute(blob)) {\r\n                    metadata = new FileMetadata(blobKey, properties.getLastModified().getTime(), getPermissionStatus(blob), BlobMaterialization.Explicit, hadoopBlockSize);\r\n                } else {\r\n                    metadata = new FileMetadata(blobKey, getDataLength(blob, properties), properties.getLastModified().getTime(), getPermissionStatus(blob), hadoopBlockSize);\r\n                }\r\n                fileMetadata.put(blobKey, metadata);\r\n            } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\r\n                CloudBlobDirectoryWrapper directory = (CloudBlobDirectoryWrapper) blobItem;\r\n                String dirKey = normalizeKey(directory);\r\n                if (dirKey.endsWith(PATH_DELIMITER)) {\r\n                    dirKey = dirKey.substring(0, dirKey.length() - 1);\r\n                }\r\n                FileMetadata directoryMetadata = new FileMetadata(dirKey, 0, defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit, hadoopBlockSize);\r\n                if (!fileMetadata.containsKey(dirKey)) {\r\n                    fileMetadata.put(dirKey, directoryMetadata);\r\n                }\r\n                if (!enableFlatListing) {\r\n                    buildUpList(directory, fileMetadata, maxListingCount, maxListingDepth - 1);\r\n                }\r\n            }\r\n        }\r\n        return fileMetadata.values().toArray(new FileMetadata[fileMetadata.size()]);\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "buildUpList",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void buildUpList(CloudBlobDirectoryWrapper aCloudBlobDirectory, HashMap<String, FileMetadata> metadataHashMap, final int maxListingCount, final int maxListingDepth) throws Exception\n{\r\n    AzureLinkedStack<Iterator<ListBlobItem>> dirIteratorStack = new AzureLinkedStack<Iterator<ListBlobItem>>();\r\n    Iterable<ListBlobItem> blobItems = aCloudBlobDirectory.listBlobs(null, false, EnumSet.of(BlobListingDetails.METADATA), null, getInstrumentedContext());\r\n    Iterator<ListBlobItem> blobItemIterator = blobItems.iterator();\r\n    if (0 == maxListingDepth || 0 == maxListingCount) {\r\n        return;\r\n    }\r\n    final boolean isUnboundedDepth = (maxListingDepth < 0);\r\n    int listingDepth = 1;\r\n    while (null != blobItemIterator && (maxListingCount <= 0 || metadataHashMap.size() < maxListingCount)) {\r\n        while (blobItemIterator.hasNext()) {\r\n            if (0 < maxListingCount && metadataHashMap.size() >= maxListingCount) {\r\n                break;\r\n            }\r\n            ListBlobItem blobItem = blobItemIterator.next();\r\n            if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {\r\n                String blobKey = null;\r\n                CloudBlobWrapper blob = (CloudBlobWrapper) blobItem;\r\n                BlobProperties properties = blob.getProperties();\r\n                blobKey = normalizeKey(blob);\r\n                FileMetadata metadata;\r\n                if (retrieveFolderAttribute(blob)) {\r\n                    metadata = new FileMetadata(blobKey, properties.getLastModified().getTime(), getPermissionStatus(blob), BlobMaterialization.Explicit, hadoopBlockSize);\r\n                } else {\r\n                    metadata = new FileMetadata(blobKey, getDataLength(blob, properties), properties.getLastModified().getTime(), getPermissionStatus(blob), hadoopBlockSize);\r\n                }\r\n                metadataHashMap.put(blobKey, metadata);\r\n            } else if (blobItem instanceof CloudBlobDirectoryWrapper) {\r\n                CloudBlobDirectoryWrapper directory = (CloudBlobDirectoryWrapper) blobItem;\r\n                if (isUnboundedDepth || maxListingDepth > listingDepth) {\r\n                    dirIteratorStack.push(blobItemIterator);\r\n                    ++listingDepth;\r\n                    blobItems = directory.listBlobs(null, false, EnumSet.noneOf(BlobListingDetails.class), null, getInstrumentedContext());\r\n                    blobItemIterator = blobItems.iterator();\r\n                } else {\r\n                    String dirKey = normalizeKey(directory);\r\n                    if (!metadataHashMap.containsKey(dirKey)) {\r\n                        FileMetadata directoryMetadata = new FileMetadata(dirKey, 0, defaultPermissionNoBlobMetadata(), BlobMaterialization.Implicit, hadoopBlockSize);\r\n                        metadataHashMap.put(dirKey, directoryMetadata);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (dirIteratorStack.isEmpty()) {\r\n            blobItemIterator = null;\r\n        } else {\r\n            blobItemIterator = dirIteratorStack.pop();\r\n            --listingDepth;\r\n            if (listingDepth < 0) {\r\n                throw new AssertionError(\"Non-negative listing depth expected\");\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getDataLength",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getDataLength(CloudBlobWrapper blob, BlobProperties properties) throws AzureException\n{\r\n    if (blob instanceof CloudPageBlobWrapper) {\r\n        try {\r\n            return PageBlobInputStream.getPageBlobDataSize((CloudPageBlobWrapper) blob, getInstrumentedContext(isConcurrentOOBAppendAllowed()));\r\n        } catch (Exception e) {\r\n            throw new AzureException(\"Unexpected exception getting page blob actual data size.\", e);\r\n        }\r\n    }\r\n    return properties.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "safeDelete",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void safeDelete(CloudBlobWrapper blob, SelfRenewingLease lease) throws StorageException\n{\r\n    OperationContext operationContext = getInstrumentedContext();\r\n    try {\r\n        blob.delete(operationContext, lease);\r\n    } catch (StorageException e) {\r\n        if (!NativeAzureFileSystemHelper.isFileNotFoundException(e)) {\r\n            LOG.error(\"Encountered Storage Exception for delete on Blob: {}\" + \", Exception Details: {} Error Code: {}\", blob.getUri(), e.getMessage(), e.getErrorCode());\r\n        }\r\n        if (e.getErrorCode() != null && \"BlobNotFound\".equals(e.getErrorCode()) && operationContext.getRequestResults().size() > 1 && operationContext.getRequestResults().get(0).getException() != null) {\r\n            LOG.debug(\"Swallowing delete exception on retry: {}\", e.getMessage());\r\n            return;\r\n        } else {\r\n            throw e;\r\n        }\r\n    } finally {\r\n        if (lease != null) {\r\n            lease.free();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "delete",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean delete(String key, SelfRenewingLease lease) throws IOException\n{\r\n    try {\r\n        if (checkContainer(ContainerAccessType.ReadThenWrite) == ContainerState.DoesntExist) {\r\n            return true;\r\n        }\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        safeDelete(blob, lease);\r\n        return true;\r\n    } catch (Exception e) {\r\n        if (e instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) e)) {\r\n            return false;\r\n        }\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "delete",
  "errType" : [ "IOException", "AzureException", "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean delete(String key) throws IOException\n{\r\n    try {\r\n        return delete(key, null);\r\n    } catch (IOException e) {\r\n        Throwable t = e.getCause();\r\n        if (t instanceof StorageException) {\r\n            StorageException se = (StorageException) t;\r\n            if (\"LeaseIdMissing\".equals(se.getErrorCode())) {\r\n                SelfRenewingLease lease = null;\r\n                try {\r\n                    lease = acquireLease(key);\r\n                    return delete(key, lease);\r\n                } catch (AzureException e3) {\r\n                    LOG.warn(\"Got unexpected exception trying to acquire lease on \" + key + \".\" + e3.getMessage());\r\n                    throw e3;\r\n                } finally {\r\n                    try {\r\n                        if (lease != null) {\r\n                            lease.free();\r\n                        }\r\n                    } catch (Exception e4) {\r\n                        LOG.error(\"Unable to free lease on \" + key, e4);\r\n                    }\r\n                }\r\n            } else {\r\n                throw e;\r\n            }\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void rename(String srcKey, String dstKey) throws IOException\n{\r\n    rename(srcKey, dstKey, false, null, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void rename(String srcKey, String dstKey, boolean acquireLease, SelfRenewingLease existingLease) throws IOException\n{\r\n    rename(srcKey, dstKey, acquireLease, existingLease, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "rename",
  "errType" : [ "StorageException", "URISyntaxException", "StorageException", "StorageException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void rename(String srcKey, String dstKey, boolean acquireLease, SelfRenewingLease existingLease, boolean overwriteDestination) throws IOException\n{\r\n    LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\r\n    if (acquireLease && existingLease != null) {\r\n        throw new IOException(\"Cannot acquire new lease if one already exists.\");\r\n    }\r\n    CloudBlobWrapper srcBlob = null;\r\n    CloudBlobWrapper dstBlob = null;\r\n    SelfRenewingLease lease = null;\r\n    try {\r\n        if (null == storageInteractionLayer) {\r\n            final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n            throw new AssertionError(errMsg);\r\n        }\r\n        checkContainer(ContainerAccessType.ReadThenWrite);\r\n        srcBlob = getBlobReference(srcKey);\r\n        if (!srcBlob.exists(getInstrumentedContext())) {\r\n            throw new AzureException(\"Source blob \" + srcKey + \" does not exist.\");\r\n        }\r\n        if (acquireLease) {\r\n            lease = srcBlob.acquireLease();\r\n        } else if (existingLease != null) {\r\n            lease = existingLease;\r\n        }\r\n        dstBlob = getBlobReference(dstKey);\r\n        try {\r\n            dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext(), overwriteDestination);\r\n        } catch (StorageException se) {\r\n            if (se.getHttpStatusCode() == HttpURLConnection.HTTP_UNAVAILABLE) {\r\n                int copyBlobMinBackoff = sessionConfiguration.getInt(KEY_COPYBLOB_MIN_BACKOFF_INTERVAL, DEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\r\n                int copyBlobMaxBackoff = sessionConfiguration.getInt(KEY_COPYBLOB_MAX_BACKOFF_INTERVAL, DEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\r\n                int copyBlobDeltaBackoff = sessionConfiguration.getInt(KEY_COPYBLOB_BACKOFF_INTERVAL, DEFAULT_COPYBLOB_BACKOFF_INTERVAL);\r\n                int copyBlobMaxRetries = sessionConfiguration.getInt(KEY_COPYBLOB_MAX_IO_RETRIES, DEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\r\n                BlobRequestOptions options = new BlobRequestOptions();\r\n                options.setRetryPolicyFactory(new RetryExponentialRetry(copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, copyBlobMaxRetries));\r\n                dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext(), overwriteDestination);\r\n            } else {\r\n                throw se;\r\n            }\r\n        }\r\n        waitForCopyToComplete(dstBlob, getInstrumentedContext());\r\n        safeDelete(srcBlob, lease);\r\n    } catch (StorageException e) {\r\n        if (e.getHttpStatusCode() == HttpURLConnection.HTTP_UNAVAILABLE) {\r\n            LOG.warn(\"Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob\");\r\n            InputStream ipStream = null;\r\n            OutputStream opStream = null;\r\n            try {\r\n                if (srcBlob.getProperties().getBlobType() == BlobType.PAGE_BLOB) {\r\n                    ipStream = openInputStream(srcBlob, Optional.empty());\r\n                    opStream = openOutputStream(dstBlob);\r\n                    byte[] buffer = new byte[PageBlobFormatHelpers.PAGE_SIZE];\r\n                    int len;\r\n                    while ((len = ipStream.read(buffer)) != -1) {\r\n                        opStream.write(buffer, 0, len);\r\n                    }\r\n                    opStream.flush();\r\n                    opStream.close();\r\n                    ipStream.close();\r\n                } else {\r\n                    throw new AzureException(e);\r\n                }\r\n                safeDelete(srcBlob, lease);\r\n            } catch (StorageException se) {\r\n                LOG.warn(\"Rename: CopyBlob: StorageException: Failed\");\r\n                throw new AzureException(se);\r\n            } finally {\r\n                IOUtils.closeStream(ipStream);\r\n                IOUtils.closeStream(opStream);\r\n            }\r\n        } else {\r\n            throw new AzureException(e);\r\n        }\r\n    } catch (URISyntaxException e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "waitForCopyToComplete",
  "errType" : [ "StorageException", "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitForCopyToComplete(CloudBlobWrapper blob, OperationContext opContext)\n{\r\n    boolean copyInProgress = true;\r\n    while (copyInProgress) {\r\n        try {\r\n            blob.downloadAttributes(opContext);\r\n        } catch (StorageException se) {\r\n        }\r\n        copyInProgress = (blob.getCopyState() != null && blob.getCopyState().getStatus() == CopyStatus.PENDING);\r\n        if (copyInProgress) {\r\n            try {\r\n                Thread.sleep(1000);\r\n            } catch (InterruptedException ie) {\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "explicitFileExists",
  "errType" : [ "StorageException", "URISyntaxException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean explicitFileExists(String key) throws AzureException\n{\r\n    CloudBlobWrapper blob;\r\n    try {\r\n        blob = getBlobReference(key);\r\n        if (null != blob && blob.exists(getInstrumentedContext())) {\r\n            return true;\r\n        }\r\n        return false;\r\n    } catch (StorageException e) {\r\n        throw new AzureException(e);\r\n    } catch (URISyntaxException e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "changePermissionStatus",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void changePermissionStatus(String key, PermissionStatus newPermission) throws AzureException\n{\r\n    try {\r\n        checkContainer(ContainerAccessType.ReadThenWrite);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        blob.downloadAttributes(getInstrumentedContext());\r\n        storePermissionStatus(blob, newPermission);\r\n        blob.uploadMetadata(getInstrumentedContext());\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "purge",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void purge(String prefix) throws IOException\n{\r\n    try {\r\n        if (null == storageInteractionLayer) {\r\n            final String errMsg = String.format(\"Storage session expected for URI '%s' but does not exist.\", sessionUri);\r\n            throw new AssertionError(errMsg);\r\n        }\r\n        if (checkContainer(ContainerAccessType.ReadThenWrite) == ContainerState.DoesntExist) {\r\n            return;\r\n        }\r\n        Iterable<ListBlobItem> objects = listRootBlobs(prefix, false, false);\r\n        for (ListBlobItem blobItem : objects) {\r\n            ((CloudBlob) blobItem).delete(DeleteSnapshotsOption.NONE, null, null, getInstrumentedContext());\r\n        }\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "acquireLease",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SelfRenewingLease acquireLease(String key) throws AzureException\n{\r\n    LOG.debug(\"acquiring lease on {}\", key);\r\n    try {\r\n        checkContainer(ContainerAccessType.ReadThenWrite);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        return blob.acquireLease();\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateFolderLastModifiedTime",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updateFolderLastModifiedTime(String key, Date lastModified, SelfRenewingLease folderLease) throws AzureException\n{\r\n    try {\r\n        checkContainer(ContainerAccessType.ReadThenWrite);\r\n        CloudBlobWrapper blob = getBlobReference(key);\r\n        blob.uploadProperties(getInstrumentedContext(), folderLease);\r\n    } catch (Exception e) {\r\n        throw new AzureException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "updateFolderLastModifiedTime",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateFolderLastModifiedTime(String key, SelfRenewingLease folderLease) throws AzureException\n{\r\n    final Calendar lastModifiedCalendar = Calendar.getInstance(Utility.LOCALE_US);\r\n    lastModifiedCalendar.setTimeZone(Utility.UTC_ZONE);\r\n    Date lastModified = lastModifiedCalendar.getTime();\r\n    updateFolderLastModifiedTime(key, lastModified, folderLease);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "dump",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void dump() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    if (bandwidthGaugeUpdater != null) {\r\n        bandwidthGaugeUpdater.close();\r\n        bandwidthGaugeUpdater = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "finalize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finalize() throws Throwable\n{\r\n    LOG.debug(\"finalize() called\");\r\n    close();\r\n    super.finalize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "retrieveAppendStream",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DataOutputStream retrieveAppendStream(String key, int bufferSize) throws IOException\n{\r\n    try {\r\n        if (isPageBlobKey(key)) {\r\n            throw new UnsupportedOperationException(\"Append not supported for Page Blobs\");\r\n        }\r\n        CloudBlobWrapper blob = this.container.getBlockBlobReference(key);\r\n        OutputStream outputStream;\r\n        BlockBlobAppendStream blockBlobOutputStream = new BlockBlobAppendStream((CloudBlockBlobWrapper) blob, key, bufferSize, isBlockBlobWithCompactionKey(key), getInstrumentedContext());\r\n        outputStream = blockBlobOutputStream;\r\n        DataOutputStream dataOutStream = new SyncableDataOutputStream(outputStream);\r\n        return dataOutStream;\r\n    } catch (Exception ex) {\r\n        throw new AzureException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isSecureScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSecureScheme()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return FileSystemUriSchemes.ABFS_SECURE_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getExponentialRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExponentialRetryPolicy getExponentialRetryPolicy()\n{\r\n    return exponentialRetryPolicy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsPerfTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfTracker getAbfsPerfTracker()\n{\r\n    return abfsPerfTracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsCounters getAbfsCounters()\n{\r\n    return abfsCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPageBlobDataSize",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "long getPageBlobDataSize(CloudPageBlobWrapper blob, OperationContext opContext) throws IOException, StorageException\n{\r\n    ArrayList<PageRange> pageRanges = blob.downloadPageRanges(new BlobRequestOptions(), opContext);\r\n    if (pageRanges.size() == 0) {\r\n        return 0;\r\n    }\r\n    if (pageRanges.get(0).getStartOffset() != 0) {\r\n        throw badStartRangeException(blob, pageRanges.get(0));\r\n    }\r\n    long totalRawBlobSize = pageRanges.get(0).getEndOffset() + 1;\r\n    long lastPageStart = totalRawBlobSize - PAGE_SIZE;\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream(PageBlobFormatHelpers.PAGE_SIZE);\r\n    blob.downloadRange(lastPageStart, PAGE_SIZE, baos, new BlobRequestOptions(), opContext);\r\n    byte[] lastPage = baos.toByteArray();\r\n    short lastPageSize = getPageSize(blob, lastPage, 0);\r\n    long totalNumberOfPages = totalRawBlobSize / PAGE_SIZE;\r\n    return (totalNumberOfPages - 1) * PAGE_DATA_SIZE + lastPageSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "available",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int available() throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(\"Stream closed\");\r\n    }\r\n    if (pageBlobSize == -1) {\r\n        try {\r\n            pageBlobSize = getPageBlobDataSize(blob, opContext);\r\n        } catch (StorageException e) {\r\n            throw new IOException(\"Unable to get page blob size.\", e);\r\n        }\r\n    }\r\n    final long remaining = pageBlobSize - filePosition;\r\n    return remaining <= Integer.MAX_VALUE ? (int) remaining : Integer.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    closed = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "dataAvailableInBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean dataAvailableInBuffer()\n{\r\n    return currentBuffer != null && currentBufferOffset < currentBufferLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "ensureDataInBuffer",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean ensureDataInBuffer() throws IOException\n{\r\n    if (dataAvailableInBuffer()) {\r\n        return true;\r\n    }\r\n    currentBuffer = null;\r\n    currentBufferOffset = 0;\r\n    currentBufferLength = 0;\r\n    if (numberOfPagesRemaining == 0) {\r\n        return false;\r\n    }\r\n    final long pagesToRead = Math.min(MAX_PAGES_PER_DOWNLOAD, numberOfPagesRemaining);\r\n    final int bufferSize = (int) (pagesToRead * PAGE_SIZE);\r\n    try {\r\n        ByteArrayOutputStream baos = new ByteArrayOutputStream(bufferSize);\r\n        blob.downloadRange(currentOffsetInBlob, bufferSize, baos, withMD5Checking(), opContext);\r\n        validateDataIntegrity(baos.toByteArray());\r\n    } catch (StorageException e) {\r\n        throw new IOException(e);\r\n    }\r\n    numberOfPagesRemaining -= pagesToRead;\r\n    currentOffsetInBlob += bufferSize;\r\n    return true;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "validateDataIntegrity",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateDataIntegrity(byte[] buffer) throws IOException\n{\r\n    if (buffer.length % PAGE_SIZE != 0) {\r\n        throw new AssertionError(\"Unexpected buffer size: \" + buffer.length);\r\n    }\r\n    int bufferLength = 0;\r\n    int numberOfPages = buffer.length / PAGE_SIZE;\r\n    long totalPagesAfterCurrent = numberOfPagesRemaining;\r\n    for (int page = 0; page < numberOfPages; page++) {\r\n        totalPagesAfterCurrent--;\r\n        short currentPageSize = getPageSize(blob, buffer, page * PAGE_SIZE);\r\n        if (currentPageSize < PAGE_DATA_SIZE && totalPagesAfterCurrent > 0) {\r\n            throw fileCorruptException(blob, String.format(\"Page with partial data found in the middle (%d pages from the\" + \" end) that only has %d bytes of data.\", totalPagesAfterCurrent, currentPageSize));\r\n        }\r\n        bufferLength += currentPageSize + PAGE_HEADER_SIZE;\r\n    }\r\n    currentBufferOffset = PAGE_HEADER_SIZE;\r\n    currentBufferLength = bufferLength;\r\n    currentBuffer = buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPageSize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "short getPageSize(CloudPageBlobWrapper blob, byte[] data, int offset) throws IOException\n{\r\n    short pageSize = toShort(data[offset], data[offset + 1]);\r\n    if (pageSize < 0 || pageSize > PAGE_DATA_SIZE) {\r\n        throw fileCorruptException(blob, String.format(\"Unexpected page size in the header: %d.\", pageSize));\r\n    }\r\n    return pageSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int read(byte[] outputBuffer, int offset, int len) throws IOException\n{\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    int numberOfBytesRead = 0;\r\n    while (len > 0) {\r\n        if (!ensureDataInBuffer()) {\r\n            break;\r\n        }\r\n        int bytesRemainingInCurrentPage = getBytesRemainingInCurrentPage();\r\n        int numBytesToRead = Math.min(len, bytesRemainingInCurrentPage);\r\n        System.arraycopy(currentBuffer, currentBufferOffset, outputBuffer, offset, numBytesToRead);\r\n        numberOfBytesRead += numBytesToRead;\r\n        offset += numBytesToRead;\r\n        len -= numBytesToRead;\r\n        if (numBytesToRead == bytesRemainingInCurrentPage) {\r\n            advancePagesInBuffer(1);\r\n        } else {\r\n            currentBufferOffset += numBytesToRead;\r\n        }\r\n    }\r\n    if (numberOfBytesRead == 0) {\r\n        return -1;\r\n    }\r\n    filePosition += numberOfBytesRead;\r\n    return numberOfBytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    byte[] oneByte = new byte[1];\r\n    int result = read(oneByte);\r\n    if (result < 0) {\r\n        return result;\r\n    }\r\n    return oneByte[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    long skipped = skipImpl(n);\r\n    filePosition += skipped;\r\n    return skipped;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "skipImpl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long skipImpl(long n) throws IOException\n{\r\n    if (n == 0) {\r\n        return 0;\r\n    }\r\n    long skippedWithinBuffer = skipWithinBuffer(n);\r\n    if (skippedWithinBuffer > n) {\r\n        throw new AssertionError(String.format(\"Bug in skipWithinBuffer: it skipped over %d bytes when asked to \" + \"skip %d bytes.\", skippedWithinBuffer, n));\r\n    }\r\n    n -= skippedWithinBuffer;\r\n    long skipped = skippedWithinBuffer;\r\n    if (n == 0) {\r\n        return skipped;\r\n    }\r\n    if (numberOfPagesRemaining == 0) {\r\n        throw new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF);\r\n    } else if (numberOfPagesRemaining > 1) {\r\n        long pagesToSkipOver = Math.min(n / PAGE_DATA_SIZE, numberOfPagesRemaining - 1);\r\n        numberOfPagesRemaining -= pagesToSkipOver;\r\n        currentOffsetInBlob += pagesToSkipOver * PAGE_SIZE;\r\n        skipped += pagesToSkipOver * PAGE_DATA_SIZE;\r\n        n -= pagesToSkipOver * PAGE_DATA_SIZE;\r\n    }\r\n    if (n == 0) {\r\n        return skipped;\r\n    }\r\n    if (!ensureDataInBuffer()) {\r\n        return skipped;\r\n    }\r\n    return skipped + skipWithinBuffer(n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "skipWithinBuffer",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "long skipWithinBuffer(long n) throws IOException\n{\r\n    if (!dataAvailableInBuffer()) {\r\n        return 0;\r\n    }\r\n    long skipped = 0;\r\n    skipped = skipWithinCurrentPage(n);\r\n    if (skipped > n) {\r\n        throw new AssertionError(String.format(\"Bug in skipWithinCurrentPage: it skipped over %d bytes when asked\" + \" to skip %d bytes.\", skipped, n));\r\n    }\r\n    n -= skipped;\r\n    if (n == 0 || !dataAvailableInBuffer()) {\r\n        return skipped;\r\n    }\r\n    int currentPageIndex = currentBufferOffset / PAGE_SIZE;\r\n    int numberOfPagesInBuffer = currentBuffer.length / PAGE_SIZE;\r\n    int wholePagesRemaining = numberOfPagesInBuffer - currentPageIndex - 1;\r\n    if (n < (PAGE_DATA_SIZE * wholePagesRemaining)) {\r\n        advancePagesInBuffer((int) (n / PAGE_DATA_SIZE));\r\n        currentBufferOffset += n % PAGE_DATA_SIZE;\r\n        return n + skipped;\r\n    }\r\n    advancePagesInBuffer(wholePagesRemaining);\r\n    skipped += wholePagesRemaining * PAGE_DATA_SIZE;\r\n    n -= wholePagesRemaining * PAGE_DATA_SIZE;\r\n    return skipWithinCurrentPage(n) + skipped;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "skipWithinCurrentPage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long skipWithinCurrentPage(long n) throws IOException\n{\r\n    int remainingBytesInCurrentPage = getBytesRemainingInCurrentPage();\r\n    if (n <= remainingBytesInCurrentPage) {\r\n        currentBufferOffset += n;\r\n        return n;\r\n    } else {\r\n        advancePagesInBuffer(1);\r\n        return remainingBytesInCurrentPage;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBytesRemainingInCurrentPage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getBytesRemainingInCurrentPage() throws IOException\n{\r\n    if (!dataAvailableInBuffer()) {\r\n        return 0;\r\n    }\r\n    int currentDataOffsetInPage = (currentBufferOffset % PAGE_SIZE) - PAGE_HEADER_SIZE;\r\n    int pageBoundary = getCurrentPageStartInBuffer();\r\n    short sizeOfCurrentPage = getPageSize(blob, currentBuffer, pageBoundary);\r\n    return sizeOfCurrentPage - currentDataOffsetInPage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "badStartRangeException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException badStartRangeException(CloudPageBlobWrapper blob, PageRange startRange)\n{\r\n    return fileCorruptException(blob, String.format(\"Page blobs for ASV should always use a page range starting at byte 0. \" + \"This starts at byte %d.\", startRange.getStartOffset()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "advancePagesInBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void advancePagesInBuffer(int numberOfPages)\n{\r\n    currentBufferOffset = getCurrentPageStartInBuffer() + (numberOfPages * PAGE_SIZE) + PAGE_HEADER_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCurrentPageStartInBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCurrentPageStartInBuffer()\n{\r\n    return PAGE_SIZE * (currentBufferOffset / PAGE_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "fileCorruptException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException fileCorruptException(CloudPageBlobWrapper blob, String reason)\n{\r\n    return new IOException(String.format(\"The page blob: '%s' is corrupt or has an unexpected format: %s.\", blob.getUri(), reason));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "addQuery",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addQuery(final String name, final String value)\n{\r\n    if (value != null && !value.isEmpty()) {\r\n        this.parameters.put(name, value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setSASToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSASToken(final String sasToken)\n{\r\n    this.sasToken = sasToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "toString",
  "errType" : [ "AzureBlobFileSystemException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    boolean first = true;\r\n    for (Map.Entry<String, String> entry : parameters.entrySet()) {\r\n        if (first) {\r\n            sb.append(AbfsHttpConstants.QUESTION_MARK);\r\n            first = false;\r\n        } else {\r\n            sb.append(AbfsHttpConstants.AND_MARK);\r\n        }\r\n        try {\r\n            sb.append(entry.getKey()).append(AbfsHttpConstants.EQUAL).append(AbfsClient.urlEncode(entry.getValue()));\r\n        } catch (AzureBlobFileSystemException ex) {\r\n            throw new IllegalArgumentException(\"Query string param is not encode-able: \" + entry.getKey() + \"=\" + entry.getValue());\r\n        }\r\n    }\r\n    if (sasToken != null) {\r\n        if (first) {\r\n            sb.append(AbfsHttpConstants.QUESTION_MARK);\r\n        } else {\r\n            sb.append(AbfsHttpConstants.AND_MARK);\r\n        }\r\n        sb.append(sasToken);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T validate(final String configValue) throws InvalidConfigurationValueException\n{\r\n    if (configValue == null) {\r\n        if (this.throwIfInvalid) {\r\n            throw new InvalidConfigurationValueException(this.configKey);\r\n        }\r\n        return this.defaultVal;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "getDefaultVal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getDefaultVal()\n{\r\n    return this.defaultVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "getConfigKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getConfigKey()\n{\r\n    return this.configKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\diagnostics",
  "methodName" : "getThrowIfInvalid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getThrowIfInvalid()\n{\r\n    return this.throwIfInvalid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getPath()\n{\r\n    Path p = super.getPath();\r\n    if (p == null) {\r\n        p = NativeAzureFileSystem.keyToPath(key);\r\n    }\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getBlobMaterialization",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BlobMaterialization getBlobMaterialization()\n{\r\n    return blobMaterialization;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "removeKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeKey()\n{\r\n    key = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void initialize(Configuration conf) throws IOException\n{\r\n    LOG.debug(\"Initializing RemoteSASKeyGeneratorImpl instance\");\r\n    this.retryPolicy = RetryUtils.getMultipleLinearRandomRetry(conf, SAS_KEY_GENERATOR_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY, true, SAS_KEY_GENERATOR_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY, SAS_KEY_GENERATOR_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT);\r\n    this.isKerberosSupportEnabled = conf.getBoolean(Constants.AZURE_KERBEROS_SUPPORT_PROPERTY_NAME, false);\r\n    this.isSpnegoTokenCacheEnabled = conf.getBoolean(Constants.AZURE_ENABLE_SPNEGO_TOKEN_CACHE, true);\r\n    this.commaSeparatedUrls = conf.getTrimmedStrings(KEY_CRED_SERVICE_URLS);\r\n    if (this.commaSeparatedUrls == null || this.commaSeparatedUrls.length <= 0) {\r\n        throw new IOException(KEY_CRED_SERVICE_URLS + \" config not set\" + \" in configuration.\");\r\n    }\r\n    if (isKerberosSupportEnabled && UserGroupInformation.isSecurityEnabled()) {\r\n        this.remoteCallHelper = new SecureWasbRemoteCallHelper(retryPolicy, false, isSpnegoTokenCacheEnabled);\r\n    } else {\r\n        this.remoteCallHelper = new WasbRemoteCallHelper(retryPolicy);\r\n    }\r\n    long sasKeyExpiryPeriodInMinutes = getSasKeyExpiryPeriod() * HOURS_IN_DAY * MINUTES_IN_HOUR;\r\n    long cacheEntryDurationInMinutes = conf.getTimeDuration(SASKEY_CACHEENTRY_EXPIRY_PERIOD, sasKeyExpiryPeriodInMinutes, TimeUnit.MINUTES);\r\n    cacheEntryDurationInMinutes = (cacheEntryDurationInMinutes > (sasKeyExpiryPeriodInMinutes - 5)) ? (sasKeyExpiryPeriodInMinutes - 5) : cacheEntryDurationInMinutes;\r\n    this.cache = new CachingAuthorizer<>(cacheEntryDurationInMinutes, \"SASKEY\");\r\n    this.cache.init(conf);\r\n    LOG.debug(\"Initialization of RemoteSASKeyGenerator instance successful\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerSASUri",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "URI getContainerSASUri(String storageAccount, String container) throws SASKeyGenerationException\n{\r\n    RemoteSASKeyGenerationResponse sasKeyResponse = null;\r\n    try {\r\n        CachedSASKeyEntry cacheKey = new CachedSASKeyEntry(storageAccount, container, \"/\");\r\n        URI cacheResult = cache.get(cacheKey);\r\n        if (cacheResult != null) {\r\n            return cacheResult;\r\n        }\r\n        LOG.debug(\"Generating Container SAS Key: Storage Account {}, Container {}\", storageAccount, container);\r\n        URIBuilder uriBuilder = new URIBuilder();\r\n        uriBuilder.setPath(\"/\" + CONTAINER_SAS_OP);\r\n        uriBuilder.addParameter(STORAGE_ACCOUNT_QUERY_PARAM_NAME, storageAccount);\r\n        uriBuilder.addParameter(CONTAINER_QUERY_PARAM_NAME, container);\r\n        uriBuilder.addParameter(SAS_EXPIRY_QUERY_PARAM_NAME, \"\" + getSasKeyExpiryPeriod());\r\n        sasKeyResponse = makeRemoteRequest(commaSeparatedUrls, uriBuilder.getPath(), uriBuilder.getQueryParams());\r\n        if (sasKeyResponse.getResponseCode() == REMOTE_CALL_SUCCESS_CODE) {\r\n            URI sasKey = new URI(sasKeyResponse.getSasKey());\r\n            cache.put(cacheKey, sasKey);\r\n            return sasKey;\r\n        } else {\r\n            throw new SASKeyGenerationException(\"Remote Service encountered error in SAS Key generation : \" + sasKeyResponse.getResponseMessage());\r\n        }\r\n    } catch (URISyntaxException uriSyntaxEx) {\r\n        throw new SASKeyGenerationException(\"Encountered URISyntaxException\" + \" while building the HttpGetRequest to remote service for \", uriSyntaxEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getRelativeBlobSASUri",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "URI getRelativeBlobSASUri(String storageAccount, String container, String relativePath) throws SASKeyGenerationException\n{\r\n    try {\r\n        CachedSASKeyEntry cacheKey = new CachedSASKeyEntry(storageAccount, container, relativePath);\r\n        URI cacheResult = cache.get(cacheKey);\r\n        if (cacheResult != null) {\r\n            return cacheResult;\r\n        }\r\n        LOG.debug(\"Generating RelativePath SAS Key for relativePath {} inside Container {} inside Storage Account {}\", relativePath, container, storageAccount);\r\n        URIBuilder uriBuilder = new URIBuilder();\r\n        uriBuilder.setPath(\"/\" + BLOB_SAS_OP);\r\n        uriBuilder.addParameter(STORAGE_ACCOUNT_QUERY_PARAM_NAME, storageAccount);\r\n        uriBuilder.addParameter(CONTAINER_QUERY_PARAM_NAME, container);\r\n        uriBuilder.addParameter(RELATIVE_PATH_QUERY_PARAM_NAME, relativePath);\r\n        uriBuilder.addParameter(SAS_EXPIRY_QUERY_PARAM_NAME, \"\" + getSasKeyExpiryPeriod());\r\n        RemoteSASKeyGenerationResponse sasKeyResponse = makeRemoteRequest(commaSeparatedUrls, uriBuilder.getPath(), uriBuilder.getQueryParams());\r\n        if (sasKeyResponse.getResponseCode() == REMOTE_CALL_SUCCESS_CODE) {\r\n            URI sasKey = new URI(sasKeyResponse.getSasKey());\r\n            cache.put(cacheKey, sasKey);\r\n            return sasKey;\r\n        } else {\r\n            throw new SASKeyGenerationException(\"Remote Service encountered error in SAS Key generation : \" + sasKeyResponse.getResponseMessage());\r\n        }\r\n    } catch (URISyntaxException uriSyntaxEx) {\r\n        throw new SASKeyGenerationException(\"Encountered URISyntaxException\" + \" while building the HttpGetRequest to \" + \" remote service\", uriSyntaxEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "makeRemoteRequest",
  "errType" : [ "WasbRemoteCallException", "JsonParseException", "JsonMappingException", "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteSASKeyGenerationResponse makeRemoteRequest(String[] urls, String path, List<NameValuePair> queryParams) throws SASKeyGenerationException\n{\r\n    try {\r\n        String responseBody = remoteCallHelper.makeRemoteRequest(urls, path, queryParams, HttpGet.METHOD_NAME);\r\n        return RESPONSE_READER.readValue(responseBody);\r\n    } catch (WasbRemoteCallException remoteCallEx) {\r\n        throw new SASKeyGenerationException(\"Encountered RemoteCallException\" + \" while retrieving SAS key from remote service\", remoteCallEx);\r\n    } catch (JsonParseException jsonParserEx) {\r\n        throw new SASKeyGenerationException(\"Encountered JsonParseException \" + \"while parsing the response from remote\" + \" service into RemoteSASKeyGenerationResponse object\", jsonParserEx);\r\n    } catch (JsonMappingException jsonMappingEx) {\r\n        throw new SASKeyGenerationException(\"Encountered JsonMappingException\" + \" while mapping the response from remote service into \" + \"RemoteSASKeyGenerationResponse object\", jsonMappingEx);\r\n    } catch (IOException ioEx) {\r\n        throw new SASKeyGenerationException(\"Encountered IOException while \" + \"accessing remote service to retrieve SAS Key\", ioEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getResponseCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getResponseCode()\n{\r\n    return responseCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setResponseCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResponseCode(int responseCode)\n{\r\n    this.responseCode = responseCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getResponseMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getResponseMessage()\n{\r\n    return responseMessage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setResponseMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResponseMessage(String responseMessage)\n{\r\n    this.responseMessage = responseMessage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getSasKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSasKey()\n{\r\n    return sasKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "setSasKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSasKey(String sasKey)\n{\r\n    this.sasKey = sasKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "getToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AuthenticatedURL.Token getToken()\n{\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "getExpiryTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpiryTime()\n{\r\n    return expiryTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "isTokenValid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isTokenValid()\n{\r\n    return (expiryTime >= System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getContainerSASUri",
  "errType" : [ "StorageException", "URISyntaxException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "URI getContainerSASUri(String accountName, String container) throws SASKeyGenerationException\n{\r\n    LOG.debug(\"Retrieving Container SAS URI For {}@{}\", container, accountName);\r\n    try {\r\n        CachedSASKeyEntry cacheKey = new CachedSASKeyEntry(accountName, container, \"/\");\r\n        URI cacheResult = cache.get(cacheKey);\r\n        if (cacheResult != null) {\r\n            return cacheResult;\r\n        }\r\n        CloudStorageAccount account = getSASKeyBasedStorageAccountInstance(accountName);\r\n        CloudBlobClient client = account.createCloudBlobClient();\r\n        URI sasKey = client.getCredentials().transformUri(client.getContainerReference(container).getUri());\r\n        cache.put(cacheKey, sasKey);\r\n        return sasKey;\r\n    } catch (StorageException stoEx) {\r\n        throw new SASKeyGenerationException(\"Encountered StorageException while\" + \" generating SAS Key for container \" + container + \" inside \" + \"storage account \" + accountName, stoEx);\r\n    } catch (URISyntaxException uriSyntaxEx) {\r\n        throw new SASKeyGenerationException(\"Encountered URISyntaxException while\" + \" generating SAS Key for container \" + container + \" inside storage\" + \" account \" + accountName, uriSyntaxEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getSASKeyBasedStorageAccountInstance",
  "errType" : [ "KeyProviderException", "InvalidKeyException", "StorageException", "URISyntaxException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "CloudStorageAccount getSASKeyBasedStorageAccountInstance(String accountName) throws SASKeyGenerationException\n{\r\n    LOG.debug(\"Creating SAS key from account instance {}\", accountName);\r\n    try {\r\n        String accountNameWithoutDomain = getAccountNameWithoutDomain(accountName);\r\n        CloudStorageAccount account = getStorageAccountInstance(accountNameWithoutDomain, AzureNativeFileSystemStore.getAccountKeyFromConfiguration(accountName, getConf()));\r\n        return new CloudStorageAccount(new StorageCredentialsSharedAccessSignature(account.generateSharedAccessSignature(getDefaultAccountAccessPolicy())), false, account.getEndpointSuffix(), accountNameWithoutDomain);\r\n    } catch (KeyProviderException keyProviderEx) {\r\n        throw new SASKeyGenerationException(\"Encountered KeyProviderException\" + \" while retrieving Storage key from configuration for account \" + accountName, keyProviderEx);\r\n    } catch (InvalidKeyException invalidKeyEx) {\r\n        throw new SASKeyGenerationException(\"Encoutered InvalidKeyException \" + \"while generating Account level SAS key for account\" + accountName, invalidKeyEx);\r\n    } catch (StorageException storeEx) {\r\n        throw new SASKeyGenerationException(\"Encoutered StorageException while \" + \"generating Account level SAS key for account\" + accountName, storeEx);\r\n    } catch (URISyntaxException uriSyntaxEx) {\r\n        throw new SASKeyGenerationException(\"Encountered URISyntaxException for\" + \" account \" + accountName, uriSyntaxEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getRelativeBlobSASUri",
  "errType" : [ "URISyntaxException", "StorageException", "URISyntaxException", "StorageException", "StorageException", "URISyntaxException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "URI getRelativeBlobSASUri(String accountName, String container, String relativePath) throws SASKeyGenerationException\n{\r\n    CloudBlobContainer sc = null;\r\n    CloudBlobClient client = null;\r\n    CachedSASKeyEntry cacheKey = null;\r\n    try {\r\n        cacheKey = new CachedSASKeyEntry(accountName, container, relativePath);\r\n        URI cacheResult = cache.get(cacheKey);\r\n        if (cacheResult != null) {\r\n            return cacheResult;\r\n        }\r\n        CloudStorageAccount account = getSASKeyBasedStorageAccountInstance(accountName);\r\n        client = account.createCloudBlobClient();\r\n        sc = client.getContainerReference(container);\r\n    } catch (URISyntaxException uriSyntaxEx) {\r\n        throw new SASKeyGenerationException(\"Encountered URISyntaxException \" + \"while getting container references for container \" + container + \" inside storage account : \" + accountName, uriSyntaxEx);\r\n    } catch (StorageException stoEx) {\r\n        throw new SASKeyGenerationException(\"Encountered StorageException while \" + \"getting  container references for container \" + container + \" inside storage account : \" + accountName, stoEx);\r\n    }\r\n    CloudBlockBlob blob = null;\r\n    try {\r\n        blob = sc.getBlockBlobReference(relativePath);\r\n    } catch (URISyntaxException uriSyntaxEx) {\r\n        throw new SASKeyGenerationException(\"Encountered URISyntaxException while \" + \"getting Block Blob references for container \" + container + \" inside storage account : \" + accountName, uriSyntaxEx);\r\n    } catch (StorageException stoEx) {\r\n        throw new SASKeyGenerationException(\"Encountered StorageException while \" + \"getting Block Blob references for container \" + container + \" inside storage account : \" + accountName, stoEx);\r\n    }\r\n    try {\r\n        URI sasKey = client.getCredentials().transformUri(blob.getUri());\r\n        cache.put(cacheKey, sasKey);\r\n        return sasKey;\r\n    } catch (StorageException stoEx) {\r\n        throw new SASKeyGenerationException(\"Encountered StorageException while \" + \"generating SAS key for Blob: \" + relativePath + \" inside \" + \"container : \" + container + \" in Storage Account : \" + accountName, stoEx);\r\n    } catch (URISyntaxException uriSyntaxEx) {\r\n        throw new SASKeyGenerationException(\"Encountered URISyntaxException \" + \"while generating SAS key for Blob: \" + relativePath + \" inside \" + \"container: \" + container + \" in Storage Account : \" + accountName, uriSyntaxEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getStorageAccountInstance",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "CloudStorageAccount getStorageAccountInstance(String accountName, String accountKey) throws SASKeyGenerationException\n{\r\n    if (!storageAccountMap.containsKey(accountName)) {\r\n        if (accountKey == null || accountKey.isEmpty()) {\r\n            throw new SASKeyGenerationException(\"No key for Storage account \" + accountName);\r\n        }\r\n        CloudStorageAccount account = null;\r\n        try {\r\n            account = new CloudStorageAccount(new StorageCredentialsAccountAndKey(accountName, accountKey));\r\n        } catch (URISyntaxException uriSyntaxEx) {\r\n            throw new SASKeyGenerationException(\"Encountered URISyntaxException \" + \"for account \" + accountName, uriSyntaxEx);\r\n        }\r\n        storageAccountMap.put(accountName, account);\r\n    }\r\n    return storageAccountMap.get(accountName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getAccountNameWithoutDomain",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAccountNameWithoutDomain(String fullAccountName)\n{\r\n    StringTokenizer tokenizer = new StringTokenizer(fullAccountName, \".\");\r\n    return tokenizer.nextToken();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getDefaultAccountAccessPolicy",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SharedAccessAccountPolicy getDefaultAccountAccessPolicy()\n{\r\n    SharedAccessAccountPolicy ap = new SharedAccessAccountPolicy();\r\n    Calendar cal = new GregorianCalendar(TimeZone.getTimeZone(\"UTC\"));\r\n    cal.setTime(new Date());\r\n    cal.add(Calendar.HOUR, (int) getSasKeyExpiryPeriod() * HOURS_IN_DAY);\r\n    ap.setSharedAccessExpiryTime(cal.getTime());\r\n    ap.setPermissions(getDefaultAccoutSASKeyPermissions());\r\n    ap.setResourceTypes(EnumSet.of(SharedAccessAccountResourceType.CONTAINER, SharedAccessAccountResourceType.OBJECT));\r\n    ap.setServices(EnumSet.of(SharedAccessAccountService.BLOB));\r\n    return ap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getDefaultAccoutSASKeyPermissions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnumSet<SharedAccessAccountPermissions> getDefaultAccoutSASKeyPermissions()\n{\r\n    return EnumSet.of(SharedAccessAccountPermissions.ADD, SharedAccessAccountPermissions.CREATE, SharedAccessAccountPermissions.DELETE, SharedAccessAccountPermissions.LIST, SharedAccessAccountPermissions.READ, SharedAccessAccountPermissions.UPDATE, SharedAccessAccountPermissions.WRITE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getAccessToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAccessToken()\n{\r\n    return this.accessToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "setAccessToken",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAccessToken(String accessToken)\n{\r\n    this.accessToken = accessToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getExpiry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Date getExpiry()\n{\r\n    return new Date(this.expiry.getTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "setExpiry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setExpiry(Date expiry)\n{\r\n    this.expiry = new Date(expiry.getTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "free",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void free() throws StorageException\n{\r\n    AccessCondition accessCondition = AccessCondition.generateEmptyCondition();\r\n    accessCondition.setLeaseID(leaseID);\r\n    try {\r\n        blobWrapper.getBlob().releaseLease(accessCondition);\r\n    } catch (StorageException e) {\r\n        if (\"BlobNotFound\".equals(e.getErrorCode())) {\r\n        } else {\r\n            LOG.warn(\"Unanticipated exception when trying to free lease \" + leaseID + \" on \" + blobWrapper.getStorageUri());\r\n            throw (e);\r\n        }\r\n    } finally {\r\n        leaseFreed = true;\r\n        LOG.debug(\"Freed lease \" + leaseID + \" on \" + blobWrapper.getUri() + \" managed by thread \" + renewer.getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "isFreed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFreed()\n{\r\n    return leaseFreed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getLeaseID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLeaseID()\n{\r\n    return leaseID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getCloudBlob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CloudBlob getCloudBlob()\n{\r\n    return blobWrapper.getBlob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void bind(final URI fsURI, final Configuration conf) throws IOException\n{\r\n    Preconditions.checkNotNull(fsURI, \"Np Filesystem URI\");\r\n    ExtensionHelper.bind(tokenManager, fsURI, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getCanonicalServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCanonicalServiceName()\n{\r\n    return ExtensionHelper.getCanonicalServiceName(tokenManager, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    if (tokenManager instanceof Closeable) {\r\n        IOUtils.cleanupWithLogger(LOG, (Closeable) tokenManager);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(String renewer) throws IOException\n{\r\n    LOG.debug(\"Requesting Delegation token for {}\", renewer);\r\n    Token<DelegationTokenIdentifier> token = tokenManager.getDelegationToken(renewer);\r\n    if (token.getKind() == null) {\r\n        token.setKind(AbfsDelegationTokenIdentifier.TOKEN_KIND);\r\n    }\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(Token<?> token) throws IOException\n{\r\n    return tokenManager.renewDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(Token<?> token) throws IOException\n{\r\n    tokenManager.cancelDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getTokenManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CustomDelegationTokenManager getTokenManager()\n{\r\n    return tokenManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AbfsDelegationTokenManager{\");\r\n    sb.append(\"tokenManager=\").append(tokenManager);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "checkState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkState() throws IOException\n{\r\n    if (closed) {\r\n        throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "resetStreamBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetStreamBuffer()\n{\r\n    streamBufferPosition = 0;\r\n    streamBufferLength = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    checkState();\r\n    return (streamBuffer != null) ? streamPosition - streamBufferLength + streamBufferPosition : streamPosition;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void seek(long pos) throws IOException\n{\r\n    checkState();\r\n    if (pos < 0) {\r\n        throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK + \" \" + pos);\r\n    }\r\n    if (pos > streamLength) {\r\n        throw new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF + \" \" + pos);\r\n    }\r\n    long offset = pos - getPos();\r\n    if (offset == 0) {\r\n        return;\r\n    }\r\n    if (offset > 0) {\r\n        if (skip(offset) != offset) {\r\n            throw new EOFException(FSExceptionMessages.EOF_IN_READ_FULLY);\r\n        }\r\n        return;\r\n    }\r\n    if (streamBuffer != null) {\r\n        if (streamBufferPosition + offset >= 0) {\r\n            streamBufferPosition += offset;\r\n        } else {\r\n            resetStreamBuffer();\r\n            streamPosition = pos;\r\n        }\r\n    } else {\r\n        streamPosition = pos;\r\n    }\r\n    closeBlobInputStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean seekToNewSource(long targetPos) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "available",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int available() throws IOException\n{\r\n    checkState();\r\n    if (blobInputStream != null) {\r\n        return blobInputStream.available();\r\n    } else {\r\n        return (streamBuffer == null) ? 0 : streamBufferLength - streamBufferPosition;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "closeBlobInputStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeBlobInputStream() throws IOException\n{\r\n    if (blobInputStream != null) {\r\n        try {\r\n            blobInputStream.close();\r\n        } finally {\r\n            blobInputStream = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    closed = true;\r\n    closeBlobInputStream();\r\n    streamBuffer = null;\r\n    streamBufferPosition = 0;\r\n    streamBufferLength = 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "doNetworkRead",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int doNetworkRead(byte[] buffer, int offset, int len) throws IOException\n{\r\n    MemoryOutputStream outputStream;\r\n    boolean needToCopy = false;\r\n    if (streamPositionAfterLastRead == streamPosition) {\r\n        if (streamBuffer == null) {\r\n            streamBuffer = new byte[(int) Math.min(minimumReadSizeInBytes, streamLength)];\r\n        }\r\n        resetStreamBuffer();\r\n        outputStream = new MemoryOutputStream(streamBuffer, streamBufferPosition, streamBuffer.length);\r\n        needToCopy = true;\r\n    } else {\r\n        outputStream = new MemoryOutputStream(buffer, offset, len);\r\n    }\r\n    long bytesToRead = Math.min(minimumReadSizeInBytes, Math.min(outputStream.capacity(), streamLength - streamPosition));\r\n    try {\r\n        blob.downloadRange(streamPosition, bytesToRead, outputStream, options, opContext);\r\n    } catch (StorageException e) {\r\n        throw new IOException(e);\r\n    }\r\n    int bytesRead = outputStream.size();\r\n    if (bytesRead > 0) {\r\n        streamPosition += bytesRead;\r\n        streamPositionAfterLastRead = streamPosition;\r\n        int count = Math.min(bytesRead, len);\r\n        if (needToCopy) {\r\n            streamBufferLength = bytesRead;\r\n            System.arraycopy(streamBuffer, streamBufferPosition, buffer, offset, count);\r\n            streamBufferPosition += count;\r\n        }\r\n        return count;\r\n    } else {\r\n        throw new EOFException(\"End of stream reached unexpectedly.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "read",
  "errType" : [ "StorageException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int read(long position, byte[] buffer, int offset, int length) throws IOException\n{\r\n    synchronized (this) {\r\n        checkState();\r\n    }\r\n    if (!bufferedPreadDisabled) {\r\n        return super.read(position, buffer, offset, length);\r\n    }\r\n    validatePositionedReadArgs(position, buffer, offset, length);\r\n    if (length == 0) {\r\n        return 0;\r\n    }\r\n    if (position >= streamLength) {\r\n        throw new EOFException(\"position is beyond stream capacity\");\r\n    }\r\n    MemoryOutputStream os = new MemoryOutputStream(buffer, offset, length);\r\n    long bytesToRead = Math.min(minimumReadSizeInBytes, Math.min(os.capacity(), streamLength - position));\r\n    try {\r\n        blob.downloadRange(position, bytesToRead, os, options, opContext);\r\n    } catch (StorageException e) {\r\n        throw new IOException(e);\r\n    }\r\n    int bytesRead = os.size();\r\n    if (bytesRead == 0) {\r\n        throw new EOFException(\"End of stream reached unexpectedly.\");\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int read(byte[] b, int offset, int len) throws IOException\n{\r\n    checkState();\r\n    NativeAzureFileSystemHelper.validateReadArgs(b, offset, len);\r\n    if (blobInputStream != null) {\r\n        int numberOfBytesRead = blobInputStream.read(b, offset, len);\r\n        streamPosition += numberOfBytesRead;\r\n        return numberOfBytesRead;\r\n    } else {\r\n        if (offset < 0 || len < 0 || len > b.length - offset) {\r\n            throw new IndexOutOfBoundsException(\"read arguments out of range\");\r\n        }\r\n        if (len == 0) {\r\n            return 0;\r\n        }\r\n        int bytesRead = 0;\r\n        int available = available();\r\n        if (available > 0) {\r\n            bytesRead = Math.min(available, len);\r\n            System.arraycopy(streamBuffer, streamBufferPosition, b, offset, bytesRead);\r\n            streamBufferPosition += bytesRead;\r\n        }\r\n        if (len == bytesRead) {\r\n            return len;\r\n        }\r\n        if (streamPosition >= streamLength) {\r\n            return (bytesRead > 0) ? bytesRead : -1;\r\n        }\r\n        offset += bytesRead;\r\n        len -= bytesRead;\r\n        return bytesRead + doNetworkRead(b, offset, len);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    byte[] buffer = new byte[1];\r\n    int numberOfBytesRead = read(buffer, 0, 1);\r\n    return (numberOfBytesRead < 1) ? -1 : buffer[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    checkState();\r\n    if (blobInputStream != null) {\r\n        long skipped = blobInputStream.skip(n);\r\n        streamPosition += skipped;\r\n        return skipped;\r\n    }\r\n    if (n < 0 || n > streamLength - getPos()) {\r\n        throw new IndexOutOfBoundsException(\"skip range\");\r\n    }\r\n    if (streamBuffer != null) {\r\n        if (n < streamBufferLength - streamBufferPosition) {\r\n            streamBufferPosition += (int) n;\r\n        } else {\r\n            streamPosition = getPos() + n;\r\n            resetStreamBuffer();\r\n        }\r\n    } else {\r\n        streamPosition += n;\r\n    }\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getListingException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOException getListingException()\n{\r\n    return listException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFileStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Iterator<FileStatus> getFileStatusIterator()\n{\r\n    return fileStatusIterator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isFailedListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFailedListing()\n{\r\n    return (listException != null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\commit",
  "methodName" : "createOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ManifestCommitter createOutputCommitter(final Path outputPath, final TaskAttemptContext context) throws IOException\n{\r\n    final Configuration conf = context.getConfiguration();\r\n    conf.set(OPT_STORE_OPERATIONS_CLASS, AbfsManifestStoreOperations.NAME);\r\n    return super.createOutputCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isAppendBlobKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isAppendBlobKey(String key)\n{\r\n    return isKeyForDirectorySet(key, appendBlobDirSet);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUser()\n{\r\n    return this.userName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getPrimaryGroup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrimaryGroup()\n{\r\n    return this.primaryUserGroup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "close",
  "errType" : [ "InterruptedException", "ExecutionException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    List<ListenableFuture<?>> futures = new ArrayList<>();\r\n    for (AbfsLease lease : leaseRefs.keySet()) {\r\n        if (lease == null) {\r\n            continue;\r\n        }\r\n        ListenableFuture<?> future = client.submit(() -> lease.free());\r\n        futures.add(future);\r\n    }\r\n    try {\r\n        Futures.allAsList(futures).get();\r\n        HadoopExecutors.shutdown(boundedThreadPool, LOG, 30, TimeUnit.SECONDS);\r\n        boundedThreadPool = null;\r\n    } catch (InterruptedException e) {\r\n        LOG.error(\"Interrupted freeing leases\", e);\r\n        Thread.currentThread().interrupt();\r\n    } catch (ExecutionException e) {\r\n        LOG.error(\"Error freeing leases\", e);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, client);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "encodeAttribute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] encodeAttribute(String value) throws UnsupportedEncodingException\n{\r\n    return value.getBytes(XMS_PROPERTIES_ENCODING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "decodeAttribute",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String decodeAttribute(byte[] value) throws UnsupportedEncodingException\n{\r\n    return new String(value, XMS_PROPERTIES_ENCODING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "authorityParts",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String[] authorityParts(URI uri) throws InvalidUriAuthorityException, InvalidUriException\n{\r\n    final String authority = uri.getRawAuthority();\r\n    if (null == authority) {\r\n        throw new InvalidUriAuthorityException(uri.toString());\r\n    }\r\n    if (!authority.contains(AbfsHttpConstants.AZURE_DISTRIBUTED_FILE_SYSTEM_AUTHORITY_DELIMITER)) {\r\n        throw new InvalidUriAuthorityException(uri.toString());\r\n    }\r\n    final String[] authorityParts = authority.split(AbfsHttpConstants.AZURE_DISTRIBUTED_FILE_SYSTEM_AUTHORITY_DELIMITER, 2);\r\n    if (authorityParts.length < 2 || authorityParts[0] != null && authorityParts[0].isEmpty()) {\r\n        final String errMsg = String.format(\"'%s' has a malformed authority, expected container name. \" + \"Authority takes the form \" + FileSystemUriSchemes.ABFS_SCHEME + \"://[<container name>@]<account name>\", uri.toString());\r\n        throw new InvalidUriException(errMsg);\r\n    }\r\n    return authorityParts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getIsNamespaceEnabled",
  "errType" : [ "TrileanConversionException", "AbfsRestOperationException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean getIsNamespaceEnabled(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try {\r\n        return this.isNamespaceEnabled.toBoolean();\r\n    } catch (TrileanConversionException e) {\r\n        LOG.debug(\"isNamespaceEnabled is UNKNOWN; fall back and determine through\" + \" getAcl server call\", e);\r\n    }\r\n    LOG.debug(\"Get root ACL status\");\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"getIsNamespaceEnabled\", \"getAclStatus\")) {\r\n        AbfsRestOperation op = client.getAclStatus(AbfsHttpConstants.ROOT_PATH, tracingContext);\r\n        perfInfo.registerResult(op.getResult());\r\n        isNamespaceEnabled = Trilean.getTrilean(true);\r\n        perfInfo.registerSuccess(true);\r\n    } catch (AbfsRestOperationException ex) {\r\n        if (HttpURLConnection.HTTP_BAD_REQUEST != ex.getStatusCode()) {\r\n            throw ex;\r\n        }\r\n        isNamespaceEnabled = Trilean.getTrilean(false);\r\n    }\r\n    return isNamespaceEnabled.toBoolean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getURIBuilder",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "URIBuilder getURIBuilder(final String hostName, boolean isSecure)\n{\r\n    String scheme = isSecure ? FileSystemUriSchemes.HTTPS_SCHEME : FileSystemUriSchemes.HTTP_SCHEME;\r\n    final URIBuilder uriBuilder = new URIBuilder();\r\n    uriBuilder.setScheme(scheme);\r\n    String endPoint = abfsConfiguration.get(AZURE_ABFS_ENDPOINT);\r\n    if (endPoint == null || !endPoint.contains(AbfsHttpConstants.COLON)) {\r\n        uriBuilder.setHost(hostName);\r\n        return uriBuilder;\r\n    }\r\n    String[] data = endPoint.split(AbfsHttpConstants.COLON);\r\n    if (data.length != 2) {\r\n        throw new RuntimeException(String.format(\"ABFS endpoint is not set correctly : %s, \" + \"Do not specify scheme when using {IP}:{PORT}\", endPoint));\r\n    }\r\n    uriBuilder.setHost(data[0].trim());\r\n    uriBuilder.setPort(Integer.parseInt(data[1].trim()));\r\n    uriBuilder.setPath(\"/\" + UriUtils.extractAccountNameFromHostName(hostName));\r\n    return uriBuilder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAbfsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsConfiguration getAbfsConfiguration()\n{\r\n    return this.abfsConfiguration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFilesystemProperties",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Hashtable<String, String> getFilesystemProperties(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"getFilesystemProperties\", \"getFilesystemProperties\")) {\r\n        LOG.debug(\"getFilesystemProperties for filesystem: {}\", client.getFileSystem());\r\n        final Hashtable<String, String> parsedXmsProperties;\r\n        final AbfsRestOperation op = client.getFilesystemProperties(tracingContext);\r\n        perfInfo.registerResult(op.getResult());\r\n        final String xMsProperties = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_PROPERTIES);\r\n        parsedXmsProperties = parseCommaSeparatedXmsProperties(xMsProperties);\r\n        perfInfo.registerSuccess(true);\r\n        return parsedXmsProperties;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setFilesystemProperties",
  "errType" : [ "CharacterCodingException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setFilesystemProperties(final Hashtable<String, String> properties, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (properties == null || properties.isEmpty()) {\r\n        LOG.trace(\"setFilesystemProperties no properties present\");\r\n        return;\r\n    }\r\n    LOG.debug(\"setFilesystemProperties for filesystem: {} with properties: {}\", client.getFileSystem(), properties);\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"setFilesystemProperties\", \"setFilesystemProperties\")) {\r\n        final String commaSeparatedProperties;\r\n        try {\r\n            commaSeparatedProperties = convertXmsPropertiesToCommaSeparatedString(properties);\r\n        } catch (CharacterCodingException ex) {\r\n            throw new InvalidAbfsRestOperationException(ex);\r\n        }\r\n        final AbfsRestOperation op = client.setFilesystemProperties(commaSeparatedProperties, tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getPathStatus",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Hashtable<String, String> getPathStatus(final Path path, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"getPathStatus\", \"getPathStatus\")) {\r\n        LOG.debug(\"getPathStatus for filesystem: {} path: {}\", client.getFileSystem(), path);\r\n        final Hashtable<String, String> parsedXmsProperties;\r\n        final AbfsRestOperation op = client.getPathStatus(getRelativePath(path), true, tracingContext);\r\n        perfInfo.registerResult(op.getResult());\r\n        final String xMsProperties = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_PROPERTIES);\r\n        parsedXmsProperties = parseCommaSeparatedXmsProperties(xMsProperties);\r\n        perfInfo.registerSuccess(true);\r\n        return parsedXmsProperties;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setPathProperties",
  "errType" : [ "CharacterCodingException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setPathProperties(final Path path, final Hashtable<String, String> properties, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"setPathProperties\", \"setPathProperties\")) {\r\n        LOG.debug(\"setFilesystemProperties for filesystem: {} path: {} with properties: {}\", client.getFileSystem(), path, properties);\r\n        final String commaSeparatedProperties;\r\n        try {\r\n            commaSeparatedProperties = convertXmsPropertiesToCommaSeparatedString(properties);\r\n        } catch (CharacterCodingException ex) {\r\n            throw new InvalidAbfsRestOperationException(ex);\r\n        }\r\n        final AbfsRestOperation op = client.setPathProperties(getRelativePath(path), commaSeparatedProperties, tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFilesystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createFilesystem(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"createFilesystem\", \"createFilesystem\")) {\r\n        LOG.debug(\"createFilesystem for filesystem: {}\", client.getFileSystem());\r\n        final AbfsRestOperation op = client.createFilesystem(tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "deleteFilesystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteFilesystem(TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"deleteFilesystem\", \"deleteFilesystem\")) {\r\n        LOG.debug(\"deleteFilesystem for filesystem: {}\", client.getFileSystem());\r\n        final AbfsRestOperation op = client.deleteFilesystem(tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "OutputStream createFile(final Path path, final FileSystem.Statistics statistics, final boolean overwrite, final FsPermission permission, final FsPermission umask, TracingContext tracingContext) throws IOException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"createFile\", \"createPath\")) {\r\n        boolean isNamespaceEnabled = getIsNamespaceEnabled(tracingContext);\r\n        LOG.debug(\"createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}\", client.getFileSystem(), path, overwrite, permission, umask, isNamespaceEnabled);\r\n        String relativePath = getRelativePath(path);\r\n        boolean isAppendBlob = false;\r\n        if (isAppendBlobKey(path.toString())) {\r\n            isAppendBlob = true;\r\n        }\r\n        boolean triggerConditionalCreateOverwrite = false;\r\n        if (overwrite && abfsConfiguration.isConditionalCreateOverwriteEnabled()) {\r\n            triggerConditionalCreateOverwrite = true;\r\n        }\r\n        AbfsRestOperation op;\r\n        if (triggerConditionalCreateOverwrite) {\r\n            op = conditionalCreateOverwriteFile(relativePath, statistics, isNamespaceEnabled ? getOctalNotation(permission) : null, isNamespaceEnabled ? getOctalNotation(umask) : null, isAppendBlob, tracingContext);\r\n        } else {\r\n            op = client.createPath(relativePath, true, overwrite, isNamespaceEnabled ? getOctalNotation(permission) : null, isNamespaceEnabled ? getOctalNotation(umask) : null, isAppendBlob, null, tracingContext);\r\n        }\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n        AbfsLease lease = maybeCreateLease(relativePath, tracingContext);\r\n        return new AbfsOutputStream(populateAbfsOutputStreamContext(isAppendBlob, lease, client, statistics, relativePath, 0, tracingContext));\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "conditionalCreateOverwriteFile",
  "errType" : [ "AbfsRestOperationException", "AbfsRestOperationException", "AbfsRestOperationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AbfsRestOperation conditionalCreateOverwriteFile(final String relativePath, final FileSystem.Statistics statistics, final String permission, final String umask, final boolean isAppendBlob, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    AbfsRestOperation op;\r\n    try {\r\n        op = client.createPath(relativePath, true, false, permission, umask, isAppendBlob, null, tracingContext);\r\n    } catch (AbfsRestOperationException e) {\r\n        if (e.getStatusCode() == HttpURLConnection.HTTP_CONFLICT) {\r\n            try {\r\n                op = client.getPathStatus(relativePath, false, tracingContext);\r\n            } catch (AbfsRestOperationException ex) {\r\n                if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\r\n                    throw new ConcurrentWriteOperationDetectedException(\"Parallel access to the create path detected. Failing request \" + \"to honor single writer semantics\");\r\n                } else {\r\n                    throw ex;\r\n                }\r\n            }\r\n            String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n            try {\r\n                op = client.createPath(relativePath, true, true, permission, umask, isAppendBlob, eTag, tracingContext);\r\n            } catch (AbfsRestOperationException ex) {\r\n                if (ex.getStatusCode() == HttpURLConnection.HTTP_PRECON_FAILED) {\r\n                    throw new ConcurrentWriteOperationDetectedException(\"Parallel access to the create path detected. Failing request \" + \"to honor single writer semantics\");\r\n                } else {\r\n                    throw ex;\r\n                }\r\n            }\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n    return op;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "populateAbfsOutputStreamContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbfsOutputStreamContext populateAbfsOutputStreamContext(boolean isAppendBlob, AbfsLease lease, AbfsClient client, FileSystem.Statistics statistics, String path, long position, TracingContext tracingContext)\n{\r\n    int bufferSize = abfsConfiguration.getWriteBufferSize();\r\n    if (isAppendBlob && bufferSize > FileSystemConfigurations.APPENDBLOB_MAX_WRITE_BUFFER_SIZE) {\r\n        bufferSize = FileSystemConfigurations.APPENDBLOB_MAX_WRITE_BUFFER_SIZE;\r\n    }\r\n    return new AbfsOutputStreamContext(abfsConfiguration.getSasTokenRenewPeriodForStreamsInSeconds()).withWriteBufferSize(bufferSize).enableFlush(abfsConfiguration.isFlushEnabled()).enableSmallWriteOptimization(abfsConfiguration.isSmallWriteOptimizationEnabled()).disableOutputStreamFlush(abfsConfiguration.isOutputStreamFlushDisabled()).withStreamStatistics(new AbfsOutputStreamStatisticsImpl()).withAppendBlob(isAppendBlob).withWriteMaxConcurrentRequestCount(abfsConfiguration.getWriteMaxConcurrentRequestCount()).withMaxWriteRequestsToQueue(abfsConfiguration.getMaxWriteRequestsToQueue()).withLease(lease).withBlockFactory(blockFactory).withBlockOutputActiveBlocks(blockOutputActiveBlocks).withClient(client).withPosition(position).withFsStatistics(statistics).withPath(path).withExecutorService(new SemaphoredDelegatingExecutor(boundedThreadPool, blockOutputActiveBlocks, true)).withTracingContext(tracingContext).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "createDirectory",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createDirectory(final Path path, final FsPermission permission, final FsPermission umask, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"createDirectory\", \"createPath\")) {\r\n        boolean isNamespaceEnabled = getIsNamespaceEnabled(tracingContext);\r\n        LOG.debug(\"createDirectory filesystem: {} path: {} permission: {} umask: {} isNamespaceEnabled: {}\", client.getFileSystem(), path, permission, umask, isNamespaceEnabled);\r\n        boolean overwrite = !isNamespaceEnabled || abfsConfiguration.isEnabledMkdirOverwrite();\r\n        final AbfsRestOperation op = client.createPath(getRelativePath(path), false, overwrite, isNamespaceEnabled ? getOctalNotation(permission) : null, isNamespaceEnabled ? getOctalNotation(umask) : null, false, null, tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "openFileForRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsInputStream openFileForRead(final Path path, final FileSystem.Statistics statistics, TracingContext tracingContext) throws IOException\n{\r\n    return openFileForRead(path, Optional.empty(), statistics, tracingContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "openFileForRead",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "AbfsInputStream openFileForRead(Path path, final Optional<OpenFileParameters> parameters, final FileSystem.Statistics statistics, TracingContext tracingContext) throws IOException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"openFileForRead\", \"getPathStatus\")) {\r\n        LOG.debug(\"openFileForRead filesystem: {} path: {}\", client.getFileSystem(), path);\r\n        FileStatus fileStatus = parameters.map(OpenFileParameters::getStatus).orElse(null);\r\n        String relativePath = getRelativePath(path);\r\n        String resourceType, eTag;\r\n        long contentLength;\r\n        if (fileStatus instanceof VersionedFileStatus) {\r\n            path = path.makeQualified(this.uri, path);\r\n            Preconditions.checkArgument(fileStatus.getPath().equals(path), String.format(\"Filestatus path [%s] does not match with given path [%s]\", fileStatus.getPath(), path));\r\n            resourceType = fileStatus.isFile() ? FILE : DIRECTORY;\r\n            contentLength = fileStatus.getLen();\r\n            eTag = ((VersionedFileStatus) fileStatus).getVersion();\r\n        } else {\r\n            if (fileStatus != null) {\r\n                LOG.warn(\"Fallback to getPathStatus REST call as provided filestatus \" + \"is not of type VersionedFileStatus\");\r\n            }\r\n            AbfsHttpOperation op = client.getPathStatus(relativePath, false, tracingContext).getResult();\r\n            resourceType = op.getResponseHeader(HttpHeaderConfigurations.X_MS_RESOURCE_TYPE);\r\n            contentLength = Long.parseLong(op.getResponseHeader(HttpHeaderConfigurations.CONTENT_LENGTH));\r\n            eTag = op.getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n        }\r\n        if (parseIsDirectory(resourceType)) {\r\n            throw new AbfsRestOperationException(AzureServiceErrorCode.PATH_NOT_FOUND.getStatusCode(), AzureServiceErrorCode.PATH_NOT_FOUND.getErrorCode(), \"openFileForRead must be used with files and not directories\", null);\r\n        }\r\n        perfInfo.registerSuccess(true);\r\n        return new AbfsInputStream(client, statistics, relativePath, contentLength, populateAbfsInputStreamContext(parameters.map(OpenFileParameters::getOptions)), eTag, tracingContext);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "populateAbfsInputStreamContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbfsInputStreamContext populateAbfsInputStreamContext(Optional<Configuration> options)\n{\r\n    boolean bufferedPreadDisabled = options.map(c -> c.getBoolean(FS_AZURE_BUFFERED_PREAD_DISABLE, false)).orElse(false);\r\n    return new AbfsInputStreamContext(abfsConfiguration.getSasTokenRenewPeriodForStreamsInSeconds()).withReadBufferSize(abfsConfiguration.getReadBufferSize()).withReadAheadQueueDepth(abfsConfiguration.getReadAheadQueueDepth()).withTolerateOobAppends(abfsConfiguration.getTolerateOobAppends()).withReadSmallFilesCompletely(abfsConfiguration.readSmallFilesCompletely()).withOptimizeFooterRead(abfsConfiguration.optimizeFooterRead()).withReadAheadRange(abfsConfiguration.getReadAheadRange()).withStreamStatistics(new AbfsInputStreamStatisticsImpl()).withShouldReadBufferSizeAlways(abfsConfiguration.shouldReadBufferSizeAlways()).withReadAheadBlockSize(abfsConfiguration.getReadAheadBlockSize()).withBufferedPreadDisabled(bufferedPreadDisabled).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "openFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "OutputStream openFileForWrite(final Path path, final FileSystem.Statistics statistics, final boolean overwrite, TracingContext tracingContext) throws IOException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"openFileForWrite\", \"getPathStatus\")) {\r\n        LOG.debug(\"openFileForWrite filesystem: {} path: {} overwrite: {}\", client.getFileSystem(), path, overwrite);\r\n        String relativePath = getRelativePath(path);\r\n        final AbfsRestOperation op = client.getPathStatus(relativePath, false, tracingContext);\r\n        perfInfo.registerResult(op.getResult());\r\n        final String resourceType = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_RESOURCE_TYPE);\r\n        final Long contentLength = Long.valueOf(op.getResult().getResponseHeader(HttpHeaderConfigurations.CONTENT_LENGTH));\r\n        if (parseIsDirectory(resourceType)) {\r\n            throw new AbfsRestOperationException(AzureServiceErrorCode.PATH_NOT_FOUND.getStatusCode(), AzureServiceErrorCode.PATH_NOT_FOUND.getErrorCode(), \"openFileForRead must be used with files and not directories\", null);\r\n        }\r\n        final long offset = overwrite ? 0 : contentLength;\r\n        perfInfo.registerSuccess(true);\r\n        boolean isAppendBlob = false;\r\n        if (isAppendBlobKey(path.toString())) {\r\n            isAppendBlob = true;\r\n        }\r\n        AbfsLease lease = maybeCreateLease(relativePath, tracingContext);\r\n        return new AbfsOutputStream(populateAbfsOutputStreamContext(isAppendBlob, lease, client, statistics, relativePath, offset, tracingContext));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "breakLease",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void breakLease(final Path path, final TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    LOG.debug(\"lease path: {}\", path);\r\n    client.breakLease(getRelativePath(path), tracingContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "boolean rename(final Path source, final Path destination, final TracingContext tracingContext, final String sourceEtag) throws AzureBlobFileSystemException\n{\r\n    final Instant startAggregate = abfsPerfTracker.getLatencyInstant();\r\n    long countAggregate = 0;\r\n    boolean shouldContinue;\r\n    if (isAtomicRenameKey(source.getName())) {\r\n        LOG.warn(\"The atomic rename feature is not supported by the ABFS scheme; however rename,\" + \" create and delete operations are atomic if Namespace is enabled for your Azure Storage account.\");\r\n    }\r\n    LOG.debug(\"renameAsync filesystem: {} source: {} destination: {}\", client.getFileSystem(), source, destination);\r\n    String continuation = null;\r\n    String sourceRelativePath = getRelativePath(source);\r\n    String destinationRelativePath = getRelativePath(destination);\r\n    boolean recovered = false;\r\n    do {\r\n        try (AbfsPerfInfo perfInfo = startTracking(\"rename\", \"renamePath\")) {\r\n            final Pair<AbfsRestOperation, Boolean> pair = client.renamePath(sourceRelativePath, destinationRelativePath, continuation, tracingContext, sourceEtag);\r\n            AbfsRestOperation op = pair.getLeft();\r\n            perfInfo.registerResult(op.getResult());\r\n            continuation = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_CONTINUATION);\r\n            perfInfo.registerSuccess(true);\r\n            countAggregate++;\r\n            shouldContinue = continuation != null && !continuation.isEmpty();\r\n            recovered |= pair.getRight();\r\n            if (!shouldContinue) {\r\n                perfInfo.registerAggregates(startAggregate, countAggregate);\r\n            }\r\n        }\r\n    } while (shouldContinue);\r\n    return recovered;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void delete(final Path path, final boolean recursive, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    final Instant startAggregate = abfsPerfTracker.getLatencyInstant();\r\n    long countAggregate = 0;\r\n    boolean shouldContinue = true;\r\n    LOG.debug(\"delete filesystem: {} path: {} recursive: {}\", client.getFileSystem(), path, String.valueOf(recursive));\r\n    String continuation = null;\r\n    String relativePath = getRelativePath(path);\r\n    do {\r\n        try (AbfsPerfInfo perfInfo = startTracking(\"delete\", \"deletePath\")) {\r\n            AbfsRestOperation op = client.deletePath(relativePath, recursive, continuation, tracingContext);\r\n            perfInfo.registerResult(op.getResult());\r\n            continuation = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_CONTINUATION);\r\n            perfInfo.registerSuccess(true);\r\n            countAggregate++;\r\n            shouldContinue = continuation != null && !continuation.isEmpty();\r\n            if (!shouldContinue) {\r\n                perfInfo.registerAggregates(startAggregate, countAggregate);\r\n            }\r\n        }\r\n    } while (shouldContinue);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "FileStatus getFileStatus(final Path path, TracingContext tracingContext) throws IOException\n{\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"getFileStatus\", \"undetermined\")) {\r\n        boolean isNamespaceEnabled = getIsNamespaceEnabled(tracingContext);\r\n        LOG.debug(\"getFileStatus filesystem: {} path: {} isNamespaceEnabled: {}\", client.getFileSystem(), path, isNamespaceEnabled);\r\n        final AbfsRestOperation op;\r\n        if (path.isRoot()) {\r\n            if (isNamespaceEnabled) {\r\n                perfInfo.registerCallee(\"getAclStatus\");\r\n                op = client.getAclStatus(getRelativePath(path), tracingContext);\r\n            } else {\r\n                perfInfo.registerCallee(\"getFilesystemProperties\");\r\n                op = client.getFilesystemProperties(tracingContext);\r\n            }\r\n        } else {\r\n            perfInfo.registerCallee(\"getPathStatus\");\r\n            op = client.getPathStatus(getRelativePath(path), false, tracingContext);\r\n        }\r\n        perfInfo.registerResult(op.getResult());\r\n        final long blockSize = abfsConfiguration.getAzureBlockSize();\r\n        final AbfsHttpOperation result = op.getResult();\r\n        String eTag = extractEtagHeader(result);\r\n        final String lastModified = result.getResponseHeader(HttpHeaderConfigurations.LAST_MODIFIED);\r\n        final String permissions = result.getResponseHeader((HttpHeaderConfigurations.X_MS_PERMISSIONS));\r\n        final boolean hasAcl = AbfsPermission.isExtendedAcl(permissions);\r\n        final long contentLength;\r\n        final boolean resourceIsDir;\r\n        if (path.isRoot()) {\r\n            contentLength = 0;\r\n            resourceIsDir = true;\r\n        } else {\r\n            contentLength = parseContentLength(result.getResponseHeader(HttpHeaderConfigurations.CONTENT_LENGTH));\r\n            resourceIsDir = parseIsDirectory(result.getResponseHeader(HttpHeaderConfigurations.X_MS_RESOURCE_TYPE));\r\n        }\r\n        final String transformedOwner = identityTransformer.transformIdentityForGetRequest(result.getResponseHeader(HttpHeaderConfigurations.X_MS_OWNER), true, userName);\r\n        final String transformedGroup = identityTransformer.transformIdentityForGetRequest(result.getResponseHeader(HttpHeaderConfigurations.X_MS_GROUP), false, primaryUserGroup);\r\n        perfInfo.registerSuccess(true);\r\n        return new VersionedFileStatus(transformedOwner, transformedGroup, permissions == null ? new AbfsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL) : AbfsPermission.valueOf(permissions), hasAcl, contentLength, resourceIsDir, 1, blockSize, DateTimeUtils.parseLastModifiedTime(lastModified), path, eTag);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus[] listStatus(final Path path, TracingContext tracingContext) throws IOException\n{\r\n    return listStatus(path, null, tracingContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus[] listStatus(final Path path, final String startFrom, TracingContext tracingContext) throws IOException\n{\r\n    List<FileStatus> fileStatuses = new ArrayList<>();\r\n    listStatus(path, startFrom, fileStatuses, true, null, tracingContext);\r\n    return fileStatuses.toArray(new FileStatus[fileStatuses.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "String listStatus(final Path path, final String startFrom, List<FileStatus> fileStatuses, final boolean fetchAll, String continuation, TracingContext tracingContext) throws IOException\n{\r\n    final Instant startAggregate = abfsPerfTracker.getLatencyInstant();\r\n    long countAggregate = 0;\r\n    boolean shouldContinue = true;\r\n    LOG.debug(\"listStatus filesystem: {} path: {}, startFrom: {}\", client.getFileSystem(), path, startFrom);\r\n    final String relativePath = getRelativePath(path);\r\n    if (continuation == null || continuation.isEmpty()) {\r\n        if (startFrom != null && !startFrom.isEmpty()) {\r\n            continuation = getIsNamespaceEnabled(tracingContext) ? generateContinuationTokenForXns(startFrom) : generateContinuationTokenForNonXns(relativePath, startFrom);\r\n        }\r\n    }\r\n    do {\r\n        try (AbfsPerfInfo perfInfo = startTracking(\"listStatus\", \"listPath\")) {\r\n            AbfsRestOperation op = client.listPath(relativePath, false, abfsConfiguration.getListMaxResults(), continuation, tracingContext);\r\n            perfInfo.registerResult(op.getResult());\r\n            continuation = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_CONTINUATION);\r\n            ListResultSchema retrievedSchema = op.getResult().getListResultSchema();\r\n            if (retrievedSchema == null) {\r\n                throw new AbfsRestOperationException(AzureServiceErrorCode.PATH_NOT_FOUND.getStatusCode(), AzureServiceErrorCode.PATH_NOT_FOUND.getErrorCode(), \"listStatusAsync path not found\", null, op.getResult());\r\n            }\r\n            long blockSize = abfsConfiguration.getAzureBlockSize();\r\n            for (ListResultEntrySchema entry : retrievedSchema.paths()) {\r\n                final String owner = identityTransformer.transformIdentityForGetRequest(entry.owner(), true, userName);\r\n                final String group = identityTransformer.transformIdentityForGetRequest(entry.group(), false, primaryUserGroup);\r\n                final FsPermission fsPermission = entry.permissions() == null ? new AbfsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL) : AbfsPermission.valueOf(entry.permissions());\r\n                final boolean hasAcl = AbfsPermission.isExtendedAcl(entry.permissions());\r\n                long lastModifiedMillis = 0;\r\n                long contentLength = entry.contentLength() == null ? 0 : entry.contentLength();\r\n                boolean isDirectory = entry.isDirectory() == null ? false : entry.isDirectory();\r\n                if (entry.lastModified() != null && !entry.lastModified().isEmpty()) {\r\n                    lastModifiedMillis = DateTimeUtils.parseLastModifiedTime(entry.lastModified());\r\n                }\r\n                Path entryPath = new Path(File.separator + entry.name());\r\n                entryPath = entryPath.makeQualified(this.uri, entryPath);\r\n                fileStatuses.add(new VersionedFileStatus(owner, group, fsPermission, hasAcl, contentLength, isDirectory, 1, blockSize, lastModifiedMillis, entryPath, entry.eTag()));\r\n            }\r\n            perfInfo.registerSuccess(true);\r\n            countAggregate++;\r\n            shouldContinue = fetchAll && continuation != null && !continuation.isEmpty();\r\n            if (!shouldContinue) {\r\n                perfInfo.registerAggregates(startAggregate, countAggregate);\r\n            }\r\n        }\r\n    } while (shouldContinue);\r\n    return continuation;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "generateContinuationTokenForXns",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String generateContinuationTokenForXns(final String firstEntryName)\n{\r\n    Preconditions.checkArgument(!Strings.isNullOrEmpty(firstEntryName) && !firstEntryName.startsWith(AbfsHttpConstants.ROOT_PATH), \"startFrom must be a dir/file name and it can not be a full path\");\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(firstEntryName).append(\"#$\").append(\"0\");\r\n    CRC64 crc64 = new CRC64();\r\n    StringBuilder token = new StringBuilder();\r\n    token.append(crc64.compute(sb.toString().getBytes(StandardCharsets.UTF_8))).append(SINGLE_WHITE_SPACE).append(\"0\").append(SINGLE_WHITE_SPACE).append(firstEntryName);\r\n    return Base64.encode(token.toString().getBytes(StandardCharsets.UTF_8));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "generateContinuationTokenForNonXns",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "String generateContinuationTokenForNonXns(String path, final String firstEntryName)\n{\r\n    Preconditions.checkArgument(!Strings.isNullOrEmpty(firstEntryName) && !firstEntryName.startsWith(AbfsHttpConstants.ROOT_PATH), \"startFrom must be a dir/file name and it can not be a full path\");\r\n    path = AbfsClient.getDirectoryQueryParameter(path);\r\n    final String startFrom = (path.isEmpty() || path.equals(ROOT_PATH)) ? firstEntryName : path + ROOT_PATH + firstEntryName;\r\n    SimpleDateFormat simpleDateFormat = new SimpleDateFormat(TOKEN_DATE_PATTERN, Locale.US);\r\n    String date = simpleDateFormat.format(new Date());\r\n    String token = String.format(\"%06d!%s!%06d!%s!%06d!%s!\", path.length(), path, startFrom.length(), startFrom, date.length(), date);\r\n    String base64EncodedToken = Base64.encode(token.getBytes(StandardCharsets.UTF_8));\r\n    StringBuilder encodedTokenBuilder = new StringBuilder(base64EncodedToken.length() + 5);\r\n    encodedTokenBuilder.append(String.format(\"%s!%d!\", TOKEN_VERSION, base64EncodedToken.length()));\r\n    for (int i = 0; i < base64EncodedToken.length(); i++) {\r\n        char current = base64EncodedToken.charAt(i);\r\n        if (CHAR_FORWARD_SLASH == current) {\r\n            current = CHAR_UNDERSCORE;\r\n        } else if (CHAR_PLUS == current) {\r\n            current = CHAR_STAR;\r\n        } else if (CHAR_EQUALS == current) {\r\n            current = CHAR_HYPHEN;\r\n        }\r\n        encodedTokenBuilder.append(current);\r\n    }\r\n    return encodedTokenBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setOwner(final Path path, final String owner, final String group, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"setOwner\", \"setOwner\")) {\r\n        LOG.debug(\"setOwner filesystem: {} path: {} owner: {} group: {}\", client.getFileSystem(), path, owner, group);\r\n        final String transformedOwner = identityTransformer.transformUserOrGroupForSetRequest(owner);\r\n        final String transformedGroup = identityTransformer.transformUserOrGroupForSetRequest(group);\r\n        final AbfsRestOperation op = client.setOwner(getRelativePath(path), transformedOwner, transformedGroup, tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setPermission(final Path path, final FsPermission permission, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"setPermission\", \"setPermission\")) {\r\n        LOG.debug(\"setPermission filesystem: {} path: {} permission: {}\", client.getFileSystem(), path, permission);\r\n        final AbfsRestOperation op = client.setPermission(getRelativePath(path), String.format(AbfsHttpConstants.PERMISSION_FORMAT, permission.toOctal()), tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void modifyAclEntries(final Path path, final List<AclEntry> aclSpec, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfoGet = startTracking(\"modifyAclEntries\", \"getAclStatus\")) {\r\n        LOG.debug(\"modifyAclEntries filesystem: {} path: {} aclSpec: {}\", client.getFileSystem(), path, AclEntry.aclSpecToString(aclSpec));\r\n        identityTransformer.transformAclEntriesForSetRequest(aclSpec);\r\n        final Map<String, String> modifyAclEntries = AbfsAclHelper.deserializeAclSpec(AclEntry.aclSpecToString(aclSpec));\r\n        boolean useUpn = AbfsAclHelper.isUpnFormatAclEntries(modifyAclEntries);\r\n        String relativePath = getRelativePath(path);\r\n        final AbfsRestOperation op = client.getAclStatus(relativePath, useUpn, tracingContext);\r\n        perfInfoGet.registerResult(op.getResult());\r\n        final String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n        final Map<String, String> aclEntries = AbfsAclHelper.deserializeAclSpec(op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_ACL));\r\n        AbfsAclHelper.modifyAclEntriesInternal(aclEntries, modifyAclEntries);\r\n        perfInfoGet.registerSuccess(true).finishTracking();\r\n        try (AbfsPerfInfo perfInfoSet = startTracking(\"modifyAclEntries\", \"setAcl\")) {\r\n            final AbfsRestOperation setAclOp = client.setAcl(relativePath, AbfsAclHelper.serializeAclSpec(aclEntries), eTag, tracingContext);\r\n            perfInfoSet.registerResult(setAclOp.getResult()).registerSuccess(true).registerAggregates(perfInfoGet.getTrackingStart(), GET_SET_AGGREGATE_COUNT);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void removeAclEntries(final Path path, final List<AclEntry> aclSpec, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfoGet = startTracking(\"removeAclEntries\", \"getAclStatus\")) {\r\n        LOG.debug(\"removeAclEntries filesystem: {} path: {} aclSpec: {}\", client.getFileSystem(), path, AclEntry.aclSpecToString(aclSpec));\r\n        identityTransformer.transformAclEntriesForSetRequest(aclSpec);\r\n        final Map<String, String> removeAclEntries = AbfsAclHelper.deserializeAclSpec(AclEntry.aclSpecToString(aclSpec));\r\n        boolean isUpnFormat = AbfsAclHelper.isUpnFormatAclEntries(removeAclEntries);\r\n        String relativePath = getRelativePath(path);\r\n        final AbfsRestOperation op = client.getAclStatus(relativePath, isUpnFormat, tracingContext);\r\n        perfInfoGet.registerResult(op.getResult());\r\n        final String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n        final Map<String, String> aclEntries = AbfsAclHelper.deserializeAclSpec(op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_ACL));\r\n        AbfsAclHelper.removeAclEntriesInternal(aclEntries, removeAclEntries);\r\n        perfInfoGet.registerSuccess(true).finishTracking();\r\n        try (AbfsPerfInfo perfInfoSet = startTracking(\"removeAclEntries\", \"setAcl\")) {\r\n            final AbfsRestOperation setAclOp = client.setAcl(relativePath, AbfsAclHelper.serializeAclSpec(aclEntries), eTag, tracingContext);\r\n            perfInfoSet.registerResult(setAclOp.getResult()).registerSuccess(true).registerAggregates(perfInfoGet.getTrackingStart(), GET_SET_AGGREGATE_COUNT);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void removeDefaultAcl(final Path path, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfoGet = startTracking(\"removeDefaultAcl\", \"getAclStatus\")) {\r\n        LOG.debug(\"removeDefaultAcl filesystem: {} path: {}\", client.getFileSystem(), path);\r\n        String relativePath = getRelativePath(path);\r\n        final AbfsRestOperation op = client.getAclStatus(relativePath, tracingContext);\r\n        perfInfoGet.registerResult(op.getResult());\r\n        final String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n        final Map<String, String> aclEntries = AbfsAclHelper.deserializeAclSpec(op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_ACL));\r\n        final Map<String, String> defaultAclEntries = new HashMap<>();\r\n        for (Map.Entry<String, String> aclEntry : aclEntries.entrySet()) {\r\n            if (aclEntry.getKey().startsWith(\"default:\")) {\r\n                defaultAclEntries.put(aclEntry.getKey(), aclEntry.getValue());\r\n            }\r\n        }\r\n        aclEntries.keySet().removeAll(defaultAclEntries.keySet());\r\n        perfInfoGet.registerSuccess(true).finishTracking();\r\n        try (AbfsPerfInfo perfInfoSet = startTracking(\"removeDefaultAcl\", \"setAcl\")) {\r\n            final AbfsRestOperation setAclOp = client.setAcl(relativePath, AbfsAclHelper.serializeAclSpec(aclEntries), eTag, tracingContext);\r\n            perfInfoSet.registerResult(setAclOp.getResult()).registerSuccess(true).registerAggregates(perfInfoGet.getTrackingStart(), GET_SET_AGGREGATE_COUNT);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void removeAcl(final Path path, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfoGet = startTracking(\"removeAcl\", \"getAclStatus\")) {\r\n        LOG.debug(\"removeAcl filesystem: {} path: {}\", client.getFileSystem(), path);\r\n        String relativePath = getRelativePath(path);\r\n        final AbfsRestOperation op = client.getAclStatus(relativePath, tracingContext);\r\n        perfInfoGet.registerResult(op.getResult());\r\n        final String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n        final Map<String, String> aclEntries = AbfsAclHelper.deserializeAclSpec(op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_ACL));\r\n        final Map<String, String> newAclEntries = new HashMap<>();\r\n        newAclEntries.put(AbfsHttpConstants.ACCESS_USER, aclEntries.get(AbfsHttpConstants.ACCESS_USER));\r\n        newAclEntries.put(AbfsHttpConstants.ACCESS_GROUP, aclEntries.get(AbfsHttpConstants.ACCESS_GROUP));\r\n        newAclEntries.put(AbfsHttpConstants.ACCESS_OTHER, aclEntries.get(AbfsHttpConstants.ACCESS_OTHER));\r\n        perfInfoGet.registerSuccess(true).finishTracking();\r\n        try (AbfsPerfInfo perfInfoSet = startTracking(\"removeAcl\", \"setAcl\")) {\r\n            final AbfsRestOperation setAclOp = client.setAcl(relativePath, AbfsAclHelper.serializeAclSpec(newAclEntries), eTag, tracingContext);\r\n            perfInfoSet.registerResult(setAclOp.getResult()).registerSuccess(true).registerAggregates(perfInfoGet.getTrackingStart(), GET_SET_AGGREGATE_COUNT);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void setAcl(final Path path, final List<AclEntry> aclSpec, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfoGet = startTracking(\"setAcl\", \"getAclStatus\")) {\r\n        LOG.debug(\"setAcl filesystem: {} path: {} aclspec: {}\", client.getFileSystem(), path, AclEntry.aclSpecToString(aclSpec));\r\n        identityTransformer.transformAclEntriesForSetRequest(aclSpec);\r\n        final Map<String, String> aclEntries = AbfsAclHelper.deserializeAclSpec(AclEntry.aclSpecToString(aclSpec));\r\n        final boolean isUpnFormat = AbfsAclHelper.isUpnFormatAclEntries(aclEntries);\r\n        String relativePath = getRelativePath(path);\r\n        final AbfsRestOperation op = client.getAclStatus(relativePath, isUpnFormat, tracingContext);\r\n        perfInfoGet.registerResult(op.getResult());\r\n        final String eTag = op.getResult().getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n        final Map<String, String> getAclEntries = AbfsAclHelper.deserializeAclSpec(op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_ACL));\r\n        AbfsAclHelper.setAclEntriesInternal(aclEntries, getAclEntries);\r\n        perfInfoGet.registerSuccess(true).finishTracking();\r\n        try (AbfsPerfInfo perfInfoSet = startTracking(\"setAcl\", \"setAcl\")) {\r\n            final AbfsRestOperation setAclOp = client.setAcl(relativePath, AbfsAclHelper.serializeAclSpec(aclEntries), eTag, tracingContext);\r\n            perfInfoSet.registerResult(setAclOp.getResult()).registerSuccess(true).registerAggregates(perfInfoGet.getTrackingStart(), GET_SET_AGGREGATE_COUNT);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "AclStatus getAclStatus(final Path path, TracingContext tracingContext) throws IOException\n{\r\n    if (!getIsNamespaceEnabled(tracingContext)) {\r\n        throw new UnsupportedOperationException(\"This operation is only valid for storage accounts with the hierarchical namespace enabled.\");\r\n    }\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"getAclStatus\", \"getAclStatus\")) {\r\n        LOG.debug(\"getAclStatus filesystem: {} path: {}\", client.getFileSystem(), path);\r\n        AbfsRestOperation op = client.getAclStatus(getRelativePath(path), tracingContext);\r\n        AbfsHttpOperation result = op.getResult();\r\n        perfInfo.registerResult(result);\r\n        final String transformedOwner = identityTransformer.transformIdentityForGetRequest(result.getResponseHeader(HttpHeaderConfigurations.X_MS_OWNER), true, userName);\r\n        final String transformedGroup = identityTransformer.transformIdentityForGetRequest(result.getResponseHeader(HttpHeaderConfigurations.X_MS_GROUP), false, primaryUserGroup);\r\n        final String permissions = result.getResponseHeader(HttpHeaderConfigurations.X_MS_PERMISSIONS);\r\n        final String aclSpecString = op.getResult().getResponseHeader(HttpHeaderConfigurations.X_MS_ACL);\r\n        final List<AclEntry> aclEntries = AclEntry.parseAclSpec(AbfsAclHelper.processAclString(aclSpecString), true);\r\n        identityTransformer.transformAclEntriesForGetRequest(aclEntries, userName, primaryUserGroup);\r\n        final FsPermission fsPermission = permissions == null ? new AbfsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL) : AbfsPermission.valueOf(permissions);\r\n        final AclStatus.Builder aclStatusBuilder = new AclStatus.Builder();\r\n        aclStatusBuilder.owner(transformedOwner);\r\n        aclStatusBuilder.group(transformedGroup);\r\n        aclStatusBuilder.setPermission(fsPermission);\r\n        aclStatusBuilder.stickyBit(fsPermission.getStickyBit());\r\n        aclStatusBuilder.addEntries(aclEntries);\r\n        perfInfo.registerSuccess(true);\r\n        return aclStatusBuilder.build();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "access",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void access(final Path path, final FsAction mode, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    LOG.debug(\"access for filesystem: {}, path: {}, mode: {}\", this.client.getFileSystem(), path, mode);\r\n    if (!this.abfsConfiguration.isCheckAccessEnabled() || !getIsNamespaceEnabled(tracingContext)) {\r\n        LOG.debug(\"Returning; either check access is not enabled or the account\" + \" used is not namespace enabled\");\r\n        return;\r\n    }\r\n    try (AbfsPerfInfo perfInfo = startTracking(\"access\", \"checkAccess\")) {\r\n        final AbfsRestOperation op = this.client.checkAccess(getRelativePath(path), mode.SYMBOL, tracingContext);\r\n        perfInfo.registerResult(op.getResult()).registerSuccess(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isAtomicRenameKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isAtomicRenameKey(String key)\n{\r\n    return isKeyForDirectorySet(key, azureAtomicRenameDirSet);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isInfiniteLeaseKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isInfiniteLeaseKey(String key)\n{\r\n    if (azureInfiniteLeaseDirSet.isEmpty()) {\r\n        return false;\r\n    }\r\n    return isKeyForDirectorySet(key, azureInfiniteLeaseDirSet);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "initializeClient",
  "errType" : [ "MalformedURLException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void initializeClient(URI uri, String fileSystemName, String accountName, boolean isSecure) throws IOException\n{\r\n    if (this.client != null) {\r\n        return;\r\n    }\r\n    final URIBuilder uriBuilder = getURIBuilder(accountName, isSecure);\r\n    final String url = uriBuilder.toString() + AbfsHttpConstants.FORWARD_SLASH + fileSystemName;\r\n    URL baseUrl;\r\n    try {\r\n        baseUrl = new URL(url);\r\n    } catch (MalformedURLException e) {\r\n        throw new InvalidUriException(uri.toString());\r\n    }\r\n    SharedKeyCredentials creds = null;\r\n    AccessTokenProvider tokenProvider = null;\r\n    SASTokenProvider sasTokenProvider = null;\r\n    if (authType == AuthType.OAuth) {\r\n        AzureADAuthenticator.init(abfsConfiguration);\r\n    }\r\n    if (authType == AuthType.SharedKey) {\r\n        LOG.trace(\"Fetching SharedKey credentials\");\r\n        int dotIndex = accountName.indexOf(AbfsHttpConstants.DOT);\r\n        if (dotIndex <= 0) {\r\n            throw new InvalidUriException(uri.toString() + \" - account name is not fully qualified.\");\r\n        }\r\n        creds = new SharedKeyCredentials(accountName.substring(0, dotIndex), abfsConfiguration.getStorageAccountKey());\r\n    } else if (authType == AuthType.SAS) {\r\n        LOG.trace(\"Fetching SAS token provider\");\r\n        sasTokenProvider = abfsConfiguration.getSASTokenProvider();\r\n    } else {\r\n        LOG.trace(\"Fetching token provider\");\r\n        tokenProvider = abfsConfiguration.getTokenProvider();\r\n        ExtensionHelper.bind(tokenProvider, uri, abfsConfiguration.getRawConfiguration());\r\n    }\r\n    LOG.trace(\"Initializing AbfsClient for {}\", baseUrl);\r\n    if (tokenProvider != null) {\r\n        this.client = new AbfsClient(baseUrl, creds, abfsConfiguration, tokenProvider, populateAbfsClientContext());\r\n    } else {\r\n        this.client = new AbfsClient(baseUrl, creds, abfsConfiguration, sasTokenProvider, populateAbfsClientContext());\r\n    }\r\n    LOG.trace(\"AbfsClient init complete\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "populateAbfsClientContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsClientContext populateAbfsClientContext()\n{\r\n    return new AbfsClientContextBuilder().withExponentialRetryPolicy(new ExponentialRetryPolicy(abfsConfiguration)).withAbfsCounters(abfsCounters).withAbfsPerfTracker(abfsPerfTracker).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getOctalNotation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getOctalNotation(FsPermission fsPermission)\n{\r\n    Preconditions.checkNotNull(fsPermission, \"fsPermission\");\r\n    return String.format(AbfsHttpConstants.PERMISSION_FORMAT, fsPermission.toOctal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRelativePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRelativePath(final Path path)\n{\r\n    Preconditions.checkNotNull(path, \"path\");\r\n    return path.toUri().getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "parseContentLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long parseContentLength(final String contentLength)\n{\r\n    if (contentLength == null) {\r\n        return -1;\r\n    }\r\n    return Long.parseLong(contentLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "parseIsDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean parseIsDirectory(final String resourceType)\n{\r\n    return resourceType != null && resourceType.equalsIgnoreCase(AbfsHttpConstants.DIRECTORY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "convertXmsPropertiesToCommaSeparatedString",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String, String> properties) throws CharacterCodingException\n{\r\n    StringBuilder commaSeparatedProperties = new StringBuilder();\r\n    final CharsetEncoder encoder = Charset.forName(XMS_PROPERTIES_ENCODING).newEncoder();\r\n    for (Map.Entry<String, String> propertyEntry : properties.entrySet()) {\r\n        String key = propertyEntry.getKey();\r\n        String value = propertyEntry.getValue();\r\n        Boolean canEncodeValue = encoder.canEncode(value);\r\n        if (!canEncodeValue) {\r\n            throw new CharacterCodingException();\r\n        }\r\n        String encodedPropertyValue = Base64.encode(encoder.encode(CharBuffer.wrap(value)).array());\r\n        commaSeparatedProperties.append(key).append(AbfsHttpConstants.EQUAL).append(encodedPropertyValue);\r\n        commaSeparatedProperties.append(AbfsHttpConstants.COMMA);\r\n    }\r\n    if (commaSeparatedProperties.length() != 0) {\r\n        commaSeparatedProperties.deleteCharAt(commaSeparatedProperties.length() - 1);\r\n    }\r\n    return commaSeparatedProperties.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "parseCommaSeparatedXmsProperties",
  "errType" : [ "CharacterCodingException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Hashtable<String, String> parseCommaSeparatedXmsProperties(String xMsProperties) throws InvalidFileSystemPropertyException, InvalidAbfsRestOperationException\n{\r\n    Hashtable<String, String> properties = new Hashtable<>();\r\n    final CharsetDecoder decoder = Charset.forName(XMS_PROPERTIES_ENCODING).newDecoder();\r\n    if (xMsProperties != null && !xMsProperties.isEmpty()) {\r\n        String[] userProperties = xMsProperties.split(AbfsHttpConstants.COMMA);\r\n        if (userProperties.length == 0) {\r\n            return properties;\r\n        }\r\n        for (String property : userProperties) {\r\n            if (property.isEmpty()) {\r\n                throw new InvalidFileSystemPropertyException(xMsProperties);\r\n            }\r\n            String[] nameValue = property.split(AbfsHttpConstants.EQUAL, 2);\r\n            if (nameValue.length != 2) {\r\n                throw new InvalidFileSystemPropertyException(xMsProperties);\r\n            }\r\n            byte[] decodedValue = Base64.decode(nameValue[1]);\r\n            final String value;\r\n            try {\r\n                value = decoder.decode(ByteBuffer.wrap(decodedValue)).toString();\r\n            } catch (CharacterCodingException ex) {\r\n                throw new InvalidAbfsRestOperationException(ex);\r\n            }\r\n            properties.put(nameValue[0], value);\r\n        }\r\n    }\r\n    return properties;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isKeyForDirectorySet",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean isKeyForDirectorySet(String key, Set<String> dirSet)\n{\r\n    for (String dir : dirSet) {\r\n        if (dir.isEmpty() || key.startsWith(dir + AbfsHttpConstants.FORWARD_SLASH)) {\r\n            return true;\r\n        }\r\n        try {\r\n            URI uri = new URI(dir);\r\n            if (null == uri.getAuthority()) {\r\n                if (key.startsWith(dir + \"/\")) {\r\n                    return true;\r\n                }\r\n            }\r\n        } catch (URISyntaxException e) {\r\n            LOG.info(\"URI syntax error creating URI for {}\", dir);\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "startTracking",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsPerfInfo startTracking(String callerName, String calleeName)\n{\r\n    return new AbfsPerfInfo(abfsPerfTracker, callerName, calleeName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsClient getClient()\n{\r\n    return this.client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setClient(AbfsClient client)\n{\r\n    this.client = client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setNamespaceEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamespaceEnabled(Trilean isNamespaceEnabled)\n{\r\n    this.isNamespaceEnabled = isNamespaceEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "updateInfiniteLeaseDirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateInfiniteLeaseDirs()\n{\r\n    this.azureInfiniteLeaseDirSet = new HashSet<>(Arrays.asList(abfsConfiguration.getAzureInfiniteLeaseDirs().split(AbfsHttpConstants.COMMA)));\r\n    this.azureInfiniteLeaseDirSet.remove(\"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "maybeCreateLease",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbfsLease maybeCreateLease(String relativePath, TracingContext tracingContext) throws AzureBlobFileSystemException\n{\r\n    boolean enableInfiniteLease = isInfiniteLeaseKey(relativePath);\r\n    if (!enableInfiniteLease) {\r\n        return null;\r\n    }\r\n    AbfsLease lease = new AbfsLease(client, relativePath, tracingContext);\r\n    leaseRefs.put(lease, null);\r\n    return lease;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "areLeasesFreed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean areLeasesFreed()\n{\r\n    for (AbfsLease lease : leaseRefs.keySet()) {\r\n        if (lease != null && !lease.isFreed()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "extractEtagHeader",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String extractEtagHeader(AbfsHttpOperation result)\n{\r\n    String etag = result.getResponseHeader(HttpHeaderConfigurations.ETAG);\r\n    if (etag != null) {\r\n        if (etag.startsWith(\"W/\\\"\")) {\r\n            etag = etag.substring(3);\r\n        } else if (etag.startsWith(\"\\\"\")) {\r\n            etag = etag.substring(1);\r\n        }\r\n        if (etag.endsWith(\"\\\"\")) {\r\n            etag = etag.substring(0, etag.length() - 1);\r\n        }\r\n    }\r\n    return etag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getAbfsHttpOperationWithFixedResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbfsHttpOperation getAbfsHttpOperationWithFixedResult(final URL url, final String method, final int httpStatus)\n{\r\n    AbfsHttpOperationWithFixedResult httpOp = new AbfsHttpOperationWithFixedResult(url, method, httpStatus);\r\n    return httpOp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getConnection",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HttpURLConnection getConnection()\n{\r\n    return connection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMethod",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMethod()\n{\r\n    return method;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getHost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHost()\n{\r\n    return url.getHost();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStatusCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getStatusCode()\n{\r\n    return statusCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStatusDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStatusDescription()\n{\r\n    return statusDescription;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStorageErrorCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStorageErrorCode()\n{\r\n    return storageErrorCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getStorageErrorMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStorageErrorMessage()\n{\r\n    return storageErrorMessage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getClientRequestId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getClientRequestId()\n{\r\n    return this.connection.getRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getExpectedAppendPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getExpectedAppendPos()\n{\r\n    return expectedAppendPos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getRequestId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRequestId()\n{\r\n    return requestId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setMaskForSAS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaskForSAS()\n{\r\n    shouldMask = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesSent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBytesSent()\n{\r\n    return bytesSent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBytesReceived",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesReceived()\n{\r\n    return bytesReceived;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getListResultSchema",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListResultSchema getListResultSchema()\n{\r\n    return listResultSchema;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getResponseHeader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getResponseHeader(String httpHeader)\n{\r\n    return connection.getHeaderField(httpHeader);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder();\r\n    sb.append(statusCode);\r\n    sb.append(\",\");\r\n    sb.append(storageErrorCode);\r\n    sb.append(\",\");\r\n    sb.append(expectedAppendPos);\r\n    sb.append(\",cid=\");\r\n    sb.append(getClientRequestId());\r\n    sb.append(\",rid=\");\r\n    sb.append(requestId);\r\n    if (isTraceEnabled) {\r\n        sb.append(\",connMs=\");\r\n        sb.append(connectionTimeMs);\r\n        sb.append(\",sendMs=\");\r\n        sb.append(sendRequestTimeMs);\r\n        sb.append(\",recvMs=\");\r\n        sb.append(recvResponseTimeMs);\r\n    }\r\n    sb.append(\",sent=\");\r\n    sb.append(bytesSent);\r\n    sb.append(\",recv=\");\r\n    sb.append(bytesReceived);\r\n    sb.append(\",\");\r\n    sb.append(method);\r\n    sb.append(\",\");\r\n    sb.append(getMaskedUrl());\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getLogString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getLogString()\n{\r\n    final StringBuilder sb = new StringBuilder();\r\n    sb.append(\"s=\").append(statusCode).append(\" e=\").append(storageErrorCode).append(\" ci=\").append(getClientRequestId()).append(\" ri=\").append(requestId);\r\n    if (isTraceEnabled) {\r\n        sb.append(\" ct=\").append(connectionTimeMs).append(\" st=\").append(sendRequestTimeMs).append(\" rt=\").append(recvResponseTimeMs);\r\n    }\r\n    sb.append(\" bs=\").append(bytesSent).append(\" br=\").append(bytesReceived).append(\" m=\").append(method).append(\" u=\").append(getMaskedEncodedUrl());\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMaskedUrl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getMaskedUrl()\n{\r\n    if (!shouldMask) {\r\n        return url.toString();\r\n    }\r\n    if (maskedUrl != null) {\r\n        return maskedUrl;\r\n    }\r\n    maskedUrl = UriUtils.getMaskedUrl(url);\r\n    return maskedUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getMaskedEncodedUrl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMaskedEncodedUrl()\n{\r\n    if (maskedEncodedUrl != null) {\r\n        return maskedEncodedUrl;\r\n    }\r\n    maskedEncodedUrl = UriUtils.encodedUrlStr(getMaskedUrl());\r\n    return maskedEncodedUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "sendRequest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void sendRequest(byte[] buffer, int offset, int length) throws IOException\n{\r\n    this.connection.setDoOutput(true);\r\n    this.connection.setFixedLengthStreamingMode(length);\r\n    if (buffer == null) {\r\n        buffer = new byte[] {};\r\n        offset = 0;\r\n        length = 0;\r\n    }\r\n    long startTime = 0;\r\n    if (this.isTraceEnabled) {\r\n        startTime = System.nanoTime();\r\n    }\r\n    try (OutputStream outputStream = this.connection.getOutputStream()) {\r\n        this.bytesSent = length;\r\n        outputStream.write(buffer, offset, length);\r\n    } finally {\r\n        if (this.isTraceEnabled) {\r\n            this.sendRequestTimeMs = elapsedTimeMs(startTime);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "processResponse",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void processResponse(final byte[] buffer, final int offset, final int length) throws IOException\n{\r\n    long startTime = 0;\r\n    if (this.isTraceEnabled) {\r\n        startTime = System.nanoTime();\r\n    }\r\n    this.statusCode = this.connection.getResponseCode();\r\n    if (this.isTraceEnabled) {\r\n        this.recvResponseTimeMs = elapsedTimeMs(startTime);\r\n    }\r\n    this.statusDescription = this.connection.getResponseMessage();\r\n    this.requestId = this.connection.getHeaderField(HttpHeaderConfigurations.X_MS_REQUEST_ID);\r\n    if (this.requestId == null) {\r\n        this.requestId = AbfsHttpConstants.EMPTY_STRING;\r\n    }\r\n    AbfsIoUtils.dumpHeadersToDebugLog(\"Response Headers\", connection.getHeaderFields());\r\n    if (AbfsHttpConstants.HTTP_METHOD_HEAD.equals(this.method)) {\r\n        return;\r\n    }\r\n    if (this.isTraceEnabled) {\r\n        startTime = System.nanoTime();\r\n    }\r\n    if (statusCode >= HttpURLConnection.HTTP_BAD_REQUEST) {\r\n        processStorageErrorResponse();\r\n        if (this.isTraceEnabled) {\r\n            this.recvResponseTimeMs += elapsedTimeMs(startTime);\r\n        }\r\n        this.bytesReceived = this.connection.getHeaderFieldLong(HttpHeaderConfigurations.CONTENT_LENGTH, 0);\r\n    } else {\r\n        int totalBytesRead = 0;\r\n        try (InputStream stream = this.connection.getInputStream()) {\r\n            if (isNullInputStream(stream)) {\r\n                return;\r\n            }\r\n            boolean endOfStream = false;\r\n            if (AbfsHttpConstants.HTTP_METHOD_GET.equals(this.method) && buffer == null) {\r\n                parseListFilesResponse(stream);\r\n            } else {\r\n                if (buffer != null) {\r\n                    while (totalBytesRead < length) {\r\n                        int bytesRead = stream.read(buffer, offset + totalBytesRead, length - totalBytesRead);\r\n                        if (bytesRead == -1) {\r\n                            endOfStream = true;\r\n                            break;\r\n                        }\r\n                        totalBytesRead += bytesRead;\r\n                    }\r\n                }\r\n                if (!endOfStream && stream.read() != -1) {\r\n                    int bytesRead = 0;\r\n                    byte[] b = new byte[CLEAN_UP_BUFFER_SIZE];\r\n                    while ((bytesRead = stream.read(b)) >= 0) {\r\n                        totalBytesRead += bytesRead;\r\n                    }\r\n                }\r\n            }\r\n        } catch (IOException ex) {\r\n            LOG.warn(\"IO/Network error: {} {}: {}\", method, getMaskedUrl(), ex.getMessage());\r\n            LOG.debug(\"IO Error: \", ex);\r\n            throw ex;\r\n        } finally {\r\n            if (this.isTraceEnabled) {\r\n                this.recvResponseTimeMs += elapsedTimeMs(startTime);\r\n            }\r\n            this.bytesReceived = totalBytesRead;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setRequestProperty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRequestProperty(String key, String value)\n{\r\n    this.connection.setRequestProperty(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "openConnection",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HttpURLConnection openConnection() throws IOException\n{\r\n    if (!isTraceEnabled) {\r\n        return (HttpURLConnection) url.openConnection();\r\n    }\r\n    long start = System.nanoTime();\r\n    try {\r\n        return (HttpURLConnection) url.openConnection();\r\n    } finally {\r\n        connectionTimeMs = elapsedTimeMs(start);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "processStorageErrorResponse",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void processStorageErrorResponse()\n{\r\n    try (InputStream stream = connection.getErrorStream()) {\r\n        if (stream == null) {\r\n            return;\r\n        }\r\n        JsonFactory jf = new JsonFactory();\r\n        try (JsonParser jp = jf.createParser(stream)) {\r\n            String fieldName, fieldValue;\r\n            jp.nextToken();\r\n            jp.nextToken();\r\n            jp.nextToken();\r\n            jp.nextToken();\r\n            while (jp.hasCurrentToken()) {\r\n                if (jp.getCurrentToken() == JsonToken.FIELD_NAME) {\r\n                    fieldName = jp.getCurrentName();\r\n                    jp.nextToken();\r\n                    fieldValue = jp.getText();\r\n                    switch(fieldName) {\r\n                        case \"code\":\r\n                            storageErrorCode = fieldValue;\r\n                            break;\r\n                        case \"message\":\r\n                            storageErrorMessage = fieldValue;\r\n                            break;\r\n                        case \"ExpectedAppendPos\":\r\n                            expectedAppendPos = fieldValue;\r\n                            break;\r\n                        default:\r\n                            break;\r\n                    }\r\n                }\r\n                jp.nextToken();\r\n            }\r\n        }\r\n    } catch (IOException ex) {\r\n        LOG.debug(\"ExpectedError: \", ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "elapsedTimeMs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long elapsedTimeMs(final long startTime)\n{\r\n    return (System.nanoTime() - startTime) / ONE_MILLION;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "parseListFilesResponse",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void parseListFilesResponse(final InputStream stream) throws IOException\n{\r\n    if (stream == null) {\r\n        return;\r\n    }\r\n    if (listResultSchema != null) {\r\n        return;\r\n    }\r\n    try {\r\n        final ObjectMapper objectMapper = new ObjectMapper();\r\n        this.listResultSchema = objectMapper.readValue(stream, ListResultSchema.class);\r\n    } catch (IOException ex) {\r\n        LOG.error(\"Unable to deserialize list results\", ex);\r\n        throw ex;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isNullInputStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isNullInputStream(InputStream stream)\n{\r\n    return stream == null ? true : false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getIsNamespaceEnabledAccount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Trilean getIsNamespaceEnabledAccount()\n{\r\n    return Trilean.getTrilean(isNamespaceEnabledAccount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAccountName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAccountName()\n{\r\n    return accountName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getClientCorrelationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClientCorrelationId()\n{\r\n    return clientCorrelationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "accountConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String accountConf(String key)\n{\r\n    return key + \".\" + accountName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String get(String key)\n{\r\n    return rawConfig.get(accountConf(key), rawConfig.get(key));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getString(String key, String defaultValue)\n{\r\n    return rawConfig.get(accountConf(key), rawConfig.get(key, defaultValue));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getBoolean",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getBoolean(String key, boolean defaultValue)\n{\r\n    return rawConfig.getBoolean(accountConf(key), rawConfig.getBoolean(key, defaultValue));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLong(String key, long defaultValue)\n{\r\n    return rawConfig.getLong(accountConf(key), rawConfig.getLong(key, defaultValue));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getPasswordString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getPasswordString(String key) throws IOException\n{\r\n    char[] passchars = rawConfig.getPassword(accountConf(key));\r\n    if (passchars == null) {\r\n        passchars = rawConfig.getPassword(key);\r\n    }\r\n    if (passchars != null) {\r\n        return new String(passchars);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMandatoryPasswordString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMandatoryPasswordString(String key) throws IOException\n{\r\n    String value = getPasswordString(key);\r\n    if (value == null) {\r\n        throw new ConfigurationPropertyNotFoundException(key);\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTokenProviderClass",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Class<? extends U> getTokenProviderClass(AuthType authType, String name, Class<? extends U> defaultValue, Class<U> xface)\n{\r\n    Class<?> tokenProviderClass = getAccountSpecificClass(name, defaultValue, xface);\r\n    if ((tokenProviderClass == null) && (authType == getAccountAgnosticEnum(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SharedKey))) {\r\n        tokenProviderClass = getAccountAgnosticClass(name, defaultValue, xface);\r\n    }\r\n    return (tokenProviderClass == null) ? null : tokenProviderClass.asSubclass(xface);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAccountSpecificClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends U> getAccountSpecificClass(String name, Class<? extends U> defaultValue, Class<U> xface)\n{\r\n    return rawConfig.getClass(accountConf(name), defaultValue, xface);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAccountAgnosticClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends U> getAccountAgnosticClass(String name, Class<? extends U> defaultValue, Class<U> xface)\n{\r\n    return rawConfig.getClass(name, defaultValue, xface);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getEnum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T getEnum(String name, T defaultValue)\n{\r\n    return rawConfig.getEnum(accountConf(name), rawConfig.getEnum(name, defaultValue));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAccountAgnosticEnum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T getAccountAgnosticEnum(String name, T defaultValue)\n{\r\n    return rawConfig.getEnum(name, defaultValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "unset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unset(String key)\n{\r\n    rawConfig.unset(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void set(String key, String value)\n{\r\n    rawConfig.set(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setBoolean",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setBoolean(String key, boolean value)\n{\r\n    rawConfig.setBoolean(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isSecureMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSecureMode()\n{\r\n    return isSecure;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getStorageAccountKey",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getStorageAccountKey() throws AzureBlobFileSystemException\n{\r\n    String key;\r\n    String keyProviderClass = get(AZURE_KEY_ACCOUNT_KEYPROVIDER);\r\n    KeyProvider keyProvider;\r\n    if (keyProviderClass == null) {\r\n        keyProvider = new SimpleKeyProvider();\r\n    } else {\r\n        Object keyProviderObject;\r\n        try {\r\n            Class<?> clazz = rawConfig.getClassByName(keyProviderClass);\r\n            keyProviderObject = clazz.newInstance();\r\n        } catch (Exception e) {\r\n            throw new KeyProviderException(\"Unable to load key provider class.\", e);\r\n        }\r\n        if (!(keyProviderObject instanceof KeyProvider)) {\r\n            throw new KeyProviderException(keyProviderClass + \" specified in config is not a valid KeyProvider class.\");\r\n        }\r\n        keyProvider = (KeyProvider) keyProviderObject;\r\n    }\r\n    key = keyProvider.getStorageAccountKey(accountName, rawConfig);\r\n    if (key == null) {\r\n        throw new ConfigurationPropertyNotFoundException(accountName);\r\n    }\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRawConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getRawConfiguration()\n{\r\n    return this.rawConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getWriteBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getWriteBufferSize()\n{\r\n    return this.writeBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isSmallWriteOptimizationEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSmallWriteOptimizationEnabled()\n{\r\n    return this.enableSmallWriteOptimization;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "readSmallFilesCompletely",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean readSmallFilesCompletely()\n{\r\n    return this.readSmallFilesCompletely;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "optimizeFooterRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean optimizeFooterRead()\n{\r\n    return this.optimizeFooterRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getReadBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadBufferSize()\n{\r\n    return this.readBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMinBackoffIntervalMilliseconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMinBackoffIntervalMilliseconds()\n{\r\n    return this.minBackoffInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMaxBackoffIntervalMilliseconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxBackoffIntervalMilliseconds()\n{\r\n    return this.maxBackoffInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getBackoffIntervalMilliseconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBackoffIntervalMilliseconds()\n{\r\n    return this.backoffInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMaxIoRetries",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxIoRetries()\n{\r\n    return this.maxIoRetries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getCustomTokenFetchRetryCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCustomTokenFetchRetryCount()\n{\r\n    return this.customTokenFetchRetryCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAzureBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAzureBlockSize()\n{\r\n    return this.azureBlockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isCheckAccessEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCheckAccessEnabled()\n{\r\n    return this.isCheckAccessEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getSasTokenRenewPeriodForStreamsInSeconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSasTokenRenewPeriodForStreamsInSeconds()\n{\r\n    return this.sasTokenRenewPeriodForStreamsInSeconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAzureBlockLocationHost",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAzureBlockLocationHost()\n{\r\n    return this.azureBlockLocationHost;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMaxConcurrentWriteThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxConcurrentWriteThreads()\n{\r\n    return this.maxConcurrentWriteThreads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMaxConcurrentReadThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxConcurrentReadThreads()\n{\r\n    return this.maxConcurrentReadThreads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getListMaxResults",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getListMaxResults()\n{\r\n    return this.listMaxResults;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTolerateOobAppends",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getTolerateOobAppends()\n{\r\n    return this.tolerateOobAppends;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAzureAtomicRenameDirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAzureAtomicRenameDirs()\n{\r\n    return this.azureAtomicDirs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isConditionalCreateOverwriteEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isConditionalCreateOverwriteEnabled()\n{\r\n    return this.enableConditionalCreateOverwrite;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isEnabledMkdirOverwrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnabledMkdirOverwrite()\n{\r\n    return mkdirOverwrite;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAppendBlobDirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAppendBlobDirs()\n{\r\n    return this.azureAppendBlobDirs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAzureInfiniteLeaseDirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAzureInfiniteLeaseDirs()\n{\r\n    return this.azureInfiniteLeaseDirs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getNumLeaseThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumLeaseThreads()\n{\r\n    return this.numLeaseThreads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getCreateRemoteFileSystemDuringInitialization",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getCreateRemoteFileSystemDuringInitialization()\n{\r\n    return this.createRemoteFileSystemDuringInitialization && this.getAuthType(this.accountName) != AuthType.SAS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getSkipUserGroupMetadataDuringInitialization",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getSkipUserGroupMetadataDuringInitialization()\n{\r\n    return this.skipUserGroupMetadataDuringInitialization;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getReadAheadQueueDepth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadQueueDepth()\n{\r\n    return this.readAheadQueueDepth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getReadAheadBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadBlockSize()\n{\r\n    return this.readAheadBlockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "shouldReadBufferSizeAlways",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldReadBufferSizeAlways()\n{\r\n    return this.alwaysReadBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isFlushEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFlushEnabled()\n{\r\n    return this.enableFlush;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isOutputStreamFlushDisabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isOutputStreamFlushDisabled()\n{\r\n    return this.disableOutputStreamFlush;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isAutoThrottlingEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isAutoThrottlingEnabled()\n{\r\n    return this.enableAutoThrottling;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getRateLimit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRateLimit()\n{\r\n    return rateLimit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getCustomUserAgentPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCustomUserAgentPrefix()\n{\r\n    return this.userAgentId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getClusterName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClusterName()\n{\r\n    return this.clusterName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getClusterType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClusterType()\n{\r\n    return this.clusterType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getPreferredSSLFactoryOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DelegatingSSLSocketFactory.SSLChannelMode getPreferredSSLFactoryOption()\n{\r\n    return getEnum(FS_AZURE_SSL_CHANNEL_MODE_KEY, DEFAULT_FS_AZURE_SSL_CHANNEL_MODE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTracingHeaderFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TracingHeaderFormat getTracingHeaderFormat()\n{\r\n    return getEnum(FS_AZURE_TRACINGHEADER_FORMAT, TracingHeaderFormat.ALL_ID_FORMAT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getAuthType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AuthType getAuthType(String accountName)\n{\r\n    return getEnum(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SharedKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isDelegationTokenManagerEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDelegationTokenManagerEnabled()\n{\r\n    return enableDelegationToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getDelegationTokenManager",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AbfsDelegationTokenManager getDelegationTokenManager() throws IOException\n{\r\n    return new AbfsDelegationTokenManager(getRawConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isHttpsAlwaysUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isHttpsAlwaysUsed()\n{\r\n    return this.alwaysUseHttps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "isUpnUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUpnUsed()\n{\r\n    return this.useUpn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "shouldTrackLatency",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldTrackLatency()\n{\r\n    return this.trackLatency;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTokenProvider",
  "errType" : [ "IllegalArgumentException", "Exception", "IllegalArgumentException", "Exception" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "AccessTokenProvider getTokenProvider() throws TokenAccessProviderException\n{\r\n    AuthType authType = getEnum(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SharedKey);\r\n    if (authType == AuthType.OAuth) {\r\n        try {\r\n            Class<? extends AccessTokenProvider> tokenProviderClass = getTokenProviderClass(authType, FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME, null, AccessTokenProvider.class);\r\n            AccessTokenProvider tokenProvider;\r\n            if (tokenProviderClass == ClientCredsTokenProvider.class) {\r\n                String authEndpoint = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT);\r\n                String clientId = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID);\r\n                String clientSecret = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_SECRET);\r\n                tokenProvider = new ClientCredsTokenProvider(authEndpoint, clientId, clientSecret);\r\n                LOG.trace(\"ClientCredsTokenProvider initialized\");\r\n            } else if (tokenProviderClass == UserPasswordTokenProvider.class) {\r\n                String authEndpoint = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT);\r\n                String username = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_USER_NAME);\r\n                String password = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_USER_PASSWORD);\r\n                tokenProvider = new UserPasswordTokenProvider(authEndpoint, username, password);\r\n                LOG.trace(\"UserPasswordTokenProvider initialized\");\r\n            } else if (tokenProviderClass == MsiTokenProvider.class) {\r\n                String authEndpoint = getTrimmedPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_ENDPOINT);\r\n                String tenantGuid = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_TENANT);\r\n                String clientId = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID);\r\n                String authority = getTrimmedPasswordString(FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_MSI_AUTHORITY);\r\n                authority = appendSlashIfNeeded(authority);\r\n                tokenProvider = new MsiTokenProvider(authEndpoint, tenantGuid, clientId, authority);\r\n                LOG.trace(\"MsiTokenProvider initialized\");\r\n            } else if (tokenProviderClass == RefreshTokenBasedTokenProvider.class) {\r\n                String authEndpoint = getTrimmedPasswordString(FS_AZURE_ACCOUNT_OAUTH_REFRESH_TOKEN_ENDPOINT, AuthConfigurations.DEFAULT_FS_AZURE_ACCOUNT_OAUTH_REFRESH_TOKEN_ENDPOINT);\r\n                String refreshToken = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_REFRESH_TOKEN);\r\n                String clientId = getMandatoryPasswordString(FS_AZURE_ACCOUNT_OAUTH_CLIENT_ID);\r\n                tokenProvider = new RefreshTokenBasedTokenProvider(authEndpoint, clientId, refreshToken);\r\n                LOG.trace(\"RefreshTokenBasedTokenProvider initialized\");\r\n            } else {\r\n                throw new IllegalArgumentException(\"Failed to initialize \" + tokenProviderClass);\r\n            }\r\n            return tokenProvider;\r\n        } catch (IllegalArgumentException e) {\r\n            throw e;\r\n        } catch (Exception e) {\r\n            throw new TokenAccessProviderException(\"Unable to load OAuth token provider class.\", e);\r\n        }\r\n    } else if (authType == AuthType.Custom) {\r\n        try {\r\n            String configKey = FS_AZURE_ACCOUNT_TOKEN_PROVIDER_TYPE_PROPERTY_NAME;\r\n            Class<? extends CustomTokenProviderAdaptee> customTokenProviderClass = getTokenProviderClass(authType, configKey, null, CustomTokenProviderAdaptee.class);\r\n            if (customTokenProviderClass == null) {\r\n                throw new IllegalArgumentException(String.format(\"The configuration value for \\\"%s\\\" is invalid.\", configKey));\r\n            }\r\n            CustomTokenProviderAdaptee azureTokenProvider = ReflectionUtils.newInstance(customTokenProviderClass, rawConfig);\r\n            if (azureTokenProvider == null) {\r\n                throw new IllegalArgumentException(\"Failed to initialize \" + customTokenProviderClass);\r\n            }\r\n            LOG.trace(\"Initializing {}\", customTokenProviderClass.getName());\r\n            azureTokenProvider.initialize(rawConfig, accountName);\r\n            LOG.trace(\"{} init complete\", customTokenProviderClass.getName());\r\n            return new CustomTokenProviderAdapter(azureTokenProvider, getCustomTokenFetchRetryCount());\r\n        } catch (IllegalArgumentException e) {\r\n            throw e;\r\n        } catch (Exception e) {\r\n            throw new TokenAccessProviderException(\"Unable to load custom token provider class: \" + e, e);\r\n        }\r\n    } else {\r\n        throw new TokenAccessProviderException(String.format(\"Invalid auth type: %s is being used, expecting OAuth\", authType));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getSASTokenProvider",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "SASTokenProvider getSASTokenProvider() throws AzureBlobFileSystemException\n{\r\n    AuthType authType = getEnum(FS_AZURE_ACCOUNT_AUTH_TYPE_PROPERTY_NAME, AuthType.SharedKey);\r\n    if (authType != AuthType.SAS) {\r\n        throw new SASTokenProviderException(String.format(\"Invalid auth type: %s is being used, expecting SAS\", authType));\r\n    }\r\n    try {\r\n        String configKey = FS_AZURE_SAS_TOKEN_PROVIDER_TYPE;\r\n        Class<? extends SASTokenProvider> sasTokenProviderClass = getTokenProviderClass(authType, configKey, null, SASTokenProvider.class);\r\n        Preconditions.checkArgument(sasTokenProviderClass != null, String.format(\"The configuration value for \\\"%s\\\" is invalid.\", configKey));\r\n        SASTokenProvider sasTokenProvider = ReflectionUtils.newInstance(sasTokenProviderClass, rawConfig);\r\n        Preconditions.checkArgument(sasTokenProvider != null, String.format(\"Failed to initialize %s\", sasTokenProviderClass));\r\n        LOG.trace(\"Initializing {}\", sasTokenProviderClass.getName());\r\n        sasTokenProvider.initialize(rawConfig, accountName);\r\n        LOG.trace(\"{} init complete\", sasTokenProviderClass.getName());\r\n        return sasTokenProvider;\r\n    } catch (Exception e) {\r\n        throw new TokenAccessProviderException(\"Unable to load SAS token provider class: \" + e, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getReadAheadRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadRange()\n{\r\n    return this.readAheadRange;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateInt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int validateInt(Field field) throws IllegalAccessException, InvalidConfigurationValueException\n{\r\n    IntegerConfigurationValidatorAnnotation validator = field.getAnnotation(IntegerConfigurationValidatorAnnotation.class);\r\n    String value = get(validator.ConfigurationKey());\r\n    return new IntegerConfigurationBasicValidator(validator.MinValue(), validator.MaxValue(), validator.DefaultValue(), validator.ConfigurationKey(), validator.ThrowIfInvalid()).validate(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateIntWithOutlier",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int validateIntWithOutlier(Field field) throws IllegalAccessException, InvalidConfigurationValueException\n{\r\n    IntegerWithOutlierConfigurationValidatorAnnotation validator = field.getAnnotation(IntegerWithOutlierConfigurationValidatorAnnotation.class);\r\n    String value = get(validator.ConfigurationKey());\r\n    return new IntegerConfigurationBasicValidator(validator.OutlierValue(), validator.MinValue(), validator.MaxValue(), validator.DefaultValue(), validator.ConfigurationKey(), validator.ThrowIfInvalid()).validate(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateLong",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long validateLong(Field field) throws IllegalAccessException, InvalidConfigurationValueException\n{\r\n    LongConfigurationValidatorAnnotation validator = field.getAnnotation(LongConfigurationValidatorAnnotation.class);\r\n    String value = rawConfig.get(validator.ConfigurationKey());\r\n    return new LongConfigurationBasicValidator(validator.MinValue(), validator.MaxValue(), validator.DefaultValue(), validator.ConfigurationKey(), validator.ThrowIfInvalid()).validate(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String validateString(Field field) throws IllegalAccessException, InvalidConfigurationValueException\n{\r\n    StringConfigurationValidatorAnnotation validator = field.getAnnotation(StringConfigurationValidatorAnnotation.class);\r\n    String value = rawConfig.get(validator.ConfigurationKey());\r\n    return new StringConfigurationBasicValidator(validator.ConfigurationKey(), validator.DefaultValue(), validator.ThrowIfInvalid()).validate(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateBase64String",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String validateBase64String(Field field) throws IllegalAccessException, InvalidConfigurationValueException\n{\r\n    Base64StringConfigurationValidatorAnnotation validator = field.getAnnotation((Base64StringConfigurationValidatorAnnotation.class));\r\n    String value = rawConfig.get(validator.ConfigurationKey());\r\n    return new Base64StringConfigurationBasicValidator(validator.ConfigurationKey(), validator.DefaultValue(), validator.ThrowIfInvalid()).validate(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "validateBoolean",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean validateBoolean(Field field) throws IllegalAccessException, InvalidConfigurationValueException\n{\r\n    BooleanConfigurationValidatorAnnotation validator = field.getAnnotation(BooleanConfigurationValidatorAnnotation.class);\r\n    String value = rawConfig.get(validator.ConfigurationKey());\r\n    return new BooleanConfigurationBasicValidator(validator.ConfigurationKey(), validator.DefaultValue(), validator.ThrowIfInvalid()).validate(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getOauthTokenFetchRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExponentialRetryPolicy getOauthTokenFetchRetryPolicy()\n{\r\n    return new ExponentialRetryPolicy(oauthTokenFetchRetryCount, oauthTokenFetchRetryMinBackoff, oauthTokenFetchRetryMaxBackoff, oauthTokenFetchRetryDeltaBackoff);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getWriteMaxConcurrentRequestCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getWriteMaxConcurrentRequestCount()\n{\r\n    if (this.writeMaxConcurrentRequestCount < 1) {\r\n        return 4 * Runtime.getRuntime().availableProcessors();\r\n    }\r\n    return this.writeMaxConcurrentRequestCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getMaxWriteRequestsToQueue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxWriteRequestsToQueue()\n{\r\n    if (this.maxWriteRequestsToQueue < 1) {\r\n        return 2 * getWriteMaxConcurrentRequestCount();\r\n    }\r\n    return this.maxWriteRequestsToQueue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "enableAbfsListIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean enableAbfsListIterator()\n{\r\n    return this.enableAbfsListIterator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getClientProvidedEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getClientProvidedEncryptionKey()\n{\r\n    String accSpecEncKey = accountConf(FS_AZURE_CLIENT_PROVIDED_ENCRYPTION_KEY);\r\n    return rawConfig.get(accSpecEncKey, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setReadBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReadBufferSize(int bufferSize)\n{\r\n    this.readBufferSize = bufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setWriteBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWriteBufferSize(int bufferSize)\n{\r\n    this.writeBufferSize = bufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setEnableFlush",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEnableFlush(boolean enableFlush)\n{\r\n    this.enableFlush = enableFlush;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setDisableOutputStreamFlush",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisableOutputStreamFlush(boolean disableOutputStreamFlush)\n{\r\n    this.disableOutputStreamFlush = disableOutputStreamFlush;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setListMaxResults",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setListMaxResults(int listMaxResults)\n{\r\n    this.listMaxResults = listMaxResults;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setMaxIoRetries",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaxIoRetries(int maxIoRetries)\n{\r\n    this.maxIoRetries = maxIoRetries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setMaxBackoffIntervalMilliseconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaxBackoffIntervalMilliseconds(int maxBackoffInterval)\n{\r\n    this.maxBackoffInterval = maxBackoffInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setIsNamespaceEnabledAccount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIsNamespaceEnabledAccount(String isNamespaceEnabledAccount)\n{\r\n    this.isNamespaceEnabledAccount = isNamespaceEnabledAccount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "getTrimmedPasswordString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getTrimmedPasswordString(String key, String defaultValue) throws IOException\n{\r\n    String value = getPasswordString(key);\r\n    if (StringUtils.isBlank(value)) {\r\n        value = defaultValue;\r\n    }\r\n    return value.trim();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "appendSlashIfNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String appendSlashIfNeeded(String authority)\n{\r\n    if (!authority.endsWith(AbfsHttpConstants.FORWARD_SLASH)) {\r\n        authority = authority + AbfsHttpConstants.FORWARD_SLASH;\r\n    }\r\n    return authority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setReadSmallFilesCompletely",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReadSmallFilesCompletely(boolean readSmallFilesCompletely)\n{\r\n    this.readSmallFilesCompletely = readSmallFilesCompletely;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setOptimizeFooterRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOptimizeFooterRead(boolean optimizeFooterRead)\n{\r\n    this.optimizeFooterRead = optimizeFooterRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs",
  "methodName" : "setEnableAbfsListIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEnableAbfsListIterator(boolean enableAbfsListIterator)\n{\r\n    this.enableAbfsListIterator = enableAbfsListIterator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\security",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return TOKEN_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\security",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return FileSystemUriSchemes.ABFS_SECURE_SCHEME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBufferManager",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ReadBufferManager getBufferManager()\n{\r\n    if (bufferManager == null) {\r\n        LOCK.lock();\r\n        try {\r\n            if (bufferManager == null) {\r\n                bufferManager = new ReadBufferManager();\r\n                bufferManager.init();\r\n            }\r\n        } finally {\r\n            LOCK.unlock();\r\n        }\r\n    }\r\n    return bufferManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setReadBufferManagerConfigs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadBufferManagerConfigs(int readAheadBlockSize)\n{\r\n    if (bufferManager == null) {\r\n        LOGGER.debug(\"ReadBufferManager not initialized yet. Overriding readAheadBlockSize as {}\", readAheadBlockSize);\r\n        blockSize = readAheadBlockSize;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void init()\n{\r\n    buffers = new byte[NUM_BUFFERS][];\r\n    for (int i = 0; i < NUM_BUFFERS; i++) {\r\n        buffers[i] = new byte[blockSize];\r\n        freeList.add(i);\r\n    }\r\n    for (int i = 0; i < NUM_THREADS; i++) {\r\n        Thread t = new Thread(new ReadBufferWorker(i));\r\n        t.setDaemon(true);\r\n        threads[i] = t;\r\n        t.setName(\"ABFS-prefetch-\" + i);\r\n        t.start();\r\n    }\r\n    ReadBufferWorker.UNLEASH_WORKERS.countDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "queueReadAhead",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength, TracingContext tracingContext)\n{\r\n    if (LOGGER.isTraceEnabled()) {\r\n        LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\", stream.getPath(), requestedOffset, requestedLength);\r\n    }\r\n    ReadBuffer buffer;\r\n    synchronized (this) {\r\n        if (isAlreadyQueued(stream, requestedOffset)) {\r\n            return;\r\n        }\r\n        if (freeList.isEmpty() && !tryEvict()) {\r\n            return;\r\n        }\r\n        buffer = new ReadBuffer();\r\n        buffer.setStream(stream);\r\n        buffer.setOffset(requestedOffset);\r\n        buffer.setLength(0);\r\n        buffer.setRequestedLength(requestedLength);\r\n        buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\r\n        buffer.setLatch(new CountDownLatch(1));\r\n        buffer.setTracingContext(tracingContext);\r\n        Integer bufferIndex = freeList.pop();\r\n        buffer.setBuffer(buffers[bufferIndex]);\r\n        buffer.setBufferindex(bufferIndex);\r\n        readAheadQueue.add(buffer);\r\n        notifyAll();\r\n        if (LOGGER.isTraceEnabled()) {\r\n            LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\", stream.getPath(), requestedOffset, buffer.getBufferindex());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) throws IOException\n{\r\n    if (LOGGER.isTraceEnabled()) {\r\n        LOGGER.trace(\"getBlock for file {}  position {}  thread {}\", stream.getPath(), position, Thread.currentThread().getName());\r\n    }\r\n    waitForProcess(stream, position);\r\n    int bytesRead = 0;\r\n    synchronized (this) {\r\n        bytesRead = getBlockFromCompletedQueue(stream, position, length, buffer);\r\n    }\r\n    if (bytesRead > 0) {\r\n        if (LOGGER.isTraceEnabled()) {\r\n            LOGGER.trace(\"Done read from Cache for {} position {} length {}\", stream.getPath(), position, bytesRead);\r\n        }\r\n        return bytesRead;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "waitForProcess",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void waitForProcess(final AbfsInputStream stream, final long position)\n{\r\n    ReadBuffer readBuf;\r\n    synchronized (this) {\r\n        clearFromReadAheadQueue(stream, position);\r\n        readBuf = getFromList(inProgressList, stream, position);\r\n    }\r\n    if (readBuf != null) {\r\n        try {\r\n            if (LOGGER.isTraceEnabled()) {\r\n                LOGGER.trace(\"got a relevant read buffer for file {} offset {} buffer idx {}\", stream.getPath(), readBuf.getOffset(), readBuf.getBufferindex());\r\n            }\r\n            readBuf.getLatch().await();\r\n        } catch (InterruptedException ex) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n        if (LOGGER.isTraceEnabled()) {\r\n            LOGGER.trace(\"latch done for file {} buffer idx {} length {}\", stream.getPath(), readBuf.getBufferindex(), readBuf.getLength());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "tryEvict",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "boolean tryEvict()\n{\r\n    ReadBuffer nodeToEvict = null;\r\n    if (completedReadList.size() <= 0) {\r\n        return false;\r\n    }\r\n    long currentTimeInMs = currentTimeMillis();\r\n    for (ReadBuffer buf : completedReadList) {\r\n        if (buf.isFirstByteConsumed() && buf.isLastByteConsumed()) {\r\n            nodeToEvict = buf;\r\n            break;\r\n        }\r\n    }\r\n    if (nodeToEvict != null) {\r\n        return evict(nodeToEvict);\r\n    }\r\n    for (ReadBuffer buf : completedReadList) {\r\n        if (buf.isAnyByteConsumed()) {\r\n            nodeToEvict = buf;\r\n            break;\r\n        }\r\n    }\r\n    if (nodeToEvict != null) {\r\n        return evict(nodeToEvict);\r\n    }\r\n    long earliestBirthday = Long.MAX_VALUE;\r\n    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\r\n    for (ReadBuffer buf : completedReadList) {\r\n        if ((buf.getBufferindex() != -1) && (buf.getTimeStamp() < earliestBirthday)) {\r\n            nodeToEvict = buf;\r\n            earliestBirthday = buf.getTimeStamp();\r\n        } else if ((buf.getBufferindex() == -1) && (currentTimeInMs - buf.getTimeStamp()) > thresholdAgeMilliseconds) {\r\n            oldFailedBuffers.add(buf);\r\n        }\r\n    }\r\n    for (ReadBuffer buf : oldFailedBuffers) {\r\n        evict(buf);\r\n    }\r\n    if ((currentTimeInMs - earliestBirthday > thresholdAgeMilliseconds) && (nodeToEvict != null)) {\r\n        return evict(nodeToEvict);\r\n    }\r\n    LOGGER.trace(\"No buffer eligible for eviction\");\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "evict",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean evict(final ReadBuffer buf)\n{\r\n    if (buf.getBufferindex() != -1) {\r\n        freeList.push(buf.getBufferindex());\r\n    }\r\n    completedReadList.remove(buf);\r\n    buf.setTracingContext(null);\r\n    if (LOGGER.isTraceEnabled()) {\r\n        LOGGER.trace(\"Evicting buffer idx {}; was used for file {} offset {} length {}\", buf.getBufferindex(), buf.getStream().getPath(), buf.getOffset(), buf.getLength());\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isAlreadyQueued",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isAlreadyQueued(final AbfsInputStream stream, final long requestedOffset)\n{\r\n    return (isInList(readAheadQueue, stream, requestedOffset) || isInList(inProgressList, stream, requestedOffset) || isInList(completedReadList, stream, requestedOffset));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "isInList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isInList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset)\n{\r\n    return (getFromList(list, stream, requestedOffset) != null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFromList",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset)\n{\r\n    for (ReadBuffer buffer : list) {\r\n        if (buffer.getStream() == stream) {\r\n            if (buffer.getStatus() == ReadBufferStatus.AVAILABLE && requestedOffset >= buffer.getOffset() && requestedOffset < buffer.getOffset() + buffer.getLength()) {\r\n                return buffer;\r\n            } else if (requestedOffset >= buffer.getOffset() && requestedOffset < buffer.getOffset() + buffer.getRequestedLength()) {\r\n                return buffer;\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBufferFromCompletedQueue",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ReadBuffer getBufferFromCompletedQueue(final AbfsInputStream stream, final long requestedOffset)\n{\r\n    for (ReadBuffer buffer : completedReadList) {\r\n        if ((buffer.getStream() == stream) && (requestedOffset >= buffer.getOffset()) && ((requestedOffset < buffer.getOffset() + buffer.getLength()) || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\r\n            return buffer;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "clearFromReadAheadQueue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void clearFromReadAheadQueue(final AbfsInputStream stream, final long requestedOffset)\n{\r\n    ReadBuffer buffer = getFromList(readAheadQueue, stream, requestedOffset);\r\n    if (buffer != null) {\r\n        readAheadQueue.remove(buffer);\r\n        notifyAll();\r\n        freeList.push(buffer.getBufferindex());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getBlockFromCompletedQueue",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) throws IOException\n{\r\n    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\r\n    if (buf == null) {\r\n        return 0;\r\n    }\r\n    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\r\n        if ((currentTimeMillis() - (buf.getTimeStamp()) < thresholdAgeMilliseconds)) {\r\n            throw buf.getErrException();\r\n        } else {\r\n            return 0;\r\n        }\r\n    }\r\n    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE) || (position >= buf.getOffset() + buf.getLength())) {\r\n        return 0;\r\n    }\r\n    int cursor = (int) (position - buf.getOffset());\r\n    int availableLengthInBuffer = buf.getLength() - cursor;\r\n    int lengthToCopy = Math.min(length, availableLengthInBuffer);\r\n    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\r\n    if (cursor == 0) {\r\n        buf.setFirstByteConsumed(true);\r\n    }\r\n    if (cursor + lengthToCopy == buf.getLength()) {\r\n        buf.setLastByteConsumed(true);\r\n    }\r\n    buf.setAnyByteConsumed(true);\r\n    return lengthToCopy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getNextBlockToRead",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ReadBuffer getNextBlockToRead() throws InterruptedException\n{\r\n    ReadBuffer buffer = null;\r\n    synchronized (this) {\r\n        while (readAheadQueue.size() == 0) {\r\n            wait();\r\n        }\r\n        buffer = readAheadQueue.remove();\r\n        notifyAll();\r\n        if (buffer == null) {\r\n            return null;\r\n        }\r\n        buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\r\n        inProgressList.add(buffer);\r\n    }\r\n    if (LOGGER.isTraceEnabled()) {\r\n        LOGGER.trace(\"ReadBufferWorker picked file {} for offset {}\", buffer.getStream().getPath(), buffer.getOffset());\r\n    }\r\n    return buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "doneReading",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead)\n{\r\n    if (LOGGER.isTraceEnabled()) {\r\n        LOGGER.trace(\"ReadBufferWorker completed read file {} for offset {} outcome {} bytes {}\", buffer.getStream().getPath(), buffer.getOffset(), result, bytesActuallyRead);\r\n    }\r\n    synchronized (this) {\r\n        if (inProgressList.contains(buffer)) {\r\n            inProgressList.remove(buffer);\r\n            if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\r\n                buffer.setStatus(ReadBufferStatus.AVAILABLE);\r\n                buffer.setLength(bytesActuallyRead);\r\n            } else {\r\n                freeList.push(buffer.getBufferindex());\r\n            }\r\n            buffer.setStatus(result);\r\n            buffer.setTimeStamp(currentTimeMillis());\r\n            completedReadList.add(buffer);\r\n        }\r\n    }\r\n    buffer.getLatch().countDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "currentTimeMillis",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long currentTimeMillis()\n{\r\n    return System.nanoTime() / 1000 / 1000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getThresholdAgeMilliseconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getThresholdAgeMilliseconds()\n{\r\n    return thresholdAgeMilliseconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setThresholdAgeMilliseconds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setThresholdAgeMilliseconds(int thresholdAgeMs)\n{\r\n    thresholdAgeMilliseconds = thresholdAgeMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getCompletedReadListSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCompletedReadListSize()\n{\r\n    return completedReadList.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getCompletedReadListCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ReadBuffer> getCompletedReadListCopy()\n{\r\n    return new ArrayList<>(completedReadList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getFreeListCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Integer> getFreeListCopy()\n{\r\n    return new ArrayList<>(freeList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadQueueCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ReadBuffer> getReadAheadQueueCopy()\n{\r\n    return new ArrayList<>(readAheadQueue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getInProgressCopiedList",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ReadBuffer> getInProgressCopiedList()\n{\r\n    return new ArrayList<>(inProgressList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "callTryEvict",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void callTryEvict()\n{\r\n    tryEvict();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "purgeBuffersForStream",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void purgeBuffersForStream(AbfsInputStream stream)\n{\r\n    LOGGER.debug(\"Purging stale buffers for AbfsInputStream {} \", stream);\r\n    readAheadQueue.removeIf(readBuffer -> readBuffer.getStream() == stream);\r\n    purgeList(stream, completedReadList);\r\n    purgeList(stream, inProgressList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "purgeList",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list)\n{\r\n    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext(); ) {\r\n        ReadBuffer readBuffer = it.next();\r\n        if (readBuffer.getStream() == stream) {\r\n            it.remove();\r\n            if (readBuffer.getBufferindex() != -1) {\r\n                freeList.push(readBuffer.getBufferindex());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testResetReadBufferManager",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testResetReadBufferManager()\n{\r\n    synchronized (this) {\r\n        ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\r\n        for (ReadBuffer buf : completedReadList) {\r\n            if (buf != null) {\r\n                completedBuffers.add(buf);\r\n            }\r\n        }\r\n        for (ReadBuffer buf : completedBuffers) {\r\n            evict(buf);\r\n        }\r\n        readAheadQueue.clear();\r\n        inProgressList.clear();\r\n        completedReadList.clear();\r\n        freeList.clear();\r\n        for (int i = 0; i < NUM_BUFFERS; i++) {\r\n            buffers[i] = null;\r\n        }\r\n        buffers = null;\r\n        resetBufferManager();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "resetBufferManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetBufferManager()\n{\r\n    bufferManager = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testResetReadBufferManager",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds)\n{\r\n    setBlockSize(readAheadBlockSize);\r\n    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\r\n    testResetReadBufferManager();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "setBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlockSize(int readAheadBlockSize)\n{\r\n    blockSize = readAheadBlockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "getReadAheadBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReadAheadBlockSize()\n{\r\n    return blockSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\services",
  "methodName" : "testMimicFullUseAndAddFailedBuffer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMimicFullUseAndAddFailedBuffer(ReadBuffer buf)\n{\r\n    freeList.clear();\r\n    completedReadList.add(buf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "refreshToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AzureADToken refreshToken() throws IOException\n{\r\n    LOG.debug(\"AADToken: refreshing user-password based token\");\r\n    return AzureADAuthenticator.getTokenUsingClientCreds(authEndpoint, username, password);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "refreshToken",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "AzureADToken refreshToken() throws IOException\n{\r\n    LOG.debug(\"AADToken: refreshing custom based token\");\r\n    AzureADToken azureADToken = new AzureADToken();\r\n    String accessToken = null;\r\n    Exception ex;\r\n    boolean succeeded = false;\r\n    int retryCount = fetchTokenRetryCount;\r\n    do {\r\n        ex = null;\r\n        try {\r\n            accessToken = adaptee.getAccessToken();\r\n            LOG.trace(\"CustomTokenProvider Access token fetch was successful with retry count {}\", (fetchTokenRetryCount - retryCount));\r\n        } catch (Exception e) {\r\n            LOG.debug(\"CustomTokenProvider Access token fetch failed with retry count {}\", (fetchTokenRetryCount - retryCount));\r\n            ex = e;\r\n        }\r\n        succeeded = (ex == null);\r\n        retryCount--;\r\n    } while (!succeeded && (retryCount) >= 0);\r\n    if (!succeeded) {\r\n        HttpException httpEx = new HttpException(-1, \"\", String.format(\"CustomTokenProvider getAccessToken threw %s : %s\", ex.getClass().getTypeName(), ex.getMessage()), \"\", \"\", \"\");\r\n        throw httpEx;\r\n    }\r\n    azureADToken.setAccessToken(accessToken);\r\n    azureADToken.setExpiry(adaptee.getExpiryTime());\r\n    return azureADToken;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "bind",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void bind(final URI fsURI, final Configuration conf) throws IOException\n{\r\n    ExtensionHelper.bind(adaptee, fsURI, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    ExtensionHelper.close(adaptee);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "getUserAgentSuffix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUserAgentSuffix()\n{\r\n    String suffix = ExtensionHelper.getUserAgentSuffix(adaptee, \"\");\r\n    return suffix != null ? suffix : \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "fileSystemStarted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fileSystemStarted()\n{\r\n    if (numFileSystems == 0) {\r\n        instance = new MetricsSystemImpl();\r\n        instance.init(\"azure-file-system\");\r\n    }\r\n    numFileSystems++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "fileSystemClosed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void fileSystemClosed()\n{\r\n    if (numFileSystems == 1) {\r\n        instance.publishMetricsNow();\r\n        instance.stop();\r\n        instance.shutdown();\r\n        instance = null;\r\n    }\r\n    numFileSystems--;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "registerSource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerSource(String name, String desc, MetricsSource source)\n{\r\n    instance.register(name, desc, source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azure\\metrics",
  "methodName" : "unregisterSource",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void unregisterSource(String name)\n{\r\n    if (instance != null) {\r\n        instance.publishMetricsNow();\r\n        instance.unregisterSource(name);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(String name, String value)\n{\r\n    params.put(name, value);\r\n    serializedString = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "setApiVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setApiVersion(String apiVersion)\n{\r\n    this.apiVersion = apiVersion;\r\n    serializedString = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-azure\\src\\main\\java\\org\\apache\\hadoop\\fs\\azurebfs\\oauth2",
  "methodName" : "serialize",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String serialize()\n{\r\n    if (serializedString == null) {\r\n        StringBuilder sb = new StringBuilder();\r\n        for (Map.Entry<String, String> entry : params.entrySet()) {\r\n            String name = entry.getKey();\r\n            try {\r\n                sb.append(separator);\r\n                sb.append(URLEncoder.encode(name, \"UTF-8\"));\r\n                sb.append('=');\r\n                sb.append(URLEncoder.encode(entry.getValue(), \"UTF-8\"));\r\n                separator = \"&\";\r\n            } catch (UnsupportedEncodingException ex) {\r\n            }\r\n        }\r\n        if (apiVersion != null) {\r\n            sb.append(separator);\r\n            sb.append(\"api-version=\");\r\n            sb.append(apiVersion);\r\n            separator = \"&\";\r\n        }\r\n        serializedString = sb.toString();\r\n    }\r\n    return serializedString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
} ]
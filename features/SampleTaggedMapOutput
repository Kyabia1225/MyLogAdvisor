[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "getData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Writable getData()\n{\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    this.tag.write(out);\r\n    this.data.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.tag.readFields(in);\r\n    this.data.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "testDataJoin",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testDataJoin() throws Exception\n{\r\n    final int srcs = 4;\r\n    JobConf job = new JobConf();\r\n    job.setBoolean(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", false);\r\n    Path base = cluster.getFileSystem().makeQualified(new Path(\"/inner\"));\r\n    Path[] src = writeSimpleSrc(base, job, srcs);\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    Path outdir = new Path(base, \"out\");\r\n    FileOutputFormat.setOutputPath(job, outdir);\r\n    job.setMapperClass(SampleDataJoinMapper.class);\r\n    job.setReducerClass(SampleDataJoinReducer.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(SampleTaggedMapOutput.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setOutputFormat(TextOutputFormat.class);\r\n    job.setNumMapTasks(1);\r\n    job.setNumReduceTasks(1);\r\n    FileInputFormat.setInputPaths(job, src);\r\n    try {\r\n        JobClient.runJob(job);\r\n        confirmOutput(outdir, job, srcs);\r\n    } finally {\r\n        base.getFileSystem(job).delete(base, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "confirmOutput",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void confirmOutput(Path out, JobConf job, int srcs) throws IOException\n{\r\n    FileSystem fs = out.getFileSystem(job);\r\n    FileStatus[] outlist = fs.listStatus(out);\r\n    assertEquals(1, outlist.length);\r\n    assertTrue(0 < outlist[0].getLen());\r\n    FSDataInputStream in = fs.open(outlist[0].getPath());\r\n    LineRecordReader rr = new LineRecordReader(in, 0, Integer.MAX_VALUE, job);\r\n    LongWritable k = new LongWritable();\r\n    Text v = new Text();\r\n    int count = 0;\r\n    while (rr.next(k, v)) {\r\n        String[] vals = v.toString().split(\"\\t\");\r\n        assertEquals(srcs + 1, vals.length);\r\n        int[] ivals = new int[vals.length];\r\n        for (int i = 0; i < vals.length; ++i) ivals[i] = Integer.parseInt(vals[i]);\r\n        assertEquals(0, ivals[0] % (srcs * srcs));\r\n        for (int i = 1; i < vals.length; ++i) {\r\n            assertEquals((ivals[i] - (i - 1)) * srcs, 10 * ivals[0]);\r\n        }\r\n        ++count;\r\n    }\r\n    assertEquals(4, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "createWriters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SequenceFile.Writer[] createWriters(Path testdir, JobConf conf, int srcs, Path[] src) throws IOException\n{\r\n    for (int i = 0; i < srcs; ++i) {\r\n        src[i] = new Path(testdir, Integer.toString(i + 10, 36));\r\n    }\r\n    SequenceFile.Writer[] out = new SequenceFile.Writer[srcs];\r\n    for (int i = 0; i < srcs; ++i) {\r\n        out[i] = new SequenceFile.Writer(testdir.getFileSystem(conf), conf, src[i], Text.class, Text.class);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "writeSimpleSrc",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path[] writeSimpleSrc(Path testdir, JobConf conf, int srcs) throws IOException\n{\r\n    SequenceFile.Writer[] out = null;\r\n    Path[] src = new Path[srcs];\r\n    try {\r\n        out = createWriters(testdir, conf, srcs, src);\r\n        final int capacity = srcs * 2 + 1;\r\n        Text key = new Text();\r\n        key.set(\"ignored\");\r\n        Text val = new Text();\r\n        for (int k = 0; k < capacity; ++k) {\r\n            for (int i = 0; i < srcs; ++i) {\r\n                val.set(Integer.toString(k % srcs == 0 ? k * srcs : k * srcs + i) + \"\\t\" + Integer.toString(10 * k + i));\r\n                out[i].append(key, val);\r\n                if (i == k) {\r\n                    out[i].append(key, val);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (out != null) {\r\n            for (int i = 0; i < srcs; ++i) {\r\n                if (out[i] != null)\r\n                    out[i].close();\r\n            }\r\n        }\r\n    }\r\n    return src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaggedMapOutput combine(Object[] tags, Object[] values)\n{\r\n    if (tags.length < 2)\r\n        return null;\r\n    String joinedStr = \"\";\r\n    for (int i = 0; i < tags.length; i++) {\r\n        if (i > 0)\r\n            joinedStr += \"\\t\";\r\n        String line = ((Text) (((TaggedMapOutput) values[i]).getData())).toString();\r\n        String[] tokens = line.split(\"\\\\t\", 2);\r\n        joinedStr += tokens[1];\r\n    }\r\n    TaggedMapOutput retv = new SampleTaggedMapOutput(new Text(joinedStr));\r\n    retv.setTag((Text) tags[0]);\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "generateInputTag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text generateInputTag(String inputFile)\n{\r\n    return new Text(inputFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "generateGroupKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Text generateGroupKey(TaggedMapOutput aRecord)\n{\r\n    String line = ((Text) aRecord.getData()).toString();\r\n    String groupKey = \"\";\r\n    String[] tokens = line.split(\"\\\\t\", 2);\r\n    groupKey = tokens[0];\r\n    return new Text(groupKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-datajoin\\src\\test\\java\\org\\apache\\hadoop\\contrib\\utils\\join",
  "methodName" : "generateTaggedMapOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaggedMapOutput generateTaggedMapOutput(Object value)\n{\r\n    TaggedMapOutput retv = new SampleTaggedMapOutput((Text) value);\r\n    retv.setTag(new Text(this.inputTag));\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
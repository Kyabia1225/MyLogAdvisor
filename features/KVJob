[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean runJob() throws Exception\n{\r\n    return job.waitForCompletion(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "createSequenceTestFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createSequenceTestFile(String filepath) throws Exception\n{\r\n    int FULL_BYTE_SPACE = 256;\r\n    createSequenceTestFile(filepath, FULL_BYTE_SPACE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "createSequenceTestFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createSequenceTestFile(String filepath, int base) throws Exception\n{\r\n    createSequenceTestFile(filepath, base, (byte) 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "createSequenceTestFile",
  "errType" : [ "ClassNotFoundException", "ClassNotFoundException", "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void createSequenceTestFile(String filepath, int base, byte start) throws Exception\n{\r\n    LOG.info(\"creating file \" + filepath + \"(\" + filesize + \" bytes)\");\r\n    LOG.info(keyClsName + \" \" + valueClsName);\r\n    Class<?> tmpkeycls, tmpvaluecls;\r\n    try {\r\n        tmpkeycls = Class.forName(keyClsName);\r\n    } catch (final ClassNotFoundException e) {\r\n        throw new Exception(\"key class not found: \", e);\r\n    }\r\n    try {\r\n        tmpvaluecls = Class.forName(valueClsName);\r\n    } catch (final ClassNotFoundException e) {\r\n        throw new Exception(\"key class not found: \", e);\r\n    }\r\n    try {\r\n        final Path outputfilepath = new Path(filepath);\r\n        final ScenarioConfiguration conf = new ScenarioConfiguration();\r\n        writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(outputfilepath), SequenceFile.Writer.keyClass(tmpkeycls), SequenceFile.Writer.valueClass(tmpvaluecls));\r\n    } catch (final Exception e) {\r\n        e.printStackTrace();\r\n    }\r\n    int tmpfilesize = this.filesize;\r\n    while (tmpfilesize > DATABUFSIZE) {\r\n        nextRandomBytes(databuf, base, start);\r\n        final int size = flushBuf(DATABUFSIZE);\r\n        tmpfilesize -= size;\r\n    }\r\n    nextRandomBytes(databuf, base, start);\r\n    flushBuf(tmpfilesize);\r\n    if (writer != null) {\r\n        IOUtils.closeStream(writer);\r\n    } else {\r\n        throw new Exception(\"no writer to create sequenceTestFile!\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "nextRandomBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nextRandomBytes(byte[] buf, int base)\n{\r\n    nextRandomBytes(buf, base, (byte) 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "nextRandomBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void nextRandomBytes(byte[] buf, int base, byte start)\n{\r\n    r.nextBytes(buf);\r\n    for (int i = 0; i < buf.length; i++) {\r\n        buf[i] = (byte) ((buf[i] & 0xFF) % base + start);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "flushBuf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "int flushBuf(int buflen) throws Exception\n{\r\n    final Random r = new Random();\r\n    int keybytesnum = 0;\r\n    int valuebytesnum = 0;\r\n    int offset = 0;\r\n    Writable keyWritable = BytesFactory.newObject(null, keyClsName);\r\n    Writable valWritable = BytesFactory.newObject(null, valueClsName);\r\n    while (offset < buflen) {\r\n        final int remains = buflen - offset;\r\n        keybytesnum = keyMaxBytesNum;\r\n        if (keyMaxBytesNum != keyMinBytesNum) {\r\n            keybytesnum = keyMinBytesNum + r.nextInt(keyMaxBytesNum - keyMinBytesNum);\r\n        }\r\n        valuebytesnum = valueMaxBytesNum;\r\n        if (valueMaxBytesNum != valueMinBytesNum) {\r\n            valuebytesnum = valueMinBytesNum + r.nextInt(valueMaxBytesNum - valueMinBytesNum);\r\n        }\r\n        if (keybytesnum + valuebytesnum > remains) {\r\n            break;\r\n        }\r\n        final byte[] key = new byte[keybytesnum];\r\n        final byte[] value = new byte[valuebytesnum];\r\n        System.arraycopy(databuf, offset, key, 0, keybytesnum);\r\n        offset += keybytesnum;\r\n        System.arraycopy(databuf, offset, value, 0, valuebytesnum);\r\n        offset += valuebytesnum;\r\n        BytesFactory.updateObject(keyWritable, key);\r\n        BytesFactory.updateObject(valWritable, value);\r\n        try {\r\n            writer.append(keyWritable, valWritable);\r\n        } catch (final IOException e) {\r\n            e.printStackTrace();\r\n            throw new Exception(\"sequence file create failed\", e);\r\n        }\r\n    }\r\n    return offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "startUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startUp() throws Exception\n{\r\n    Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());\r\n    Assume.assumeTrue(NativeRuntime.isNativeLibraryLoaded());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "testLargeValueCombiner",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testLargeValueCombiner() throws Exception\n{\r\n    final Configuration normalConf = ScenarioConfiguration.getNormalConfiguration();\r\n    final Configuration nativeConf = ScenarioConfiguration.getNativeConfiguration();\r\n    normalConf.addResource(TestConstants.COMBINER_CONF_PATH);\r\n    nativeConf.addResource(TestConstants.COMBINER_CONF_PATH);\r\n    final int deafult_KVSize_Maximum = 1 << 22;\r\n    final int KVSize_Maximum = normalConf.getInt(TestConstants.NATIVETASK_KVSIZE_MAX_LARGEKV_TEST, deafult_KVSize_Maximum);\r\n    final String inputPath = TestConstants.NATIVETASK_COMBINER_TEST_INPUTDIR + \"/largeKV\";\r\n    final String nativeOutputPath = TestConstants.NATIVETASK_COMBINER_TEST_NATIVE_OUTPUTDIR + \"/nativeLargeKV\";\r\n    final String hadoopOutputPath = TestConstants.NATIVETASK_COMBINER_TEST_NORMAL_OUTPUTDIR + \"/normalLargeKV\";\r\n    final FileSystem fs = FileSystem.get(normalConf);\r\n    for (int i = 65536; i <= KVSize_Maximum; i *= 4) {\r\n        int max = i;\r\n        int min = Math.max(i / 4, max - 10);\r\n        LOG.info(\"===KV Size Test: min size: \" + min + \", max size: \" + max);\r\n        normalConf.set(TestConstants.NATIVETASK_KVSIZE_MIN, String.valueOf(min));\r\n        normalConf.set(TestConstants.NATIVETASK_KVSIZE_MAX, String.valueOf(max));\r\n        nativeConf.set(TestConstants.NATIVETASK_KVSIZE_MIN, String.valueOf(min));\r\n        nativeConf.set(TestConstants.NATIVETASK_KVSIZE_MAX, String.valueOf(max));\r\n        fs.delete(new Path(inputPath), true);\r\n        new TestInputFile(normalConf.getInt(TestConstants.NATIVETASK_COMBINER_WORDCOUNT_FILESIZE, 1000000), IntWritable.class.getName(), Text.class.getName(), normalConf).createSequenceTestFile(inputPath, 1);\r\n        final Job normaljob = CombinerTest.getJob(\"normalwordcount\", normalConf, inputPath, hadoopOutputPath);\r\n        final Job nativejob = CombinerTest.getJob(\"nativewordcount\", nativeConf, inputPath, nativeOutputPath);\r\n        assertThat(nativejob.waitForCompletion(true)).isTrue();\r\n        assertThat(normaljob.waitForCompletion(true)).isTrue();\r\n        final boolean compareRet = ResultVerifier.verify(nativeOutputPath, hadoopOutputPath);\r\n        final String reason = \"LargeKVCombinerTest failed with, min size: \" + min + \", max size: \" + max + \", normal out: \" + hadoopOutputPath + \", native Out: \" + nativeOutputPath;\r\n        assertThat(compareRet).withFailMessage(reason).isTrue();\r\n        ResultVerifier.verifyCounters(normaljob, nativejob, true);\r\n    }\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new ScenarioConfiguration());\r\n    fs.delete(new Path(TestConstants.NATIVETASK_COMBINER_TEST_DIR), true);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "updateObject",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void updateObject(Writable obj, byte[] seed)\n{\r\n    if (obj instanceof IntWritable) {\r\n        ((IntWritable) obj).set(Ints.fromByteArray(seed));\r\n    } else if (obj instanceof FloatWritable) {\r\n        ((FloatWritable) obj).set(r.nextFloat());\r\n    } else if (obj instanceof DoubleWritable) {\r\n        ((DoubleWritable) obj).set(r.nextDouble());\r\n    } else if (obj instanceof LongWritable) {\r\n        ((LongWritable) obj).set(Longs.fromByteArray(seed));\r\n    } else if (obj instanceof VIntWritable) {\r\n        ((VIntWritable) obj).set(Ints.fromByteArray(seed));\r\n    } else if (obj instanceof VLongWritable) {\r\n        ((VLongWritable) obj).set(Longs.fromByteArray(seed));\r\n    } else if (obj instanceof BooleanWritable) {\r\n        ((BooleanWritable) obj).set(seed[0] % 2 == 1 ? true : false);\r\n    } else if (obj instanceof Text) {\r\n        ((Text) obj).set(BytesUtil.toStringBinary(seed));\r\n    } else if (obj instanceof ByteWritable) {\r\n        ((ByteWritable) obj).set(seed.length > 0 ? seed[0] : 0);\r\n    } else if (obj instanceof BytesWritable) {\r\n        ((BytesWritable) obj).set(seed, 0, seed.length);\r\n    } else if (obj instanceof UTF8) {\r\n        ((UTF8) obj).set(BytesUtil.toStringBinary(seed));\r\n    } else if (obj instanceof MockValueClass) {\r\n        ((MockValueClass) obj).set(seed);\r\n    } else {\r\n        throw new IllegalArgumentException(\"unknown writable: \" + obj.getClass().getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "newObject",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Writable newObject(byte[] seed, String className)\n{\r\n    Writable ret;\r\n    try {\r\n        Class<?> clazz = Class.forName(className);\r\n        Preconditions.checkArgument(Writable.class.isAssignableFrom(clazz));\r\n        ret = (Writable) clazz.newInstance();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    if (seed != null) {\r\n        updateObject(ret, seed);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "fromBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] fromBytes(byte[] bytes) throws Exception\n{\r\n    throw new Exception(\"Not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "byte[] toBytes(VTYPE obj)\n{\r\n    final String className = obj.getClass().getName();\r\n    if (className.equals(IntWritable.class.getName())) {\r\n        return Ints.toByteArray(((IntWritable) obj).get());\r\n    } else if (className.equals(FloatWritable.class.getName())) {\r\n        return BytesUtil.toBytes(((FloatWritable) obj).get());\r\n    } else if (className.equals(DoubleWritable.class.getName())) {\r\n        return BytesUtil.toBytes(((DoubleWritable) obj).get());\r\n    } else if (className.equals(LongWritable.class.getName())) {\r\n        return Longs.toByteArray(((LongWritable) obj).get());\r\n    } else if (className.equals(VIntWritable.class.getName())) {\r\n        return Ints.toByteArray(((VIntWritable) obj).get());\r\n    } else if (className.equals(VLongWritable.class.getName())) {\r\n        return Longs.toByteArray(((VLongWritable) obj).get());\r\n    } else if (className.equals(BooleanWritable.class.getName())) {\r\n        return BytesUtil.toBytes(((BooleanWritable) obj).get());\r\n    } else if (className.equals(Text.class.getName())) {\r\n        return ((Text) obj).copyBytes();\r\n    } else if (className.equals(ByteWritable.class.getName())) {\r\n        return Ints.toByteArray((int) ((ByteWritable) obj).get());\r\n    } else if (className.equals(BytesWritable.class.getName())) {\r\n        return ((BytesWritable) obj).getBytes();\r\n    } else {\r\n        return new byte[0];\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "testWordCountCombiner",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testWordCountCombiner() throws Exception\n{\r\n    final Configuration nativeConf = ScenarioConfiguration.getNativeConfiguration();\r\n    nativeConf.addResource(TestConstants.COMBINER_CONF_PATH);\r\n    final Job nativejob = getJob(\"nativewordcount\", nativeConf, inputpath, nativeoutputpath);\r\n    final Configuration commonConf = ScenarioConfiguration.getNormalConfiguration();\r\n    commonConf.addResource(TestConstants.COMBINER_CONF_PATH);\r\n    final Job normaljob = getJob(\"normalwordcount\", commonConf, inputpath, hadoopoutputpath);\r\n    assertThat(nativejob.waitForCompletion(true)).isTrue();\r\n    assertThat(normaljob.waitForCompletion(true)).isTrue();\r\n    assertThat(ResultVerifier.verify(nativeoutputpath, hadoopoutputpath)).isTrue();\r\n    ResultVerifier.verifyCounters(normaljob, nativejob, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "startUp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void startUp() throws Exception\n{\r\n    Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());\r\n    Assume.assumeTrue(NativeRuntime.isNativeLibraryLoaded());\r\n    final ScenarioConfiguration conf = new ScenarioConfiguration();\r\n    conf.addcombinerConf();\r\n    this.fs = FileSystem.get(conf);\r\n    this.inputpath = TestConstants.NATIVETASK_COMBINER_TEST_INPUTDIR + \"/wordcount\";\r\n    if (!fs.exists(new Path(inputpath))) {\r\n        new TestInputFile(conf.getInt(TestConstants.NATIVETASK_COMBINER_WORDCOUNT_FILESIZE, 1000000), Text.class.getName(), Text.class.getName(), conf).createSequenceTestFile(inputpath, 1, (byte) ('a'));\r\n    }\r\n    this.nativeoutputpath = TestConstants.NATIVETASK_COMBINER_TEST_NATIVE_OUTPUTDIR + \"/nativewordcount\";\r\n    this.hadoopoutputpath = TestConstants.NATIVETASK_COMBINER_TEST_NORMAL_OUTPUTDIR + \"/normalwordcount\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new ScenarioConfiguration());\r\n    fs.delete(new Path(TestConstants.NATIVETASK_COMBINER_TEST_DIR), true);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Job getJob(String jobname, Configuration inputConf, String inputpath, String outputpath) throws Exception\n{\r\n    final Configuration conf = new Configuration(inputConf);\r\n    conf.set(\"fileoutputpath\", outputpath);\r\n    final FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(new Path(outputpath))) {\r\n        fs.delete(new Path(outputpath), true);\r\n    }\r\n    fs.close();\r\n    final Job job = Job.getInstance(conf, jobname);\r\n    job.setJarByClass(WordCount.class);\r\n    job.setMapperClass(TokenizerMapper.class);\r\n    job.setCombinerClass(IntSumReducer.class);\r\n    job.setReducerClass(IntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.setInputFormatClass(SequenceFileInputFormat.class);\r\n    FileInputFormat.addInputPath(job, new Path(inputpath));\r\n    FileOutputFormat.setOutputPath(job, new Path(outputpath));\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\utils",
  "methodName" : "testBytesIntConversion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBytesIntConversion()\n{\r\n    final int a = 1000;\r\n    final byte[] intBytes = Ints.toByteArray(a);\r\n    Assert.assertEquals(a, BytesUtil.toInt(intBytes, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\utils",
  "methodName" : "testBytesLongConversion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBytesLongConversion()\n{\r\n    final long l = 1000000L;\r\n    final byte[] longBytes = Longs.toByteArray(l);\r\n    Assert.assertEquals(l, BytesUtil.toLong(longBytes, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\utils",
  "methodName" : "testBytesFloatConversion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBytesFloatConversion()\n{\r\n    final float f = 3.14f;\r\n    final byte[] floatBytes = BytesUtil.toBytes(f);\r\n    Assert.assertEquals(f, BytesUtil.toFloat(floatBytes), 0.0f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\utils",
  "methodName" : "testBytesDoubleConversion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBytesDoubleConversion()\n{\r\n    final double d = 3.14;\r\n    final byte[] doubleBytes = BytesUtil.toBytes(d);\r\n    Assert.assertEquals(d, BytesUtil.toDouble(doubleBytes), 0.0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\utils",
  "methodName" : "testToStringBinary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testToStringBinary()\n{\r\n    Assert.assertEquals(\"\\\\x01\\\\x02ABC\", BytesUtil.toStringBinary(new byte[] { 1, 2, 65, 66, 67 }));\r\n    Assert.assertEquals(\"\\\\x10\\\\x11\", BytesUtil.toStringBinary(new byte[] { 16, 17 }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "init",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init(Context context) throws IOException, ClassNotFoundException\n{\r\n    try {\r\n        super.init(context);\r\n        nativetaskloaded = true;\r\n    } catch (final Exception e) {\r\n        nativetaskloaded = false;\r\n        LOG.error(\"load nativetask lib failed, Native-Task Delegation is disabled\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "collect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void collect(K key, V value, int partition) throws IOException, InterruptedException\n{\r\n    if (this.nativetaskloaded) {\r\n        super.collect(key, value, partition);\r\n    } else {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\compresstest",
  "methodName" : "testSnappyCompress",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSnappyCompress() throws Exception\n{\r\n    final String snappyCodec = \"org.apache.hadoop.io.compress.SnappyCodec\";\r\n    nativeConf.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, snappyCodec);\r\n    final String nativeOutputPath = TestConstants.NATIVETASK_COMPRESS_TEST_NATIVE_OUTPUTDIR + \"/snappy\";\r\n    final Job job = CompressMapper.getCompressJob(\"nativesnappy\", nativeConf, TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR, nativeOutputPath);\r\n    assertThat(job.waitForCompletion(true)).isTrue();\r\n    hadoopConf.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, snappyCodec);\r\n    final String hadoopOutputPath = TestConstants.NATIVETASK_COMPRESS_TEST_NORMAL_OUTPUTDIR + \"/snappy\";\r\n    final Job hadoopjob = CompressMapper.getCompressJob(\"hadoopsnappy\", hadoopConf, TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR, hadoopOutputPath);\r\n    assertThat(hadoopjob.waitForCompletion(true)).isTrue();\r\n    final boolean compareRet = ResultVerifier.verify(nativeOutputPath, hadoopOutputPath);\r\n    assertThat(compareRet).withFailMessage(\"file compare result: if they are the same ,then return true\").isTrue();\r\n    ResultVerifier.verifyCounters(hadoopjob, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\compresstest",
  "methodName" : "testGzipCompress",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGzipCompress() throws Exception\n{\r\n    final String gzipCodec = \"org.apache.hadoop.io.compress.GzipCodec\";\r\n    nativeConf.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, gzipCodec);\r\n    final String nativeOutputPath = TestConstants.NATIVETASK_COMPRESS_TEST_NATIVE_OUTPUTDIR + \"/gzip\";\r\n    final Job job = CompressMapper.getCompressJob(\"nativegzip\", nativeConf, TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR, nativeOutputPath);\r\n    assertThat(job.waitForCompletion(true)).isTrue();\r\n    hadoopConf.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, gzipCodec);\r\n    final String hadoopOutputPath = TestConstants.NATIVETASK_COMPRESS_TEST_NORMAL_OUTPUTDIR + \"/gzip\";\r\n    final Job hadoopjob = CompressMapper.getCompressJob(\"hadoopgzip\", hadoopConf, TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR, hadoopOutputPath);\r\n    assertThat(hadoopjob.waitForCompletion(true)).isTrue();\r\n    final boolean compareRet = ResultVerifier.verify(nativeOutputPath, hadoopOutputPath);\r\n    assertThat(compareRet).withFailMessage(\"file compare result: if they are the same ,then return true\").isTrue();\r\n    ResultVerifier.verifyCounters(hadoopjob, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\compresstest",
  "methodName" : "testLz4Compress",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLz4Compress() throws Exception\n{\r\n    final String lz4Codec = \"org.apache.hadoop.io.compress.Lz4Codec\";\r\n    nativeConf.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, lz4Codec);\r\n    final String nativeOutputPath = TestConstants.NATIVETASK_COMPRESS_TEST_NATIVE_OUTPUTDIR + \"/lz4\";\r\n    final Job nativeJob = CompressMapper.getCompressJob(\"nativelz4\", nativeConf, TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR, nativeOutputPath);\r\n    assertThat(nativeJob.waitForCompletion(true)).isTrue();\r\n    hadoopConf.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, lz4Codec);\r\n    final String hadoopOutputPath = TestConstants.NATIVETASK_COMPRESS_TEST_NORMAL_OUTPUTDIR + \"/lz4\";\r\n    final Job hadoopJob = CompressMapper.getCompressJob(\"hadooplz4\", hadoopConf, TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR, hadoopOutputPath);\r\n    assertThat(hadoopJob.waitForCompletion(true)).isTrue();\r\n    final boolean compareRet = ResultVerifier.verify(nativeOutputPath, hadoopOutputPath);\r\n    assertThat(compareRet).withFailMessage(\"file compare result: if they are the same ,then return true\").isTrue();\r\n    ResultVerifier.verifyCounters(hadoopJob, nativeJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\compresstest",
  "methodName" : "startUp",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void startUp() throws Exception\n{\r\n    Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());\r\n    Assume.assumeTrue(NativeRuntime.isNativeLibraryLoaded());\r\n    final ScenarioConfiguration conf = new ScenarioConfiguration();\r\n    final FileSystem fs = FileSystem.get(conf);\r\n    final Path path = new Path(TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR);\r\n    fs.delete(path, true);\r\n    if (!fs.exists(path)) {\r\n        new TestInputFile(hadoopConf.getInt(TestConstants.NATIVETASK_COMPRESS_FILESIZE, 100000), Text.class.getName(), Text.class.getName(), conf).createSequenceTestFile(TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR);\r\n    }\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\compresstest",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new ScenarioConfiguration());\r\n    fs.delete(new Path(TestConstants.NATIVETASK_COMPRESS_TEST_DIR), true);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testInputBuffer",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testInputBuffer() throws IOException\n{\r\n    final int size = 100;\r\n    final InputBuffer input1 = new InputBuffer(BufferType.DIRECT_BUFFER, size);\r\n    assertThat(input1.getType()).isEqualTo(BufferType.DIRECT_BUFFER);\r\n    assertThat(input1.position()).isZero();\r\n    assertThat(input1.length()).isZero();\r\n    assertThat(input1.remaining()).isZero();\r\n    assertThat(input1.capacity()).isEqualTo(size);\r\n    final InputBuffer input2 = new InputBuffer(BufferType.HEAP_BUFFER, size);\r\n    assertThat(input2.getType()).isEqualTo(BufferType.HEAP_BUFFER);\r\n    assertThat(input2.position()).isZero();\r\n    assertThat(input2.length()).isZero();\r\n    assertThat(input2.remaining()).isZero();\r\n    assertThat(input2.capacity()).isEqualTo(size);\r\n    final InputBuffer input3 = new InputBuffer(new byte[size]);\r\n    assertThat(input3.getType()).isEqualTo(BufferType.HEAP_BUFFER);\r\n    assertThat(input3.position()).isZero();\r\n    assertThat(input3.length()).isZero();\r\n    assertThat(input3.remaining()).isZero();\r\n    assertThat(input3.capacity()).isEqualTo(size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "testRegisterAndGet",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRegisterAndGet() throws IOException\n{\r\n    final NativeSerialization serialization = NativeSerialization.getInstance();\r\n    serialization.reset();\r\n    serialization.register(WritableKey.class.getName(), ComparableKeySerializer.class);\r\n    INativeSerializer serializer = serialization.getSerializer(WritableKey.class);\r\n    Assert.assertEquals(ComparableKeySerializer.class.getName(), serializer.getClass().getName());\r\n    serializer = serialization.getSerializer(WritableValue.class);\r\n    Assert.assertEquals(DefaultSerializer.class.getName(), serializer.getClass().getName());\r\n    boolean ioExceptionThrown = false;\r\n    try {\r\n        serializer = serialization.getSerializer(NonWritableValue.class);\r\n    } catch (final IOException e) {\r\n        ioExceptionThrown = true;\r\n    }\r\n    Assert.assertTrue(ioExceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "getMapInputs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KV[] getMapInputs(int size)\n{\r\n    final KV[] dataInput = new KV[size];\r\n    for (int i = 0; i < size; i++) {\r\n        dataInput[i] = getSingleMapInput(i);\r\n    }\r\n    return dataInput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "getSingleMapInput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KV getSingleMapInput(int i)\n{\r\n    final char character = CHAR_SET[i % CHAR_SET.length];\r\n    final byte b = (byte) character;\r\n    final byte[] bytes = new byte[i];\r\n    Arrays.fill(bytes, b);\r\n    final BytesWritable result = new BytesWritable(bytes);\r\n    final KV kv = new KV();\r\n    kv.key = result;\r\n    kv.value = result;\r\n    return kv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testReadWrite",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testReadWrite() throws IOException\n{\r\n    byte[] buff = new byte[10000];\r\n    InputBuffer input = new InputBuffer(buff);\r\n    MockDataTarget target = new MockDataTarget(buff);\r\n    ByteBufferDataWriter writer = new ByteBufferDataWriter(target);\r\n    writer.write(1);\r\n    writer.write(new byte[] { 2, 2 }, 0, 2);\r\n    writer.writeBoolean(true);\r\n    writer.writeByte(4);\r\n    writer.writeShort(5);\r\n    writer.writeChar(6);\r\n    writer.writeInt(7);\r\n    writer.writeLong(8);\r\n    writer.writeFloat(9);\r\n    writer.writeDouble(10);\r\n    writer.writeBytes(\"goodboy\");\r\n    writer.writeChars(\"hello\");\r\n    writer.writeUTF(\"native task\");\r\n    int length = target.getOutputBuffer().length();\r\n    input.rewind(0, length);\r\n    ByteBufferDataReader reader = new ByteBufferDataReader(input);\r\n    assertThat(reader.read()).isOne();\r\n    byte[] two = new byte[2];\r\n    reader.read(two);\r\n    assertThat(two[0]).isEqualTo(two[1]);\r\n    assertThat(two[0]).isEqualTo((byte) 2);\r\n    assertThat(reader.readBoolean()).isTrue();\r\n    assertThat(reader.readByte()).isEqualTo((byte) 4);\r\n    assertThat(reader.readShort()).isEqualTo((short) 5);\r\n    assertThat(reader.readChar()).isEqualTo((char) 6);\r\n    assertThat(reader.readInt()).isEqualTo(7);\r\n    assertThat(reader.readLong()).isEqualTo(8);\r\n    assertThat(reader.readFloat()).isEqualTo(9f, offset(0.0001f));\r\n    assertThat(reader.readDouble()).isEqualTo(10, offset(0.0001));\r\n    byte[] goodboy = new byte[\"goodboy\".length()];\r\n    reader.read(goodboy);\r\n    assertThat(toString(goodboy)).isEqualTo(\"goodboy\");\r\n    char[] hello = new char[\"hello\".length()];\r\n    for (int i = 0; i < hello.length; i++) {\r\n        hello[i] = reader.readChar();\r\n    }\r\n    String helloString = new String(hello);\r\n    assertThat(helloString).isEqualTo(\"hello\");\r\n    assertThat(reader.readUTF()).isEqualTo(\"native task\");\r\n    assertThat(input.remaining()).isZero();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testCatFace",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCatFace() throws IOException\n{\r\n    byte[] buff = new byte[10];\r\n    MockDataTarget target = new MockDataTarget(buff);\r\n    ByteBufferDataWriter writer = new ByteBufferDataWriter(target);\r\n    String catFace = \"\\uD83D\\uDE38\";\r\n    writer.writeUTF(catFace);\r\n    InputBuffer input = new InputBuffer(buff);\r\n    input.rewind(0, buff.length);\r\n    ByteBufferDataReader reader = new ByteBufferDataReader(input);\r\n    assertThat(reader.readUTF()).isEqualTo(catFace);\r\n    String fromJava = new java.io.DataInputStream(new ByteArrayInputStream(buff)).readUTF();\r\n    assertThat(fromJava).isEqualTo(catFace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testShortOfSpace",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testShortOfSpace() throws IOException\n{\r\n    byte[] buff = new byte[10];\r\n    MockDataTarget target = new MockDataTarget(buff);\r\n    ByteBufferDataWriter writer = new ByteBufferDataWriter(target);\r\n    assertThat(writer.hasUnFlushedData()).isFalse();\r\n    writer.write(1);\r\n    writer.write(new byte[] { 2, 2 }, 0, 2);\r\n    assertThat(writer.hasUnFlushedData()).isTrue();\r\n    assertThat(writer.shortOfSpace(100)).isTrue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testFlush",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFlush() throws IOException\n{\r\n    byte[] buff = new byte[10];\r\n    MockDataTarget target = Mockito.spy(new MockDataTarget(buff));\r\n    ByteBufferDataWriter writer = new ByteBufferDataWriter(target);\r\n    assertThat(writer.hasUnFlushedData()).isFalse();\r\n    writer.write(1);\r\n    writer.write(new byte[100]);\r\n    assertThat(writer.hasUnFlushedData()).isTrue();\r\n    writer.close();\r\n    Mockito.verify(target, Mockito.times(11)).sendData();\r\n    Mockito.verify(target).finishSendData();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString(byte[] str) throws UnsupportedEncodingException\n{\r\n    return new String(str, 0, str.length, \"UTF-8\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    final String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r\n    if (otherArgs.length != 2) {\r\n        System.err.println(\"Usage: wordcount <in> <out>\");\r\n        System.exit(2);\r\n    }\r\n    final Job job = Job.getInstance(conf, conf.get(MRJobConfig.JOB_NAME, \"word count\"));\r\n    job.setJarByClass(WordCount.class);\r\n    job.setMapperClass(TokenizerMapper.class);\r\n    job.setCombinerClass(IntSumReducer.class);\r\n    job.setReducerClass(IntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\r\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reduce(KTYPE key, Iterable<VTYPE> values, Context context) throws IOException, InterruptedException\n{\r\n    int hashSum = 0;\r\n    for (final VTYPE val : values) {\r\n        if (val instanceof Writable) {\r\n            os.reset();\r\n            ((Writable) val).write(dos);\r\n            final int hash = Arrays.hashCode(os.toByteArray());\r\n            hashSum += hash;\r\n        }\r\n    }\r\n    context.write(key, new IntWritable(hashSum));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\utils",
  "methodName" : "testSizedWritable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSizedWritable()\n{\r\n    final SizedWritable w = new SizedWritable(BytesWritable.class);\r\n    Assert.assertTrue(w.length == SizedWritable.INVALID_LENGTH);\r\n    Assert.assertFalse(w.v == null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "startUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startUp() throws Exception\n{\r\n    Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());\r\n    Assume.assumeTrue(NativeRuntime.isNativeLibraryLoaded());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "testKeySize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testKeySize() throws Exception\n{\r\n    runKVSizeTests(Text.class, IntWritable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "testValueSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testValueSize() throws Exception\n{\r\n    runKVSizeTests(IntWritable.class, Text.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new ScenarioConfiguration());\r\n    fs.delete(new Path(TestConstants.NATIVETASK_KVTEST_DIR), true);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "runKVSizeTests",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void runKVSizeTests(Class<?> keyClass, Class<?> valueClass) throws Exception\n{\r\n    if (!keyClass.equals(Text.class) && !valueClass.equals(Text.class)) {\r\n        return;\r\n    }\r\n    final int deafultKVSizeMaximum = 1 << 22;\r\n    final int kvSizeMaximum = normalConf.getInt(TestConstants.NATIVETASK_KVSIZE_MAX_LARGEKV_TEST, deafultKVSizeMaximum);\r\n    final FileSystem fs = FileSystem.get(normalConf);\r\n    for (int i = 65536; i <= kvSizeMaximum; i *= 4) {\r\n        int min = i / 4;\r\n        int max = i;\r\n        nativeConf.set(TestConstants.NATIVETASK_KVSIZE_MIN, String.valueOf(min));\r\n        nativeConf.set(TestConstants.NATIVETASK_KVSIZE_MAX, String.valueOf(max));\r\n        normalConf.set(TestConstants.NATIVETASK_KVSIZE_MIN, String.valueOf(min));\r\n        normalConf.set(TestConstants.NATIVETASK_KVSIZE_MAX, String.valueOf(max));\r\n        LOG.info(\"===KV Size Test: min size: \" + min + \", max size: \" + max + \", keyClass: \" + keyClass.getName() + \", valueClass: \" + valueClass.getName());\r\n        final String inputPath = TestConstants.NATIVETASK_KVTEST_INPUTDIR + \"/LargeKV/\" + keyClass.getName() + \"/\" + valueClass.getName();\r\n        final String nativeOutputPath = TestConstants.NATIVETASK_KVTEST_NATIVE_OUTPUTDIR + \"/LargeKV/\" + keyClass.getName() + \"/\" + valueClass.getName();\r\n        fs.delete(new Path(nativeOutputPath), true);\r\n        final KVJob nativeJob = new KVJob(\"Test Large Value Size:\" + String.valueOf(i), nativeConf, keyClass, valueClass, inputPath, nativeOutputPath);\r\n        assertTrue(\"job should complete successfully\", nativeJob.runJob());\r\n        final String normalOutputPath = TestConstants.NATIVETASK_KVTEST_NORMAL_OUTPUTDIR + \"/LargeKV/\" + keyClass.getName() + \"/\" + valueClass.getName();\r\n        fs.delete(new Path(normalOutputPath), true);\r\n        final KVJob normalJob = new KVJob(\"Test Large Key Size:\" + String.valueOf(i), normalConf, keyClass, valueClass, inputPath, normalOutputPath);\r\n        assertTrue(\"job should complete successfully\", normalJob.runJob());\r\n        final boolean compareRet = ResultVerifier.verify(normalOutputPath, nativeOutputPath);\r\n        final String reason = \"keytype: \" + keyClass.getName() + \", valuetype: \" + valueClass.getName() + \", failed with \" + (keyClass.equals(Text.class) ? \"key\" : \"value\") + \", min size: \" + min + \", max size: \" + max + \", normal out: \" + normalOutputPath + \", native Out: \" + nativeOutputPath;\r\n        assertEquals(reason, true, compareRet);\r\n        ResultVerifier.verifyCounters(normalJob.job, nativeJob.job);\r\n    }\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    this.nativeHandler = Mockito.mock(INativeHandler.class);\r\n    this.pusher = Mockito.mock(BufferPusher.class);\r\n    this.combiner = Mockito.mock(ICombineHandler.class);\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(OutputUtil.NATIVE_TASK_OUTPUT_MANAGER, \"org.apache.hadoop.mapred.nativetask.util.LocalJobOutputFiles\");\r\n    jobConf.set(\"mapred.local.dir\", LOCAL_DIR);\r\n    this.taskContext = new TaskContext(jobConf, BytesWritable.class, BytesWritable.class, BytesWritable.class, BytesWritable.class, null, null);\r\n    Mockito.when(nativeHandler.getInputBuffer()).thenReturn(new InputBuffer(BufferType.HEAP_BUFFER, 100));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    FileSystem.getLocal(new Configuration()).delete(new Path(LOCAL_DIR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "testCollect",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCollect() throws IOException\n{\r\n    this.handler = new NativeCollectorOnlyHandler(taskContext, nativeHandler, pusher, combiner);\r\n    handler.collect(new BytesWritable(), new BytesWritable(), 100);\r\n    handler.close();\r\n    handler.close();\r\n    Mockito.verify(pusher, Mockito.times(1)).collect(any(BytesWritable.class), any(BytesWritable.class), anyInt());\r\n    Mockito.verify(pusher, Mockito.times(1)).close();\r\n    Mockito.verify(combiner, Mockito.times(1)).close();\r\n    Mockito.verify(nativeHandler, Mockito.times(1)).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "testGetCombiner",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetCombiner() throws IOException\n{\r\n    this.handler = new NativeCollectorOnlyHandler(taskContext, nativeHandler, pusher, combiner);\r\n    Mockito.when(combiner.getId()).thenReturn(100L);\r\n    final ReadWriteBuffer result = handler.onCall(NativeCollectorOnlyHandler.GET_COMBINE_HANDLER, null);\r\n    Assert.assertEquals(100L, result.readLong());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "testOnCall",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testOnCall() throws IOException\n{\r\n    this.handler = new NativeCollectorOnlyHandler(taskContext, nativeHandler, pusher, combiner);\r\n    boolean thrown = false;\r\n    try {\r\n        handler.onCall(new Command(-1), null);\r\n    } catch (final IOException e) {\r\n        thrown = true;\r\n    }\r\n    Assert.assertTrue(\"exception thrown\", thrown);\r\n    final String expectedOutputPath = StringUtils.join(File.separator, new String[] { LOCAL_DIR, \"output\", \"file.out\" });\r\n    final String expectedOutputIndexPath = StringUtils.join(File.separator, new String[] { LOCAL_DIR, \"output\", \"file.out.index\" });\r\n    final String expectedSpillPath = StringUtils.join(File.separator, new String[] { LOCAL_DIR, \"output\", \"spill0.out\" });\r\n    final String outputPath = handler.onCall(NativeCollectorOnlyHandler.GET_OUTPUT_PATH, null).readString();\r\n    Assert.assertEquals(expectedOutputPath, outputPath);\r\n    final String outputIndexPath = handler.onCall(NativeCollectorOnlyHandler.GET_OUTPUT_INDEX_PATH, null).readString();\r\n    Assert.assertEquals(expectedOutputIndexPath, outputIndexPath);\r\n    final String spillPath = handler.onCall(NativeCollectorOnlyHandler.GET_SPILL_PATH, null).readString();\r\n    Assert.assertEquals(expectedSpillPath, spillPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    this.nativeHandler = Mockito.mock(INativeHandler.class);\r\n    this.pusher = Mockito.mock(BufferPusher.class);\r\n    this.puller = Mockito.mock(BufferPuller.class);\r\n    this.combinerRunner = Mockito.mock(CombinerRunner.class);\r\n    Mockito.when(nativeHandler.getInputBuffer()).thenReturn(new InputBuffer(BufferType.HEAP_BUFFER, 100));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "testCommandDispatcherSetting",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommandDispatcherSetting() throws IOException\n{\r\n    this.handler = new CombinerHandler(nativeHandler, combinerRunner, puller, pusher);\r\n    Mockito.verify(nativeHandler, Mockito.times(1)).setCommandDispatcher(eq(handler));\r\n    Mockito.verify(nativeHandler, Mockito.times(1)).setDataReceiver(eq(puller));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "testCombine",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCombine() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    this.handler = new CombinerHandler(nativeHandler, combinerRunner, puller, pusher);\r\n    assertThat(handler.onCall(CombinerHandler.COMBINE, null)).isNull();\r\n    handler.close();\r\n    handler.close();\r\n    Mockito.verify(combinerRunner, Mockito.times(1)).combine(eq(puller), eq(pusher));\r\n    Mockito.verify(pusher, Mockito.times(1)).close();\r\n    Mockito.verify(puller, Mockito.times(1)).close();\r\n    Mockito.verify(nativeHandler, Mockito.times(1)).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\handlers",
  "methodName" : "testOnCall",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOnCall() throws IOException\n{\r\n    this.handler = new CombinerHandler(nativeHandler, combinerRunner, puller, pusher);\r\n    assertThat(handler.onCall(new Command(-1), null)).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "parseClassNames",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<Class<?>> parseClassNames(String spec)\n{\r\n    List<Class<?>> ret = Lists.newArrayList();\r\n    Iterable<String> classNames = Splitter.on(';').trimResults().omitEmptyStrings().split(spec);\r\n    for (String className : classNames) {\r\n        try {\r\n            ret.add(Class.forName(className));\r\n        } catch (ClassNotFoundException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Iterable<Class<?>[]> data() throws Exception\n{\r\n    final String valueClassesStr = nativekvtestconf.get(TestConstants.NATIVETASK_KVTEST_VALUECLASSES);\r\n    LOG.info(\"Parameterizing with value classes: \" + valueClassesStr);\r\n    List<Class<?>> valueClasses = parseClassNames(valueClassesStr);\r\n    final String keyClassesStr = nativekvtestconf.get(TestConstants.NATIVETASK_KVTEST_KEYCLASSES);\r\n    LOG.info(\"Parameterizing with key classes: \" + keyClassesStr);\r\n    List<Class<?>> keyClasses = parseClassNames(keyClassesStr);\r\n    List<Class<?>[]> pairs = Lists.newArrayList();\r\n    for (Class<?> keyClass : keyClasses) {\r\n        pairs.add(new Class<?>[] { keyClass, LongWritable.class });\r\n    }\r\n    for (Class<?> valueClass : valueClasses) {\r\n        pairs.add(new Class<?>[] { LongWritable.class, valueClass });\r\n    }\r\n    return pairs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "startUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startUp() throws Exception\n{\r\n    Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());\r\n    Assume.assumeTrue(NativeRuntime.isNativeLibraryLoaded());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "testKVCompability",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testKVCompability() throws Exception\n{\r\n    final FileSystem fs = FileSystem.get(nativekvtestconf);\r\n    final String jobName = \"Test:\" + keyclass.getSimpleName() + \"--\" + valueclass.getSimpleName();\r\n    final String inputPath = TestConstants.NATIVETASK_KVTEST_INPUTDIR + \"/\" + keyclass.getName() + \"/\" + valueclass.getName();\r\n    final String nativeOutputPath = TestConstants.NATIVETASK_KVTEST_NATIVE_OUTPUTDIR + \"/\" + keyclass.getName() + \"/\" + valueclass.getName();\r\n    fs.delete(new Path(nativeOutputPath), true);\r\n    nativekvtestconf.set(TestConstants.NATIVETASK_KVTEST_CREATEFILE, \"true\");\r\n    final KVJob nativeJob = new KVJob(jobName, nativekvtestconf, keyclass, valueclass, inputPath, nativeOutputPath);\r\n    assertThat(nativeJob.runJob()).withFailMessage(\"job should complete successfully\").isTrue();\r\n    final String normalOutputPath = TestConstants.NATIVETASK_KVTEST_NORMAL_OUTPUTDIR + \"/\" + keyclass.getName() + \"/\" + valueclass.getName();\r\n    fs.delete(new Path(normalOutputPath), true);\r\n    hadoopkvtestconf.set(TestConstants.NATIVETASK_KVTEST_CREATEFILE, \"false\");\r\n    final KVJob normalJob = new KVJob(jobName, hadoopkvtestconf, keyclass, valueclass, inputPath, normalOutputPath);\r\n    assertThat(normalJob.runJob()).withFailMessage(\"job should complete successfully\").isTrue();\r\n    final boolean compareRet = ResultVerifier.verify(normalOutputPath, nativeOutputPath);\r\n    assertThat(compareRet).withFailMessage(\"job output not the same\").isTrue();\r\n    ResultVerifier.verifyCounters(normalJob.job, nativeJob.job);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\kvtest",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new ScenarioConfiguration());\r\n    fs.delete(new Path(TestConstants.NATIVETASK_KVTEST_DIR), true);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    this.inputArray = TestInput.getMapInputs(inputArraySize);\r\n    this.key = new SizedWritable(BytesWritable.class);\r\n    this.value = new SizedWritable(BytesWritable.class);\r\n    this.serializer = new KVSerializer(BytesWritable.class, BytesWritable.class);\r\n    key.reset(inputArray[4].key);\r\n    value.reset(inputArray[4].value);\r\n    serializer.updateLength(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "testUpdateLength",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUpdateLength() throws IOException\n{\r\n    Mockito.mock(DataOutputStream.class);\r\n    int kvLength = 0;\r\n    for (int i = 0; i < inputArraySize; i++) {\r\n        key.reset(inputArray[i].key);\r\n        value.reset(inputArray[i].value);\r\n        serializer.updateLength(key, value);\r\n        Assert.assertTrue(key.length + value.length > kvLength);\r\n        kvLength = key.length + value.length;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "testSerializeKV",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSerializeKV() throws IOException\n{\r\n    final DataOutputStream dataOut = Mockito.mock(DataOutputStream.class);\r\n    Mockito.when(dataOut.hasUnFlushedData()).thenReturn(true);\r\n    Mockito.when(dataOut.shortOfSpace(key.length + value.length + Constants.SIZEOF_KV_LENGTH)).thenReturn(true);\r\n    final int written = serializer.serializeKV(dataOut, key, value);\r\n    Mockito.verify(dataOut, Mockito.times(1)).flush();\r\n    Mockito.verify(dataOut, Mockito.times(4)).writeInt(anyInt());\r\n    Mockito.verify(dataOut, Mockito.times(2)).write(any(byte[].class), anyInt(), anyInt());\r\n    Assert.assertEquals(written, key.length + value.length + Constants.SIZEOF_KV_LENGTH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "testSerializeNoFlush",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSerializeNoFlush() throws IOException\n{\r\n    final DataOutputStream dataOut = Mockito.mock(DataOutputStream.class);\r\n    Mockito.when(dataOut.hasUnFlushedData()).thenReturn(true);\r\n    Mockito.when(dataOut.shortOfSpace(anyInt())).thenReturn(false);\r\n    final int written = serializer.serializeKV(dataOut, key, value);\r\n    Mockito.verify(dataOut, Mockito.times(0)).flush();\r\n    Mockito.verify(dataOut, Mockito.times(4)).writeInt(anyInt());\r\n    Mockito.verify(dataOut, Mockito.times(2)).write(any(byte[].class), anyInt(), anyInt());\r\n    Assert.assertEquals(written, key.length + value.length + Constants.SIZEOF_KV_LENGTH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "testSerializePartitionKV",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSerializePartitionKV() throws IOException\n{\r\n    final DataOutputStream dataOut = Mockito.mock(DataOutputStream.class);\r\n    Mockito.when(dataOut.hasUnFlushedData()).thenReturn(true);\r\n    Mockito.when(dataOut.shortOfSpace(key.length + value.length + Constants.SIZEOF_KV_LENGTH + Constants.SIZEOF_PARTITION_LENGTH)).thenReturn(true);\r\n    final int written = serializer.serializePartitionKV(dataOut, 100, key, value);\r\n    Mockito.verify(dataOut, Mockito.times(1)).flush();\r\n    Mockito.verify(dataOut, Mockito.times(5)).writeInt(anyInt());\r\n    Mockito.verify(dataOut, Mockito.times(2)).write(any(byte[].class), anyInt(), anyInt());\r\n    Assert.assertEquals(written, key.length + value.length + Constants.SIZEOF_KV_LENGTH + Constants.SIZEOF_PARTITION_LENGTH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "testDeserializerNoData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDeserializerNoData() throws IOException\n{\r\n    final DataInputStream in = Mockito.mock(DataInputStream.class);\r\n    Mockito.when(in.hasUnReadData()).thenReturn(false);\r\n    Assert.assertEquals(0, serializer.deserializeKV(in, key, value));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\serde",
  "methodName" : "testDeserializer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDeserializer() throws IOException\n{\r\n    final DataInputStream in = Mockito.mock(DataInputStream.class);\r\n    Mockito.when(in.hasUnReadData()).thenReturn(true);\r\n    Assert.assertTrue(serializer.deserializeKV(in, key, value) > 0);\r\n    Mockito.verify(in, Mockito.times(4)).readInt();\r\n    Mockito.verify(in, Mockito.times(2)).readFully(any(byte[].class), anyInt(), anyInt());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\utils",
  "methodName" : "testReadWriteBuffer",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testReadWriteBuffer()\n{\r\n    final ReadWriteBuffer buffer = new ReadWriteBuffer();\r\n    assertThat(buffer.getBuff()).isNotNull();\r\n    assertThat(buffer.getWritePoint()).isZero();\r\n    assertThat(buffer.getReadPoint()).isZero();\r\n    buffer.writeInt(3);\r\n    buffer.writeString(\"goodboy\");\r\n    buffer.writeLong(10L);\r\n    buffer.writeBytes(bytes, 0, bytes.length);\r\n    buffer.writeLong(100L);\r\n    assertThat(buffer.getWritePoint()).isEqualTo(41);\r\n    assertThat(buffer.getReadPoint()).isZero();\r\n    assertThat(buffer.getBuff().length).isEqualTo(41);\r\n    assertThat(buffer.readInt()).isEqualTo(3);\r\n    assertThat(buffer.readString()).isEqualTo(\"goodboy\");\r\n    assertThat(buffer.readLong()).isEqualTo(10L);\r\n    final byte[] read = buffer.readBytes();\r\n    for (int i = 0; i < bytes.length; i++) {\r\n        assertThat(read[i]).isEqualTo(bytes[i]);\r\n    }\r\n    assertThat(buffer.readLong()).isEqualTo(100L);\r\n    assertThat(buffer.getReadPoint()).isEqualTo(41);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void set(byte[] seed)\n{\r\n    a = seed.length;\r\n    array = new byte[seed.length];\r\n    System.arraycopy(seed, 0, array, 0, seed.length);\r\n    longWritable.set(a);\r\n    txt.set(BytesUtil.toStringBinary(array));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeInt(a);\r\n    out.writeInt(array.length);\r\n    out.write(array);\r\n    longWritable.write(out);\r\n    txt.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    a = in.readInt();\r\n    final int length = in.readInt();\r\n    array = new byte[length];\r\n    in.readFully(array);\r\n    longWritable.readFields(in);\r\n    txt.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "testWordCountCombinerWithOldAPI",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testWordCountCombinerWithOldAPI() throws Exception\n{\r\n    final Configuration nativeConf = ScenarioConfiguration.getNativeConfiguration();\r\n    nativeConf.addResource(TestConstants.COMBINER_CONF_PATH);\r\n    final String nativeoutput = TestConstants.NATIVETASK_OLDAPI_COMBINER_TEST_NATIVE_OUTPUTPATH;\r\n    final JobConf nativeJob = getOldAPIJobconf(nativeConf, \"nativeCombinerWithOldAPI\", inputpath, nativeoutput);\r\n    RunningJob nativeRunning = JobClient.runJob(nativeJob);\r\n    Counter nativeReduceGroups = nativeRunning.getCounters().findCounter(TaskCounter.REDUCE_INPUT_RECORDS);\r\n    final Configuration normalConf = ScenarioConfiguration.getNormalConfiguration();\r\n    normalConf.addResource(TestConstants.COMBINER_CONF_PATH);\r\n    final String normaloutput = TestConstants.NATIVETASK_OLDAPI_COMBINER_TEST_NORMAL_OUTPUTPATH;\r\n    final JobConf normalJob = getOldAPIJobconf(normalConf, \"normalCombinerWithOldAPI\", inputpath, normaloutput);\r\n    RunningJob normalRunning = JobClient.runJob(normalJob);\r\n    Counter normalReduceGroups = normalRunning.getCounters().findCounter(TaskCounter.REDUCE_INPUT_RECORDS);\r\n    final boolean compareRet = ResultVerifier.verify(nativeoutput, normaloutput);\r\n    assertThat(compareRet).withFailMessage(\"file compare result: if they are the same ,then return true\").isTrue();\r\n    assertThat(nativeReduceGroups.getValue()).withFailMessage(\"The input reduce record count must be same\").isEqualTo(normalReduceGroups.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "startUp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void startUp() throws Exception\n{\r\n    Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());\r\n    Assume.assumeTrue(NativeRuntime.isNativeLibraryLoaded());\r\n    final ScenarioConfiguration conf = new ScenarioConfiguration();\r\n    conf.addcombinerConf();\r\n    this.fs = FileSystem.get(conf);\r\n    this.inputpath = TestConstants.NATIVETASK_COMBINER_TEST_INPUTDIR + \"/wordcount\";\r\n    if (!fs.exists(new Path(inputpath))) {\r\n        new TestInputFile(conf.getInt(TestConstants.NATIVETASK_COMBINER_WORDCOUNT_FILESIZE, 1000000), Text.class.getName(), Text.class.getName(), conf).createSequenceTestFile(inputpath, 1, (byte) ('a'));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new ScenarioConfiguration());\r\n    fs.delete(new Path(TestConstants.NATIVETASK_COMBINER_TEST_DIR), true);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\combinertest",
  "methodName" : "getOldAPIJobconf",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "JobConf getOldAPIJobconf(Configuration configuration, String name, String input, String output) throws Exception\n{\r\n    final JobConf jobConf = new JobConf(configuration);\r\n    final FileSystem fs = FileSystem.get(configuration);\r\n    if (fs.exists(new Path(output))) {\r\n        fs.delete(new Path(output), true);\r\n    }\r\n    fs.close();\r\n    jobConf.setJobName(name);\r\n    jobConf.setOutputKeyClass(Text.class);\r\n    jobConf.setOutputValueClass(IntWritable.class);\r\n    jobConf.setMapperClass(WordCountWithOldAPI.TokenizerMapperWithOldAPI.class);\r\n    jobConf.setCombinerClass(WordCountWithOldAPI.IntSumReducerWithOldAPI.class);\r\n    jobConf.setReducerClass(WordCountWithOldAPI.IntSumReducerWithOldAPI.class);\r\n    jobConf.setInputFormat(SequenceFileInputFormat.class);\r\n    jobConf.setOutputFormat(TextOutputFormat.class);\r\n    FileInputFormat.setInputPaths(jobConf, new Path(input));\r\n    FileOutputFormat.setOutputPath(jobConf, new Path(output));\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "addcombinerConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addcombinerConf()\n{\r\n    this.addResource(TestConstants.COMBINER_CONF_PATH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "addKVTestConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addKVTestConf()\n{\r\n    this.addResource(TestConstants.KVTEST_CONF_PATH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "addNonSortTestConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNonSortTestConf()\n{\r\n    this.addResource(TestConstants.NONSORT_TEST_CONF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "addNativeConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNativeConf()\n{\r\n    this.set(TestConstants.NATIVETASK_COLLECTOR_DELEGATOR, TestConstants.NATIVETASK_COLLECTOR_DELEGATOR_CLASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "getNormalConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getNormalConfiguration()\n{\r\n    Configuration normalConf = new Configuration();\r\n    normalConf.addResource(\"common_conf.xml\");\r\n    normalConf.addResource(\"normal_conf.xml\");\r\n    return normalConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "getNativeConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getNativeConfiguration()\n{\r\n    Configuration nativeConf = new Configuration();\r\n    nativeConf.addResource(\"common_conf.xml\");\r\n    nativeConf.addResource(\"native_conf.xml\");\r\n    return nativeConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testOutputBuffer",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testOutputBuffer()\n{\r\n    final int size = 100;\r\n    final OutputBuffer output1 = new OutputBuffer(BufferType.DIRECT_BUFFER, size);\r\n    assertThat(output1.getType()).isEqualTo(BufferType.DIRECT_BUFFER);\r\n    assertThat(output1.length()).isZero();\r\n    assertThat(output1.limit()).isEqualTo(size);\r\n    final OutputBuffer output2 = new OutputBuffer(BufferType.HEAP_BUFFER, size);\r\n    assertThat(output2.getType()).isEqualTo(BufferType.HEAP_BUFFER);\r\n    assertThat(output2.length()).isZero();\r\n    assertThat(output2.limit()).isEqualTo(size);\r\n    final OutputBuffer output3 = new OutputBuffer(new byte[size]);\r\n    assertThat(output3.getType()).isEqualTo(BufferType.HEAP_BUFFER);\r\n    assertThat(output3.length()).isZero();\r\n    assertThat(output3.limit()).isEqualTo(size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\compresstest",
  "methodName" : "getCompressJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Job getCompressJob(String jobname, Configuration conf, String inputpath, String outputpath) throws Exception\n{\r\n    Job job = Job.getInstance(conf, jobname + \"-CompressMapperJob\");\r\n    job.setJarByClass(CompressMapper.class);\r\n    job.setMapperClass(TextCompressMapper.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    final FileSystem hdfs = FileSystem.get(new ScenarioConfiguration());\r\n    if (hdfs.exists(new Path(outputpath))) {\r\n        hdfs.delete(new Path(outputpath), true);\r\n    }\r\n    hdfs.close();\r\n    job.setInputFormatClass(SequenceFileInputFormat.class);\r\n    FileInputFormat.addInputPath(job, new Path(inputpath));\r\n    FileOutputFormat.setOutputPath(job, new Path(outputpath));\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask",
  "methodName" : "testTaskContext",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTaskContext()\n{\r\n    TaskContext context = new TaskContext(null, null, null, null, null, null, null);\r\n    context.setInputKeyClass(IntWritable.class);\r\n    Assert.assertEquals(IntWritable.class.getName(), context.getInputKeyClass().getName());\r\n    context.setInputValueClass(Text.class);\r\n    Assert.assertEquals(Text.class.getName(), context.getInputValueClass().getName());\r\n    context.setOutputKeyClass(LongWritable.class);\r\n    Assert.assertEquals(LongWritable.class.getName(), context.getOutputKeyClass().getName());\r\n    context.setOutputValueClass(FloatWritable.class);\r\n    Assert.assertEquals(FloatWritable.class.getName(), context.getOutputValueClass().getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp()\n{\r\n    this.dataInput = TestInput.getMapInputs(INPUT_KV_COUNT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testPush",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testPush() throws Exception\n{\r\n    final byte[] buff = new byte[BUFFER_LENGTH];\r\n    final InputBuffer input = new InputBuffer(buff);\r\n    final OutputBuffer out = new OutputBuffer(buff);\r\n    final Class<BytesWritable> iKClass = BytesWritable.class;\r\n    final Class<BytesWritable> iVClass = BytesWritable.class;\r\n    final RecordWriterForPush writer = new RecordWriterForPush() {\r\n\r\n        @Override\r\n        public void write(BytesWritable key, BytesWritable value) throws IOException {\r\n            final KV expect = dataInput[count++];\r\n            Assert.assertEquals(expect.key.toString(), key.toString());\r\n            Assert.assertEquals(expect.value.toString(), value.toString());\r\n        }\r\n    };\r\n    final BufferPushee pushee = new BufferPushee(iKClass, iVClass, writer);\r\n    final PushTarget handler = new PushTarget(out) {\r\n\r\n        @Override\r\n        public void sendData() throws IOException {\r\n            final int outputLength = out.length();\r\n            input.rewind(0, outputLength);\r\n            out.rewind();\r\n            pushee.collect(input);\r\n        }\r\n    };\r\n    final BufferPusher pusher = new BufferPusher(iKClass, iVClass, handler);\r\n    writer.reset();\r\n    for (int i = 0; i < INPUT_KV_COUNT; i++) {\r\n        pusher.collect(dataInput[i].key, dataInput[i].value);\r\n    }\r\n    pusher.close();\r\n    pushee.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\buffer",
  "methodName" : "testPull",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testPull() throws Exception\n{\r\n    final byte[] buff = new byte[BUFFER_LENGTH];\r\n    final InputBuffer input = new InputBuffer(buff);\r\n    final OutputBuffer out = new OutputBuffer(buff);\r\n    final Class<BytesWritable> iKClass = BytesWritable.class;\r\n    final Class<BytesWritable> iVClass = BytesWritable.class;\r\n    final NativeHandlerForPull handler = new NativeHandlerForPull(input, out);\r\n    final KeyValueIterator iter = new KeyValueIterator();\r\n    final BufferPullee pullee = new BufferPullee(iKClass, iVClass, iter, handler);\r\n    handler.setDataLoader(pullee);\r\n    final BufferPuller puller = new BufferPuller(handler);\r\n    handler.setDataReceiver(puller);\r\n    int count = 0;\r\n    while (puller.next()) {\r\n        final DataInputBuffer key = puller.getKey();\r\n        final DataInputBuffer value = puller.getValue();\r\n        final BytesWritable keyBytes = new BytesWritable();\r\n        final BytesWritable valueBytes = new BytesWritable();\r\n        keyBytes.readFields(key);\r\n        valueBytes.readFields(value);\r\n        Assert.assertEquals(dataInput[count].key.toString(), keyBytes.toString());\r\n        Assert.assertEquals(dataInput[count].value.toString(), valueBytes.toString());\r\n        count++;\r\n    }\r\n    puller.close();\r\n    pullee.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "verify",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "boolean verify(String sample, String source) throws Exception\n{\r\n    FSDataInputStream sourcein = null;\r\n    FSDataInputStream samplein = null;\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = FileSystem.get(conf);\r\n    final Path hdfssource = new Path(source);\r\n    final Path[] sourcepaths = FileUtil.stat2Paths(fs.listStatus(hdfssource));\r\n    final Path hdfssample = new Path(sample);\r\n    final Path[] samplepaths = FileUtil.stat2Paths(fs.listStatus(hdfssample));\r\n    if (sourcepaths == null) {\r\n        throw new Exception(\"source file can not be found\");\r\n    }\r\n    if (samplepaths == null) {\r\n        throw new Exception(\"sample file can not be found\");\r\n    }\r\n    if (sourcepaths.length != samplepaths.length) {\r\n        return false;\r\n    }\r\n    for (int i = 0; i < sourcepaths.length; i++) {\r\n        final Path sourcepath = sourcepaths[i];\r\n        if (!sourcepath.getName().startsWith(\"part-r\")) {\r\n            continue;\r\n        }\r\n        Path samplepath = null;\r\n        for (int j = 0; j < samplepaths.length; j++) {\r\n            if (samplepaths[i].getName().equals(sourcepath.getName())) {\r\n                samplepath = samplepaths[i];\r\n                break;\r\n            }\r\n        }\r\n        if (samplepath == null) {\r\n            throw new Exception(\"cound not find file \" + samplepaths[0].getParent() + \"/\" + sourcepath.getName() + \" , as sourcepaths has such file\");\r\n        }\r\n        try {\r\n            if (fs.exists(sourcepath) && fs.exists(samplepath)) {\r\n                sourcein = fs.open(sourcepath);\r\n                samplein = fs.open(samplepath);\r\n            } else {\r\n                System.err.println(\"result file not found:\" + sourcepath + \" or \" + samplepath);\r\n                return false;\r\n            }\r\n            CRC32 sourcecrc, samplecrc;\r\n            samplecrc = new CRC32();\r\n            sourcecrc = new CRC32();\r\n            final byte[] bufin = new byte[1 << 16];\r\n            int readnum = 0;\r\n            int totalRead = 0;\r\n            while (samplein.available() > 0) {\r\n                readnum = samplein.read(bufin);\r\n                totalRead += readnum;\r\n                samplecrc.update(bufin, 0, readnum);\r\n            }\r\n            if (0 == totalRead) {\r\n                throw new Exception(\"source \" + sample + \" is empty file\");\r\n            }\r\n            totalRead = 0;\r\n            while (sourcein.available() > 0) {\r\n                readnum = sourcein.read(bufin);\r\n                totalRead += readnum;\r\n                sourcecrc.update(bufin, 0, readnum);\r\n            }\r\n            if (0 == totalRead) {\r\n                throw new Exception(\"source \" + sample + \" is empty file\");\r\n            }\r\n            if (samplecrc.getValue() == sourcecrc.getValue()) {\r\n                ;\r\n            } else {\r\n                return false;\r\n            }\r\n        } catch (final IOException e) {\r\n            throw new Exception(\"verify exception :\", e);\r\n        } finally {\r\n            try {\r\n                if (samplein != null) {\r\n                    samplein.close();\r\n                }\r\n                if (sourcein != null) {\r\n                    sourcein.close();\r\n                }\r\n            } catch (final IOException e) {\r\n                e.printStackTrace();\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "verifyCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyCounters(Job normalJob, Job nativeJob, boolean hasCombiner) throws IOException\n{\r\n    Counters normalCounters = normalJob.getCounters();\r\n    Counters nativeCounters = nativeJob.getCounters();\r\n    assertEquals(\"Counter MAP_OUTPUT_RECORDS should be equal\", normalCounters.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getValue(), nativeCounters.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getValue());\r\n    assertEquals(\"Counter REDUCE_INPUT_GROUPS should be equal\", normalCounters.findCounter(TaskCounter.REDUCE_INPUT_GROUPS).getValue(), nativeCounters.findCounter(TaskCounter.REDUCE_INPUT_GROUPS).getValue());\r\n    if (!hasCombiner) {\r\n        assertEquals(\"Counter REDUCE_INPUT_RECORDS should be equal\", normalCounters.findCounter(TaskCounter.REDUCE_INPUT_RECORDS).getValue(), nativeCounters.findCounter(TaskCounter.REDUCE_INPUT_RECORDS).getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\testutil",
  "methodName" : "verifyCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyCounters(Job normalJob, Job nativeJob) throws IOException\n{\r\n    verifyCounters(normalJob, nativeJob, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\nonsorttest",
  "methodName" : "nonSortTest",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void nonSortTest() throws Exception\n{\r\n    Configuration nativeConf = ScenarioConfiguration.getNativeConfiguration();\r\n    nativeConf.addResource(TestConstants.NONSORT_TEST_CONF);\r\n    nativeConf.set(TestConstants.NATIVETASK_MAP_OUTPUT_SORT, \"false\");\r\n    final Job nativeNonSort = getJob(nativeConf, \"NativeNonSort\", TestConstants.NATIVETASK_NONSORT_TEST_INPUTDIR, TestConstants.NATIVETASK_NONSORT_TEST_NATIVE_OUTPUT);\r\n    assertThat(nativeNonSort.waitForCompletion(true)).isTrue();\r\n    Configuration normalConf = ScenarioConfiguration.getNormalConfiguration();\r\n    normalConf.addResource(TestConstants.NONSORT_TEST_CONF);\r\n    final Job hadoopWithSort = getJob(normalConf, \"NormalJob\", TestConstants.NATIVETASK_NONSORT_TEST_INPUTDIR, TestConstants.NATIVETASK_NONSORT_TEST_NORMAL_OUTPUT);\r\n    assertThat(hadoopWithSort.waitForCompletion(true)).isTrue();\r\n    final boolean compareRet = ResultVerifier.verify(TestConstants.NATIVETASK_NONSORT_TEST_NATIVE_OUTPUT, TestConstants.NATIVETASK_NONSORT_TEST_NORMAL_OUTPUT);\r\n    assertThat(compareRet).withFailMessage(\"file compare result: if they are the same ,then return true\").isTrue();\r\n    ResultVerifier.verifyCounters(hadoopWithSort, nativeNonSort);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\nonsorttest",
  "methodName" : "startUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void startUp() throws Exception\n{\r\n    Assume.assumeTrue(NativeCodeLoader.isNativeCodeLoaded());\r\n    Assume.assumeTrue(NativeRuntime.isNativeLibraryLoaded());\r\n    final ScenarioConfiguration conf = new ScenarioConfiguration();\r\n    conf.addNonSortTestConf();\r\n    final FileSystem fs = FileSystem.get(conf);\r\n    final Path path = new Path(TestConstants.NATIVETASK_NONSORT_TEST_INPUTDIR);\r\n    if (!fs.exists(path)) {\r\n        int filesize = conf.getInt(TestConstants.NATIVETASK_NONSORTTEST_FILESIZE, 10000000);\r\n        new TestInputFile(filesize, Text.class.getName(), Text.class.getName(), conf).createSequenceTestFile(path.toString());\r\n    }\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\nonsorttest",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new ScenarioConfiguration());\r\n    fs.delete(new Path(TestConstants.NATIVETASK_NONSORT_TEST_DIR), true);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-nativetask\\src\\test\\java\\org\\apache\\hadoop\\mapred\\nativetask\\nonsorttest",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Job getJob(Configuration conf, String jobName, String inputpath, String outputpath) throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(new Path(outputpath))) {\r\n        fs.delete(new Path(outputpath), true);\r\n    }\r\n    fs.close();\r\n    final Job job = Job.getInstance(conf, jobName);\r\n    job.setJarByClass(NonSortTestMR.class);\r\n    job.setMapperClass(NonSortTestMR.Map.class);\r\n    job.setReducerClass(NonSortTestMR.KeyHashSumReduce.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(IntWritable.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    job.setInputFormatClass(SequenceFileInputFormat.class);\r\n    job.setOutputFormatClass(TextOutputFormat.class);\r\n    FileInputFormat.addInputPath(job, new Path(inputpath));\r\n    FileOutputFormat.setOutputPath(job, new Path(outputpath));\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
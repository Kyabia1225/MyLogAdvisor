[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testFailOnCloseError",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFailOnCloseError() throws Exception\n{\r\n    Mapper.Context context = mock(Mapper.Context.class);\r\n    doReturn(new Configuration()).when(context).getConfiguration();\r\n    Exception expectedEx = new IOException(\"boom\");\r\n    OutputStream out = mock(OutputStream.class);\r\n    doThrow(expectedEx).when(out).close();\r\n    File f = File.createTempFile(this.getClass().getSimpleName(), null);\r\n    f.deleteOnExit();\r\n    CopyListingFileStatus stat = new CopyListingFileStatus(new FileStatus(1L, false, 1, 1024, 0, new Path(f.toURI())));\r\n    Exception actualEx = null;\r\n    try {\r\n        new RetriableFileCopyCommand(\"testFailOnCloseError\", FileAction.OVERWRITE).copyBytes(stat, 0, out, 512, context);\r\n    } catch (Exception e) {\r\n        actualEx = e;\r\n    }\r\n    assertNotNull(\"close didn't fail\", actualEx);\r\n    assertEquals(expectedEx, actualEx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testGetNumBytesToRead",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetNumBytesToRead()\n{\r\n    long pos = 100;\r\n    long buffLength = 1024;\r\n    long fileLength = 2058;\r\n    RetriableFileCopyCommand retriableFileCopyCommand = new RetriableFileCopyCommand(\"Testing NumBytesToRead \", FileAction.OVERWRITE);\r\n    long numBytes = retriableFileCopyCommand.getNumBytesToRead(fileLength, pos, buffLength);\r\n    Assert.assertEquals(1024, numBytes);\r\n    pos += numBytes;\r\n    numBytes = retriableFileCopyCommand.getNumBytesToRead(fileLength, pos, buffLength);\r\n    Assert.assertEquals(934, numBytes);\r\n    pos += numBytes;\r\n    numBytes = retriableFileCopyCommand.getNumBytesToRead(fileLength, pos, buffLength);\r\n    Assert.assertEquals(0, numBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Mapper<Text, CopyListingFileStatus, Text, Text>.Context getContext()\n{\r\n    return mapperContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getReporter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StatusReporter getReporter()\n{\r\n    return reporter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<Text, CopyListingFileStatus> getReader()\n{\r\n    return reader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReader(RecordReader<Text, CopyListingFileStatus> reader)\n{\r\n    this.reader = reader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StubInMemoryWriter getWriter()\n{\r\n    return writer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptID(int taskId)\n{\r\n    return new TaskAttemptID(\"\", 0, TaskType.MAP, taskId, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(DATA_NUM).build();\r\n    cluster.waitActive();\r\n    webfs = WebHdfsTestUtil.getWebHdfsFileSystem(conf, WebHdfsConstants.WEBHDFS_SCHEME);\r\n    dfs = cluster.getFileSystem();\r\n    dfs.mkdirs(source);\r\n    dfs.mkdirs(target);\r\n    final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(source), target).withSyncFolder(true).withUseDiff(\"s1\", \"s2\").build();\r\n    options.appendToConf(conf);\r\n    context = new DistCpContext(options);\r\n    conf.set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, target.toString());\r\n    conf.set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, target.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    IOUtils.cleanupWithLogger(null, dfs);\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testFallback",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testFallback() throws Exception\n{\r\n    Assert.assertFalse(sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s2\");\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    context.setSourcePaths(Collections.singletonList(source));\r\n    dfs.allowSnapshot(source);\r\n    dfs.allowSnapshot(target);\r\n    Assert.assertFalse(sync());\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    context.setSourcePaths(Collections.singletonList(source));\r\n    dfs.createSnapshot(source, \"s1\");\r\n    dfs.createSnapshot(source, \"s2\");\r\n    dfs.createSnapshot(target, \"s1\");\r\n    Assert.assertTrue(sync());\r\n    context.setSourcePaths(Collections.singletonList(source));\r\n    final Path subTarget = new Path(target, \"sub\");\r\n    dfs.mkdirs(subTarget);\r\n    Assert.assertFalse(sync());\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    context.setSourcePaths(Collections.singletonList(source));\r\n    dfs.delete(subTarget, true);\r\n    Assert.assertTrue(sync());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "enableAndCreateFirstSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void enableAndCreateFirstSnapshot() throws Exception\n{\r\n    dfs.allowSnapshot(source);\r\n    dfs.allowSnapshot(target);\r\n    dfs.createSnapshot(source, \"s1\");\r\n    dfs.createSnapshot(target, \"s1\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "syncAndVerify",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void syncAndVerify() throws Exception\n{\r\n    Assert.assertTrue(sync());\r\n    verifyCopy(dfs.getFileStatus(source), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "sync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean sync() throws Exception\n{\r\n    DistCpSync distCpSync = new DistCpSync(context, conf);\r\n    return distCpSync.sync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initData(Path dir) throws Exception\n{\r\n    initData(dfs, dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void initData(FileSystem fs, Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path d1 = new Path(foo, \"d1\");\r\n    final Path f1 = new Path(foo, \"f1\");\r\n    final Path d2 = new Path(bar, \"d2\");\r\n    final Path f2 = new Path(bar, \"f2\");\r\n    final Path f3 = new Path(d1, \"f3\");\r\n    final Path f4 = new Path(d2, \"f4\");\r\n    DFSTestUtil.createFile(fs, f1, BLOCK_SIZE, DATA_NUM, 0);\r\n    DFSTestUtil.createFile(fs, f2, BLOCK_SIZE, DATA_NUM, 0);\r\n    DFSTestUtil.createFile(fs, f3, BLOCK_SIZE, DATA_NUM, 0);\r\n    DFSTestUtil.createFile(fs, f4, BLOCK_SIZE, DATA_NUM, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int changeData(FileSystem fs, Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path d1 = new Path(foo, \"d1\");\r\n    final Path f2 = new Path(bar, \"f2\");\r\n    final Path bar_d1 = new Path(bar, \"d1\");\r\n    int numCreatedModified = 0;\r\n    fs.rename(d1, bar_d1);\r\n    numCreatedModified += 1;\r\n    numCreatedModified += 1;\r\n    final Path f3 = new Path(bar_d1, \"f3\");\r\n    fs.delete(f3, true);\r\n    final Path newfoo = new Path(bar_d1, \"foo\");\r\n    fs.rename(foo, newfoo);\r\n    numCreatedModified += 1;\r\n    final Path f1 = new Path(newfoo, \"f1\");\r\n    fs.delete(f1, true);\r\n    DFSTestUtil.createFile(fs, f1, 2 * BLOCK_SIZE, DATA_NUM, 0);\r\n    numCreatedModified += 1;\r\n    DFSTestUtil.appendFile(fs, f2, (int) BLOCK_SIZE);\r\n    numCreatedModified += 1;\r\n    fs.rename(bar, new Path(dir, \"foo\"));\r\n    return numCreatedModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testSync() throws Exception\n{\r\n    initData(source);\r\n    initData(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numCreatedModified = changeData(dfs, source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    final Path toDelete = new Path(source, \"foo/d1/foo/f1\");\r\n    dfs.delete(toDelete, true);\r\n    final Path newdir = new Path(source, \"foo/d1/foo/newdir\");\r\n    dfs.mkdirs(newdir);\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    DistCpSync distCpSync = new DistCpSync(context, conf);\r\n    Assert.assertTrue(distCpSync.sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s2\");\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    final Path listingPath = new Path(\"/tmp/META/fileList.seq\");\r\n    CopyListing listing = new SimpleCopyListing(conf, new Credentials(), distCpSync);\r\n    listing.buildListing(listingPath, context);\r\n    Map<Text, CopyListingFileStatus> copyListing = getListing(listingPath);\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context mapContext = stubContext.getContext();\r\n    mapContext.getConfiguration().setBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), true);\r\n    copyMapper.setup(mapContext);\r\n    for (Map.Entry<Text, CopyListingFileStatus> entry : copyListing.entrySet()) {\r\n        copyMapper.map(entry.getKey(), entry.getValue(), mapContext);\r\n    }\r\n    Assert.assertEquals(numCreatedModified, copyListing.size());\r\n    Assert.assertEquals(BLOCK_SIZE * 3, stubContext.getReporter().getCounter(CopyMapper.Counter.BYTESCOPIED).getValue());\r\n    verifyCopy(dfs.getFileStatus(spath), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync1",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testSync1() throws Exception\n{\r\n    Path srcpath = new Path(source, \"encz-mock\");\r\n    dfs.mkdirs(srcpath);\r\n    dfs.mkdirs(new Path(source, \"encz-mock/datedir\"));\r\n    enableAndCreateFirstSnapshot();\r\n    DFSTestUtil.createFile(dfs, new Path(source, \"encz-mock/datedir/file1\"), BLOCK_SIZE, DATA_NUM, 0);\r\n    dfs.delete(new Path(source, \"encz-mock/datedir\"), true);\r\n    dfs.mkdirs(new Path(source, \"encz-mock/datedir\"));\r\n    DFSTestUtil.createFile(dfs, new Path(source, \"encz-mock/datedir/file2\"), BLOCK_SIZE, DATA_NUM, 0);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    Assert.assertTrue(dfs.exists(new Path(source, \"encz-mock/datedir/file2\")));\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    DistCpSync distCpSync = new DistCpSync(context, conf);\r\n    Assert.assertTrue(distCpSync.sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s2\");\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    final Path listingPath = new Path(\"/tmp/META/fileList.seq\");\r\n    CopyListing listing = new SimpleCopyListing(conf, new Credentials(), distCpSync);\r\n    listing.buildListing(listingPath, context);\r\n    Map<Text, CopyListingFileStatus> copyListing = getListing(listingPath);\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context mapContext = stubContext.getContext();\r\n    copyMapper.setup(mapContext);\r\n    for (Map.Entry<Text, CopyListingFileStatus> entry : copyListing.entrySet()) {\r\n        copyMapper.map(entry.getKey(), entry.getValue(), mapContext);\r\n    }\r\n    Assert.assertTrue(dfs.exists(new Path(target, \"encz-mock/datedir/file2\")));\r\n    verifyCopy(dfs.getFileStatus(spath), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncNew",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testSyncNew() throws Exception\n{\r\n    Path srcpath = new Path(source, \"encz-mock\");\r\n    dfs.mkdirs(srcpath);\r\n    dfs.mkdirs(new Path(source, \"encz-mock/datedir\"));\r\n    dfs.mkdirs(new Path(source, \"trash\"));\r\n    enableAndCreateFirstSnapshot();\r\n    DFSTestUtil.createFile(dfs, new Path(source, \"encz-mock/datedir/file1\"), BLOCK_SIZE, DATA_NUM, 0);\r\n    dfs.rename(new Path(source, \"encz-mock/datedir\"), new Path(source, \"trash\"));\r\n    dfs.mkdirs(new Path(source, \"encz-mock/datedir\"));\r\n    DFSTestUtil.createFile(dfs, new Path(source, \"encz-mock/datedir/file2\"), BLOCK_SIZE, DATA_NUM, 0);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    Assert.assertTrue(dfs.exists(new Path(source, \"encz-mock/datedir/file2\")));\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    DistCpSync distCpSync = new DistCpSync(context, conf);\r\n    Assert.assertTrue(distCpSync.sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s2\");\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    final Path listingPath = new Path(\"/tmp/META/fileList.seq\");\r\n    CopyListing listing = new SimpleCopyListing(conf, new Credentials(), distCpSync);\r\n    listing.buildListing(listingPath, context);\r\n    Map<Text, CopyListingFileStatus> copyListing = getListing(listingPath);\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context mapContext = stubContext.getContext();\r\n    copyMapper.setup(mapContext);\r\n    for (Map.Entry<Text, CopyListingFileStatus> entry : copyListing.entrySet()) {\r\n        copyMapper.map(entry.getKey(), entry.getValue(), mapContext);\r\n    }\r\n    Assert.assertTrue(dfs.exists(new Path(target, \"encz-mock/datedir/file2\")));\r\n    Assert.assertTrue(dfs.exists(new Path(target, \"trash/datedir/file1\")));\r\n    verifyCopy(dfs.getFileStatus(spath), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncWithFilters",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testSyncWithFilters() throws Exception\n{\r\n    Path srcpath = new Path(source, \"encz-mock\");\r\n    dfs.mkdirs(srcpath);\r\n    dfs.mkdirs(new Path(source, \"encz-mock/datedir\"));\r\n    dfs.mkdirs(new Path(source, \"trash\"));\r\n    enableAndCreateFirstSnapshot();\r\n    DFSTestUtil.createFile(dfs, new Path(source, \"encz-mock/datedir/file1\"), BLOCK_SIZE, DATA_NUM, 0);\r\n    dfs.rename(new Path(source, \"encz-mock/datedir\"), new Path(source, \"trash\"));\r\n    dfs.mkdirs(new Path(source, \"encz-mock/datedir\"));\r\n    DFSTestUtil.createFile(dfs, new Path(source, \"encz-mock/datedir/file2\"), BLOCK_SIZE, DATA_NUM, 0);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    Assert.assertTrue(dfs.exists(new Path(source, \"encz-mock/datedir/file2\")));\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    List<Pattern> filters = new ArrayList<>();\r\n    filters.add(Pattern.compile(\".*trash.*\"));\r\n    RegexCopyFilter regexCopyFilter = new RegexCopyFilter(\"fakeFile\");\r\n    regexCopyFilter.setFilters(filters);\r\n    DistCpSync distCpSync = new DistCpSync(context, conf);\r\n    distCpSync.setCopyFilter(regexCopyFilter);\r\n    Assert.assertTrue(distCpSync.sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s2\");\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    final Path listingPath = new Path(\"/tmp/META/fileList.seq\");\r\n    CopyListing listing = new SimpleCopyListing(conf, new Credentials(), distCpSync);\r\n    listing.buildListing(listingPath, context);\r\n    Map<Text, CopyListingFileStatus> copyListing = getListing(listingPath);\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context mapContext = stubContext.getContext();\r\n    copyMapper.setup(mapContext);\r\n    for (Map.Entry<Text, CopyListingFileStatus> entry : copyListing.entrySet()) {\r\n        copyMapper.map(entry.getKey(), entry.getValue(), mapContext);\r\n    }\r\n    Assert.assertTrue(dfs.exists(new Path(target, \"encz-mock/datedir/file2\")));\r\n    Assert.assertFalse(dfs.exists(new Path(target, \"encz-mock/datedir/file1\")));\r\n    Assert.assertFalse(dfs.exists(new Path(target, \"trash/datedir/file1\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getListing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<Text, CopyListingFileStatus> getListing(Path listingPath) throws Exception\n{\r\n    SequenceFile.Reader reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(listingPath));\r\n    Text key = new Text();\r\n    CopyListingFileStatus value = new CopyListingFileStatus();\r\n    Map<Text, CopyListingFileStatus> values = new HashMap<>();\r\n    while (reader.next(key, value)) {\r\n        values.put(key, value);\r\n        key = new Text();\r\n        value = new CopyListingFileStatus();\r\n    }\r\n    return values;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "verifyCopy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyCopy(FileStatus s, FileStatus t, boolean compareName) throws Exception\n{\r\n    verifyCopy(dfs, dfs, s, t, compareName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "verifyCopyByFs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyCopyByFs(FileSystem sfs, FileSystem tfs, FileStatus s, FileStatus t, boolean compareName) throws Exception\n{\r\n    verifyCopy(sfs, tfs, s, t, compareName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "verifyCopy",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyCopy(FileSystem sfs, FileSystem tfs, FileStatus s, FileStatus t, boolean compareName) throws Exception\n{\r\n    Assert.assertEquals(s.isDirectory(), t.isDirectory());\r\n    if (compareName) {\r\n        Assert.assertEquals(s.getPath().getName(), t.getPath().getName());\r\n    }\r\n    if (!s.isDirectory()) {\r\n        byte[] sbytes = DFSTestUtil.readFileBuffer(sfs, s.getPath());\r\n        byte[] tbytes = DFSTestUtil.readFileBuffer(tfs, t.getPath());\r\n        Assert.assertArrayEquals(sbytes, tbytes);\r\n    } else {\r\n        FileStatus[] slist = sfs.listStatus(s.getPath());\r\n        FileStatus[] tlist = tfs.listStatus(t.getPath());\r\n        Assert.assertEquals(slist.length, tlist.length);\r\n        for (int i = 0; i < slist.length; i++) {\r\n            verifyCopy(sfs, tfs, slist[i], tlist[i], true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncWithCurrent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSyncWithCurrent() throws Exception\n{\r\n    final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(source), target).withSyncFolder(true).withUseDiff(\"s1\", \".\").build();\r\n    context = new DistCpContext(options);\r\n    initData(source);\r\n    initData(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData(dfs, source);\r\n    sync();\r\n    Assert.assertEquals(source, context.getSourcePaths().get(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData2",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initData2(Path dir) throws Exception\n{\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path f1 = new Path(test, \"f1\");\r\n    final Path f2 = new Path(foo, \"f2\");\r\n    final Path f3 = new Path(bar, \"f3\");\r\n    DFSTestUtil.createFile(dfs, f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, f2, BLOCK_SIZE, DATA_NUM, 1L);\r\n    DFSTestUtil.createFile(dfs, f3, BLOCK_SIZE, DATA_NUM, 2L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData2",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void changeData2(Path dir) throws Exception\n{\r\n    final Path tmpFoo = new Path(dir, \"tmpFoo\");\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    dfs.rename(test, tmpFoo);\r\n    dfs.rename(foo, test);\r\n    dfs.rename(bar, foo);\r\n    dfs.rename(tmpFoo, bar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync2",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSync2() throws Exception\n{\r\n    initData2(source);\r\n    initData2(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData2(source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    syncAndVerify();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initData3(Path dir) throws Exception\n{\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path f1 = new Path(test, \"file\");\r\n    final Path f2 = new Path(foo, \"file\");\r\n    final Path f3 = new Path(bar, \"file\");\r\n    DFSTestUtil.createFile(dfs, f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, f2, BLOCK_SIZE * 2, DATA_NUM, 1L);\r\n    DFSTestUtil.createFile(dfs, f3, BLOCK_SIZE * 3, DATA_NUM, 2L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void changeData3(Path dir) throws Exception\n{\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path f1 = new Path(test, \"file\");\r\n    final Path f2 = new Path(foo, \"file\");\r\n    final Path f3 = new Path(bar, \"file\");\r\n    final Path newf1 = new Path(test, \"newfile\");\r\n    final Path newf2 = new Path(foo, \"newfile\");\r\n    final Path newf3 = new Path(bar, \"newfile\");\r\n    dfs.rename(f1, newf1);\r\n    dfs.rename(f2, newf2);\r\n    dfs.rename(f3, newf3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync3",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSync3() throws Exception\n{\r\n    initData3(source);\r\n    initData3(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData3(source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    syncAndVerify();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData4",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initData4(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d2 = new Path(d1, \"d2\");\r\n    final Path f1 = new Path(d2, \"f1\");\r\n    DFSTestUtil.createFile(dfs, f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData4",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void changeData4(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d11 = new Path(dir, \"d11\");\r\n    final Path d2 = new Path(d1, \"d2\");\r\n    final Path d21 = new Path(d1, \"d21\");\r\n    final Path f1 = new Path(d2, \"f1\");\r\n    dfs.delete(f1, false);\r\n    dfs.rename(d2, d21);\r\n    dfs.rename(d1, d11);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync4",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSync4() throws Exception\n{\r\n    initData4(source);\r\n    initData4(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData4(source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    syncAndVerify();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData5",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData5(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d2 = new Path(dir, \"d2\");\r\n    final Path f1 = new Path(d1, \"f1\");\r\n    final Path f2 = new Path(d2, \"f2\");\r\n    DFSTestUtil.createFile(dfs, f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, f2, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData5",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void changeData5(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d2 = new Path(dir, \"d2\");\r\n    final Path f1 = new Path(d1, \"f1\");\r\n    final Path tmp = new Path(dir, \"tmp\");\r\n    dfs.delete(f1, false);\r\n    dfs.rename(d1, tmp);\r\n    dfs.rename(d2, d1);\r\n    final Path f2 = new Path(d1, \"f2\");\r\n    dfs.delete(f2, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync5",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSync5() throws Exception\n{\r\n    initData5(source);\r\n    initData5(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData5(source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    syncAndVerify();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testAndVerify",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testAndVerify(int numCreatedModified) throws Exception\n{\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(source, \"s1\", \"s2\");\r\n    System.out.println(report);\r\n    DistCpSync distCpSync = new DistCpSync(context, conf);\r\n    Assert.assertTrue(distCpSync.sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s2\");\r\n    Assert.assertEquals(spath, context.getSourcePaths().get(0));\r\n    final Path listingPath = new Path(\"/tmp/META/fileList.seq\");\r\n    CopyListing listing = new SimpleCopyListing(conf, new Credentials(), distCpSync);\r\n    listing.buildListing(listingPath, context);\r\n    Map<Text, CopyListingFileStatus> copyListing = getListing(listingPath);\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context mapContext = stubContext.getContext();\r\n    mapContext.getConfiguration().setBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), true);\r\n    copyMapper.setup(mapContext);\r\n    for (Map.Entry<Text, CopyListingFileStatus> entry : copyListing.entrySet()) {\r\n        copyMapper.map(entry.getKey(), entry.getValue(), mapContext);\r\n    }\r\n    Assert.assertEquals(numCreatedModified, copyListing.size());\r\n    verifyCopy(dfs.getFileStatus(spath), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData6",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData6(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path bar_f1 = new Path(bar, \"f1\");\r\n    DFSTestUtil.createFile(dfs, foo_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, bar_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData6",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int changeData6(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path foo2 = new Path(dir, \"foo2\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    int numCreatedModified = 0;\r\n    dfs.rename(foo, foo2);\r\n    dfs.rename(bar, foo);\r\n    dfs.rename(foo2, bar);\r\n    DFSTestUtil.appendFile(dfs, foo_f1, (int) BLOCK_SIZE);\r\n    numCreatedModified += 1;\r\n    return numCreatedModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync6",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSync6() throws Exception\n{\r\n    initData6(source);\r\n    initData6(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numCreatedModified = changeData6(source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    testAndVerify(numCreatedModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData7",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData7(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path bar_f1 = new Path(bar, \"f1\");\r\n    DFSTestUtil.createFile(dfs, foo_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, bar_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData7",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int changeData7(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path foo2 = new Path(dir, \"foo2\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path foo2_f2 = new Path(foo2, \"f2\");\r\n    final Path foo_d1 = new Path(foo, \"d1\");\r\n    final Path foo_d1_f3 = new Path(foo_d1, \"f3\");\r\n    int numCreatedModified = 0;\r\n    dfs.rename(foo, foo2);\r\n    DFSTestUtil.createFile(dfs, foo_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    numCreatedModified += 2;\r\n    DFSTestUtil.appendFile(dfs, foo_f1, (int) BLOCK_SIZE);\r\n    dfs.rename(foo_f1, foo2_f2);\r\n    numCreatedModified -= 1;\r\n    numCreatedModified += 2;\r\n    DFSTestUtil.createFile(dfs, foo_d1_f3, BLOCK_SIZE, DATA_NUM, 0L);\r\n    numCreatedModified += 2;\r\n    return numCreatedModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync7",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSync7() throws Exception\n{\r\n    initData7(source);\r\n    initData7(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numCreatedModified = changeData7(source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    testAndVerify(numCreatedModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData8",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initData8(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path bar_f1 = new Path(bar, \"f1\");\r\n    final Path d1_f1 = new Path(d1, \"f1\");\r\n    DFSTestUtil.createFile(dfs, foo_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, bar_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, d1_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData8",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int changeData8(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path createdDir = new Path(dir, \"c\");\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d1_f1 = new Path(d1, \"f1\");\r\n    final Path createdDir_f1 = new Path(createdDir, \"f1\");\r\n    final Path foo_f3 = new Path(foo, \"f3\");\r\n    final Path new_foo = new Path(createdDir, \"foo\");\r\n    final Path foo_f4 = new Path(foo, \"f4\");\r\n    final Path foo_d1 = new Path(foo, \"d1\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path bar1 = new Path(dir, \"bar1\");\r\n    int numCreatedModified = 0;\r\n    DFSTestUtil.createFile(dfs, foo_f3, BLOCK_SIZE, DATA_NUM, 0L);\r\n    numCreatedModified += 1;\r\n    DFSTestUtil.createFile(dfs, createdDir_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    numCreatedModified += 1;\r\n    dfs.rename(createdDir_f1, foo_f4);\r\n    numCreatedModified += 1;\r\n    dfs.rename(d1_f1, createdDir_f1);\r\n    numCreatedModified += 1;\r\n    dfs.rename(d1, foo_d1);\r\n    numCreatedModified += 1;\r\n    dfs.rename(foo, new_foo);\r\n    dfs.rename(bar, bar1);\r\n    return numCreatedModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync8",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSync8() throws Exception\n{\r\n    initData8(source);\r\n    initData8(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numCreatedModified = changeData8(source);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    testAndVerify(numCreatedModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData9",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initData9(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    DFSTestUtil.createFile(dfs, foo_f1, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData9",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void changeData9(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path foo_f2 = new Path(foo, \"f2\");\r\n    DFSTestUtil.createFile(dfs, foo_f2, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync9",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSync9() throws Exception\n{\r\n    Path sourcePath = new Path(dfs.getWorkingDirectory(), \"source\");\r\n    initData9(sourcePath);\r\n    initData9(target);\r\n    dfs.allowSnapshot(sourcePath);\r\n    dfs.allowSnapshot(target);\r\n    dfs.createSnapshot(sourcePath, \"s1\");\r\n    dfs.createSnapshot(target, \"s1\");\r\n    changeData9(sourcePath);\r\n    dfs.createSnapshot(sourcePath, \"s2\");\r\n    String[] args = new String[] { \"-update\", \"-diff\", \"s1\", \"s2\", \"source\", target.toString() };\r\n    new DistCp(conf, OptionsParser.parse(args)).execute();\r\n    verifyCopy(dfs.getFileStatus(sourcePath), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncSnapshotTimeStampChecking",
  "errType" : [ "HadoopIllegalArgumentException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSyncSnapshotTimeStampChecking() throws Exception\n{\r\n    initData(source);\r\n    initData(target);\r\n    dfs.allowSnapshot(source);\r\n    dfs.allowSnapshot(target);\r\n    dfs.createSnapshot(source, \"s2\");\r\n    dfs.createSnapshot(target, \"s1\");\r\n    Thread.sleep(1000);\r\n    dfs.createSnapshot(source, \"s1\");\r\n    boolean threwException = false;\r\n    try {\r\n        DistCpSync distCpSync = new DistCpSync(context, conf);\r\n        distCpSync.sync();\r\n    } catch (HadoopIllegalArgumentException e) {\r\n        threwException = true;\r\n        GenericTestUtils.assertExceptionContains(\"Snapshot s2 should be newer than s1\", e);\r\n    }\r\n    Assert.assertTrue(threwException);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData10",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData10(Path dir) throws Exception\n{\r\n    final Path staging = new Path(dir, \".staging\");\r\n    final Path stagingF1 = new Path(staging, \"f1\");\r\n    final Path data = new Path(dir, \"data\");\r\n    final Path dataF1 = new Path(data, \"f1\");\r\n    DFSTestUtil.createFile(dfs, stagingF1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, dataF1, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData10",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void changeData10(Path dir) throws Exception\n{\r\n    final Path staging = new Path(dir, \".staging\");\r\n    final Path prod = new Path(dir, \"prod\");\r\n    dfs.rename(staging, prod);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "generateFilterFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "java.nio.file.Path generateFilterFile(String fileName) throws IOException\n{\r\n    java.nio.file.Path tmpFile = Files.createTempFile(fileName, \"txt\");\r\n    String str = \".*\\\\.staging.*\";\r\n    try (BufferedWriter writer = new BufferedWriter(new FileWriter(tmpFile.toString()))) {\r\n        writer.write(str);\r\n    }\r\n    return tmpFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "deleteFilterFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteFilterFile(java.nio.file.Path filePath) throws IOException\n{\r\n    Files.delete(filePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync10",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSync10() throws Exception\n{\r\n    java.nio.file.Path filterFile = null;\r\n    try {\r\n        Path sourcePath = new Path(dfs.getWorkingDirectory(), \"source\");\r\n        initData10(sourcePath);\r\n        dfs.allowSnapshot(sourcePath);\r\n        dfs.createSnapshot(sourcePath, \"s1\");\r\n        filterFile = generateFilterFile(\"filters\");\r\n        final DistCpOptions.Builder builder = new DistCpOptions.Builder(new ArrayList<>(Arrays.asList(sourcePath)), target).withFiltersFile(filterFile.toString()).withSyncFolder(true);\r\n        new DistCp(conf, builder.build()).execute();\r\n        verifySync(dfs.getFileStatus(sourcePath), dfs.getFileStatus(target), false, \".staging\");\r\n        dfs.allowSnapshot(target);\r\n        dfs.createSnapshot(target, \"s1\");\r\n        changeData10(sourcePath);\r\n        dfs.createSnapshot(sourcePath, \"s2\");\r\n        final DistCpOptions.Builder diffBuilder = new DistCpOptions.Builder(new ArrayList<>(Arrays.asList(sourcePath)), target).withUseDiff(\"s1\", \"s2\").withFiltersFile(filterFile.toString()).withSyncFolder(true);\r\n        new DistCp(conf, diffBuilder.build()).execute();\r\n        verifyCopy(dfs.getFileStatus(sourcePath), dfs.getFileStatus(target), false);\r\n    } finally {\r\n        deleteFilterFile(filterFile);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData11",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData11(Path dir) throws Exception\n{\r\n    final Path staging = new Path(dir, \"prod\");\r\n    final Path stagingF1 = new Path(staging, \"f1\");\r\n    final Path data = new Path(dir, \"data\");\r\n    final Path dataF1 = new Path(data, \"f1\");\r\n    DFSTestUtil.createFile(dfs, stagingF1, BLOCK_SIZE, DATA_NUM, 0L);\r\n    DFSTestUtil.createFile(dfs, dataF1, BLOCK_SIZE, DATA_NUM, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData11",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void changeData11(Path dir) throws Exception\n{\r\n    final Path staging = new Path(dir, \"prod\");\r\n    final Path prod = new Path(dir, \".staging\");\r\n    dfs.rename(staging, prod);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "verifySync",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void verifySync(FileStatus s, FileStatus t, boolean compareName, String deletedName) throws Exception\n{\r\n    Assert.assertEquals(s.isDirectory(), t.isDirectory());\r\n    if (compareName) {\r\n        Assert.assertEquals(s.getPath().getName(), t.getPath().getName());\r\n    }\r\n    if (!s.isDirectory()) {\r\n        byte[] sbytes = DFSTestUtil.readFileBuffer(dfs, s.getPath());\r\n        byte[] tbytes = DFSTestUtil.readFileBuffer(dfs, t.getPath());\r\n        Assert.assertArrayEquals(sbytes, tbytes);\r\n    } else {\r\n        FileStatus[] slist = dfs.listStatus(s.getPath());\r\n        FileStatus[] tlist = dfs.listStatus(t.getPath());\r\n        int minFiles = tlist.length;\r\n        if (slist.length < tlist.length) {\r\n            minFiles = slist.length;\r\n        }\r\n        for (int i = 0; i < minFiles; i++) {\r\n            if (slist[i].getPath().getName().contains(deletedName)) {\r\n                if (tlist[i].getPath().getName().contains(deletedName)) {\r\n                    throw new Exception(\"Target is not synced as per exclusion filter\");\r\n                }\r\n                continue;\r\n            }\r\n            verifySync(slist[i], tlist[i], true, deletedName);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync11",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSync11() throws Exception\n{\r\n    java.nio.file.Path filterFile = null;\r\n    try {\r\n        Path sourcePath = new Path(dfs.getWorkingDirectory(), \"source\");\r\n        initData11(sourcePath);\r\n        dfs.allowSnapshot(sourcePath);\r\n        dfs.createSnapshot(sourcePath, \"s1\");\r\n        filterFile = generateFilterFile(\"filters\");\r\n        final DistCpOptions.Builder builder = new DistCpOptions.Builder(new ArrayList<>(Arrays.asList(sourcePath)), target).withFiltersFile(filterFile.toString()).withSyncFolder(true);\r\n        new DistCp(conf, builder.build()).execute();\r\n        verifyCopy(dfs.getFileStatus(sourcePath), dfs.getFileStatus(target), false);\r\n        dfs.allowSnapshot(target);\r\n        dfs.createSnapshot(target, \"s1\");\r\n        changeData11(sourcePath);\r\n        dfs.createSnapshot(sourcePath, \"s2\");\r\n        final DistCpOptions.Builder diffBuilder = new DistCpOptions.Builder(new ArrayList<>(Arrays.asList(sourcePath)), target).withUseDiff(\"s1\", \"s2\").withFiltersFile(filterFile.toString()).withSyncFolder(true);\r\n        new DistCp(conf, diffBuilder.build()).execute();\r\n        verifySync(dfs.getFileStatus(sourcePath), dfs.getFileStatus(target), false, \".staging\");\r\n    } finally {\r\n        deleteFilterFile(filterFile);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncSnapshotDiffWithWebHdfs1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSyncSnapshotDiffWithWebHdfs1() throws Exception\n{\r\n    Path dfsSource = new Path(dfs.getUri().toString(), source);\r\n    Path webHdfsTarget = new Path(webfs.getUri().toString(), target);\r\n    snapshotDiffWithPaths(dfsSource, webHdfsTarget);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncSnapshotDiffWithWebHdfs2",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSyncSnapshotDiffWithWebHdfs2() throws Exception\n{\r\n    Path webHdfsSource = new Path(webfs.getUri().toString(), source);\r\n    Path dfsTarget = new Path(dfs.getUri().toString(), target);\r\n    snapshotDiffWithPaths(webHdfsSource, dfsTarget);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncSnapshotDiffWithWebHdfs3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSyncSnapshotDiffWithWebHdfs3() throws Exception\n{\r\n    Path webHdfsSource = new Path(webfs.getUri().toString(), source);\r\n    Path webHdfsTarget = new Path(webfs.getUri().toString(), target);\r\n    snapshotDiffWithPaths(webHdfsSource, webHdfsTarget);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testRenameWithFilter",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testRenameWithFilter() throws Exception\n{\r\n    java.nio.file.Path filterFile = null;\r\n    try {\r\n        Path sourcePath = new Path(dfs.getWorkingDirectory(), \"source\");\r\n        dfs.mkdirs(new Path(sourcePath, \"dir1\"));\r\n        dfs.mkdirs(new Path(sourcePath, \"dir2\"));\r\n        dfs.allowSnapshot(sourcePath);\r\n        dfs.createSnapshot(sourcePath, \"s1\");\r\n        filterFile = Files.createTempFile(\"filters\", \"txt\");\r\n        String str = \".*filterDir1.*\";\r\n        try (BufferedWriter writer = new BufferedWriter(new FileWriter(filterFile.toString()))) {\r\n            writer.write(str);\r\n        }\r\n        final DistCpOptions.Builder builder = new DistCpOptions.Builder(new ArrayList<>(Arrays.asList(sourcePath)), target).withFiltersFile(filterFile.toString()).withSyncFolder(true);\r\n        new DistCp(conf, builder.build()).execute();\r\n        ContractTestUtils.assertPathExists(dfs, \"dir1 should get copied to target\", new Path(target, \"dir1\"));\r\n        ContractTestUtils.assertPathExists(dfs, \"dir2 should get copied to target\", new Path(target, \"dir2\"));\r\n        dfs.allowSnapshot(target);\r\n        dfs.createSnapshot(target, \"s1\");\r\n        dfs.rename(new Path(sourcePath, \"dir1\"), new Path(sourcePath, \"filterDir1\"));\r\n        ContractTestUtils.assertPathExists(dfs, \"'filterDir1' should be there on source\", new Path(sourcePath, \"filterDir1\"));\r\n        dfs.createSnapshot(sourcePath, \"s2\");\r\n        final DistCpOptions.Builder diffBuilder = new DistCpOptions.Builder(new ArrayList<>(Arrays.asList(sourcePath)), target).withUseDiff(\"s1\", \"s2\").withFiltersFile(filterFile.toString()).withSyncFolder(true);\r\n        new DistCp(conf, diffBuilder.build()).execute();\r\n        ContractTestUtils.assertPathExists(dfs, \"dir2 should be there on target\", new Path(target, \"dir2\"));\r\n        ContractTestUtils.assertPathDoesNotExist(dfs, \"Filtered directory 'filterDir1' shouldn't get copied\", new Path(target, \"filterDir1\"));\r\n        ContractTestUtils.assertPathDoesNotExist(dfs, \"Renamed directory 'dir1' should get deleted\", new Path(target, \"dir1\"));\r\n        ContractTestUtils.assertPathDoesNotExist(dfs, \"Filtered directory 'filterDir1' shouldn't get copied to home directory\", new Path(\"filterDir1\"));\r\n    } finally {\r\n        deleteFilterFile(filterFile);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "snapshotDiffWithPaths",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void snapshotDiffWithPaths(Path sourceFSPath, Path targetFSPath) throws Exception\n{\r\n    FileSystem sourceFS = sourceFSPath.getFileSystem(conf);\r\n    FileSystem targetFS = targetFSPath.getFileSystem(conf);\r\n    initData(sourceFS, sourceFSPath);\r\n    initData(targetFS, targetFSPath);\r\n    List<Path> paths = Arrays.asList(sourceFSPath, targetFSPath);\r\n    for (Path path : paths) {\r\n        FileSystem fs = path.getFileSystem(conf);\r\n        if (fs instanceof DistributedFileSystem) {\r\n            ((DistributedFileSystem) fs).allowSnapshot(path);\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            ((WebHdfsFileSystem) fs).allowSnapshot(path);\r\n        } else {\r\n            throw new IOException(\"Unsupported fs: \" + fs.getScheme());\r\n        }\r\n        fs.createSnapshot(path, \"s1\");\r\n    }\r\n    changeData(sourceFS, sourceFSPath);\r\n    sourceFS.createSnapshot(sourceFSPath, \"s2\");\r\n    final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(sourceFSPath), targetFSPath).withUseDiff(\"s1\", \"s2\").withSyncFolder(true).build();\r\n    options.appendToConf(conf);\r\n    new DistCp(conf, options).execute();\r\n    verifyCopyByFs(sourceFS, targetFS, sourceFS.getFileStatus(sourceFSPath), targetFS.getFileStatus(targetFSPath), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).format(true).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroy()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    Object[][] data = new Object[][] { { 1 }, { 2 }, { 10 }, { 20 } };\r\n    return Arrays.asList(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBytesToCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesToCopy()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getNumberOfPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumberOfPaths()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultipleSrcToFile",
  "errType" : [ "IOException", "InvalidInputException", "InvalidInputException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testMultipleSrcToFile()\n{\r\n    FileSystem fs = null;\r\n    try {\r\n        fs = FileSystem.get(getConf());\r\n        List<Path> srcPaths = new ArrayList<Path>();\r\n        srcPaths.add(new Path(\"/tmp/in/1\"));\r\n        srcPaths.add(new Path(\"/tmp/in/2\"));\r\n        final Path target = new Path(\"/tmp/out/1\");\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/1\");\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/2\");\r\n        fs.mkdirs(target);\r\n        final DistCpOptions options = new DistCpOptions.Builder(srcPaths, target).build();\r\n        validatePaths(new DistCpContext(options));\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n        fs.create(target).close();\r\n        try {\r\n            validatePaths(new DistCpContext(options));\r\n            Assert.fail(\"Invalid inputs accepted\");\r\n        } catch (InvalidInputException ignore) {\r\n        }\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n        srcPaths.clear();\r\n        srcPaths.add(new Path(\"/tmp/in/1\"));\r\n        fs.mkdirs(new Path(\"/tmp/in/1\"));\r\n        fs.create(target).close();\r\n        try {\r\n            validatePaths(new DistCpContext(options));\r\n            Assert.fail(\"Invalid inputs accepted\");\r\n        } catch (InvalidInputException ignore) {\r\n        }\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test input validation failed\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testDuplicates",
  "errType" : [ "IOException", "DuplicateFileException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDuplicates()\n{\r\n    FileSystem fs = null;\r\n    try {\r\n        fs = FileSystem.get(getConf());\r\n        List<Path> srcPaths = new ArrayList<Path>();\r\n        srcPaths.add(new Path(\"/tmp/in/*/*\"));\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/src1/1.txt\");\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/src2/1.txt\");\r\n        Path target = new Path(\"/tmp/out\");\r\n        Path listingFile = new Path(\"/tmp/list\");\r\n        final DistCpOptions options = new DistCpOptions.Builder(srcPaths, target).build();\r\n        final DistCpContext context = new DistCpContext(options);\r\n        CopyListing listing = CopyListing.getCopyListing(getConf(), CREDENTIALS, context);\r\n        try {\r\n            listing.buildListing(listingFile, context);\r\n            Assert.fail(\"Duplicates not detected\");\r\n        } catch (DuplicateFileException ignore) {\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered in test\", e);\r\n        Assert.fail(\"Test failed \" + e.getMessage());\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testDuplicateSourcePaths",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDuplicateSourcePaths() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getConf());\r\n    List<Path> srcPaths = new ArrayList<Path>();\r\n    try {\r\n        srcPaths.add(new Path(\"/tmp/in\"));\r\n        srcPaths.add(new Path(\"/tmp/in\"));\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/src1/1.txt\");\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/src2/1.txt\");\r\n        Path target = new Path(\"/tmp/out\");\r\n        Path listingFile = new Path(\"/tmp/list\");\r\n        final DistCpOptions options = new DistCpOptions.Builder(srcPaths, target).build();\r\n        final DistCpContext context = new DistCpContext(options);\r\n        CopyListing listing = CopyListing.getCopyListing(getConf(), CREDENTIALS, context);\r\n        listing.buildListing(listingFile, context);\r\n        Assert.assertTrue(fs.exists(listingFile));\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testBuildListing",
  "errType" : [ "IOException", "DuplicateFileException", "InvalidInputException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testBuildListing()\n{\r\n    FileSystem fs = null;\r\n    try {\r\n        fs = FileSystem.get(getConf());\r\n        List<Path> srcPaths = new ArrayList<Path>();\r\n        Path p1 = new Path(\"/tmp/in/1\");\r\n        Path p2 = new Path(\"/tmp/in/2\");\r\n        Path p3 = new Path(\"/tmp/in2/2\");\r\n        Path target = new Path(\"/tmp/out/1\");\r\n        srcPaths.add(p1.getParent());\r\n        srcPaths.add(p3.getParent());\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/1\");\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in/2\");\r\n        TestDistCpUtils.createFile(fs, \"/tmp/in2/2\");\r\n        fs.mkdirs(target);\r\n        OutputStream out = fs.create(p1);\r\n        out.write(\"ABC\".getBytes());\r\n        out.close();\r\n        out = fs.create(p2);\r\n        out.write(\"DEF\".getBytes());\r\n        out.close();\r\n        out = fs.create(p3);\r\n        out.write(\"GHIJ\".getBytes());\r\n        out.close();\r\n        Path listingFile = new Path(\"/tmp/file\");\r\n        final DistCpOptions options = new DistCpOptions.Builder(srcPaths, target).withSyncFolder(true).build();\r\n        CopyListing listing = new SimpleCopyListing(getConf(), CREDENTIALS);\r\n        try {\r\n            listing.buildListing(listingFile, new DistCpContext(options));\r\n            Assert.fail(\"Duplicates not detected\");\r\n        } catch (DuplicateFileException ignore) {\r\n        }\r\n        assertThat(listing.getBytesToCopy()).isEqualTo(10);\r\n        assertThat(listing.getNumberOfPaths()).isEqualTo(3);\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n        try {\r\n            listing.buildListing(listingFile, new DistCpContext(options));\r\n            Assert.fail(\"Invalid input not detected\");\r\n        } catch (InvalidInputException ignore) {\r\n        }\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test build listing failed\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testWithRandomFileListing",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testWithRandomFileListing() throws IOException\n{\r\n    FileSystem fs = null;\r\n    try {\r\n        fs = FileSystem.get(getConf());\r\n        List<Path> srcPaths = new ArrayList<>();\r\n        List<Path> srcFiles = new ArrayList<>();\r\n        Path target = new Path(\"/tmp/out/1\");\r\n        final int pathCount = 25;\r\n        for (int i = 0; i < pathCount; i++) {\r\n            Path p = new Path(\"/tmp\", String.valueOf(i));\r\n            srcPaths.add(p);\r\n            fs.mkdirs(p);\r\n            Path fileName = new Path(p, i + \".txt\");\r\n            srcFiles.add(fileName);\r\n            try (OutputStream out = fs.create(fileName)) {\r\n                out.write(i);\r\n            }\r\n        }\r\n        Path listingFile = new Path(\"/tmp/file\");\r\n        final DistCpOptions options = new DistCpOptions.Builder(srcPaths, target).withSyncFolder(true).build();\r\n        getConf().setBoolean(DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES, false);\r\n        SimpleCopyListing listing = new SimpleCopyListing(getConf(), CREDENTIALS);\r\n        listing.buildListing(listingFile, new DistCpContext(options));\r\n        Assert.assertEquals(listing.getNumberOfPaths(), pathCount);\r\n        validateFinalListing(listingFile, srcFiles);\r\n        fs.delete(listingFile, true);\r\n        getConf().setBoolean(DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES, true);\r\n        listing = new SimpleCopyListing(getConf(), CREDENTIALS);\r\n        long seed = System.nanoTime();\r\n        listing.setSeedForRandomListing(seed);\r\n        listing.buildListing(listingFile, new DistCpContext(options));\r\n        Assert.assertEquals(listing.getNumberOfPaths(), pathCount);\r\n        Collections.shuffle(srcFiles, new Random(seed));\r\n        validateFinalListing(listingFile, srcFiles);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "validateFinalListing",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateFinalListing(Path pathToListFile, List<Path> srcFiles) throws IOException\n{\r\n    FileSystem fs = pathToListFile.getFileSystem(config);\r\n    try (SequenceFile.Reader reader = new SequenceFile.Reader(config, SequenceFile.Reader.file(pathToListFile))) {\r\n        CopyListingFileStatus currentVal = new CopyListingFileStatus();\r\n        Text currentKey = new Text();\r\n        int idx = 0;\r\n        while (reader.next(currentKey)) {\r\n            reader.getCurrentValue(currentVal);\r\n            Assert.assertEquals(\"srcFiles.size=\" + srcFiles.size() + \", idx=\" + idx, fs.makeQualified(srcFiles.get(idx)), currentVal.getPath());\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"val=\" + fs.makeQualified(srcFiles.get(idx)));\r\n            }\r\n            idx++;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testBuildListingForSingleFile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testBuildListingForSingleFile()\n{\r\n    FileSystem fs = null;\r\n    String testRootString = \"/singleFileListing\";\r\n    Path testRoot = new Path(testRootString);\r\n    SequenceFile.Reader reader = null;\r\n    try {\r\n        fs = FileSystem.get(getConf());\r\n        if (fs.exists(testRoot))\r\n            TestDistCpUtils.delete(fs, testRootString);\r\n        Path sourceFile = new Path(testRoot, \"/source/foo/bar/source.txt\");\r\n        Path decoyFile = new Path(testRoot, \"/target/moo/source.txt\");\r\n        Path targetFile = new Path(testRoot, \"/target/moo/target.txt\");\r\n        TestDistCpUtils.createFile(fs, sourceFile.toString());\r\n        TestDistCpUtils.createFile(fs, decoyFile.toString());\r\n        TestDistCpUtils.createFile(fs, targetFile.toString());\r\n        List<Path> srcPaths = new ArrayList<Path>();\r\n        srcPaths.add(sourceFile);\r\n        DistCpOptions options = new DistCpOptions.Builder(srcPaths, targetFile).build();\r\n        CopyListing listing = new SimpleCopyListing(getConf(), CREDENTIALS);\r\n        final Path listFile = new Path(testRoot, \"/tmp/fileList.seq\");\r\n        listing.buildListing(listFile, new DistCpContext(options));\r\n        reader = new SequenceFile.Reader(getConf(), SequenceFile.Reader.file(listFile));\r\n        CopyListingFileStatus fileStatus = new CopyListingFileStatus();\r\n        Text relativePath = new Text();\r\n        Assert.assertTrue(reader.next(relativePath, fileStatus));\r\n        Assert.assertTrue(relativePath.toString().equals(\"\"));\r\n    } catch (Exception e) {\r\n        Assert.fail(\"Unexpected exception encountered.\");\r\n        LOG.error(\"Unexpected exception: \", e);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, testRootString);\r\n        IOUtils.closeStream(reader);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testFailOnCloseError",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFailOnCloseError() throws IOException\n{\r\n    File inFile = File.createTempFile(\"TestCopyListingIn\", null);\r\n    inFile.deleteOnExit();\r\n    File outFile = File.createTempFile(\"TestCopyListingOut\", null);\r\n    outFile.deleteOnExit();\r\n    List<Path> srcs = new ArrayList<Path>();\r\n    srcs.add(new Path(inFile.toURI()));\r\n    Exception expectedEx = new IOException(\"boom\");\r\n    SequenceFile.Writer writer = mock(SequenceFile.Writer.class);\r\n    doThrow(expectedEx).when(writer).close();\r\n    SimpleCopyListing listing = new SimpleCopyListing(getConf(), CREDENTIALS);\r\n    final DistCpOptions options = new DistCpOptions.Builder(srcs, new Path(outFile.toURI())).build();\r\n    Exception actualEx = null;\r\n    try {\r\n        listing.doBuildListing(writer, new DistCpContext(options));\r\n    } catch (Exception e) {\r\n        actualEx = e;\r\n    }\r\n    Assert.assertNotNull(\"close writer didn't fail\", actualEx);\r\n    Assert.assertEquals(expectedEx, actualEx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getJobForClient",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Job getJobForClient() throws IOException\n{\r\n    Job job = Job.getInstance(new Configuration());\r\n    job.getConfiguration().set(\"mapred.job.tracker\", \"localhost:\" + PORT);\r\n    job.setInputFormatClass(NullInputFormat.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    job.setNumReduceTasks(0);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    clusterConfig = getJobForClient().getConfiguration();\r\n    clusterConfig.setLong(DistCpConstants.CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED, 0);\r\n    clusterConfig.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, BLOCK_SIZE);\r\n    clusterConfig.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);\r\n    cluster = new MiniDFSCluster.Builder(clusterConfig).numDataNodes(1).format(true).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroy()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createMetaFolder",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createMetaFolder() throws IOException\n{\r\n    config = new Configuration(clusterConfig);\r\n    config.set(DistCpConstants.CONF_LABEL_META_FOLDER, \"/meta\");\r\n    Path meta = new Path(\"/meta\");\r\n    cluster.getFileSystem().mkdirs(meta);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "cleanupMetaFolder",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanupMetaFolder() throws IOException\n{\r\n    Path meta = new Path(\"/meta\");\r\n    if (cluster.getFileSystem().exists(meta)) {\r\n        cluster.getFileSystem().delete(meta, true);\r\n        Assert.fail(\"Expected meta folder to be deleted\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testNoCommitAction",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testNoCommitAction() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n    committer.commitJob(jobContext);\r\n    Assert.assertEquals(\"Commit Successful\", taskAttemptContext.getStatus());\r\n    committer.commitJob(jobContext);\r\n    Assert.assertEquals(\"Commit Successful\", taskAttemptContext.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserveStatus",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testPreserveStatus() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String sourceBase;\r\n    String targetBase;\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        FsPermission sourcePerm = new FsPermission((short) 511);\r\n        FsPermission initialPerm = new FsPermission((short) 448);\r\n        sourceBase = TestDistCpUtils.createTestSetup(fs, sourcePerm);\r\n        targetBase = TestDistCpUtils.createTestSetup(fs, initialPerm);\r\n        final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(sourceBase)), new Path(\"/out\")).preserve(FileAttribute.PERMISSION).build();\r\n        options.appendToConf(conf);\r\n        final DistCpContext context = new DistCpContext(options);\r\n        context.setTargetPathExists(false);\r\n        CopyListing listing = new GlobbedCopyListing(conf, CREDENTIALS);\r\n        Path listingFile = new Path(\"/tmp1/\" + rand.nextLong());\r\n        listing.buildListing(listingFile, context);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, targetBase);\r\n        committer.commitJob(jobContext);\r\n        checkDirectoryPermissions(fs, targetBase, sourcePerm);\r\n        committer.commitJob(jobContext);\r\n        checkDirectoryPermissions(fs, targetBase, sourcePerm);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n        conf.unset(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserveStatusWithAtomicCommit",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testPreserveStatusWithAtomicCommit() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String sourceBase;\r\n    String workBase;\r\n    String targetBase;\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        FsPermission sourcePerm = new FsPermission((short) 511);\r\n        FsPermission initialPerm = new FsPermission((short) 448);\r\n        sourceBase = TestDistCpUtils.createTestSetup(fs, sourcePerm);\r\n        workBase = TestDistCpUtils.createTestSetup(fs, initialPerm);\r\n        targetBase = \"/tmp1/\" + rand.nextLong();\r\n        final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(sourceBase)), new Path(\"/out\")).preserve(FileAttribute.PERMISSION).build();\r\n        options.appendToConf(conf);\r\n        final DistCpContext context = new DistCpContext(options);\r\n        context.setTargetPathExists(false);\r\n        CopyListing listing = new GlobbedCopyListing(conf, CREDENTIALS);\r\n        Path listingFile = new Path(\"/tmp1/\" + rand.nextLong());\r\n        listing.buildListing(listingFile, context);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, targetBase);\r\n        conf.set(CONF_LABEL_TARGET_WORK_PATH, workBase);\r\n        conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, true);\r\n        committer.commitJob(jobContext);\r\n        checkDirectoryPermissions(fs, targetBase, sourcePerm);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n        conf.unset(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteMissing",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testDeleteMissing() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String sourceBase;\r\n    String targetBase;\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        sourceBase = TestDistCpUtils.createTestSetup(fs, FsPermission.getDefault());\r\n        targetBase = TestDistCpUtils.createTestSetup(fs, FsPermission.getDefault());\r\n        String targetBaseAdd = TestDistCpUtils.createTestSetup(fs, FsPermission.getDefault());\r\n        fs.rename(new Path(targetBaseAdd), new Path(targetBase));\r\n        final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(sourceBase)), new Path(\"/out\")).withSyncFolder(true).withDeleteMissing(true).build();\r\n        options.appendToConf(conf);\r\n        final DistCpContext context = new DistCpContext(options);\r\n        CopyListing listing = new GlobbedCopyListing(conf, CREDENTIALS);\r\n        Path listingFile = new Path(\"/tmp1/\" + String.valueOf(rand.nextLong()));\r\n        listing.buildListing(listingFile, context);\r\n        conf.set(CONF_LABEL_TARGET_WORK_PATH, targetBase);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, targetBase);\r\n        committer.commitJob(jobContext);\r\n        verifyFoldersAreInSync(fs, targetBase, sourceBase);\r\n        verifyFoldersAreInSync(fs, sourceBase, targetBase);\r\n        committer.commitJob(jobContext);\r\n        verifyFoldersAreInSync(fs, targetBase, sourceBase);\r\n        verifyFoldersAreInSync(fs, sourceBase, targetBase);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n        conf.set(DistCpConstants.CONF_LABEL_DELETE_MISSING, \"false\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteMissingWithOnlyFile",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testDeleteMissingWithOnlyFile() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String sourceBase;\r\n    String targetBase;\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        sourceBase = TestDistCpUtils.createTestSetupWithOnlyFile(fs, FsPermission.getDefault());\r\n        targetBase = TestDistCpUtils.createTestSetupWithOnlyFile(fs, FsPermission.getDefault());\r\n        final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(sourceBase)), new Path(targetBase)).withSyncFolder(true).withDeleteMissing(true).build();\r\n        options.appendToConf(conf);\r\n        final DistCpContext context = new DistCpContext(options);\r\n        CopyListing listing = new GlobbedCopyListing(conf, CREDENTIALS);\r\n        Path listingFile = new Path(sourceBase);\r\n        listing.buildListing(listingFile, context);\r\n        conf.set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, targetBase);\r\n        conf.set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, targetBase);\r\n        committer.commitJob(jobContext);\r\n        verifyFoldersAreInSync(fs, targetBase, sourceBase);\r\n        verifyFoldersAreInSync(fs, sourceBase, targetBase);\r\n        committer.commitJob(jobContext);\r\n        verifyFoldersAreInSync(fs, targetBase, sourceBase);\r\n        verifyFoldersAreInSync(fs, sourceBase, targetBase);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n        conf.set(DistCpConstants.CONF_LABEL_DELETE_MISSING, \"false\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserveTimeWithDeleteMiss",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testPreserveTimeWithDeleteMiss() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        String sourceBase = TestDistCpUtils.createTestSetup(fs, FsPermission.getDefault());\r\n        String targetBase = TestDistCpUtils.createTestSetup(fs, FsPermission.getDefault());\r\n        String targetBaseAdd = TestDistCpUtils.createTestSetup(fs, FsPermission.getDefault());\r\n        fs.rename(new Path(targetBaseAdd), new Path(targetBase));\r\n        final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(sourceBase)), new Path(\"/out\")).withSyncFolder(true).withDeleteMissing(true).preserve(FileAttribute.TIMES).build();\r\n        options.appendToConf(conf);\r\n        final DistCpContext context = new DistCpContext(options);\r\n        CopyListing listing = new GlobbedCopyListing(conf, CREDENTIALS);\r\n        Path listingFile = new Path(\"/tmp1/\" + String.valueOf(rand.nextLong()));\r\n        listing.buildListing(listingFile, context);\r\n        conf.set(CONF_LABEL_TARGET_WORK_PATH, targetBase);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, targetBase);\r\n        Path sourceListing = new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));\r\n        SequenceFile.Reader sourceReader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(sourceListing));\r\n        Path targetRoot = new Path(targetBase);\r\n        committer.commitJob(jobContext);\r\n        checkDirectoryTimes(fs, sourceReader, targetRoot);\r\n        committer.commitJob(jobContext);\r\n        checkDirectoryTimes(fs, sourceReader, targetRoot);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n        conf.unset(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);\r\n        conf.set(DistCpConstants.CONF_LABEL_DELETE_MISSING, \"false\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteMissingFlatInterleavedFiles",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void testDeleteMissingFlatInterleavedFiles() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String sourceBase;\r\n    String targetBase;\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        sourceBase = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n        targetBase = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n        createFile(fs, sourceBase + \"/1\");\r\n        createFile(fs, sourceBase + \"/3\");\r\n        createFile(fs, sourceBase + \"/4\");\r\n        createFile(fs, sourceBase + \"/5\");\r\n        createFile(fs, sourceBase + \"/7\");\r\n        createFile(fs, sourceBase + \"/8\");\r\n        createFile(fs, sourceBase + \"/9\");\r\n        createFile(fs, targetBase + \"/2\");\r\n        createFile(fs, targetBase + \"/4\");\r\n        createFile(fs, targetBase + \"/5\");\r\n        createFile(fs, targetBase + \"/7\");\r\n        createFile(fs, targetBase + \"/9\");\r\n        createFile(fs, targetBase + \"/A\");\r\n        final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(sourceBase)), new Path(\"/out\")).withSyncFolder(true).withDeleteMissing(true).build();\r\n        options.appendToConf(conf);\r\n        final DistCpContext context = new DistCpContext(options);\r\n        CopyListing listing = new GlobbedCopyListing(conf, CREDENTIALS);\r\n        Path listingFile = new Path(\"/tmp1/\" + String.valueOf(rand.nextLong()));\r\n        listing.buildListing(listingFile, context);\r\n        conf.set(CONF_LABEL_TARGET_WORK_PATH, targetBase);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, targetBase);\r\n        committer.commitJob(jobContext);\r\n        verifyFoldersAreInSync(fs, targetBase, sourceBase);\r\n        Assert.assertEquals(4, fs.listStatus(new Path(targetBase)).length);\r\n        committer.commitJob(jobContext);\r\n        verifyFoldersAreInSync(fs, targetBase, sourceBase);\r\n        Assert.assertEquals(4, fs.listStatus(new Path(targetBase)).length);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n        conf.set(DistCpConstants.CONF_LABEL_DELETE_MISSING, \"false\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testAtomicCommitMissingFinal",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testAtomicCommitMissingFinal() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String workPath = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n    String finalPath = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        fs.mkdirs(new Path(workPath));\r\n        conf.set(CONF_LABEL_TARGET_WORK_PATH, workPath);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, finalPath);\r\n        conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, true);\r\n        assertPathExists(fs, \"Work path\", new Path(workPath));\r\n        assertPathDoesNotExist(fs, \"Final path\", new Path(finalPath));\r\n        committer.commitJob(jobContext);\r\n        assertPathDoesNotExist(fs, \"Work path\", new Path(workPath));\r\n        assertPathExists(fs, \"Final path\", new Path(finalPath));\r\n        committer.commitJob(jobContext);\r\n        assertPathDoesNotExist(fs, \"Work path\", new Path(workPath));\r\n        assertPathExists(fs, \"Final path\", new Path(finalPath));\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, workPath);\r\n        TestDistCpUtils.delete(fs, finalPath);\r\n        conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testAtomicCommitExistingFinal",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testAtomicCommitExistingFinal() throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String workPath = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n    String finalPath = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n    FileSystem fs = null;\r\n    try {\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        fs = FileSystem.get(conf);\r\n        fs.mkdirs(new Path(workPath));\r\n        fs.mkdirs(new Path(finalPath));\r\n        conf.set(CONF_LABEL_TARGET_WORK_PATH, workPath);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, finalPath);\r\n        conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, true);\r\n        assertPathExists(fs, \"Work path\", new Path(workPath));\r\n        assertPathExists(fs, \"Final path\", new Path(finalPath));\r\n        try {\r\n            committer.commitJob(jobContext);\r\n            Assert.fail(\"Should not be able to atomic-commit to pre-existing path.\");\r\n        } catch (Exception exception) {\r\n            assertPathExists(fs, \"Work path\", new Path(workPath));\r\n            assertPathExists(fs, \"Final path\", new Path(finalPath));\r\n            LOG.info(\"Atomic-commit Test pass.\");\r\n        }\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, workPath);\r\n        TestDistCpUtils.delete(fs, finalPath);\r\n        conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCommitWithChecksumMismatchAndSkipCrc",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitWithChecksumMismatchAndSkipCrc() throws IOException\n{\r\n    testCommitWithChecksumMismatch(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCommitWithChecksumMismatchWithoutSkipCrc",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommitWithChecksumMismatchWithoutSkipCrc() throws IOException\n{\r\n    testCommitWithChecksumMismatch(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCommitWithChecksumMismatch",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testCommitWithChecksumMismatch(boolean skipCrc) throws IOException\n{\r\n    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);\r\n    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(), taskAttemptContext.getTaskAttemptID().getJobID());\r\n    Configuration conf = jobContext.getConfiguration();\r\n    String sourceBase;\r\n    String targetBase;\r\n    FileSystem fs = null;\r\n    try {\r\n        fs = FileSystem.get(conf);\r\n        sourceBase = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n        targetBase = \"/tmp1/\" + String.valueOf(rand.nextLong());\r\n        int blocksPerChunk = 5;\r\n        String srcFilename = \"/srcdata\";\r\n        createSrcAndWorkFilesWithDifferentChecksum(fs, targetBase, sourceBase, srcFilename, blocksPerChunk);\r\n        DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(sourceBase)), new Path(\"/out\")).withBlocksPerChunk(blocksPerChunk).withCRC(skipCrc).build();\r\n        options.appendToConf(conf);\r\n        conf.setBoolean(DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES, false);\r\n        DistCpContext context = new DistCpContext(options);\r\n        context.setTargetPathExists(false);\r\n        CopyListing listing = new GlobbedCopyListing(conf, CREDENTIALS);\r\n        Path listingFile = new Path(\"/tmp1/\" + String.valueOf(rand.nextLong()));\r\n        listing.buildListing(listingFile, context);\r\n        conf.set(CONF_LABEL_TARGET_WORK_PATH, targetBase);\r\n        conf.set(CONF_LABEL_TARGET_FINAL_PATH, targetBase);\r\n        OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);\r\n        try {\r\n            committer.commitJob(jobContext);\r\n            if (!skipCrc) {\r\n                Assert.fail(\"Expected commit to fail\");\r\n            }\r\n            Path sourcePath = new Path(sourceBase + srcFilename);\r\n            CopyListingFileStatus sourceCurrStatus = new CopyListingFileStatus(fs.getFileStatus(sourcePath));\r\n            Assert.assertFalse(DistCpUtils.checksumsAreEqual(fs, new Path(sourceBase + srcFilename), null, fs, new Path(targetBase + srcFilename), sourceCurrStatus.getLen()));\r\n        } catch (IOException exception) {\r\n            if (skipCrc) {\r\n                LOG.error(\"Unexpected exception is found\", exception);\r\n                throw exception;\r\n            }\r\n            Throwable cause = exception.getCause();\r\n            GenericTestUtils.assertExceptionContains(\"Checksum mismatch\", cause);\r\n        }\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n        TestDistCpUtils.delete(fs, \"/meta\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createSrcAndWorkFilesWithDifferentChecksum",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createSrcAndWorkFilesWithDifferentChecksum(FileSystem fs, String targetBase, String sourceBase, String filename, int blocksPerChunk) throws IOException\n{\r\n    long srcSeed = System.currentTimeMillis();\r\n    long dstSeed = srcSeed + rand.nextLong();\r\n    int bufferLen = 128;\r\n    short replFactor = 2;\r\n    Path srcData = new Path(sourceBase + filename);\r\n    long firstChunkLength = BLOCK_SIZE * blocksPerChunk;\r\n    long secondChunkLength = BLOCK_SIZE / 2;\r\n    DFSTestUtil.createFile(fs, srcData, bufferLen, firstChunkLength, BLOCK_SIZE, replFactor, srcSeed);\r\n    DFSTestUtil.appendFileNewBlock((DistributedFileSystem) fs, srcData, (int) secondChunkLength);\r\n    DFSTestUtil.createFile(fs, new Path(targetBase + filename + \".____distcpSplit____0.\" + firstChunkLength), bufferLen, firstChunkLength, BLOCK_SIZE, replFactor, dstSeed);\r\n    DFSTestUtil.createFile(fs, new Path(targetBase + filename + \".____distcpSplit____\" + firstChunkLength + \".\" + secondChunkLength), bufferLen, secondChunkLength, BLOCK_SIZE, replFactor, dstSeed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getTaskAttemptContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptContext getTaskAttemptContext(Configuration conf)\n{\r\n    return new TaskAttemptContextImpl(conf, new TaskAttemptID(\"200707121733\", 1, TaskType.MAP, 1, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "checkDirectoryPermissions",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void checkDirectoryPermissions(FileSystem fs, String targetBase, FsPermission sourcePerm) throws IOException\n{\r\n    Path base = new Path(targetBase);\r\n    Stack<Path> stack = new Stack<>();\r\n    stack.push(base);\r\n    while (!stack.isEmpty()) {\r\n        Path file = stack.pop();\r\n        if (!fs.exists(file))\r\n            continue;\r\n        FileStatus[] fStatus = fs.listStatus(file);\r\n        if (fStatus == null || fStatus.length == 0)\r\n            continue;\r\n        for (FileStatus status : fStatus) {\r\n            if (status.isDirectory()) {\r\n                stack.push(status.getPath());\r\n                Assert.assertEquals(sourcePerm, status.getPermission());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "checkDirectoryTimes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkDirectoryTimes(FileSystem fs, SequenceFile.Reader sourceReader, Path targetRoot) throws IOException\n{\r\n    try {\r\n        CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\r\n        Text srcRelPath = new Text();\r\n        while (sourceReader.next(srcRelPath, srcFileStatus)) {\r\n            Path targetFile = new Path(targetRoot.toString() + \"/\" + srcRelPath);\r\n            FileStatus targetStatus = fs.getFileStatus(targetFile);\r\n            Assert.assertEquals(srcFileStatus.getModificationTime(), targetStatus.getModificationTime());\r\n            Assert.assertEquals(srcFileStatus.getAccessTime(), targetStatus.getAccessTime());\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(sourceReader);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetIgnoreFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetIgnoreFailure()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldIgnoreFailures());\r\n    builder.withIgnoreFailures(true);\r\n    Assert.assertTrue(builder.build().shouldIgnoreFailures());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetOverwrite",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetOverwrite()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldOverwrite());\r\n    builder.withOverwrite(true);\r\n    Assert.assertTrue(builder.build().shouldOverwrite());\r\n    try {\r\n        builder.withSyncFolder(true).build();\r\n        Assert.fail(\"Update and overwrite aren't allowed together\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testLogPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testLogPath()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertNull(builder.build().getLogPath());\r\n    final Path logPath = new Path(\"hdfs://localhost:8020/logs\");\r\n    builder.withLogPath(logPath);\r\n    Assert.assertEquals(logPath, builder.build().getLogPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetBlokcing",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetBlokcing()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertTrue(builder.build().shouldBlock());\r\n    builder.withBlocking(false);\r\n    Assert.assertFalse(builder.build().shouldBlock());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetBandwidth",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetBandwidth()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertEquals(0, builder.build().getMapBandwidth(), DELTA);\r\n    builder.withMapBandwidth(11);\r\n    Assert.assertEquals(11, builder.build().getMapBandwidth(), DELTA);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetNonPositiveBandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetNonPositiveBandwidth()\n{\r\n    new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withMapBandwidth(-11).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetZeroBandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSetZeroBandwidth()\n{\r\n    new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withMapBandwidth(0).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetSkipCRC",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSetSkipCRC()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldSkipCRC());\r\n    final DistCpOptions options = builder.withSyncFolder(true).withCRC(true).build();\r\n    Assert.assertTrue(options.shouldSyncFolder());\r\n    Assert.assertTrue(options.shouldSkipCRC());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetAtomicCommit",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetAtomicCommit()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldAtomicCommit());\r\n    builder.withAtomicCommit(true);\r\n    Assert.assertTrue(builder.build().shouldAtomicCommit());\r\n    try {\r\n        builder.withSyncFolder(true).build();\r\n        Assert.fail(\"Atomic and sync folders were mutually exclusive\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetWorkPath",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetWorkPath()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertNull(builder.build().getAtomicWorkPath());\r\n    builder.withAtomicCommit(true);\r\n    Assert.assertNull(builder.build().getAtomicWorkPath());\r\n    final Path workPath = new Path(\"hdfs://localhost:8020/work\");\r\n    builder.withAtomicWorkPath(workPath);\r\n    Assert.assertEquals(workPath, builder.build().getAtomicWorkPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetSyncFolders",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetSyncFolders()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldSyncFolder());\r\n    builder.withSyncFolder(true);\r\n    Assert.assertTrue(builder.build().shouldSyncFolder());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetDeleteMissing",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSetDeleteMissing()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldDeleteMissing());\r\n    DistCpOptions options = builder.withSyncFolder(true).withDeleteMissing(true).build();\r\n    Assert.assertTrue(options.shouldSyncFolder());\r\n    Assert.assertTrue(options.shouldDeleteMissing());\r\n    options = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withOverwrite(true).withDeleteMissing(true).build();\r\n    Assert.assertTrue(options.shouldOverwrite());\r\n    Assert.assertTrue(options.shouldDeleteMissing());\r\n    try {\r\n        new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withDeleteMissing(true).build();\r\n        fail(\"Delete missing should fail without update or overwrite options\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"Delete missing is applicable only with update \" + \"or overwrite options\", e);\r\n    }\r\n    try {\r\n        new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).withSyncFolder(true).withDeleteMissing(true).withUseDiff(\"s1\", \"s2\").build();\r\n        fail(\"Should have failed as -delete and -diff are mutually exclusive.\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-delete and -diff/-rdiff are mutually exclusive.\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetMaps",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetMaps()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertEquals(DistCpConstants.DEFAULT_MAPS, builder.build().getMaxMaps());\r\n    builder.maxMaps(1);\r\n    Assert.assertEquals(1, builder.build().getMaxMaps());\r\n    builder.maxMaps(0);\r\n    Assert.assertEquals(1, builder.build().getMaxMaps());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetNumListtatusThreads",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSetNumListtatusThreads()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertEquals(0, builder.build().getNumListstatusThreads());\r\n    builder.withNumListstatusThreads(12);\r\n    Assert.assertEquals(12, builder.build().getNumListstatusThreads());\r\n    builder.withNumListstatusThreads(0);\r\n    Assert.assertEquals(0, builder.build().getNumListstatusThreads());\r\n    builder.withNumListstatusThreads(MAX_NUM_LISTSTATUS_THREADS * 2);\r\n    Assert.assertEquals(MAX_NUM_LISTSTATUS_THREADS, builder.build().getNumListstatusThreads());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSourceListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSourceListing()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertEquals(new Path(\"hdfs://localhost:8020/source/first\"), builder.build().getSourceFileListing());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMissingTarget",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testMissingTarget()\n{\r\n    new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testToString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testToString()\n{\r\n    DistCpOptions option = new DistCpOptions.Builder(new Path(\"abc\"), new Path(\"xyz\")).build();\r\n    String val = \"DistCpOptions{atomicCommit=false, syncFolder=false, \" + \"deleteMissing=false, ignoreFailures=false, overwrite=false, \" + \"append=false, useDiff=false, useRdiff=false, \" + \"fromSnapshot=null, toSnapshot=null, \" + \"skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, \" + \"mapBandwidth=0.0, copyStrategy='uniformsize', preserveStatus=[], \" + \"atomicWorkPath=null, logPath=null, sourceFileListing=abc, \" + \"sourcePaths=null, targetPath=xyz, filtersFile='null', \" + \"blocksPerChunk=0, copyBufferSize=8192, verboseLog=false, \" + \"directWrite=false, useiterator=false, updateRoot=false}\";\r\n    String optionString = option.toString();\r\n    Assert.assertEquals(val, optionString);\r\n    Assert.assertNotSame(DistCpOptionSwitch.ATOMIC_COMMIT.toString(), DistCpOptionSwitch.ATOMIC_COMMIT.name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCopyStrategy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCopyStrategy()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertEquals(DistCpConstants.UNIFORMSIZE, builder.build().getCopyStrategy());\r\n    builder.withCopyStrategy(\"dynamic\");\r\n    Assert.assertEquals(\"dynamic\", builder.build().getCopyStrategy());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testTargetPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTargetPath()\n{\r\n    final DistCpOptions options = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).build();\r\n    Assert.assertEquals(new Path(\"hdfs://localhost:8020/target/\"), options.getTargetPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserve",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testPreserve()\n{\r\n    DistCpOptions options = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).build();\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    options = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).preserve(FileAttribute.ACL).build();\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.ACL));\r\n    options = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).preserve(FileAttribute.BLOCKSIZE).preserve(FileAttribute.REPLICATION).preserve(FileAttribute.PERMISSION).preserve(FileAttribute.USER).preserve(FileAttribute.GROUP).preserve(FileAttribute.CHECKSUMTYPE).build();\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.XATTR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testAppendOption",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAppendOption()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withSyncFolder(true).withAppend(true);\r\n    Assert.assertTrue(builder.build().shouldAppend());\r\n    try {\r\n        new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withAppend(true).build();\r\n        fail(\"Append should fail if update option is not specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"Append is valid only with update options\", e);\r\n    }\r\n    try {\r\n        new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withSyncFolder(true).withAppend(true).withCRC(true).build();\r\n        fail(\"Append should fail if skipCrc option is specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"Append is disallowed when skipping CRC\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testDiffOption",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testDiffOption()\n{\r\n    DistCpOptions options = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).withSyncFolder(true).withUseDiff(\"s1\", \"s2\").build();\r\n    Assert.assertTrue(options.shouldUseDiff());\r\n    Assert.assertEquals(\"s1\", options.getFromSnapshot());\r\n    Assert.assertEquals(\"s2\", options.getToSnapshot());\r\n    options = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).withSyncFolder(true).withUseDiff(\"s1\", \".\").build();\r\n    Assert.assertTrue(options.shouldUseDiff());\r\n    Assert.assertEquals(\"s1\", options.getFromSnapshot());\r\n    Assert.assertEquals(\".\", options.getToSnapshot());\r\n    try {\r\n        new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).withUseDiff(\"s1\", \"s2\").build();\r\n        fail(\"-diff should fail if -update option is not specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-diff/-rdiff is valid only with -update option\", e);\r\n    }\r\n    try {\r\n        new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).withSyncFolder(true).withUseDiff(\"s1\", \"s2\").withDeleteMissing(true).build();\r\n        fail(\"Should fail as -delete and -diff/-rdiff are mutually exclusive.\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-delete and -diff/-rdiff are mutually exclusive.\", e);\r\n    }\r\n    try {\r\n        new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).withUseDiff(\"s1\", \"s2\").withDeleteMissing(true).build();\r\n        fail(\"-diff should fail if -update option is not specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-delete and -diff/-rdiff are mutually exclusive.\", e);\r\n    }\r\n    try {\r\n        new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\")).withDeleteMissing(true).withUseDiff(\"s1\", \"s2\").build();\r\n        fail(\"Should have failed as -delete and -diff are mutually exclusive\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-delete and -diff/-rdiff are mutually exclusive\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testExclusionsOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testExclusionsOption()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertNull(builder.build().getFiltersFile());\r\n    builder.withFiltersFile(\"/tmp/filters.txt\");\r\n    Assert.assertEquals(\"/tmp/filters.txt\", builder.build().getFiltersFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetOptionsForSplitLargeFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetOptionsForSplitLargeFile()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/\"), new Path(\"hdfs://localhost:8020/target/\")).withAppend(true).withSyncFolder(true);\r\n    Assert.assertFalse(builder.build().shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertTrue(builder.build().shouldAppend());\r\n    builder.withBlocksPerChunk(5440);\r\n    Assert.assertTrue(builder.build().shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertFalse(builder.build().shouldAppend());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSetCopyBufferSize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetCopyBufferSize()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertEquals(DistCpConstants.COPY_BUFFER_SIZE_DEFAULT, builder.build().getCopyBufferSize());\r\n    builder.withCopyBufferSize(4194304);\r\n    Assert.assertEquals(4194304, builder.build().getCopyBufferSize());\r\n    builder.withCopyBufferSize(-1);\r\n    Assert.assertEquals(DistCpConstants.COPY_BUFFER_SIZE_DEFAULT, builder.build().getCopyBufferSize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testVerboseLog",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testVerboseLog()\n{\r\n    final DistCpOptions.Builder builder = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldVerboseLog());\r\n    try {\r\n        builder.withVerboseLog(true).build();\r\n        fail(\"-v should fail if -log option is not specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-v is valid only with -log option\", e);\r\n    }\r\n    final Path logPath = new Path(\"hdfs://localhost:8020/logs\");\r\n    builder.withLogPath(logPath).withVerboseLog(true);\r\n    Assert.assertTrue(builder.build().shouldVerboseLog());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testAppendToConf",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAppendToConf()\n{\r\n    final int expectedBlocksPerChunk = 999;\r\n    final String expectedValForEmptyConfigKey = \"VALUE_OF_EMPTY_CONFIG_KEY\";\r\n    DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withBlocksPerChunk(expectedBlocksPerChunk).build();\r\n    Configuration config = new Configuration();\r\n    config.set(\"\", expectedValForEmptyConfigKey);\r\n    options.appendToConf(config);\r\n    Assert.assertEquals(expectedBlocksPerChunk, config.getInt(DistCpOptionSwitch.BLOCKS_PER_CHUNK.getConfigLabel(), 0));\r\n    Assert.assertEquals(\"Some DistCpOptionSwitch's config label is empty! \" + \"Pls ensure the config label is provided when apply to config, \" + \"otherwise it may not be fetched properly\", expectedValForEmptyConfigKey, config.get(\"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateRoot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testUpdateRoot()\n{\r\n    final DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(new Path(\"hdfs://localhost:8020/source\")), new Path(\"hdfs://localhost:8020/target/\")).withUpdateRoot(true).build();\r\n    Assert.assertTrue(options.shouldUpdateRoot());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testShouldCopy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testShouldCopy()\n{\r\n    Configuration configuration = new Configuration(false);\r\n    configuration.set(DistCpConstants.DISTCP_EXCLUDE_FILE_REGEX, \"\\\\/.*_COPYING_$|\\\\/.*_COPYING$|^.*\\\\/\\\\.[^\\\\/]*$|\" + \"\\\\/_temporary$|\\\\/\\\\_temporary\\\\/|.*\\\\/\\\\.Trash\\\\/.*\");\r\n    RegexpInConfigurationFilter defaultCopyFilter = new RegexpInConfigurationFilter(configuration);\r\n    Path shouldCopyPath = new Path(\"/user/bar\");\r\n    assertTrue(shouldCopyPath.toString() + \" should be copied\", defaultCopyFilter.shouldCopy(shouldCopyPath));\r\n    shouldCopyPath = new Path(\"/user/bar/_COPYING\");\r\n    assertFalse(shouldCopyPath.toString() + \" shouldn't be copied\", defaultCopyFilter.shouldCopy(shouldCopyPath));\r\n    shouldCopyPath = new Path(\"/user/bar/_COPYING_\");\r\n    assertFalse(shouldCopyPath.toString() + \" shouldn't be copied\", defaultCopyFilter.shouldCopy(shouldCopyPath));\r\n    shouldCopyPath = new Path(\"/temp/\");\r\n    assertTrue(shouldCopyPath.toString() + \" should be copied\", defaultCopyFilter.shouldCopy(shouldCopyPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initSourcePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initSourcePath()\n{\r\n    setSource(new Path(\"/target\"));\r\n    setSrcNotSameAsTgt(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "assertXAttrs",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void assertXAttrs(Path path, FileSystem fs, Map<String, byte[]> expectedXAttrs) throws Exception\n{\r\n    Map<String, byte[]> xAttrs = fs.getXAttrs(path);\r\n    assertEquals(path.toString(), expectedXAttrs.size(), xAttrs.size());\r\n    Iterator<Entry<String, byte[]>> i = expectedXAttrs.entrySet().iterator();\r\n    while (i.hasNext()) {\r\n        Entry<String, byte[]> e = i.next();\r\n        String name = e.getKey();\r\n        byte[] value = e.getValue();\r\n        if (value == null) {\r\n            assertTrue(xAttrs.containsKey(name) && xAttrs.get(name) == null);\r\n        } else {\r\n            assertArrayEquals(value, xAttrs.get(name));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "assertRunDistCp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertRunDistCp(int exitCode, String src, String dst, String options, Configuration conf) throws Exception\n{\r\n    assertRunDistCp(exitCode, src, dst, options == null ? new String[0] : options.trim().split(\" \"), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "assertRunDistCp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertRunDistCp(int exitCode, String src, String dst, String[] options, Configuration conf) throws Exception\n{\r\n    DistCp distCp = new DistCp(conf, null);\r\n    String[] optsArr = new String[options.length + 2];\r\n    System.arraycopy(options, 0, optsArr, 0, options.length);\r\n    optsArr[optsArr.length - 2] = src;\r\n    optsArr[optsArr.length - 1] = dst;\r\n    assertEquals(exitCode, ToolRunner.run(conf, distCp, optsArr));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    Object[][] data = new Object[][] { { 1 }, { 2 }, { 10 } };\r\n    return Arrays.asList(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.default.name\", \"file:///\");\r\n    conf.set(\"mapred.job.tracker\", \"local\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup()\n{\r\n    try {\r\n        fs = FileSystem.get(getConf());\r\n        listFile = new Path(\"target/tmp/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        target = new Path(\"target/tmp/target\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        root = new Path(\"target/tmp\").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();\r\n        TestDistCpUtils.delete(fs, root);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileMissingTarget",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileMissingTarget()\n{\r\n    caseSingleFileMissingTarget(false);\r\n    caseSingleFileMissingTarget(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileMissingTarget",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseSingleFileMissingTarget(boolean sync)\n{\r\n    try {\r\n        addEntries(listFile, \"singlefile1/file1\");\r\n        createFiles(\"singlefile1/file1\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 1);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileTargetFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileTargetFile()\n{\r\n    caseSingleFileTargetFile(false);\r\n    caseSingleFileTargetFile(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileTargetFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseSingleFileTargetFile(boolean sync)\n{\r\n    try {\r\n        addEntries(listFile, \"singlefile1/file1\");\r\n        createFiles(\"singlefile1/file1\", \"target\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 1);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileTargetDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileTargetDir()\n{\r\n    caseSingleFileTargetDir(false);\r\n    caseSingleFileTargetDir(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileTargetDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void caseSingleFileTargetDir(boolean sync)\n{\r\n    try {\r\n        addEntries(listFile, \"singlefile2/file2\");\r\n        createFiles(\"singlefile2/file2\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, sync);\r\n        checkResult(target, 1, \"file2\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleDirTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleDirTargetMissing()\n{\r\n    caseSingleDirTargetMissing(false);\r\n    caseSingleDirTargetMissing(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleDirTargetMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseSingleDirTargetMissing(boolean sync)\n{\r\n    try {\r\n        addEntries(listFile, \"singledir\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 1, \"dir1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSingleDirTargetPresent()\n{\r\n    try {\r\n        addEntries(listFile, \"singledir\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, false);\r\n        checkResult(target, 1, \"singledir/dir1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateSingleDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateSingleDirTargetPresent()\n{\r\n    try {\r\n        addEntries(listFile, \"Usingledir\");\r\n        mkdirs(root + \"/Usingledir/Udir1\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, true);\r\n        checkResult(target, 1, \"Udir1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiFileTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiFileTargetPresent()\n{\r\n    caseMultiFileTargetPresent(false);\r\n    caseMultiFileTargetPresent(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseMultiFileTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void caseMultiFileTargetPresent(boolean sync)\n{\r\n    try {\r\n        addEntries(listFile, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, sync);\r\n        checkResult(target, 3, \"file3\", \"file4\", \"file5\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiFileTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiFileTargetMissing()\n{\r\n    caseMultiFileTargetMissing(false);\r\n    caseMultiFileTargetMissing(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseMultiFileTargetMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseMultiFileTargetMissing(boolean sync)\n{\r\n    try {\r\n        addEntries(listFile, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 3, \"file3\", \"file4\", \"file5\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMultiDirTargetPresent()\n{\r\n    try {\r\n        addEntries(listFile, \"multifile\", \"singledir\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(target.toString(), root + \"/singledir/dir1\");\r\n        runTest(listFile, target, true, false);\r\n        checkResult(target, 2, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\", \"singledir/dir1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateMultiDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateMultiDirTargetPresent()\n{\r\n    try {\r\n        addEntries(listFile, \"Umultifile\", \"Usingledir\");\r\n        createFiles(\"Umultifile/Ufile3\", \"Umultifile/Ufile4\", \"Umultifile/Ufile5\");\r\n        mkdirs(target.toString(), root + \"/Usingledir/Udir1\");\r\n        runTest(listFile, target, true, true);\r\n        checkResult(target, 4, \"Ufile3\", \"Ufile4\", \"Ufile5\", \"Udir1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiDirTargetMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMultiDirTargetMissing()\n{\r\n    try {\r\n        addEntries(listFile, \"multifile\", \"singledir\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        runTest(listFile, target, false, false);\r\n        checkResult(target, 2, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\", \"singledir/dir1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateMultiDirTargetMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateMultiDirTargetMissing()\n{\r\n    try {\r\n        addEntries(listFile, \"multifile\", \"singledir\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        runTest(listFile, target, false, true);\r\n        checkResult(target, 4, \"file3\", \"file4\", \"file5\", \"dir1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testDeleteMissingInDestination",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDeleteMissingInDestination()\n{\r\n    try {\r\n        addEntries(listFile, \"srcdir\");\r\n        createFiles(\"srcdir/file1\", \"dstdir/file1\", \"dstdir/file2\");\r\n        Path target = new Path(root + \"/dstdir\");\r\n        runTest(listFile, target, false, true, true, false);\r\n        checkResult(target, 1, \"file1\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while running distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testOverwrite",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testOverwrite()\n{\r\n    byte[] contents1 = \"contents1\".getBytes();\r\n    byte[] contents2 = \"contents2\".getBytes();\r\n    Assert.assertEquals(contents1.length, contents2.length);\r\n    try {\r\n        addEntries(listFile, \"srcdir\");\r\n        createWithContents(\"srcdir/file1\", contents1);\r\n        createWithContents(\"dstdir/file1\", contents2);\r\n        Path target = new Path(root + \"/dstdir\");\r\n        runTest(listFile, target, false, false, false, true);\r\n        checkResult(target, 1, \"file1\");\r\n        FSDataInputStream is = fs.open(new Path(root + \"/dstdir/file1\"));\r\n        byte[] dstContents = new byte[contents1.length];\r\n        is.readFully(dstContents);\r\n        is.close();\r\n        Assert.assertArrayEquals(contents1, dstContents);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while running distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobTargetMissingSingleLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGlobTargetMissingSingleLevel()\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir/dir2/file6\");\r\n        runTest(listFile, target, false, false);\r\n        checkResult(target, 2, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\", \"singledir/dir2/file6\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateGlobTargetMissingSingleLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testUpdateGlobTargetMissingSingleLevel()\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir/dir2/file6\");\r\n        runTest(listFile, target, false, true);\r\n        checkResult(target, 4, \"file3\", \"file4\", \"file5\", \"dir2/file6\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while running distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobTargetMissingMultiLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGlobTargetMissingMultiLevel()\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*/*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir1/dir3/file7\", \"singledir1/dir3/file8\", \"singledir1/dir3/file9\");\r\n        runTest(listFile, target, false, false);\r\n        checkResult(target, 4, \"file3\", \"file4\", \"file5\", \"dir3/file7\", \"dir3/file8\", \"dir3/file9\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while running distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateGlobTargetMissingMultiLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testUpdateGlobTargetMissingMultiLevel()\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*/*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir1/dir3/file7\", \"singledir1/dir3/file8\", \"singledir1/dir3/file9\");\r\n        runTest(listFile, target, false, true);\r\n        checkResult(target, 6, \"file3\", \"file4\", \"file5\", \"file7\", \"file8\", \"file9\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while running distcp\", e);\r\n        Assert.fail(\"distcp failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCleanup",
  "errType" : [ "Exception", "Throwable" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCleanup()\n{\r\n    try {\r\n        Path sourcePath = new Path(\"noscheme:///file\");\r\n        List<Path> sources = new ArrayList<Path>();\r\n        sources.add(sourcePath);\r\n        DistCpOptions options = new DistCpOptions.Builder(sources, target).build();\r\n        Configuration conf = getConf();\r\n        Path stagingDir = JobSubmissionFiles.getStagingDir(new Cluster(conf), conf);\r\n        stagingDir.getFileSystem(conf).mkdirs(stagingDir);\r\n        try {\r\n            new DistCp(conf, options).execute();\r\n        } catch (Throwable t) {\r\n            Assert.assertEquals(stagingDir.getFileSystem(conf).listStatus(stagingDir).length, 0);\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"testCleanup failed \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "addEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addEntries(Path listFile, String... entries) throws IOException\n{\r\n    OutputStream out = fs.create(listFile);\r\n    try {\r\n        for (String entry : entries) {\r\n            out.write((root + \"/\" + entry).getBytes());\r\n            out.write(\"\\n\".getBytes());\r\n        }\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createFiles(String... entries) throws IOException\n{\r\n    for (String entry : entries) {\r\n        OutputStream out = fs.create(new Path(root + \"/\" + entry));\r\n        try {\r\n            out.write((root + \"/\" + entry).getBytes());\r\n            out.write(\"\\n\".getBytes());\r\n        } finally {\r\n            out.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createWithContents",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createWithContents(String entry, byte[] contents) throws IOException\n{\r\n    OutputStream out = fs.create(new Path(root + \"/\" + entry));\r\n    try {\r\n        out.write(contents);\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mkdirs(String... entries) throws IOException\n{\r\n    for (String entry : entries) {\r\n        fs.mkdirs(new Path(entry));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "runTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runTest(Path listFile, Path target, boolean targetExists, boolean sync) throws IOException\n{\r\n    runTest(listFile, target, targetExists, sync, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "runTest",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void runTest(Path listFile, Path target, boolean targetExists, boolean sync, boolean delete, boolean overwrite) throws IOException\n{\r\n    final DistCpOptions options = new DistCpOptions.Builder(listFile, target).withSyncFolder(sync).withDeleteMissing(delete).withOverwrite(overwrite).withNumListstatusThreads(numListstatusThreads).build();\r\n    try {\r\n        final DistCp distCp = new DistCp(getConf(), options);\r\n        distCp.context.setTargetPathExists(targetExists);\r\n        distCp.execute();\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkResult",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkResult(Path target, int count, String... relPaths) throws IOException\n{\r\n    Assert.assertEquals(count, fs.listStatus(target).length);\r\n    if (relPaths == null || relPaths.length == 0) {\r\n        Assert.assertTrue(target.toString(), fs.exists(target));\r\n        return;\r\n    }\r\n    for (String relPath : relPaths) {\r\n        Assert.assertTrue(new Path(target, relPath).toString(), fs.exists(new Path(target, relPath)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testRead",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRead()\n{\r\n    File tmpFile;\r\n    File outFile;\r\n    try {\r\n        tmpFile = createFile(1024);\r\n        outFile = createFile();\r\n        tmpFile.deleteOnExit();\r\n        outFile.deleteOnExit();\r\n        long maxBandwidth = copyAndAssert(tmpFile, outFile, 0, 1, -1, CB.BUFFER);\r\n        copyAndAssert(tmpFile, outFile, maxBandwidth, 20, 0, CB.BUFFER);\r\n        copyAndAssert(tmpFile, outFile, maxBandwidth, 20, 0, CB.BUFF_OFFSET);\r\n        copyAndAssert(tmpFile, outFile, maxBandwidth, 20, 0, CB.ONE_C);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "copyAndAssert",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "long copyAndAssert(File tmpFile, File outFile, long maxBandwidth, float factor, int sleepTime, CB flag) throws IOException\n{\r\n    long bandwidth;\r\n    ThrottledInputStream in;\r\n    long maxBPS = (long) (maxBandwidth / factor);\r\n    if (maxBandwidth == 0) {\r\n        in = new ThrottledInputStream(new FileInputStream(tmpFile));\r\n    } else {\r\n        in = new ThrottledInputStream(new FileInputStream(tmpFile), maxBPS);\r\n    }\r\n    OutputStream out = new FileOutputStream(outFile);\r\n    try {\r\n        if (flag == CB.BUFFER) {\r\n            copyBytes(in, out, BUFF_SIZE);\r\n        } else if (flag == CB.BUFF_OFFSET) {\r\n            copyBytesWithOffset(in, out, BUFF_SIZE);\r\n        } else {\r\n            copyByteByByte(in, out);\r\n        }\r\n        LOG.info(\"{}\", in);\r\n        bandwidth = in.getBytesPerSec();\r\n        Assert.assertEquals(in.getTotalBytesRead(), tmpFile.length());\r\n        Assert.assertTrue(in.getBytesPerSec() > maxBandwidth / (factor * 1.2));\r\n        Assert.assertTrue(in.getTotalSleepTime() > sleepTime || in.getBytesPerSec() <= maxBPS);\r\n    } finally {\r\n        IOUtils.closeStream(in);\r\n        IOUtils.closeStream(out);\r\n    }\r\n    return bandwidth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "copyBytesWithOffset",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void copyBytesWithOffset(InputStream in, OutputStream out, int buffSize) throws IOException\n{\r\n    byte[] buf = new byte[buffSize];\r\n    int bytesRead = in.read(buf, 0, buffSize);\r\n    while (bytesRead >= 0) {\r\n        out.write(buf, 0, bytesRead);\r\n        bytesRead = in.read(buf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "copyByteByByte",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void copyByteByByte(InputStream in, OutputStream out) throws IOException\n{\r\n    int ch = in.read();\r\n    while (ch >= 0) {\r\n        out.write(ch);\r\n        ch = in.read();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "copyBytes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void copyBytes(InputStream in, OutputStream out, int buffSize) throws IOException\n{\r\n    byte[] buf = new byte[buffSize];\r\n    int bytesRead = in.read(buf);\r\n    while (bytesRead >= 0) {\r\n        out.write(buf, 0, bytesRead);\r\n        bytesRead = in.read(buf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File createFile(long sizeInKB) throws IOException\n{\r\n    File tmpFile = createFile();\r\n    writeToFile(tmpFile, sizeInKB);\r\n    return tmpFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File createFile() throws IOException\n{\r\n    return File.createTempFile(\"tmp\", \"dat\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "writeToFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeToFile(File tmpFile, long sizeInKB) throws IOException\n{\r\n    OutputStream out = new FileOutputStream(tmpFile);\r\n    try {\r\n        byte[] buffer = new byte[1024];\r\n        for (long index = 0; index < sizeInKB; index++) {\r\n            out.write(buffer);\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getTestTimeoutMillis",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTestTimeoutMillis()\n{\r\n    return 15 * 60 * 1000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration newConf = new Configuration();\r\n    newConf.set(\"mapred.job.tracker\", \"local\");\r\n    return newConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    super.setup();\r\n    conf = getContract().getConf();\r\n    localFS = FileSystem.getLocal(conf);\r\n    remoteFS = getFileSystem();\r\n    String className = getClass().getSimpleName();\r\n    String testSubDir = className + \"/\" + testName.getMethodName();\r\n    localDir = localFS.makeQualified(new Path(new Path(GenericTestUtils.getTestDir().toURI()), testSubDir + \"/local\"));\r\n    localFS.delete(localDir, true);\r\n    mkdirs(localFS, localDir);\r\n    Path testSubPath = path(testSubDir);\r\n    remoteDir = new Path(testSubPath, \"remote\");\r\n    remoteFS.delete(remoteDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    logIOStatisticsAtLevel(LOG, IOSTATISTICS_LOGGING_LEVEL_INFO, getRemoteFS());\r\n    super.teardown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "initPathFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initPathFields(final Path src, final Path dest)\n{\r\n    initInputFields(src);\r\n    initOutputFields(dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "initOutputFields",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initOutputFields(final Path path)\n{\r\n    outputDir = new Path(path, \"outputDir\");\r\n    inputDirUnderOutputDir = new Path(outputDir, \"inputDir\");\r\n    outputFile1 = new Path(inputDirUnderOutputDir, \"file1\");\r\n    outputSubDir1 = new Path(inputDirUnderOutputDir, \"subDir1\");\r\n    outputFile2 = new Path(outputSubDir1, \"file2\");\r\n    outputSubDir2 = new Path(inputDirUnderOutputDir, \"subDir2/subDir2\");\r\n    outputFile3 = new Path(outputSubDir2, \"file3\");\r\n    outputSubDir4 = new Path(inputDirUnderOutputDir, \"subDir4/subDir4\");\r\n    outputFile4 = new Path(outputSubDir4, \"file4\");\r\n    outputFile5 = new Path(outputSubDir4, \"file5\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "initInputFields",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initInputFields(final Path srcDir)\n{\r\n    inputDir = new Path(srcDir, \"inputDir\");\r\n    inputFile1 = new Path(inputDir, \"file1\");\r\n    inputSubDir1 = new Path(inputDir, \"subDir1\");\r\n    inputFile2 = new Path(inputSubDir1, \"file2\");\r\n    inputSubDir2 = new Path(inputDir, \"subDir2/subDir2\");\r\n    inputFile3 = new Path(inputSubDir2, \"file3\");\r\n    inputSubDir4 = new Path(inputDir, \"subDir4/subDir4\");\r\n    inputFile4 = new Path(inputSubDir4, \"file4\");\r\n    inputFile5 = new Path(inputSubDir4, \"file5\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getLocalFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getLocalFS()\n{\r\n    return localFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getRemoteFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getRemoteFS()\n{\r\n    return remoteFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getLocalDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getLocalDir()\n{\r\n    return localDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getRemoteDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getRemoteDir()\n{\r\n    return remoteDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testUpdateDeepDirectoryStructureToRemote",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUpdateDeepDirectoryStructureToRemote() throws Exception\n{\r\n    describe(\"update a deep directory structure from local to remote\");\r\n    distCpDeepDirectoryStructure(localFS, localDir, remoteFS, remoteDir);\r\n    distCpUpdateDeepDirectoryStructure(inputDirUnderOutputDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testUpdateDeepDirectoryStructureNoChange",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUpdateDeepDirectoryStructureNoChange() throws Exception\n{\r\n    describe(\"update an unchanged directory structure\" + \" from local to remote; expect no copy\");\r\n    Path target = distCpDeepDirectoryStructure(localFS, localDir, remoteFS, remoteDir);\r\n    describe(\"\\nExecuting Update\\n\");\r\n    Job job = distCpUpdate(localDir, target);\r\n    assertCounterInRange(job, CopyMapper.Counter.SKIP, 1, -1);\r\n    assertCounterInRange(job, CopyMapper.Counter.BYTESCOPIED, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "assertCounterInRange",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertCounterInRange(Job job, Enum<?> counter, long min, long max) throws IOException\n{\r\n    Counter c = job.getCounters().findCounter(counter);\r\n    long value = c.getValue();\r\n    String description = String.format(\"%s value %s\", c.getDisplayName(), value, false);\r\n    if (min >= 0) {\r\n        assertTrue(description + \" too below minimum \" + min, value >= min);\r\n    }\r\n    if (max >= 0) {\r\n        assertTrue(description + \" above maximum \" + max, value <= max);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "distCpUpdateDeepDirectoryStructure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Job distCpUpdateDeepDirectoryStructure(final Path destDir) throws Exception\n{\r\n    describe(\"Now do an incremental update with deletion of missing files\");\r\n    Path srcDir = inputDir;\r\n    LOG.info(\"Source directory = {}, dest={}\", srcDir, destDir);\r\n    ContractTestUtils.assertPathsExist(localFS, \"Paths for test are wrong\", inputFile1, inputFile2, inputFile3, inputFile4, inputFile5);\r\n    modifySourceDirectories();\r\n    Job job = distCpUpdate(srcDir, destDir);\r\n    Path outputFileNew1 = new Path(outputSubDir2, \"newfile1\");\r\n    lsR(\"Updated Remote\", remoteFS, destDir);\r\n    ContractTestUtils.assertPathDoesNotExist(remoteFS, \" deleted from \" + inputFile1, outputFile1);\r\n    ContractTestUtils.assertIsFile(remoteFS, outputFileNew1);\r\n    ContractTestUtils.assertPathsDoNotExist(remoteFS, \"DistCP should have deleted\", outputFile3, outputFile4, outputSubDir4);\r\n    assertCounterInRange(job, CopyMapper.Counter.COPY, 1, 1);\r\n    assertCounterInRange(job, CopyMapper.Counter.SKIP, 1, -1);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "distCpUpdate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job distCpUpdate(final Path srcDir, final Path destDir) throws Exception\n{\r\n    describe(\"\\nDistcp -update from \" + srcDir + \" to \" + destDir);\r\n    lsR(\"Local to update\", localFS, srcDir);\r\n    lsR(\"Remote before update\", remoteFS, destDir);\r\n    return runDistCp(buildWithStandardOptions(new DistCpOptions.Builder(Collections.singletonList(srcDir), destDir).withDeleteMissing(true).withSyncFolder(true).withCRC(true).withDirectWrite(shouldUseDirectWrite()).withOverwrite(false)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "modifySourceDirectories",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path modifySourceDirectories() throws IOException\n{\r\n    localFS.delete(inputFile1, false);\r\n    localFS.delete(inputFile3, false);\r\n    localFS.delete(inputSubDir4, true);\r\n    Path inputFileNew1 = new Path(inputSubDir2, \"newfile1\");\r\n    ContractTestUtils.touch(localFS, inputFileNew1);\r\n    return inputFileNew1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testTrackDeepDirectoryStructureToRemote",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testTrackDeepDirectoryStructureToRemote() throws Exception\n{\r\n    describe(\"copy a deep directory structure from local to remote\");\r\n    Path destDir = distCpDeepDirectoryStructure(localFS, localDir, remoteFS, remoteDir);\r\n    ContractTestUtils.assertIsDirectory(remoteFS, destDir);\r\n    describe(\"Now do an incremental update and save of missing files\");\r\n    Path srcDir = inputDir;\r\n    Path trackDir = new Path(localDir, \"trackDir\");\r\n    describe(\"\\nDirectories\\n\");\r\n    lsR(\"Local to update\", localFS, srcDir);\r\n    lsR(\"Remote before update\", remoteFS, destDir);\r\n    ContractTestUtils.assertPathsExist(localFS, \"Paths for test are wrong\", inputFile2, inputFile3, inputFile4, inputFile5);\r\n    Path inputFileNew1 = modifySourceDirectories();\r\n    runDistCp(buildWithStandardOptions(new DistCpOptions.Builder(Collections.singletonList(srcDir), inputDirUnderOutputDir).withTrackMissing(trackDir).withSyncFolder(true).withDirectWrite(shouldUseDirectWrite()).withOverwrite(false)));\r\n    lsR(\"tracked udpate\", remoteFS, destDir);\r\n    Path outputFileNew1 = new Path(outputSubDir2, \"newfile1\");\r\n    ContractTestUtils.assertIsFile(remoteFS, outputFileNew1);\r\n    ContractTestUtils.assertPathExists(localFS, \"tracking directory\", trackDir);\r\n    Path sortedSourceListing = new Path(trackDir, DistCpConstants.SOURCE_SORTED_FILE);\r\n    ContractTestUtils.assertIsFile(localFS, sortedSourceListing);\r\n    Path sortedTargetListing = new Path(trackDir, DistCpConstants.TARGET_SORTED_FILE);\r\n    ContractTestUtils.assertIsFile(localFS, sortedTargetListing);\r\n    ContractTestUtils.assertPathsExist(remoteFS, \"DistCP should have retained\", outputFile2, outputFile3, outputFile4, outputSubDir4);\r\n    Map<String, Path> sourceFiles = new HashMap<>(10);\r\n    Map<String, Path> targetFiles = new HashMap<>(10);\r\n    try (SequenceFile.Reader sourceReader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(sortedSourceListing));\r\n        SequenceFile.Reader targetReader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(sortedTargetListing))) {\r\n        CopyListingFileStatus copyStatus = new CopyListingFileStatus();\r\n        Text name = new Text();\r\n        while (sourceReader.next(name, copyStatus)) {\r\n            String key = name.toString();\r\n            Path path = copyStatus.getPath();\r\n            LOG.info(\"{}: {}\", key, path);\r\n            sourceFiles.put(key, path);\r\n        }\r\n        while (targetReader.next(name, copyStatus)) {\r\n            String key = name.toString();\r\n            Path path = copyStatus.getPath();\r\n            LOG.info(\"{}: {}\", key, path);\r\n            targetFiles.put(name.toString(), copyStatus.getPath());\r\n        }\r\n    }\r\n    assertTrue(\"No \" + outputFileNew1 + \" in source listing\", sourceFiles.containsValue(inputFileNew1));\r\n    assertTrue(\"No \" + outputFileNew1 + \" in target listing\", targetFiles.containsValue(outputFileNew1));\r\n    assertTrue(\"No \" + outputSubDir4 + \" in target listing\", targetFiles.containsValue(outputSubDir4));\r\n    assertFalse(\"Found \" + inputSubDir4 + \" in source listing\", sourceFiles.containsValue(inputSubDir4));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "lsR",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void lsR(final String description, final FileSystem fs, final Path dir) throws IOException\n{\r\n    RemoteIterator<LocatedFileStatus> files = fs.listFiles(dir, true);\r\n    LOG.info(\"{}: {}:\", description, dir);\r\n    StringBuilder sb = new StringBuilder();\r\n    while (files.hasNext()) {\r\n        LocatedFileStatus status = files.next();\r\n        sb.append(String.format(\"  %s; type=%s; length=%d\", status.getPath(), status.isDirectory() ? \"dir\" : \"file\", status.getLen()));\r\n    }\r\n    LOG.info(\"{}\", sb);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "largeFilesToRemote",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void largeFilesToRemote() throws Exception\n{\r\n    describe(\"copy multiple large files from local to remote\");\r\n    largeFiles(localFS, localDir, remoteFS, remoteDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testDeepDirectoryStructureFromRemote",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDeepDirectoryStructureFromRemote() throws Exception\n{\r\n    describe(\"copy a deep directory structure from remote to local\");\r\n    distCpDeepDirectoryStructure(remoteFS, remoteDir, localFS, localDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testLargeFilesFromRemote",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testLargeFilesFromRemote() throws Exception\n{\r\n    describe(\"copy multiple large files from remote to local\");\r\n    largeFiles(remoteFS, remoteDir, localFS, localDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testSetJobId",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetJobId() throws Exception\n{\r\n    describe(\"check jobId is set in the conf\");\r\n    remoteFS.create(new Path(remoteDir, \"file1\")).close();\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, remoteDir.toString(), localDir.toString(), getDefaultCLIOptionsOrNull(), conf);\r\n    assertNotNull(\"DistCp job id isn't set\", conf.get(CONF_LABEL_DISTCP_JOB_ID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "distCpDeepDirectoryStructure",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "Path distCpDeepDirectoryStructure(FileSystem srcFS, Path srcDir, FileSystem dstFS, Path dstDir) throws Exception\n{\r\n    initPathFields(srcDir, dstDir);\r\n    mkdirs(srcFS, inputSubDir1);\r\n    mkdirs(srcFS, inputSubDir2);\r\n    byte[] data1 = dataset(100, 33, 43);\r\n    createFile(srcFS, inputFile1, true, data1);\r\n    byte[] data2 = dataset(200, 43, 53);\r\n    createFile(srcFS, inputFile2, true, data2);\r\n    byte[] data3 = dataset(300, 53, 63);\r\n    createFile(srcFS, inputFile3, true, data3);\r\n    createFile(srcFS, inputFile4, true, dataset(400, 53, 63));\r\n    createFile(srcFS, inputFile5, true, dataset(500, 53, 63));\r\n    Path target = new Path(dstDir, \"outputDir\");\r\n    runDistCp(inputDir, target);\r\n    ContractTestUtils.assertIsDirectory(dstFS, target);\r\n    lsR(\"Destination tree after distcp\", dstFS, target);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/file1\"), data1);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/subDir1/file2\"), data2);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/subDir2/subDir2/file3\"), data3);\r\n    return target;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "largeFiles",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void largeFiles(FileSystem srcFS, Path srcDir, FileSystem dstFS, Path dstDir) throws Exception\n{\r\n    int fileSizeKb = conf.getInt(SCALE_TEST_DISTCP_FILE_SIZE_KB, getDefaultDistCPSizeKb());\r\n    if (fileSizeKb < 1) {\r\n        skip(\"File size in \" + SCALE_TEST_DISTCP_FILE_SIZE_KB + \" is zero\");\r\n    }\r\n    initPathFields(srcDir, dstDir);\r\n    Path largeFile1 = new Path(inputDir, \"file1\");\r\n    Path largeFile2 = new Path(inputDir, \"file2\");\r\n    Path largeFile3 = new Path(inputDir, \"file3\");\r\n    int fileSizeMb = fileSizeKb / 1024;\r\n    getLogger().info(\"{} with file size {}\", testName.getMethodName(), fileSizeMb);\r\n    byte[] data1 = dataset((fileSizeMb + 1) * MB, 33, 43);\r\n    createFile(srcFS, largeFile1, true, data1);\r\n    byte[] data2 = dataset((fileSizeMb + 2) * MB, 43, 53);\r\n    createFile(srcFS, largeFile2, true, data2);\r\n    byte[] data3 = dataset((fileSizeMb + 3) * MB, 53, 63);\r\n    createFile(srcFS, largeFile3, true, data3);\r\n    Path target = new Path(dstDir, \"outputDir\");\r\n    runDistCp(inputDir, target);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/file1\"), data1);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/file2\"), data2);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/file3\"), data3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getDefaultDistCPSizeKb",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDefaultDistCPSizeKb()\n{\r\n    return DEFAULT_DISTCP_SIZE_KB;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "runDistCp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runDistCp(Path src, Path dst) throws Exception\n{\r\n    if (shouldUseDirectWrite()) {\r\n        runDistCpDirectWrite(src, dst);\r\n    } else {\r\n        runDistCpWithRename(src, dst);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "runDistCp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job runDistCp(final DistCpOptions options) throws Exception\n{\r\n    Job job = new DistCp(conf, options).execute();\r\n    assertNotNull(\"Unexpected null job returned from DistCp execution.\", job);\r\n    assertTrue(\"DistCp job did not complete.\", job.isComplete());\r\n    assertTrue(\"DistCp job did not complete successfully.\", job.isSuccessful());\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "buildWithStandardOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DistCpOptions buildWithStandardOptions(DistCpOptions.Builder builder)\n{\r\n    return builder.withNumListstatusThreads(DistCpOptions.MAX_NUM_LISTSTATUS_THREADS).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mkdirs(FileSystem fs, Path dir) throws Exception\n{\r\n    assertTrue(\"Failed to mkdir \" + dir, fs.mkdirs(dir));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDirectWrite() throws Exception\n{\r\n    describe(\"copy file from local to remote using direct write option\");\r\n    if (shouldUseDirectWrite()) {\r\n        skip(\"not needed as all other tests use the -direct option.\");\r\n    }\r\n    directWrite(localFS, localDir, remoteFS, remoteDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testNonDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testNonDirectWrite() throws Exception\n{\r\n    describe(\"copy file from local to remote without using direct write \" + \"option\");\r\n    directWrite(localFS, localDir, remoteFS, remoteDir, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testDistCpWithIterator",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDistCpWithIterator() throws Exception\n{\r\n    describe(\"Build listing in distCp using the iterator option.\");\r\n    Path source = new Path(remoteDir, \"src\");\r\n    Path dest = new Path(localDir, \"dest\");\r\n    dest = localFS.makeQualified(dest);\r\n    GenericTestUtils.createFiles(remoteFS, source, getDepth(), getWidth(), getWidth());\r\n    GenericTestUtils.LogCapturer log = GenericTestUtils.LogCapturer.captureLogs(SimpleCopyListing.LOG);\r\n    String options = \"-useiterator -update -delete\" + getDefaultCLIOptions();\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, source.toString(), dest.toString(), options, conf);\r\n    Assertions.assertThat(log.getOutput()).contains(\"Building listing using iterator mode for \" + dest.toString());\r\n    Assertions.assertThat(RemoteIterators.toList(localFS.listFiles(dest, true))).describedAs(\"files\").hasSize(getTotalFiles());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getDepth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDepth()\n{\r\n    return DEFAULT_DEPTH;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getWidth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getWidth()\n{\r\n    return DEFAULT_WIDTH;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getTotalFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getTotalFiles()\n{\r\n    int totalFiles = 0;\r\n    for (int i = 1; i <= getDepth(); i++) {\r\n        totalFiles += Math.pow(getWidth(), i);\r\n    }\r\n    return totalFiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "shouldUseDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldUseDirectWrite()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getDefaultCLIOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDefaultCLIOptions()\n{\r\n    return shouldUseDirectWrite() ? \" -direct \" : \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getDefaultCLIOptionsOrNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDefaultCLIOptionsOrNull()\n{\r\n    return shouldUseDirectWrite() ? \" -direct \" : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "directWrite",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void directWrite(FileSystem srcFS, Path srcDir, FileSystem dstFS, Path dstDir, boolean directWrite) throws Exception\n{\r\n    initPathFields(srcDir, dstDir);\r\n    mkdirs(srcFS, inputSubDir1);\r\n    byte[] data1 = dataset(64, 33, 43);\r\n    createFile(srcFS, inputFile1, true, data1);\r\n    byte[] data2 = dataset(200, 43, 53);\r\n    createFile(srcFS, inputFile2, true, data2);\r\n    Path target = new Path(dstDir, \"outputDir\");\r\n    if (directWrite) {\r\n        runDistCpDirectWrite(inputDir, target);\r\n    } else {\r\n        runDistCpWithRename(inputDir, target);\r\n    }\r\n    ContractTestUtils.assertIsDirectory(dstFS, target);\r\n    lsR(\"Destination tree after distcp\", dstFS, target);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/file1\"), data1);\r\n    verifyFileContents(dstFS, new Path(target, \"inputDir/subDir1/file2\"), data2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "runDistCpDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Job runDistCpDirectWrite(final Path srcDir, final Path destDir) throws Exception\n{\r\n    describe(\"\\nDistcp -direct from \" + srcDir + \" to \" + destDir);\r\n    return runDistCp(buildWithStandardOptions(new DistCpOptions.Builder(Collections.singletonList(srcDir), destDir).withDirectWrite(true)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "runDistCpWithRename",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Job runDistCpWithRename(Path srcDir, final Path destDir) throws Exception\n{\r\n    describe(\"\\nDistcp from \" + srcDir + \" to \" + destDir);\r\n    return runDistCp(buildWithStandardOptions(new DistCpOptions.Builder(Collections.singletonList(srcDir), destDir).withDirectWrite(false)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testDistCpWithFile",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDistCpWithFile() throws Exception\n{\r\n    describe(\"Distcp only file\");\r\n    Path source = new Path(remoteDir, \"file\");\r\n    Path dest = new Path(localDir, \"file\");\r\n    dest = localFS.makeQualified(dest);\r\n    mkdirs(localFS, localDir);\r\n    int len = 4;\r\n    int base = 0x40;\r\n    byte[] block = dataset(len, base, base + len);\r\n    ContractTestUtils.createFile(remoteFS, source, true, block);\r\n    verifyPathExists(remoteFS, \"\", source);\r\n    verifyPathExists(localFS, \"\", localDir);\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, source.toString(), dest.toString(), getDefaultCLIOptionsOrNull(), conf);\r\n    Assertions.assertThat(RemoteIterators.toList(localFS.listFiles(dest, true))).describedAs(\"files\").hasSize(1);\r\n    verifyFileContents(localFS, dest, block);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "testDistCpWithUpdateExistFile",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDistCpWithUpdateExistFile() throws Exception\n{\r\n    describe(\"Now update an existing file.\");\r\n    Path source = new Path(remoteDir, \"file\");\r\n    Path dest = new Path(localDir, \"file\");\r\n    dest = localFS.makeQualified(dest);\r\n    int len = 4;\r\n    int base = 0x40;\r\n    byte[] block = dataset(len, base, base + len);\r\n    byte[] destBlock = dataset(len, base, base + len + 1);\r\n    ContractTestUtils.createFile(remoteFS, source, true, block);\r\n    ContractTestUtils.createFile(localFS, dest, true, destBlock);\r\n    verifyPathExists(remoteFS, \"\", source);\r\n    verifyPathExists(localFS, \"\", dest);\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, source.toString(), dest.toString(), \"-delete -update\" + getDefaultCLIOptions(), conf);\r\n    Assertions.assertThat(RemoteIterators.toList(localFS.listFiles(dest, true))).hasSize(1);\r\n    verifyFileContents(localFS, dest, block);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testRetriableCommand",
  "errType" : [ "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRetriableCommand()\n{\r\n    try {\r\n        new MyRetriableCommand(5).execute(0);\r\n        Assert.assertTrue(false);\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(true);\r\n    }\r\n    try {\r\n        new MyRetriableCommand(3).execute(0);\r\n        Assert.assertTrue(true);\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(false);\r\n    }\r\n    try {\r\n        new MyRetriableCommand(5, RetryPolicies.retryUpToMaximumCountWithFixedSleep(5, 0, TimeUnit.MILLISECONDS)).execute(0);\r\n        Assert.assertTrue(true);\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testToString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testToString()\n{\r\n    CopyListingFileStatus src = new CopyListingFileStatus(4344L, false, 2, 512 << 20, 1234L, 5678L, new FsPermission((short) 0512), \"dingo\", \"yaks\", new Path(\"hdfs://localhost:4344\"));\r\n    src.toString();\r\n    src = new CopyListingFileStatus();\r\n    src.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCopyListingFileStatusSerialization",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCopyListingFileStatusSerialization() throws Exception\n{\r\n    CopyListingFileStatus src = new CopyListingFileStatus(4344L, false, 2, 512 << 20, 1234L, 5678L, new FsPermission((short) 0512), \"dingo\", \"yaks\", new Path(\"hdfs://localhost:4344\"));\r\n    DataOutputBuffer dob = new DataOutputBuffer();\r\n    src.write(dob);\r\n    DataInputBuffer dib = new DataInputBuffer();\r\n    dib.reset(dob.getData(), 0, dob.getLength());\r\n    CopyListingFileStatus dst = new CopyListingFileStatus();\r\n    dst.readFields(dib);\r\n    assertEquals(src, dst);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testFileStatusEquality",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testFileStatusEquality() throws Exception\n{\r\n    FileStatus stat = new FileStatus(4344L, false, 2, 512 << 20, 1234L, 5678L, new FsPermission((short) 0512), \"dingo\", \"yaks\", new Path(\"hdfs://localhost:4344/foo/bar/baz\"));\r\n    CopyListingFileStatus clfs = new CopyListingFileStatus(stat);\r\n    assertEquals(stat.getLen(), clfs.getLen());\r\n    assertEquals(stat.isDirectory(), clfs.isDirectory());\r\n    assertEquals(stat.getReplication(), clfs.getReplication());\r\n    assertEquals(stat.getBlockSize(), clfs.getBlockSize());\r\n    assertEquals(stat.getAccessTime(), clfs.getAccessTime());\r\n    assertEquals(stat.getModificationTime(), clfs.getModificationTime());\r\n    assertEquals(stat.getPermission(), clfs.getPermission());\r\n    assertEquals(stat.getOwner(), clfs.getOwner());\r\n    assertEquals(stat.getGroup(), clfs.getGroup());\r\n    assertEquals(stat.getPath(), clfs.getPath());\r\n    assertEquals(stat.isErasureCoded(), clfs.isErasureCoded());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getConf() throws URISyntaxException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"mapred.job.tracker\", \"local\");\r\n    conf.set(\"fs.default.name\", \"file:///\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup() throws URISyntaxException\n{\r\n    try {\r\n        Path fswd = FileSystem.get(getConf()).getWorkingDirectory();\r\n        Configuration vConf = ViewFileSystemTestSetup.createConfig(false);\r\n        ConfigUtil.addLink(vConf, \"/usr\", new URI(fswd.toString()));\r\n        fs = FileSystem.get(FsConstants.VIEWFS_URI, vConf);\r\n        fs.setWorkingDirectory(new Path(\"/usr\"));\r\n        listFile = new Path(\"target/tmp/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        target = new Path(\"target/tmp/target\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        root = new Path(\"target/tmp\").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();\r\n        TestDistCpUtils.delete(fs, root);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileMissingTarget",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileMissingTarget() throws IOException\n{\r\n    caseSingleFileMissingTarget(false);\r\n    caseSingleFileMissingTarget(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileMissingTarget",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void caseSingleFileMissingTarget(boolean sync) throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"singlefile1/file1\");\r\n        createFiles(\"singlefile1/file1\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 1);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileTargetFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileTargetFile() throws IOException\n{\r\n    caseSingleFileTargetFile(false);\r\n    caseSingleFileTargetFile(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileTargetFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void caseSingleFileTargetFile(boolean sync) throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"singlefile1/file1\");\r\n        createFiles(\"singlefile1/file1\", target.toString());\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 1);\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileTargetDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileTargetDir() throws IOException\n{\r\n    caseSingleFileTargetDir(false);\r\n    caseSingleFileTargetDir(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileTargetDir",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void caseSingleFileTargetDir(boolean sync) throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"singlefile2/file2\");\r\n        createFiles(\"singlefile2/file2\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, sync);\r\n        checkResult(target, 1, \"file2\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleDirTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleDirTargetMissing() throws IOException\n{\r\n    caseSingleDirTargetMissing(false);\r\n    caseSingleDirTargetMissing(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleDirTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void caseSingleDirTargetMissing(boolean sync) throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"singledir\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 1, \"dir1\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleDirTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSingleDirTargetPresent() throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"singledir\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, false);\r\n        checkResult(target, 1, \"singledir/dir1\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateSingleDirTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUpdateSingleDirTargetPresent() throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"Usingledir\");\r\n        mkdirs(root + \"/Usingledir/Udir1\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, true);\r\n        checkResult(target, 1, \"Udir1\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiFileTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiFileTargetPresent() throws IOException\n{\r\n    caseMultiFileTargetPresent(false);\r\n    caseMultiFileTargetPresent(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseMultiFileTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void caseMultiFileTargetPresent(boolean sync) throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, sync);\r\n        checkResult(target, 3, \"file3\", \"file4\", \"file5\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiFileTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiFileTargetMissing() throws IOException\n{\r\n    caseMultiFileTargetMissing(false);\r\n    caseMultiFileTargetMissing(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseMultiFileTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void caseMultiFileTargetMissing(boolean sync) throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(target, 3, \"file3\", \"file4\", \"file5\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiDirTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultiDirTargetPresent() throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"multifile\", \"singledir\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(target.toString(), root + \"/singledir/dir1\");\r\n        runTest(listFile, target, true, false);\r\n        checkResult(target, 2, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\", \"singledir/dir1\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateMultiDirTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUpdateMultiDirTargetPresent() throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"Umultifile\", \"Usingledir\");\r\n        createFiles(\"Umultifile/Ufile3\", \"Umultifile/Ufile4\", \"Umultifile/Ufile5\");\r\n        mkdirs(target.toString(), root + \"/Usingledir/Udir1\");\r\n        runTest(listFile, target, true, true);\r\n        checkResult(target, 4, \"Ufile3\", \"Ufile4\", \"Ufile5\", \"Udir1\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiDirTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultiDirTargetMissing() throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"multifile\", \"singledir\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        runTest(listFile, target, false, false);\r\n        checkResult(target, 2, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\", \"singledir/dir1\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateMultiDirTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUpdateMultiDirTargetMissing() throws IOException\n{\r\n    try {\r\n        addEntries(listFile, \"multifile\", \"singledir\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        mkdirs(root + \"/singledir/dir1\");\r\n        runTest(listFile, target, false, true);\r\n        checkResult(target, 4, \"file3\", \"file4\", \"file5\", \"dir1\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobTargetMissingSingleLevel",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGlobTargetMissingSingleLevel() throws IOException\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir/dir2/file6\");\r\n        runTest(listFile, target, false, false);\r\n        checkResult(target, 2, \"multifile/file3\", \"multifile/file4\", \"multifile/file5\", \"singledir/dir2/file6\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateGlobTargetMissingSingleLevel",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateGlobTargetMissingSingleLevel() throws IOException\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir/dir2/file6\");\r\n        runTest(listFile, target, false, true);\r\n        checkResult(target, 4, \"file3\", \"file4\", \"file5\", \"dir2/file6\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobTargetMissingMultiLevel",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGlobTargetMissingMultiLevel() throws IOException\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*/*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir1/dir3/file7\", \"singledir1/dir3/file8\", \"singledir1/dir3/file9\");\r\n        runTest(listFile, target, false, false);\r\n        checkResult(target, 4, \"file3\", \"file4\", \"file5\", \"dir3/file7\", \"dir3/file8\", \"dir3/file9\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateGlobTargetMissingMultiLevel",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateGlobTargetMissingMultiLevel() throws IOException\n{\r\n    try {\r\n        Path listFile = new Path(\"target/tmp1/listing\").makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        addEntries(listFile, \"*/*\");\r\n        createFiles(\"multifile/file3\", \"multifile/file4\", \"multifile/file5\");\r\n        createFiles(\"singledir1/dir3/file7\", \"singledir1/dir3/file8\", \"singledir1/dir3/file9\");\r\n        runTest(listFile, target, false, true);\r\n        checkResult(target, 6, \"file3\", \"file4\", \"file5\", \"file7\", \"file8\", \"file9\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, root);\r\n        TestDistCpUtils.delete(fs, \"target/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "addEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addEntries(Path listFile, String... entries) throws IOException\n{\r\n    OutputStream out = fs.create(listFile);\r\n    try {\r\n        for (String entry : entries) {\r\n            out.write((root + \"/\" + entry).getBytes());\r\n            out.write(\"\\n\".getBytes());\r\n        }\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createFiles(String... entries) throws IOException\n{\r\n    String e;\r\n    for (String entry : entries) {\r\n        if ((new Path(entry)).isAbsolute()) {\r\n            e = entry;\r\n        } else {\r\n            e = root + \"/\" + entry;\r\n        }\r\n        OutputStream out = fs.create(new Path(e));\r\n        try {\r\n            out.write((e).getBytes());\r\n            out.write(\"\\n\".getBytes());\r\n        } finally {\r\n            out.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mkdirs(String... entries) throws IOException\n{\r\n    for (String entry : entries) {\r\n        fs.mkdirs(new Path(entry));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "runTest",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void runTest(Path listFile, Path target, boolean targetExists, boolean sync) throws IOException\n{\r\n    final DistCpOptions options = new DistCpOptions.Builder(listFile, target).withSyncFolder(sync).build();\r\n    try {\r\n        final DistCp distcp = new DistCp(getConf(), options);\r\n        distcp.context.setTargetPathExists(targetExists);\r\n        distcp.execute();\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkResult",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkResult(Path target, int count, String... relPaths) throws IOException\n{\r\n    Assert.assertEquals(count, fs.listStatus(target).length);\r\n    if (relPaths == null || relPaths.length == 0) {\r\n        Assert.assertTrue(target.toString(), fs.exists(target));\r\n        return;\r\n    }\r\n    for (String relPath : relPaths) {\r\n        Assert.assertTrue(new Path(target, relPath).toString(), fs.exists(new Path(target, relPath)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGetCopyFilterTrueCopyFilter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetCopyFilterTrueCopyFilter()\n{\r\n    Configuration configuration = new Configuration(false);\r\n    CopyFilter copyFilter = CopyFilter.getCopyFilter(configuration);\r\n    assertTrue(\"copyFilter should be instance of TrueCopyFilter\", copyFilter instanceof TrueCopyFilter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGetCopyFilterRegexCopyFilter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetCopyFilterRegexCopyFilter()\n{\r\n    Configuration configuration = new Configuration(false);\r\n    configuration.set(DistCpConstants.CONF_LABEL_FILTERS_FILE, \"random\");\r\n    CopyFilter copyFilter = CopyFilter.getCopyFilter(configuration);\r\n    assertTrue(\"copyFilter should be instance of RegexCopyFilter\", copyFilter instanceof RegexCopyFilter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGetCopyFilterRegexpInConfigurationFilter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetCopyFilterRegexpInConfigurationFilter()\n{\r\n    final String filterName = \"org.apache.hadoop.tools.RegexpInConfigurationFilter\";\r\n    Configuration configuration = new Configuration(false);\r\n    configuration.set(DistCpConstants.CONF_LABEL_FILTERS_CLASS, filterName);\r\n    CopyFilter copyFilter = CopyFilter.getCopyFilter(configuration);\r\n    assertTrue(\"copyFilter should be instance of RegexpInConfigurationFilter\", copyFilter instanceof RegexpInConfigurationFilter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGetCopyFilterNonExistingClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetCopyFilterNonExistingClass() throws Exception\n{\r\n    final String filterName = \"org.apache.hadoop.tools.RegexpInConfigurationWrongFilter\";\r\n    Configuration configuration = new Configuration(false);\r\n    configuration.set(DistCpConstants.CONF_LABEL_FILTERS_CLASS, filterName);\r\n    intercept(RuntimeException.class, DistCpConstants.CLASS_INSTANTIATION_ERROR_MSG + filterName, () -> CopyFilter.getCopyFilter(configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGetCopyFilterWrongClassType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetCopyFilterWrongClassType() throws Exception\n{\r\n    final String filterName = \"org.apache.hadoop.tools.\" + \"TestCopyFilter.FilterNotExtendingCopyFilter\";\r\n    Configuration configuration = new Configuration(false);\r\n    configuration.set(DistCpConstants.CONF_LABEL_FILTERS_CLASS, filterName);\r\n    intercept(RuntimeException.class, DistCpConstants.CLASS_INSTANTIATION_ERROR_MSG + filterName, () -> CopyFilter.getCopyFilter(configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGetCopyFilterEmptyString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetCopyFilterEmptyString() throws Exception\n{\r\n    final String filterName = \"\";\r\n    Configuration configuration = new Configuration(false);\r\n    configuration.set(DistCpConstants.CONF_LABEL_FILTERS_CLASS, filterName);\r\n    intercept(RuntimeException.class, DistCpConstants.CLASS_INSTANTIATION_ERROR_MSG + filterName, () -> CopyFilter.getCopyFilter(configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Configuration configuration = getConfigurationForCluster();\r\n    setCluster(new MiniDFSCluster.Builder(configuration).numDataNodes(1).format(true).build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectDifferentBlockSizesMultipleBlocksToSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean expectDifferentBlockSizesMultipleBlocksToSucceed()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectDifferentBytesPerCrcToSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean expectDifferentBytesPerCrcToSucceed()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCluster(MiniDFSCluster c)\n{\r\n    cluster = c;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getConfigurationForCluster",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration getConfigurationForCluster() throws IOException\n{\r\n    Configuration configuration = new Configuration();\r\n    System.setProperty(\"test.build.data\", \"target/tmp/build/TEST_COPY_MAPPER/data\");\r\n    configuration.set(\"hadoop.log.dir\", \"target/tmp\");\r\n    configuration.set(\"dfs.namenode.fs-limits.min-block-size\", \"0\");\r\n    LOG.debug(\"fs.default.name  == \" + configuration.get(\"fs.default.name\"));\r\n    LOG.debug(\"dfs.http.address == \" + configuration.get(\"dfs.http.address\"));\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Configuration getConfiguration() throws IOException\n{\r\n    Configuration configuration = getConfigurationForCluster();\r\n    final FileSystem fs = cluster.getFileSystem();\r\n    Path workPath = new Path(TARGET_PATH).makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n    configuration.set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, workPath.toString());\r\n    configuration.set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, workPath.toString());\r\n    configuration.setBoolean(DistCpOptionSwitch.OVERWRITE.getConfigLabel(), false);\r\n    configuration.setBoolean(DistCpOptionSwitch.SKIP_CRC.getConfigLabel(), false);\r\n    configuration.setBoolean(DistCpOptionSwitch.SYNC_FOLDERS.getConfigLabel(), true);\r\n    configuration.set(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel(), \"br\");\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createSourceData",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void createSourceData() throws Exception\n{\r\n    mkdirs(SOURCE_PATH + \"/1\");\r\n    mkdirs(SOURCE_PATH + \"/2\");\r\n    mkdirs(SOURCE_PATH + \"/2/3/4\");\r\n    mkdirs(SOURCE_PATH + \"/2/3\");\r\n    mkdirs(SOURCE_PATH + \"/5\");\r\n    touchFile(SOURCE_PATH + \"/5/6\");\r\n    mkdirs(SOURCE_PATH + \"/7\");\r\n    mkdirs(SOURCE_PATH + \"/7/8\");\r\n    touchFile(SOURCE_PATH + \"/7/8/9\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "appendSourceData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void appendSourceData() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    for (Path source : pathList) {\r\n        if (fs.getFileStatus(source).isFile()) {\r\n            appendFile(source, DEFAULT_FILE_SIZE * 2);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createSourceDataWithDifferentBlockSize",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void createSourceDataWithDifferentBlockSize() throws Exception\n{\r\n    mkdirs(SOURCE_PATH + \"/1\");\r\n    mkdirs(SOURCE_PATH + \"/2\");\r\n    mkdirs(SOURCE_PATH + \"/2/3/4\");\r\n    mkdirs(SOURCE_PATH + \"/2/3\");\r\n    mkdirs(SOURCE_PATH + \"/5\");\r\n    touchFile(SOURCE_PATH + \"/5/6\", true, null);\r\n    mkdirs(SOURCE_PATH + \"/7\");\r\n    mkdirs(SOURCE_PATH + \"/7/8\");\r\n    touchFile(SOURCE_PATH + \"/7/8/9\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createSourceDataWithDifferentChecksumType",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void createSourceDataWithDifferentChecksumType() throws Exception\n{\r\n    mkdirs(SOURCE_PATH + \"/1\");\r\n    mkdirs(SOURCE_PATH + \"/2\");\r\n    mkdirs(SOURCE_PATH + \"/2/3/4\");\r\n    mkdirs(SOURCE_PATH + \"/2/3\");\r\n    mkdirs(SOURCE_PATH + \"/5\");\r\n    touchFile(SOURCE_PATH + \"/5/6\", new ChecksumOpt(DataChecksum.Type.CRC32, 512));\r\n    mkdirs(SOURCE_PATH + \"/7\");\r\n    mkdirs(SOURCE_PATH + \"/7/8\");\r\n    touchFile(SOURCE_PATH + \"/7/8/9\", new ChecksumOpt(DataChecksum.Type.CRC32C, 512));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createSourceDataWithDifferentBytesPerCrc",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void createSourceDataWithDifferentBytesPerCrc() throws Exception\n{\r\n    mkdirs(SOURCE_PATH + \"/1\");\r\n    mkdirs(SOURCE_PATH + \"/2\");\r\n    mkdirs(SOURCE_PATH + \"/2/3/4\");\r\n    mkdirs(SOURCE_PATH + \"/2/3\");\r\n    mkdirs(SOURCE_PATH + \"/5\");\r\n    touchFile(SOURCE_PATH + \"/5/6\", false, new ChecksumOpt(DataChecksum.Type.CRC32C, 32));\r\n    mkdirs(SOURCE_PATH + \"/7\");\r\n    mkdirs(SOURCE_PATH + \"/7/8\");\r\n    touchFile(SOURCE_PATH + \"/7/8/9\", false, new ChecksumOpt(DataChecksum.Type.CRC32C, 64));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void mkdirs(String path) throws Exception\n{\r\n    FileSystem fileSystem = cluster.getFileSystem();\r\n    final Path qualifiedPath = new Path(path).makeQualified(fileSystem.getUri(), fileSystem.getWorkingDirectory());\r\n    pathList.add(qualifiedPath);\r\n    fileSystem.mkdirs(qualifiedPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "touchFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void touchFile(String path) throws Exception\n{\r\n    touchFile(path, false, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "touchFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void touchFile(String path, ChecksumOpt checksumOpt) throws Exception\n{\r\n    touchFile(path, true, checksumOpt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "touchFile",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void touchFile(String path, boolean createMultipleBlocks, ChecksumOpt checksumOpt) throws Exception\n{\r\n    FileSystem fs;\r\n    DataOutputStream outputStream = null;\r\n    try {\r\n        fs = cluster.getFileSystem();\r\n        final Path qualifiedPath = new Path(path).makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        final long blockSize = createMultipleBlocks ? NON_DEFAULT_BLOCK_SIZE : fs.getDefaultBlockSize(qualifiedPath) * 2;\r\n        FsPermission permission = FsPermission.getFileDefault().applyUMask(FsPermission.getUMask(fs.getConf()));\r\n        outputStream = fs.create(qualifiedPath, permission, EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), 0, (short) (fs.getDefaultReplication(qualifiedPath) * 2), blockSize, null, checksumOpt);\r\n        byte[] bytes = new byte[DEFAULT_FILE_SIZE];\r\n        outputStream.write(bytes);\r\n        long fileSize = DEFAULT_FILE_SIZE;\r\n        if (createMultipleBlocks) {\r\n            while (fileSize < 2 * blockSize) {\r\n                outputStream.write(bytes);\r\n                outputStream.flush();\r\n                fileSize += DEFAULT_FILE_SIZE;\r\n            }\r\n        }\r\n        pathList.add(qualifiedPath);\r\n        ++nFiles;\r\n        FileStatus fileStatus = fs.getFileStatus(qualifiedPath);\r\n        System.out.println(fileStatus.getBlockSize());\r\n        System.out.println(fileStatus.getReplication());\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(null, outputStream);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "appendFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void appendFile(Path p, int length) throws IOException\n{\r\n    byte[] toAppend = new byte[length];\r\n    Random random = new Random();\r\n    random.nextBytes(toAppend);\r\n    FSDataOutputStream out = cluster.getFileSystem().append(p);\r\n    try {\r\n        out.write(toAppend);\r\n    } finally {\r\n        IOUtils.closeStream(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopyWithDifferentChecksumType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCopyWithDifferentChecksumType() throws Exception\n{\r\n    testCopy(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRun() throws Exception\n{\r\n    testCopy(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopyWithAppend",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCopyWithAppend() throws Exception\n{\r\n    final FileSystem fs = cluster.getFileSystem();\r\n    testCopy(false);\r\n    appendSourceData();\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    Configuration conf = getConfiguration();\r\n    conf.setInt(DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(), DEFAULT_FILE_SIZE / 10);\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n    context.getConfiguration().setBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), true);\r\n    copyMapper.setup(context);\r\n    int numFiles = 0;\r\n    MetricsRecordBuilder rb = getMetrics(cluster.getDataNodes().get(0).getMetrics().name());\r\n    String readCounter = \"ReadsFromLocalClient\";\r\n    long readsFromClient = getLongCounter(readCounter, rb);\r\n    for (Path path : pathList) {\r\n        if (fs.getFileStatus(path).isFile()) {\r\n            numFiles++;\r\n        }\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(cluster.getFileSystem().getFileStatus(path)), context);\r\n    }\r\n    verifyCopy(fs, false, true);\r\n    Assert.assertEquals(nFiles * DEFAULT_FILE_SIZE * 2, stubContext.getReporter().getCounter(CopyMapper.Counter.BYTESCOPIED).getValue());\r\n    Assert.assertEquals(numFiles, stubContext.getReporter().getCounter(CopyMapper.Counter.COPY).getValue());\r\n    rb = getMetrics(cluster.getDataNodes().get(0).getMetrics().name());\r\n    assertCounter(readCounter, readsFromClient + numFiles, rb);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopy",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testCopy(boolean preserveChecksum) throws Exception\n{\r\n    deleteState();\r\n    if (preserveChecksum) {\r\n        createSourceDataWithDifferentChecksumType();\r\n    } else {\r\n        createSourceData();\r\n    }\r\n    FileSystem fs = cluster.getFileSystem();\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n    Configuration configuration = context.getConfiguration();\r\n    EnumSet<DistCpOptions.FileAttribute> fileAttributes = EnumSet.of(DistCpOptions.FileAttribute.REPLICATION);\r\n    if (preserveChecksum) {\r\n        fileAttributes.add(DistCpOptions.FileAttribute.CHECKSUMTYPE);\r\n    }\r\n    configuration.set(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel(), DistCpUtils.packAttributes(fileAttributes));\r\n    copyMapper.setup(context);\r\n    int numFiles = 0;\r\n    int numDirs = 0;\r\n    for (Path path : pathList) {\r\n        if (fs.getFileStatus(path).isDirectory()) {\r\n            numDirs++;\r\n        } else {\r\n            numFiles++;\r\n        }\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fs.getFileStatus(path)), context);\r\n    }\r\n    verifyCopy(fs, preserveChecksum, true);\r\n    Assert.assertEquals(numFiles, stubContext.getReporter().getCounter(CopyMapper.Counter.COPY).getValue());\r\n    Assert.assertEquals(numDirs, stubContext.getReporter().getCounter(CopyMapper.Counter.DIR_COPY).getValue());\r\n    if (!preserveChecksum) {\r\n        Assert.assertEquals(nFiles * DEFAULT_FILE_SIZE, stubContext.getReporter().getCounter(CopyMapper.Counter.BYTESCOPIED).getValue());\r\n    } else {\r\n        Assert.assertEquals(nFiles * NON_DEFAULT_BLOCK_SIZE * 2, stubContext.getReporter().getCounter(CopyMapper.Counter.BYTESCOPIED).getValue());\r\n    }\r\n    testCopyingExistingFiles(fs, copyMapper, context);\r\n    for (Text value : stubContext.getWriter().values()) {\r\n        Assert.assertTrue(value.toString() + \" is not skipped\", value.toString().startsWith(\"SKIP:\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "verifyCopy",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void verifyCopy(FileSystem fs, boolean preserveChecksum, boolean preserveReplication) throws Exception\n{\r\n    for (Path path : pathList) {\r\n        final Path targetPath = new Path(path.toString().replaceAll(SOURCE_PATH, TARGET_PATH));\r\n        Assert.assertTrue(fs.exists(targetPath));\r\n        Assert.assertTrue(fs.isFile(targetPath) == fs.isFile(path));\r\n        FileStatus sourceStatus = fs.getFileStatus(path);\r\n        FileStatus targetStatus = fs.getFileStatus(targetPath);\r\n        if (preserveReplication) {\r\n            Assert.assertEquals(sourceStatus.getReplication(), targetStatus.getReplication());\r\n        }\r\n        if (preserveChecksum) {\r\n            Assert.assertEquals(sourceStatus.getBlockSize(), targetStatus.getBlockSize());\r\n        }\r\n        Assert.assertTrue(!fs.isFile(targetPath) || fs.getFileChecksum(targetPath).equals(fs.getFileChecksum(path)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopyingExistingFiles",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCopyingExistingFiles(FileSystem fs, CopyMapper copyMapper, Mapper<Text, CopyListingFileStatus, Text, Text>.Context context)\n{\r\n    try {\r\n        for (Path path : pathList) {\r\n            copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fs.getFileStatus(path)), context);\r\n        }\r\n        Assert.assertEquals(nFiles, context.getCounter(CopyMapper.Counter.SKIP).getValue());\r\n    } catch (Exception exception) {\r\n        Assert.assertTrue(\"Caught unexpected exception:\" + exception.getMessage(), false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopyWhileAppend",
  "errType" : [ "IOException|InterruptedException", "Exception" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testCopyWhileAppend() throws Exception\n{\r\n    deleteState();\r\n    mkdirs(SOURCE_PATH + \"/1\");\r\n    touchFile(SOURCE_PATH + \"/1/3\");\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n    copyMapper.setup(context);\r\n    final Path path = new Path(SOURCE_PATH + \"/1/3\");\r\n    int manyBytes = 100000000;\r\n    appendFile(path, manyBytes);\r\n    ScheduledExecutorService scheduledExecutorService = Executors.newSingleThreadScheduledExecutor();\r\n    Runnable task = new Runnable() {\r\n\r\n        public void run() {\r\n            try {\r\n                int maxAppendAttempts = 20;\r\n                int appendCount = 0;\r\n                while (appendCount < maxAppendAttempts) {\r\n                    appendFile(path, 1000);\r\n                    Thread.sleep(200);\r\n                    appendCount++;\r\n                }\r\n            } catch (IOException | InterruptedException e) {\r\n                LOG.error(\"Exception encountered \", e);\r\n                Assert.fail(\"Test failed: \" + e.getMessage());\r\n            }\r\n        }\r\n    };\r\n    scheduledExecutorService.schedule(task, 10, TimeUnit.MILLISECONDS);\r\n    try {\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(cluster.getFileSystem().getFileStatus(path)), context);\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Exception encountered \", ex);\r\n        String exceptionAsString = StringUtils.stringifyException(ex);\r\n        if (exceptionAsString.contains(DistCpConstants.LENGTH_MISMATCH_ERROR_MSG) || exceptionAsString.contains(DistCpConstants.CHECKSUM_MISMATCH_ERROR_MSG)) {\r\n            Assert.fail(\"Test failed: \" + exceptionAsString);\r\n        }\r\n    } finally {\r\n        scheduledExecutorService.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testMakeDirFailure",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testMakeDirFailure()\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        Configuration configuration = context.getConfiguration();\r\n        String workPath = new Path(\"webhdfs://localhost:1234/*/*/*/?/\").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();\r\n        configuration.set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, workPath);\r\n        copyMapper.setup(context);\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), pathList.get(0))), new CopyListingFileStatus(fs.getFileStatus(pathList.get(0))), context);\r\n        Assert.assertTrue(\"There should have been an exception.\", false);\r\n    } catch (Exception ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testIgnoreFailures",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testIgnoreFailures()\n{\r\n    doTestIgnoreFailures(true);\r\n    doTestIgnoreFailures(false);\r\n    doTestIgnoreFailuresDoubleWrapped(true);\r\n    doTestIgnoreFailuresDoubleWrapped(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDirToFile",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDirToFile()\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        mkdirs(SOURCE_PATH + \"/src/file\");\r\n        touchFile(TARGET_PATH + \"/src/file\");\r\n        try {\r\n            copyMapper.setup(context);\r\n            copyMapper.map(new Text(\"/src/file\"), new CopyListingFileStatus(fs.getFileStatus(new Path(SOURCE_PATH + \"/src/file\"))), context);\r\n        } catch (IOException e) {\r\n            Assert.assertTrue(e.getMessage().startsWith(\"Can't replace\"));\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test failed: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserve",
  "errType" : [ "Exception", "Exception", "IOException", "AccessControlException", "Exception" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPreserve()\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser(\"guest\");\r\n        final CopyMapper copyMapper = new CopyMapper();\r\n        final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = tmpUser.doAs(new PrivilegedAction<Mapper<Text, CopyListingFileStatus, Text, Text>.Context>() {\r\n\r\n            @Override\r\n            public Mapper<Text, CopyListingFileStatus, Text, Text>.Context run() {\r\n                try {\r\n                    StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n                    return stubContext.getContext();\r\n                } catch (Exception e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n        });\r\n        EnumSet<DistCpOptions.FileAttribute> preserveStatus = EnumSet.allOf(DistCpOptions.FileAttribute.class);\r\n        preserveStatus.remove(DistCpOptions.FileAttribute.ACL);\r\n        preserveStatus.remove(DistCpOptions.FileAttribute.XATTR);\r\n        context.getConfiguration().set(DistCpConstants.CONF_LABEL_PRESERVE_STATUS, DistCpUtils.packAttributes(preserveStatus));\r\n        touchFile(SOURCE_PATH + \"/src/file\");\r\n        mkdirs(TARGET_PATH);\r\n        cluster.getFileSystem().setPermission(new Path(TARGET_PATH), new FsPermission((short) 511));\r\n        final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {\r\n\r\n            @Override\r\n            public FileSystem run() {\r\n                try {\r\n                    return FileSystem.get(cluster.getConfiguration(0));\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    Assert.fail(\"Test failed: \" + e.getMessage());\r\n                    throw new RuntimeException(\"Test ought to fail here\");\r\n                }\r\n            }\r\n        });\r\n        tmpUser.doAs(new PrivilegedAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() {\r\n                try {\r\n                    copyMapper.setup(context);\r\n                    copyMapper.map(new Text(\"/src/file\"), new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + \"/src/file\"))), context);\r\n                    Assert.fail(\"Expected copy to fail\");\r\n                } catch (AccessControlException e) {\r\n                    Assert.assertTrue(\"Got exception: \" + e.getMessage(), true);\r\n                } catch (Exception e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test failed: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopyReadableFiles",
  "errType" : [ "Exception", "Exception", "IOException", "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCopyReadableFiles()\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser(\"guest\");\r\n        final CopyMapper copyMapper = new CopyMapper();\r\n        final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = tmpUser.doAs(new PrivilegedAction<Mapper<Text, CopyListingFileStatus, Text, Text>.Context>() {\r\n\r\n            @Override\r\n            public Mapper<Text, CopyListingFileStatus, Text, Text>.Context run() {\r\n                try {\r\n                    StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n                    return stubContext.getContext();\r\n                } catch (Exception e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n        });\r\n        touchFile(SOURCE_PATH + \"/src/file\");\r\n        mkdirs(TARGET_PATH);\r\n        cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + \"/src/file\"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));\r\n        cluster.getFileSystem().setPermission(new Path(TARGET_PATH), new FsPermission((short) 511));\r\n        final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {\r\n\r\n            @Override\r\n            public FileSystem run() {\r\n                try {\r\n                    return FileSystem.get(cluster.getConfiguration(0));\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    Assert.fail(\"Test failed: \" + e.getMessage());\r\n                    throw new RuntimeException(\"Test ought to fail here\");\r\n                }\r\n            }\r\n        });\r\n        tmpUser.doAs(new PrivilegedAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() {\r\n                try {\r\n                    copyMapper.setup(context);\r\n                    copyMapper.map(new Text(\"/src/file\"), new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + \"/src/file\"))), context);\r\n                } catch (Exception e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test failed: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testSkipCopyNoPerms",
  "errType" : [ "Exception", "Exception", "IOException", "Exception" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testSkipCopyNoPerms()\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser(\"guest\");\r\n        final CopyMapper copyMapper = new CopyMapper();\r\n        final StubContext stubContext = tmpUser.doAs(new PrivilegedAction<StubContext>() {\r\n\r\n            @Override\r\n            public StubContext run() {\r\n                try {\r\n                    return new StubContext(getConfiguration(), null, 0);\r\n                } catch (Exception e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n        });\r\n        final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        EnumSet<DistCpOptions.FileAttribute> preserveStatus = EnumSet.allOf(DistCpOptions.FileAttribute.class);\r\n        preserveStatus.remove(DistCpOptions.FileAttribute.ACL);\r\n        preserveStatus.remove(DistCpOptions.FileAttribute.XATTR);\r\n        preserveStatus.remove(DistCpOptions.FileAttribute.TIMES);\r\n        context.getConfiguration().set(DistCpConstants.CONF_LABEL_PRESERVE_STATUS, DistCpUtils.packAttributes(preserveStatus));\r\n        touchFile(SOURCE_PATH + \"/src/file\");\r\n        touchFile(TARGET_PATH + \"/src/file\");\r\n        cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + \"/src/file\"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));\r\n        cluster.getFileSystem().setPermission(new Path(TARGET_PATH + \"/src/file\"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));\r\n        final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {\r\n\r\n            @Override\r\n            public FileSystem run() {\r\n                try {\r\n                    return FileSystem.get(cluster.getConfiguration(0));\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    Assert.fail(\"Test failed: \" + e.getMessage());\r\n                    throw new RuntimeException(\"Test ought to fail here\");\r\n                }\r\n            }\r\n        });\r\n        tmpUser.doAs(new PrivilegedAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() {\r\n                try {\r\n                    copyMapper.setup(context);\r\n                    copyMapper.map(new Text(\"/src/file\"), new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + \"/src/file\"))), context);\r\n                    assertThat(stubContext.getWriter().values().size()).isEqualTo(1);\r\n                    Assert.assertTrue(stubContext.getWriter().values().get(0).toString().startsWith(\"SKIP\"));\r\n                    Assert.assertTrue(stubContext.getWriter().values().get(0).toString().contains(SOURCE_PATH + \"/src/file\"));\r\n                } catch (Exception e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test failed: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testFailCopyWithAccessControlException",
  "errType" : [ "Exception", "Exception", "IOException", "AccessControlException", "Exception" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testFailCopyWithAccessControlException()\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser(\"guest\");\r\n        final CopyMapper copyMapper = new CopyMapper();\r\n        final StubContext stubContext = tmpUser.doAs(new PrivilegedAction<StubContext>() {\r\n\r\n            @Override\r\n            public StubContext run() {\r\n                try {\r\n                    return new StubContext(getConfiguration(), null, 0);\r\n                } catch (Exception e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n        });\r\n        EnumSet<DistCpOptions.FileAttribute> preserveStatus = EnumSet.allOf(DistCpOptions.FileAttribute.class);\r\n        preserveStatus.remove(DistCpOptions.FileAttribute.ACL);\r\n        preserveStatus.remove(DistCpOptions.FileAttribute.XATTR);\r\n        final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        context.getConfiguration().set(DistCpConstants.CONF_LABEL_PRESERVE_STATUS, DistCpUtils.packAttributes(preserveStatus));\r\n        touchFile(SOURCE_PATH + \"/src/file\");\r\n        OutputStream out = cluster.getFileSystem().create(new Path(TARGET_PATH + \"/src/file\"));\r\n        out.write(\"hello world\".getBytes());\r\n        out.close();\r\n        cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + \"/src/file\"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));\r\n        cluster.getFileSystem().setPermission(new Path(TARGET_PATH + \"/src/file\"), new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ));\r\n        final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {\r\n\r\n            @Override\r\n            public FileSystem run() {\r\n                try {\r\n                    return FileSystem.get(cluster.getConfiguration(0));\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Exception encountered \", e);\r\n                    Assert.fail(\"Test failed: \" + e.getMessage());\r\n                    throw new RuntimeException(\"Test ought to fail here\");\r\n                }\r\n            }\r\n        });\r\n        tmpUser.doAs(new PrivilegedAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() {\r\n                try {\r\n                    copyMapper.setup(context);\r\n                    copyMapper.map(new Text(\"/src/file\"), new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + \"/src/file\"))), context);\r\n                    Assert.fail(\"Didn't expect the file to be copied\");\r\n                } catch (AccessControlException ignore) {\r\n                } catch (Exception e) {\r\n                    if (e.getCause() == null || e.getCause().getCause() == null || !(e.getCause().getCause() instanceof AccessControlException)) {\r\n                        throw new RuntimeException(e);\r\n                    }\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test failed: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testFileToDir",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFileToDir()\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        touchFile(SOURCE_PATH + \"/src/file\");\r\n        mkdirs(TARGET_PATH + \"/src/file\");\r\n        try {\r\n            copyMapper.setup(context);\r\n            copyMapper.map(new Text(\"/src/file\"), new CopyListingFileStatus(fs.getFileStatus(new Path(SOURCE_PATH + \"/src/file\"))), context);\r\n        } catch (IOException e) {\r\n            Assert.assertTrue(e.getMessage().startsWith(\"Can't replace\"));\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Test failed: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "doTestIgnoreFailures",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void doTestIgnoreFailures(boolean ignoreFailures)\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        Configuration configuration = context.getConfiguration();\r\n        configuration.setBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), ignoreFailures);\r\n        configuration.setBoolean(DistCpOptionSwitch.OVERWRITE.getConfigLabel(), true);\r\n        configuration.setBoolean(DistCpOptionSwitch.SKIP_CRC.getConfigLabel(), true);\r\n        copyMapper.setup(context);\r\n        for (Path path : pathList) {\r\n            final FileStatus fileStatus = fs.getFileStatus(path);\r\n            if (!fileStatus.isDirectory()) {\r\n                fs.delete(path, true);\r\n                copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fileStatus), context);\r\n            }\r\n        }\r\n        if (ignoreFailures) {\r\n            for (Text value : stubContext.getWriter().values()) {\r\n                Assert.assertTrue(value.toString() + \" is not skipped\", value.toString().startsWith(\"FAIL:\"));\r\n            }\r\n        }\r\n        Assert.assertTrue(\"There should have been an exception.\", ignoreFailures);\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(\"Unexpected exception: \" + e.getMessage(), !ignoreFailures);\r\n        e.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "doTestIgnoreFailuresDoubleWrapped",
  "errType" : [ "Exception", "Exception", "IOException", "IOException", "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void doTestIgnoreFailuresDoubleWrapped(final boolean ignoreFailures)\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        final UserGroupInformation tmpUser = UserGroupInformation.createRemoteUser(\"guest\");\r\n        final CopyMapper copyMapper = new CopyMapper();\r\n        final Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = tmpUser.doAs(new PrivilegedAction<Mapper<Text, CopyListingFileStatus, Text, Text>.Context>() {\r\n\r\n            @Override\r\n            public Mapper<Text, CopyListingFileStatus, Text, Text>.Context run() {\r\n                try {\r\n                    StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n                    return stubContext.getContext();\r\n                } catch (Exception e) {\r\n                    LOG.error(\"Exception encountered when get stub context\", e);\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n        });\r\n        touchFile(SOURCE_PATH + \"/src/file\");\r\n        mkdirs(TARGET_PATH);\r\n        cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + \"/src/file\"), new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));\r\n        cluster.getFileSystem().setPermission(new Path(TARGET_PATH), new FsPermission((short) 511));\r\n        context.getConfiguration().setBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), ignoreFailures);\r\n        final FileSystem tmpFS = tmpUser.doAs(new PrivilegedAction<FileSystem>() {\r\n\r\n            @Override\r\n            public FileSystem run() {\r\n                try {\r\n                    return FileSystem.get(cluster.getConfiguration(0));\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Exception encountered when get FileSystem.\", e);\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n        });\r\n        tmpUser.doAs(new PrivilegedAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() {\r\n                try {\r\n                    copyMapper.setup(context);\r\n                    copyMapper.map(new Text(\"/src/file\"), new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + \"/src/file\"))), context);\r\n                    Assert.assertTrue(\"Should have thrown an IOException if not \" + \"ignoring failures\", ignoreFailures);\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Unexpected exception encountered. \", e);\r\n                    Assert.assertFalse(\"Should not have thrown an IOException if \" + \"ignoring failures\", ignoreFailures);\r\n                } catch (Exception e) {\r\n                    LOG.error(\"Exception encountered when the mapper copies file.\", e);\r\n                    throw new RuntimeException(e);\r\n                }\r\n                return null;\r\n            }\r\n        });\r\n    } catch (Exception e) {\r\n        LOG.error(\"Unexpected exception encountered. \", e);\r\n        Assert.fail(\"Test failed: \" + e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "deleteState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteState() throws IOException\n{\r\n    pathList.clear();\r\n    nFiles = 0;\r\n    cluster.getFileSystem().delete(new Path(SOURCE_PATH), true);\r\n    cluster.getFileSystem().delete(new Path(TARGET_PATH), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserveBlockSizeAndReplication",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testPreserveBlockSizeAndReplication()\n{\r\n    testPreserveBlockSizeAndReplicationImpl(true);\r\n    testPreserveBlockSizeAndReplicationImpl(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopyWithDifferentBlockSizes",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testCopyWithDifferentBlockSizes() throws Exception\n{\r\n    try {\r\n        deleteState();\r\n        createSourceDataWithDifferentBlockSize();\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        Configuration configuration = context.getConfiguration();\r\n        EnumSet<DistCpOptions.FileAttribute> fileAttributes = EnumSet.noneOf(DistCpOptions.FileAttribute.class);\r\n        configuration.set(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel(), DistCpUtils.packAttributes(fileAttributes));\r\n        copyMapper.setup(context);\r\n        for (Path path : pathList) {\r\n            final FileStatus fileStatus = fs.getFileStatus(path);\r\n            copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fileStatus), context);\r\n        }\r\n        if (expectDifferentBlockSizesMultipleBlocksToSucceed()) {\r\n            verifyCopy(fs, false, false);\r\n        } else {\r\n            Assert.fail(\"Copy should have failed because of block-size difference.\");\r\n        }\r\n    } catch (Exception exception) {\r\n        if (expectDifferentBlockSizesMultipleBlocksToSucceed()) {\r\n            throw exception;\r\n        } else {\r\n            Throwable cause = exception.getCause().getCause();\r\n            GenericTestUtils.assertExceptionContains(\"-pb\", cause);\r\n            GenericTestUtils.assertExceptionContains(\"-skipcrccheck\", cause);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCopyWithDifferentBytesPerCrc",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCopyWithDifferentBytesPerCrc() throws Exception\n{\r\n    try {\r\n        deleteState();\r\n        createSourceDataWithDifferentBytesPerCrc();\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        Configuration configuration = context.getConfiguration();\r\n        EnumSet<DistCpOptions.FileAttribute> fileAttributes = EnumSet.noneOf(DistCpOptions.FileAttribute.class);\r\n        configuration.set(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel(), DistCpUtils.packAttributes(fileAttributes));\r\n        copyMapper.setup(context);\r\n        for (Path path : pathList) {\r\n            final FileStatus fileStatus = fs.getFileStatus(path);\r\n            copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fileStatus), context);\r\n        }\r\n        if (expectDifferentBytesPerCrcToSucceed()) {\r\n            verifyCopy(fs, false, false);\r\n        } else {\r\n            Assert.fail(\"Copy should have failed because of bytes-per-crc difference.\");\r\n        }\r\n    } catch (Exception exception) {\r\n        if (expectDifferentBytesPerCrcToSucceed()) {\r\n            throw exception;\r\n        } else {\r\n            Throwable cause = exception.getCause().getCause();\r\n            GenericTestUtils.assertExceptionContains(\"mismatch\", cause);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserveBlockSizeAndReplicationImpl",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testPreserveBlockSizeAndReplicationImpl(boolean preserve)\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        Configuration configuration = context.getConfiguration();\r\n        EnumSet<DistCpOptions.FileAttribute> fileAttributes = EnumSet.noneOf(DistCpOptions.FileAttribute.class);\r\n        if (preserve) {\r\n            fileAttributes.add(DistCpOptions.FileAttribute.BLOCKSIZE);\r\n            fileAttributes.add(DistCpOptions.FileAttribute.REPLICATION);\r\n        }\r\n        configuration.set(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel(), DistCpUtils.packAttributes(fileAttributes));\r\n        copyMapper.setup(context);\r\n        for (Path path : pathList) {\r\n            final FileStatus fileStatus = fs.getFileStatus(path);\r\n            copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fileStatus), context);\r\n        }\r\n        for (Path path : pathList) {\r\n            final Path targetPath = new Path(path.toString().replaceAll(SOURCE_PATH, TARGET_PATH));\r\n            final FileStatus source = fs.getFileStatus(path);\r\n            final FileStatus target = fs.getFileStatus(targetPath);\r\n            if (!source.isDirectory()) {\r\n                Assert.assertTrue(preserve || source.getBlockSize() != target.getBlockSize());\r\n                Assert.assertTrue(preserve || source.getReplication() != target.getReplication());\r\n                Assert.assertTrue(!preserve || source.getBlockSize() == target.getBlockSize());\r\n                Assert.assertTrue(!preserve || source.getReplication() == target.getReplication());\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(\"Unexpected exception: \" + e.getMessage(), false);\r\n        e.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "changeUserGroup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void changeUserGroup(String user, String group) throws IOException\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    FsPermission changedPermission = new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL);\r\n    for (Path path : pathList) if (fs.isFile(path)) {\r\n        fs.setOwner(path, user, group);\r\n        fs.setPermission(path, changedPermission);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testSingleFileCopy",
  "errType" : [ "Exception", "Throwable" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testSingleFileCopy()\n{\r\n    try {\r\n        deleteState();\r\n        touchFile(SOURCE_PATH + \"/1\");\r\n        Path sourceFilePath = pathList.get(0);\r\n        Path targetFilePath = new Path(sourceFilePath.toString().replaceAll(SOURCE_PATH, TARGET_PATH));\r\n        touchFile(targetFilePath.toString());\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        context.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, targetFilePath.getParent().toString());\r\n        copyMapper.setup(context);\r\n        final CopyListingFileStatus sourceFileStatus = new CopyListingFileStatus(fs.getFileStatus(sourceFilePath));\r\n        long before = fs.getFileStatus(targetFilePath).getModificationTime();\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), sourceFilePath)), sourceFileStatus, context);\r\n        long after = fs.getFileStatus(targetFilePath).getModificationTime();\r\n        Assert.assertTrue(\"File should have been skipped\", before == after);\r\n        context.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, targetFilePath.toString());\r\n        copyMapper.setup(context);\r\n        before = fs.getFileStatus(targetFilePath).getModificationTime();\r\n        try {\r\n            Thread.sleep(2);\r\n        } catch (Throwable ignore) {\r\n        }\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), sourceFilePath)), sourceFileStatus, context);\r\n        after = fs.getFileStatus(targetFilePath).getModificationTime();\r\n        Assert.assertTrue(\"File should have been overwritten.\", before < after);\r\n    } catch (Exception exception) {\r\n        Assert.fail(\"Unexpected exception: \" + exception.getMessage());\r\n        exception.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserveUserGroup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testPreserveUserGroup()\n{\r\n    testPreserveUserGroupImpl(true);\r\n    testPreserveUserGroupImpl(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testPreserveUserGroupImpl",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testPreserveUserGroupImpl(boolean preserve)\n{\r\n    try {\r\n        deleteState();\r\n        createSourceData();\r\n        changeUserGroup(\"Michael\", \"Corleone\");\r\n        FileSystem fs = cluster.getFileSystem();\r\n        CopyMapper copyMapper = new CopyMapper();\r\n        StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n        Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n        Configuration configuration = context.getConfiguration();\r\n        EnumSet<DistCpOptions.FileAttribute> fileAttributes = EnumSet.noneOf(DistCpOptions.FileAttribute.class);\r\n        if (preserve) {\r\n            fileAttributes.add(DistCpOptions.FileAttribute.USER);\r\n            fileAttributes.add(DistCpOptions.FileAttribute.GROUP);\r\n            fileAttributes.add(DistCpOptions.FileAttribute.PERMISSION);\r\n        }\r\n        configuration.set(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel(), DistCpUtils.packAttributes(fileAttributes));\r\n        copyMapper.setup(context);\r\n        for (Path path : pathList) {\r\n            final FileStatus fileStatus = fs.getFileStatus(path);\r\n            copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fileStatus), context);\r\n        }\r\n        for (Path path : pathList) {\r\n            final Path targetPath = new Path(path.toString().replaceAll(SOURCE_PATH, TARGET_PATH));\r\n            final FileStatus source = fs.getFileStatus(path);\r\n            final FileStatus target = fs.getFileStatus(targetPath);\r\n            if (!source.isDirectory()) {\r\n                Assert.assertTrue(!preserve || source.getOwner().equals(target.getOwner()));\r\n                Assert.assertTrue(!preserve || source.getGroup().equals(target.getGroup()));\r\n                Assert.assertTrue(!preserve || source.getPermission().equals(target.getPermission()));\r\n                Assert.assertTrue(preserve || !source.getOwner().equals(target.getOwner()));\r\n                Assert.assertTrue(preserve || !source.getGroup().equals(target.getGroup()));\r\n                Assert.assertTrue(preserve || !source.getPermission().equals(target.getPermission()));\r\n                Assert.assertTrue(source.isDirectory() || source.getReplication() != target.getReplication());\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(\"Unexpected exception: \" + e.getMessage(), false);\r\n        e.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testVerboseLogging",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testVerboseLogging() throws Exception\n{\r\n    deleteState();\r\n    createSourceData();\r\n    FileSystem fs = cluster.getFileSystem();\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(getConfiguration(), null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n    copyMapper.setup(context);\r\n    int numFiles = 0;\r\n    for (Path path : pathList) {\r\n        if (fs.getFileStatus(path).isFile()) {\r\n            numFiles++;\r\n        }\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fs.getFileStatus(path)), context);\r\n    }\r\n    Assert.assertEquals(numFiles, stubContext.getReporter().getCounter(CopyMapper.Counter.COPY).getValue());\r\n    testCopyingExistingFiles(fs, copyMapper, context);\r\n    for (Text value : stubContext.getWriter().values()) {\r\n        Assert.assertTrue(!value.toString().startsWith(\"FILE_COPIED:\"));\r\n        Assert.assertTrue(!value.toString().startsWith(\"FILE_SKIPPED:\"));\r\n    }\r\n    deleteState();\r\n    createSourceData();\r\n    stubContext = new StubContext(getConfiguration(), null, 0);\r\n    context = stubContext.getContext();\r\n    copyMapper.setup(context);\r\n    context.getConfiguration().setBoolean(DistCpOptionSwitch.VERBOSE_LOG.getConfigLabel(), true);\r\n    copyMapper.setup(context);\r\n    for (Path path : pathList) {\r\n        copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH), path)), new CopyListingFileStatus(fs.getFileStatus(path)), context);\r\n    }\r\n    Assert.assertEquals(numFiles, stubContext.getReporter().getCounter(CopyMapper.Counter.COPY).getValue());\r\n    int numFileCopied = 0;\r\n    for (Text value : stubContext.getWriter().values()) {\r\n        if (value.toString().startsWith(\"FILE_COPIED:\")) {\r\n            numFileCopied++;\r\n        }\r\n    }\r\n    Assert.assertEquals(numFiles, numFileCopied);\r\n    int numFileSkipped = 0;\r\n    testCopyingExistingFiles(fs, copyMapper, context);\r\n    for (Text value : stubContext.getWriter().values()) {\r\n        if (value.toString().startsWith(\"FILE_SKIPPED:\")) {\r\n            numFileSkipped++;\r\n        }\r\n    }\r\n    Assert.assertEquals(numFiles, numFileSkipped);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createCluster() throws IOException\n{\r\n    HDFSContract.createCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "teardownCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardownCluster() throws IOException\n{\r\n    HDFSContract.destroyCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new HDFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "getDefaultDistCPSizeKb",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDefaultDistCPSizeKb()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "beforeClass",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void beforeClass() throws IOException\n{\r\n    conf = new Configuration();\r\n    conf.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, BLOCK_SIZE);\r\n    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n    cluster.waitActive();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "afterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void afterClass() throws IOException\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "execCmd",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String execCmd(FsShell shell, String... args) throws Exception\n{\r\n    ByteArrayOutputStream baout = new ByteArrayOutputStream();\r\n    PrintStream out = new PrintStream(baout, true);\r\n    PrintStream old = System.out;\r\n    System.setOut(out);\r\n    shell.run(args);\r\n    out.close();\r\n    System.setOut(old);\r\n    return baout.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void createFiles(DistributedFileSystem fs, String topdir, FileEntry[] entries, long chunkSize) throws IOException\n{\r\n    long seed = System.currentTimeMillis();\r\n    Random rand = new Random(seed);\r\n    short replicationFactor = 2;\r\n    for (FileEntry entry : entries) {\r\n        Path newPath = new Path(topdir + \"/\" + entry.getPath());\r\n        if (entry.isDirectory()) {\r\n            fs.mkdirs(newPath);\r\n        } else {\r\n            long fileSize = BLOCK_SIZE * 100;\r\n            int bufSize = 128;\r\n            if (chunkSize == -1) {\r\n                DFSTestUtil.createFile(fs, newPath, bufSize, fileSize, BLOCK_SIZE, replicationFactor, seed);\r\n            } else {\r\n                long seg1 = chunkSize * BLOCK_SIZE - BLOCK_SIZE / 2;\r\n                long seg2 = fileSize - seg1;\r\n                DFSTestUtil.createFile(fs, newPath, bufSize, seg1, BLOCK_SIZE, replicationFactor, seed);\r\n                DFSTestUtil.appendFileNewBlock(fs, newPath, (int) seg2);\r\n            }\r\n        }\r\n        seed = System.currentTimeMillis() + rand.nextLong();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createFiles(DistributedFileSystem fs, String topdir, FileEntry[] entries) throws IOException\n{\r\n    createFiles(fs, topdir, entries, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus[] getFileStatus(FileSystem fs, String topdir, FileEntry[] files) throws IOException\n{\r\n    Path root = new Path(topdir);\r\n    List<FileStatus> statuses = new ArrayList<FileStatus>();\r\n    for (int idx = 0; idx < files.length; ++idx) {\r\n        Path newpath = new Path(root, files[idx].getPath());\r\n        statuses.add(fs.getFileStatus(newpath));\r\n    }\r\n    return statuses.toArray(new FileStatus[statuses.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "deldir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deldir(FileSystem fs, String topdir) throws IOException\n{\r\n    fs.delete(new Path(topdir), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveUserHelper",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testPreserveUserHelper(String testRoot, FileEntry[] srcEntries, FileEntry[] dstEntries, boolean createSrcDir, boolean createTgtDir, boolean update) throws Exception\n{\r\n    final String testSrcRel = SRCDAT;\r\n    final String testSrc = testRoot + \"/\" + testSrcRel;\r\n    final String testDstRel = DSTDAT;\r\n    final String testDst = testRoot + \"/\" + testDstRel;\r\n    String nnUri = FileSystem.getDefaultUri(conf).toString();\r\n    DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(URI.create(nnUri), conf);\r\n    fs.mkdirs(new Path(testRoot));\r\n    if (createSrcDir) {\r\n        fs.mkdirs(new Path(testSrc));\r\n    }\r\n    if (createTgtDir) {\r\n        fs.mkdirs(new Path(testDst));\r\n    }\r\n    createFiles(fs, testRoot, srcEntries);\r\n    FileStatus[] srcstats = getFileStatus(fs, testRoot, srcEntries);\r\n    for (int i = 0; i < srcEntries.length; i++) {\r\n        fs.setOwner(srcstats[i].getPath(), \"u\" + i, null);\r\n    }\r\n    String[] args = update ? new String[] { \"-pub\", \"-update\", nnUri + testSrc, nnUri + testDst } : new String[] { \"-pub\", nnUri + testSrc, nnUri + testDst };\r\n    ToolRunner.run(conf, new DistCp(), args);\r\n    String realTgtPath = testDst;\r\n    if (!createTgtDir) {\r\n        realTgtPath = testRoot;\r\n    }\r\n    FileStatus[] dststat = getFileStatus(fs, realTgtPath, dstEntries);\r\n    for (int i = 0; i < dststat.length; i++) {\r\n        assertEquals(\"i=\" + i, \"u\" + i, dststat[i].getOwner());\r\n    }\r\n    deldir(fs, testRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "compareFiles",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void compareFiles(FileSystem fs, FileStatus srcStat, FileStatus dstStat) throws Exception\n{\r\n    LOG.info(\"Comparing \" + srcStat + \" and \" + dstStat);\r\n    assertEquals(srcStat.isDirectory(), dstStat.isDirectory());\r\n    assertEquals(srcStat.getReplication(), dstStat.getReplication());\r\n    assertEquals(\"File POSIX permission should match\", srcStat.getPermission(), dstStat.getPermission());\r\n    assertEquals(\"File user ownership should match\", srcStat.getOwner(), dstStat.getOwner());\r\n    assertEquals(\"File group ownership should match\", srcStat.getGroup(), dstStat.getGroup());\r\n    if (srcStat.isDirectory()) {\r\n        return;\r\n    }\r\n    assertEquals(\"File length should match (\" + srcStat.getPath() + \")\", srcStat.getLen(), dstStat.getLen());\r\n    FSDataInputStream srcIn = fs.open(srcStat.getPath());\r\n    FSDataInputStream dstIn = fs.open(dstStat.getPath());\r\n    try {\r\n        byte[] readSrc = new byte[(int) HdfsClientConfigKeys.DFS_BLOCK_SIZE_DEFAULT];\r\n        byte[] readDst = new byte[(int) HdfsClientConfigKeys.DFS_BLOCK_SIZE_DEFAULT];\r\n        int srcBytesRead = 0, tgtBytesRead = 0;\r\n        int srcIdx = 0, tgtIdx = 0;\r\n        long totalComparedBytes = 0;\r\n        while (true) {\r\n            if (srcBytesRead == 0) {\r\n                srcBytesRead = srcIn.read(readSrc);\r\n                srcIdx = 0;\r\n            }\r\n            if (tgtBytesRead == 0) {\r\n                tgtBytesRead = dstIn.read(readDst);\r\n                tgtIdx = 0;\r\n            }\r\n            if (srcBytesRead == 0 || tgtBytesRead == 0) {\r\n                LOG.info(\"______ compared src and dst files for \" + totalComparedBytes + \" bytes, content match.\");\r\n                if (srcBytesRead != tgtBytesRead) {\r\n                    Assert.fail(\"Read mismatching size, compared \" + totalComparedBytes + \" bytes between src and dst file \" + srcStat + \" and \" + dstStat);\r\n                }\r\n                if (totalComparedBytes != srcStat.getLen()) {\r\n                    Assert.fail(\"Only read/compared \" + totalComparedBytes + \" bytes between src and dst file \" + srcStat + \" and \" + dstStat);\r\n                } else {\r\n                    break;\r\n                }\r\n            }\r\n            for (; srcIdx < srcBytesRead && tgtIdx < tgtBytesRead; ++srcIdx, ++tgtIdx) {\r\n                if (readSrc[srcIdx] != readDst[tgtIdx]) {\r\n                    Assert.fail(\"src and dst file does not match at \" + totalComparedBytes + \" between \" + srcStat + \" and \" + dstStat);\r\n                }\r\n                ++totalComparedBytes;\r\n            }\r\n            LOG.info(\"______ compared src and dst files for \" + totalComparedBytes + \" bytes, content match. FileLength: \" + srcStat.getLen());\r\n            if (totalComparedBytes == srcStat.getLen()) {\r\n                LOG.info(\"______ Final:\" + srcIdx + \" \" + srcBytesRead + \" \" + tgtIdx + \" \" + tgtBytesRead);\r\n                break;\r\n            }\r\n            if (srcIdx == srcBytesRead) {\r\n                srcBytesRead = 0;\r\n            }\r\n            if (tgtIdx == tgtBytesRead) {\r\n                tgtBytesRead = 0;\r\n            }\r\n        }\r\n    } finally {\r\n        if (srcIn != null) {\r\n            srcIn.close();\r\n        }\r\n        if (dstIn != null) {\r\n            dstIn.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createDestDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createDestDir(FileSystem fs, String testDst, FileStatus[] srcStats, FileEntry[] srcFiles) throws IOException\n{\r\n    fs.mkdirs(new Path(testDst));\r\n    for (int i = 0; i < srcStats.length; i++) {\r\n        FileStatus srcStat = srcStats[i];\r\n        if (srcStat.isDirectory()) {\r\n            Path dstPath = new Path(testDst, srcFiles[i].getPath());\r\n            fs.mkdirs(dstPath);\r\n            fs.setOwner(dstPath, srcStat.getOwner(), srcStat.getGroup());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "copyAndVerify",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void copyAndVerify(final DistributedFileSystem fs, final FileEntry[] srcFiles, final FileStatus[] srcStats, final String testDst, final String[] args) throws Exception\n{\r\n    final String testRoot = \"/testdir\";\r\n    FsShell shell = new FsShell(fs.getConf());\r\n    LOG.info(\"ls before distcp\");\r\n    LOG.info(execCmd(shell, \"-lsr\", testRoot));\r\n    LOG.info(\"_____ running distcp: \" + args[0] + \" \" + args[1]);\r\n    ToolRunner.run(conf, new DistCp(), args);\r\n    LOG.info(\"ls after distcp\");\r\n    LOG.info(execCmd(shell, \"-lsr\", testRoot));\r\n    FileStatus[] dstStat = getFileStatus(fs, testDst, srcFiles);\r\n    for (int i = 0; i < dstStat.length; i++) {\r\n        compareFiles(fs, srcStats[i], dstStat[i]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "chunkCopy",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void chunkCopy(FileEntry[] srcFiles) throws Exception\n{\r\n    final String testRoot = \"/testdir\";\r\n    final String testSrcRel = SRCDAT;\r\n    final String testSrc = testRoot + \"/\" + testSrcRel;\r\n    final String testDstRel = DSTDAT;\r\n    final String testDst = testRoot + \"/\" + testDstRel;\r\n    long chunkSize = 8;\r\n    String nnUri = FileSystem.getDefaultUri(conf).toString();\r\n    DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(URI.create(nnUri), conf);\r\n    createFiles(fs, testRoot, srcFiles, chunkSize);\r\n    FileStatus[] srcStats = getFileStatus(fs, testRoot, srcFiles);\r\n    for (int i = 0; i < srcFiles.length; i++) {\r\n        fs.setOwner(srcStats[i].getPath(), \"u\" + i, \"g\" + i);\r\n    }\r\n    srcStats = getFileStatus(fs, testRoot, srcFiles);\r\n    createDestDir(fs, testDst, srcStats, srcFiles);\r\n    String[] args = new String[] { \"-pugp\", \"-blocksperchunk\", String.valueOf(chunkSize), nnUri + testSrc, nnUri + testDst };\r\n    copyAndVerify(fs, srcFiles, srcStats, testDst, args);\r\n    copyAndVerify(fs, srcFiles, srcStats, testDst, args);\r\n    LOG.info(\"Modify a file and copy again\");\r\n    for (int i = srcFiles.length - 1; i >= 0; --i) {\r\n        if (!srcFiles[i].isDirectory()) {\r\n            LOG.info(\"Modifying \" + srcStats[i].getPath());\r\n            DFSTestUtil.appendFileNewBlock(fs, srcStats[i].getPath(), (int) BLOCK_SIZE * 3);\r\n            break;\r\n        }\r\n    }\r\n    srcStats = getFileStatus(fs, testRoot, srcFiles);\r\n    args = new String[] { \"-pugp\", \"-update\", \"-blocksperchunk\", String.valueOf(chunkSize), nnUri + testSrc, nnUri + testDst + \"/\" + testSrcRel };\r\n    copyAndVerify(fs, srcFiles, srcStats, testDst, args);\r\n    deldir(fs, testRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testRecursiveChunkCopy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRecursiveChunkCopy() throws Exception\n{\r\n    FileEntry[] srcFiles = { new FileEntry(SRCDAT, true), new FileEntry(SRCDAT + \"/file0\", false), new FileEntry(SRCDAT + \"/dir1\", true), new FileEntry(SRCDAT + \"/dir2\", true), new FileEntry(SRCDAT + \"/dir1/file1\", false) };\r\n    chunkCopy(srcFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testChunkCopyOneFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testChunkCopyOneFile() throws Exception\n{\r\n    FileEntry[] srcFiles = { new FileEntry(SRCDAT, true), new FileEntry(SRCDAT + \"/file0\", false) };\r\n    chunkCopy(srcFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testDistcpLargeFile",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testDistcpLargeFile() throws Exception\n{\r\n    FileEntry[] srcfiles = { new FileEntry(SRCDAT, true), new FileEntry(SRCDAT + \"/file\", false) };\r\n    final String testRoot = \"/testdir\";\r\n    final String testSrcRel = SRCDAT;\r\n    final String testSrc = testRoot + \"/\" + testSrcRel;\r\n    final String testDstRel = DSTDAT;\r\n    final String testDst = testRoot + \"/\" + testDstRel;\r\n    String nnUri = FileSystem.getDefaultUri(conf).toString();\r\n    DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(URI.create(nnUri), conf);\r\n    fs.mkdirs(new Path(testRoot));\r\n    fs.mkdirs(new Path(testSrc));\r\n    fs.mkdirs(new Path(testDst));\r\n    long chunkSize = 6;\r\n    createFiles(fs, testRoot, srcfiles, chunkSize);\r\n    String srcFileName = testRoot + Path.SEPARATOR + srcfiles[1].getPath();\r\n    Path srcfile = new Path(srcFileName);\r\n    if (!cluster.getFileSystem().exists(srcfile)) {\r\n        throw new Exception(\"src not exist\");\r\n    }\r\n    final long srcLen = fs.getFileStatus(srcfile).getLen();\r\n    FileStatus[] srcstats = getFileStatus(fs, testRoot, srcfiles);\r\n    for (int i = 0; i < srcfiles.length; i++) {\r\n        fs.setOwner(srcstats[i].getPath(), \"u\" + i, null);\r\n    }\r\n    String[] args = new String[] { \"-blocksperchunk\", String.valueOf(chunkSize), nnUri + testSrc, nnUri + testDst };\r\n    LOG.info(\"_____ running distcp: \" + args[0] + \" \" + args[1]);\r\n    ToolRunner.run(conf, new DistCp(), args);\r\n    String realTgtPath = testDst;\r\n    FileStatus[] dststat = getFileStatus(fs, realTgtPath, srcfiles);\r\n    assertEquals(\"File length should match\", srcLen, dststat[dststat.length - 1].getLen());\r\n    this.compareFiles(fs, srcstats[srcstats.length - 1], dststat[dststat.length - 1]);\r\n    deldir(fs, testRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveUseNonEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPreserveUseNonEmptyDir() throws Exception\n{\r\n    String testRoot = \"/testdir.\" + getMethodName();\r\n    FileEntry[] srcfiles = { new FileEntry(SRCDAT, true), new FileEntry(SRCDAT + \"/a\", false), new FileEntry(SRCDAT + \"/b\", true), new FileEntry(SRCDAT + \"/b/c\", false) };\r\n    FileEntry[] dstfiles = { new FileEntry(DSTDAT, true), new FileEntry(DSTDAT + \"/a\", false), new FileEntry(DSTDAT + \"/b\", true), new FileEntry(DSTDAT + \"/b/c\", false) };\r\n    testPreserveUserHelper(testRoot, srcfiles, srcfiles, false, true, false);\r\n    testPreserveUserHelper(testRoot, srcfiles, dstfiles, false, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveUserEmptyDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPreserveUserEmptyDir() throws Exception\n{\r\n    String testRoot = \"/testdir.\" + getMethodName();\r\n    FileEntry[] srcfiles = { new FileEntry(SRCDAT, true) };\r\n    FileEntry[] dstfiles = { new FileEntry(DSTDAT, true) };\r\n    testPreserveUserHelper(testRoot, srcfiles, srcfiles, false, true, false);\r\n    testPreserveUserHelper(testRoot, srcfiles, dstfiles, false, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveUserSingleFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPreserveUserSingleFile() throws Exception\n{\r\n    String testRoot = \"/testdir.\" + getMethodName();\r\n    FileEntry[] srcfiles = { new FileEntry(SRCDAT, false) };\r\n    FileEntry[] dstfiles = { new FileEntry(DSTDAT, false) };\r\n    testPreserveUserHelper(testRoot, srcfiles, srcfiles, false, true, false);\r\n    testPreserveUserHelper(testRoot, srcfiles, dstfiles, false, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveUserNonEmptyDirWithUpdate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testPreserveUserNonEmptyDirWithUpdate() throws Exception\n{\r\n    String testRoot = \"/testdir.\" + getMethodName();\r\n    FileEntry[] srcfiles = { new FileEntry(SRCDAT + \"/a\", false), new FileEntry(SRCDAT + \"/b\", true), new FileEntry(SRCDAT + \"/b/c\", false) };\r\n    FileEntry[] dstfiles = { new FileEntry(\"a\", false), new FileEntry(\"b\", true), new FileEntry(\"b/c\", false) };\r\n    testPreserveUserHelper(testRoot, srcfiles, dstfiles, true, true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSourceRoot",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSourceRoot() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    String rootStr = fs.makeQualified(new Path(\"/\")).toString();\r\n    String testRoot = \"/testdir.\" + getMethodName();\r\n    Path tgtPath = new Path(testRoot + \"/nodir\");\r\n    String tgtStr = fs.makeQualified(tgtPath).toString();\r\n    String[] args = new String[] { rootStr, tgtStr };\r\n    Assert.assertThat(ToolRunner.run(conf, new DistCp(), args), is(0));\r\n    Path tgtPath2 = new Path(testRoot + \"/dir\");\r\n    assertTrue(fs.mkdirs(tgtPath2));\r\n    String tgtStr2 = fs.makeQualified(tgtPath2).toString();\r\n    String[] args2 = new String[] { rootStr, tgtStr2 };\r\n    Assert.assertThat(ToolRunner.run(conf, new DistCp(), args2), is(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateRoot",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testUpdateRoot() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    Path source = new Path(\"/src\");\r\n    Path dest1 = new Path(\"/dest1\");\r\n    Path dest2 = new Path(\"/dest2\");\r\n    fs.delete(source, true);\r\n    fs.delete(dest1, true);\r\n    fs.delete(dest2, true);\r\n    fs.mkdirs(source);\r\n    fs.setOwner(source, \"userA\", \"groupA\");\r\n    fs.setTimes(source, new Random().nextLong(), new Random().nextLong());\r\n    GenericTestUtils.createFiles(fs, source, 3, 5, 5);\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, source.toString(), dest1.toString(), \"-p -update\", conf);\r\n    FileStatus srcStatus = fs.getFileStatus(source);\r\n    FileStatus destStatus1 = fs.getFileStatus(dest1);\r\n    assertNotEquals(srcStatus.getOwner(), destStatus1.getOwner());\r\n    assertNotEquals(srcStatus.getModificationTime(), destStatus1.getModificationTime());\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, source.toString(), dest2.toString(), \"-p -update -updateRoot\", conf);\r\n    FileStatus destStatus2 = fs.getFileStatus(dest2);\r\n    assertEquals(srcStatus.getOwner(), destStatus2.getOwner());\r\n    assertEquals(srcStatus.getModificationTime(), destStatus2.getModificationTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testShouldCopyTrue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testShouldCopyTrue()\n{\r\n    List<Pattern> filters = new ArrayList<>();\r\n    filters.add(Pattern.compile(\"user\"));\r\n    RegexCopyFilter regexCopyFilter = new RegexCopyFilter(\"fakeFile\");\r\n    regexCopyFilter.setFilters(filters);\r\n    Path shouldCopyPath = new Path(\"/user/bar\");\r\n    Assert.assertTrue(regexCopyFilter.shouldCopy(shouldCopyPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testShouldCopyFalse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testShouldCopyFalse()\n{\r\n    List<Pattern> filters = new ArrayList<>();\r\n    filters.add(Pattern.compile(\".*test.*\"));\r\n    RegexCopyFilter regexCopyFilter = new RegexCopyFilter(\"fakeFile\");\r\n    regexCopyFilter.setFilters(filters);\r\n    Path shouldNotCopyPath = new Path(\"/user/testing\");\r\n    Assert.assertFalse(regexCopyFilter.shouldCopy(shouldNotCopyPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testShouldCopyWithMultipleFilters",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testShouldCopyWithMultipleFilters()\n{\r\n    List<Pattern> filters = new ArrayList<>();\r\n    filters.add(Pattern.compile(\".*test.*\"));\r\n    filters.add(Pattern.compile(\"/user/b.*\"));\r\n    filters.add(Pattern.compile(\".*_SUCCESS\"));\r\n    List<Path> toCopy = getTestPaths();\r\n    int shouldCopyCount = 0;\r\n    RegexCopyFilter regexCopyFilter = new RegexCopyFilter(\"fakeFile\");\r\n    regexCopyFilter.setFilters(filters);\r\n    for (Path path : toCopy) {\r\n        if (regexCopyFilter.shouldCopy(path)) {\r\n            shouldCopyCount++;\r\n        }\r\n    }\r\n    Assert.assertEquals(2, shouldCopyCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testShouldExcludeAll",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testShouldExcludeAll()\n{\r\n    List<Pattern> filters = new ArrayList<>();\r\n    filters.add(Pattern.compile(\".*test.*\"));\r\n    filters.add(Pattern.compile(\"/user/b.*\"));\r\n    filters.add(Pattern.compile(\".*\"));\r\n    List<Path> toCopy = getTestPaths();\r\n    int shouldCopyCount = 0;\r\n    RegexCopyFilter regexCopyFilter = new RegexCopyFilter(\"fakeFile\");\r\n    regexCopyFilter.setFilters(filters);\r\n    for (Path path : toCopy) {\r\n        if (regexCopyFilter.shouldCopy(path)) {\r\n            shouldCopyCount++;\r\n        }\r\n    }\r\n    Assert.assertEquals(0, shouldCopyCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTestPaths",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<Path> getTestPaths()\n{\r\n    List<Path> toCopy = new ArrayList<>();\r\n    toCopy.add(new Path(\"/user/bar\"));\r\n    toCopy.add(new Path(\"/user/foo/_SUCCESS\"));\r\n    toCopy.add(new Path(\"/hive/test_data\"));\r\n    toCopy.add(new Path(\"test\"));\r\n    toCopy.add(new Path(\"/user/foo/bar\"));\r\n    toCopy.add(new Path(\"/mapred/.staging_job\"));\r\n    return toCopy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void init() throws Exception\n{\r\n    initCluster(true, true);\r\n    fs.mkdirs(new Path(\"/src/dir1/subdir1\"));\r\n    fs.mkdirs(new Path(\"/src/dir2\"));\r\n    fs.create(new Path(\"/src/dir2/file2\")).close();\r\n    fs.create(new Path(\"/src/dir2/file3\")).close();\r\n    fs.mkdirs(new Path(\"/src/dir3sticky\"));\r\n    fs.create(new Path(\"/src/file1\")).close();\r\n    fs.modifyAclEntries(new Path(\"/src/dir1\"), Arrays.asList(aclEntry(DEFAULT, USER, \"bruce\", ALL)));\r\n    fs.modifyAclEntries(new Path(\"/src/dir2/file2\"), Arrays.asList(aclEntry(ACCESS, GROUP, \"sales\", NONE)));\r\n    fs.setPermission(new Path(\"/src/dir2/file3\"), new FsPermission((short) 0660));\r\n    fs.modifyAclEntries(new Path(\"/src/file1\"), Arrays.asList(aclEntry(ACCESS, USER, \"diana\", READ)));\r\n    fs.setPermission(new Path(\"/src/dir3sticky\"), new FsPermission((short) 01777));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdown()\n{\r\n    IOUtils.cleanupWithLogger(null, fs);\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveAcls",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPreserveAcls() throws Exception\n{\r\n    assertRunDistCp(DistCpConstants.SUCCESS, \"/dstPreserveAcls\");\r\n    assertAclEntries(\"/dstPreserveAcls/dir1\", new AclEntry[] { aclEntry(DEFAULT, USER, ALL), aclEntry(DEFAULT, USER, \"bruce\", ALL), aclEntry(DEFAULT, GROUP, READ_EXECUTE), aclEntry(DEFAULT, MASK, ALL), aclEntry(DEFAULT, OTHER, READ_EXECUTE) });\r\n    assertPermission(\"/dstPreserveAcls/dir1\", (short) 0755);\r\n    assertAclEntries(\"/dstPreserveAcls/dir1/subdir1\", new AclEntry[] {});\r\n    assertPermission(\"/dstPreserveAcls/dir1/subdir1\", (short) 0755);\r\n    assertAclEntries(\"/dstPreserveAcls/dir2\", new AclEntry[] {});\r\n    assertPermission(\"/dstPreserveAcls/dir2\", (short) 0755);\r\n    assertAclEntries(\"/dstPreserveAcls/dir2/file2\", new AclEntry[] { aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, GROUP, \"sales\", NONE) });\r\n    assertPermission(\"/dstPreserveAcls/dir2/file2\", (short) 0644);\r\n    assertAclEntries(\"/dstPreserveAcls/dir2/file3\", new AclEntry[] {});\r\n    assertPermission(\"/dstPreserveAcls/dir2/file3\", (short) 0660);\r\n    assertAclEntries(\"/dstPreserveAcls/dir3sticky\", new AclEntry[] {});\r\n    assertPermission(\"/dstPreserveAcls/dir3sticky\", (short) 01777);\r\n    assertAclEntries(\"/dstPreserveAcls/file1\", new AclEntry[] { aclEntry(ACCESS, USER, \"diana\", READ), aclEntry(ACCESS, GROUP, READ) });\r\n    assertPermission(\"/dstPreserveAcls/file1\", (short) 0644);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testAclsNotEnabled",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAclsNotEnabled() throws Exception\n{\r\n    try {\r\n        restart(false);\r\n        assertRunDistCp(DistCpConstants.ACLS_NOT_SUPPORTED, \"/dstAclsNotEnabled\");\r\n    } finally {\r\n        restart(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testAclsNotImplemented",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAclsNotImplemented() throws Exception\n{\r\n    assertRunDistCp(DistCpConstants.ACLS_NOT_SUPPORTED, \"stubfs://dstAclsNotImplemented\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "aclEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntry aclEntry(AclEntryScope scope, AclEntryType type, FsAction permission)\n{\r\n    return new AclEntry.Builder().setScope(scope).setType(type).setPermission(permission).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "aclEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclEntry aclEntry(AclEntryScope scope, AclEntryType type, String name, FsAction permission)\n{\r\n    return new AclEntry.Builder().setScope(scope).setType(type).setName(name).setPermission(permission).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "assertAclEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertAclEntries(String path, AclEntry[] entries) throws Exception\n{\r\n    assertArrayEquals(entries, fs.getAclStatus(new Path(path)).getEntries().toArray(new AclEntry[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "assertPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertPermission(String path, short perm) throws Exception\n{\r\n    assertEquals(perm, fs.getFileStatus(new Path(path)).getPermission().toShort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "assertRunDistCp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertRunDistCp(int exitCode, String dst) throws Exception\n{\r\n    DistCp distCp = new DistCp(conf, null);\r\n    assertEquals(exitCode, ToolRunner.run(conf, distCp, new String[] { \"-pa\", \"/src\", dst }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initCluster",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initCluster(boolean format, boolean aclsEnabled) throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, aclsEnabled);\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, \"stubfs:///\");\r\n    conf.setClass(\"fs.stubfs.impl\", StubFileSystem.class, FileSystem.class);\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(format).build();\r\n    cluster.waitActive();\r\n    fs = cluster.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "restart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void restart(boolean aclsEnabled) throws Exception\n{\r\n    shutdown();\r\n    initCluster(false, aclsEnabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).format(true).build();\r\n    fs = cluster.getFileSystem();\r\n    buildExpectedValuesMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroy()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "buildExpectedValuesMap",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void buildExpectedValuesMap()\n{\r\n    map.put(\"/file1\", \"/tmp/singlefile1/file1\");\r\n    map.put(\"/file2\", \"/tmp/singlefile2/file2\");\r\n    map.put(\"/file3\", \"/tmp/multifile/file3\");\r\n    map.put(\"/file4\", \"/tmp/multifile/file4\");\r\n    map.put(\"/file5\", \"/tmp/multifile/file5\");\r\n    map.put(\"/multifile/file3\", \"/tmp/multifile/file3\");\r\n    map.put(\"/multifile/file4\", \"/tmp/multifile/file4\");\r\n    map.put(\"/multifile/file5\", \"/tmp/multifile/file5\");\r\n    map.put(\"/Ufile3\", \"/tmp/Umultifile/Ufile3\");\r\n    map.put(\"/Ufile4\", \"/tmp/Umultifile/Ufile4\");\r\n    map.put(\"/Ufile5\", \"/tmp/Umultifile/Ufile5\");\r\n    map.put(\"/dir1\", \"/tmp/singledir/dir1\");\r\n    map.put(\"/singledir/dir1\", \"/tmp/singledir/dir1\");\r\n    map.put(\"/dir2\", \"/tmp/singledir/dir2\");\r\n    map.put(\"/singledir/dir2\", \"/tmp/singledir/dir2\");\r\n    map.put(\"/Udir1\", \"/tmp/Usingledir/Udir1\");\r\n    map.put(\"/Udir2\", \"/tmp/Usingledir/Udir2\");\r\n    map.put(\"/dir2/file6\", \"/tmp/singledir/dir2/file6\");\r\n    map.put(\"/singledir/dir2/file6\", \"/tmp/singledir/dir2/file6\");\r\n    map.put(\"/file7\", \"/tmp/singledir1/dir3/file7\");\r\n    map.put(\"/file8\", \"/tmp/singledir1/dir3/file8\");\r\n    map.put(\"/file9\", \"/tmp/singledir1/dir3/file9\");\r\n    map.put(\"/dir3/file7\", \"/tmp/singledir1/dir3/file7\");\r\n    map.put(\"/dir3/file8\", \"/tmp/singledir1/dir3/file8\");\r\n    map.put(\"/dir3/file9\", \"/tmp/singledir1/dir3/file9\");\r\n    map.put(\"/Ufile7\", \"/tmp/Usingledir1/Udir3/Ufile7\");\r\n    map.put(\"/Ufile8\", \"/tmp/Usingledir1/Udir3/Ufile8\");\r\n    map.put(\"/Ufile9\", \"/tmp/Usingledir1/Udir3/Ufile9\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileMissingTarget",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileMissingTarget()\n{\r\n    caseSingleFileMissingTarget(false);\r\n    caseSingleFileMissingTarget(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileMissingTarget",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseSingleFileMissingTarget(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/singlefile1/file1\");\r\n        createFiles(\"/tmp/singlefile1/file1\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(listFile, 0);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileTargetFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileTargetFile()\n{\r\n    caseSingleFileTargetFile(false);\r\n    caseSingleFileTargetFile(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileTargetFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseSingleFileTargetFile(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/singlefile1/file1\");\r\n        createFiles(\"/tmp/singlefile1/file1\", target.toString());\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(listFile, 0);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFileTargetDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleFileTargetDir()\n{\r\n    caseSingleFileTargetDir(false);\r\n    caseSingleFileTargetDir(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleFileTargetDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void caseSingleFileTargetDir(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/singlefile2/file2\");\r\n        createFiles(\"/tmp/singlefile2/file2\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, sync);\r\n        checkResult(listFile, 1);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleDirTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleDirTargetMissing()\n{\r\n    caseSingleDirTargetMissing(false);\r\n    caseSingleDirTargetMissing(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseSingleDirTargetMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseSingleDirTargetMissing(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/singledir\");\r\n        mkdirs(\"/tmp/singledir/dir1\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(listFile, 1);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSingleDirTargetPresent()\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/singledir\");\r\n        mkdirs(\"/tmp/singledir/dir1\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true);\r\n        checkResult(listFile, 1);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateSingleDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateSingleDirTargetPresent()\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/Usingledir\");\r\n        mkdirs(\"/tmp/Usingledir/Udir1\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, true);\r\n        checkResult(listFile, 1);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiFileTargetPresent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiFileTargetPresent()\n{\r\n    caseMultiFileTargetPresent(false);\r\n    caseMultiFileTargetPresent(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseMultiFileTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void caseMultiFileTargetPresent(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        createFiles(\"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true, sync);\r\n        checkResult(listFile, 3);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiFileTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiFileTargetMissing()\n{\r\n    caseMultiFileTargetMissing(false);\r\n    caseMultiFileTargetMissing(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseMultiFileTargetMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void caseMultiFileTargetMissing(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        createFiles(\"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        runTest(listFile, target, false, sync);\r\n        checkResult(listFile, 3);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMultiDirTargetPresent()\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/multifile\", \"/tmp/singledir\");\r\n        createFiles(\"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        mkdirs(target.toString(), \"/tmp/singledir/dir1\");\r\n        runTest(listFile, target, true);\r\n        checkResult(listFile, 4);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateMultiDirTargetPresent",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateMultiDirTargetPresent()\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/Umultifile\", \"/tmp/Usingledir\");\r\n        createFiles(\"/tmp/Umultifile/Ufile3\", \"/tmp/Umultifile/Ufile4\", \"/tmp/Umultifile/Ufile5\");\r\n        mkdirs(target.toString(), \"/tmp/Usingledir/Udir1\");\r\n        runTest(listFile, target, true);\r\n        checkResult(listFile, 4);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMultiDirTargetMissing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultiDirTargetMissing()\n{\r\n    caseMultiDirTargetMissing(false);\r\n    caseMultiDirTargetMissing(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseMultiDirTargetMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void caseMultiDirTargetMissing(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/multifile\", \"/tmp/singledir\");\r\n        createFiles(\"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        mkdirs(\"/tmp/singledir/dir1\");\r\n        runTest(listFile, target, sync);\r\n        checkResult(listFile, 4);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobTargetMissingSingleLevel",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGlobTargetMissingSingleLevel()\n{\r\n    caseGlobTargetMissingSingleLevel(false);\r\n    caseGlobTargetMissingSingleLevel(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseGlobTargetMissingSingleLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void caseGlobTargetMissingSingleLevel(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp1/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/*\");\r\n        createFiles(\"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        createFiles(\"/tmp/singledir/dir2/file6\");\r\n        runTest(listFile, target, sync);\r\n        checkResult(listFile, 5);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobTargetMissingMultiLevel",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGlobTargetMissingMultiLevel()\n{\r\n    caseGlobTargetMissingMultiLevel(false);\r\n    caseGlobTargetMissingMultiLevel(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "caseGlobTargetMissingMultiLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void caseGlobTargetMissingMultiLevel(boolean sync)\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp1/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/*/*\");\r\n        createFiles(\"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        createFiles(\"/tmp/singledir1/dir3/file7\", \"/tmp/singledir1/dir3/file8\", \"/tmp/singledir1/dir3/file9\");\r\n        runTest(listFile, target, sync);\r\n        checkResult(listFile, 6);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobTargetDirMultiLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGlobTargetDirMultiLevel()\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp1/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/*/*\");\r\n        createFiles(\"/tmp/multifile/file3\", \"/tmp/multifile/file4\", \"/tmp/multifile/file5\");\r\n        createFiles(\"/tmp/singledir1/dir3/file7\", \"/tmp/singledir1/dir3/file8\", \"/tmp/singledir1/dir3/file9\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true);\r\n        checkResult(listFile, 6);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUpdateGlobTargetDirMultiLevel",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testUpdateGlobTargetDirMultiLevel()\n{\r\n    try {\r\n        Path listFile = new Path(\"/tmp1/listing\");\r\n        Path target = new Path(\"/tmp/target\");\r\n        addEntries(listFile, \"/tmp/*/*\");\r\n        createFiles(\"/tmp/Umultifile/Ufile3\", \"/tmp/Umultifile/Ufile4\", \"/tmp/Umultifile/Ufile5\");\r\n        createFiles(\"/tmp/Usingledir1/Udir3/Ufile7\", \"/tmp/Usingledir1/Udir3/Ufile8\", \"/tmp/Usingledir1/Udir3/Ufile9\");\r\n        mkdirs(target.toString());\r\n        runTest(listFile, target, true);\r\n        checkResult(listFile, 6);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing build listing\", e);\r\n        Assert.fail(\"build listing failure\");\r\n    } finally {\r\n        TestDistCpUtils.delete(fs, \"/tmp\");\r\n        TestDistCpUtils.delete(fs, \"/tmp1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "addEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addEntries(Path listFile, String... entries) throws IOException\n{\r\n    OutputStream out = fs.create(listFile);\r\n    try {\r\n        for (String entry : entries) {\r\n            out.write(entry.getBytes());\r\n            out.write(\"\\n\".getBytes());\r\n        }\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createFiles(String... entries) throws IOException\n{\r\n    for (String entry : entries) {\r\n        OutputStream out = fs.create(new Path(entry));\r\n        try {\r\n            out.write(entry.getBytes());\r\n            out.write(\"\\n\".getBytes());\r\n        } finally {\r\n            out.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mkdirs(String... entries) throws IOException\n{\r\n    for (String entry : entries) {\r\n        fs.mkdirs(new Path(entry));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "runTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runTest(Path listFile, Path target, boolean targetExists) throws IOException\n{\r\n    runTest(listFile, target, targetExists, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "runTest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runTest(Path listFile, Path target, boolean targetExists, boolean sync) throws IOException\n{\r\n    CopyListing listing = new FileBasedCopyListing(config, CREDENTIALS);\r\n    final DistCpOptions options = new DistCpOptions.Builder(listFile, target).withSyncFolder(sync).build();\r\n    final DistCpContext context = new DistCpContext(options);\r\n    context.setTargetPathExists(targetExists);\r\n    listing.buildListing(listFile, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkResult",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkResult(Path listFile, int count) throws IOException\n{\r\n    if (count == 0) {\r\n        return;\r\n    }\r\n    int recCount = 0;\r\n    SequenceFile.Reader reader = new SequenceFile.Reader(config, SequenceFile.Reader.file(listFile));\r\n    try {\r\n        Text relPath = new Text();\r\n        CopyListingFileStatus fileStatus = new CopyListingFileStatus();\r\n        while (reader.next(relPath, fileStatus)) {\r\n            if (fileStatus.isDirectory() && relPath.toString().equals(\"\")) {\r\n                continue;\r\n            }\r\n            Assert.assertEquals(fileStatus.getPath().toUri().getPath(), map.get(relPath.toString()));\r\n            recCount++;\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(reader);\r\n    }\r\n    Assert.assertEquals(recCount, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void init() throws Exception\n{\r\n    initCluster(true, true);\r\n    fs.mkdirs(subDir1);\r\n    fs.create(file1).close();\r\n    fs.mkdirs(dir2);\r\n    fs.create(file2).close();\r\n    fs.create(file3).close();\r\n    fs.create(file4).close();\r\n    fs.setXAttr(dir1, name1, value1);\r\n    fs.setXAttr(dir1, name2, value2);\r\n    fs.setXAttr(subDir1, name1, value1);\r\n    fs.setXAttr(subDir1, name3, value3);\r\n    fs.setXAttr(file1, name1, value1);\r\n    fs.setXAttr(file1, name2, value2);\r\n    fs.setXAttr(file1, name3, value3);\r\n    fs.setXAttr(dir2, name2, value2);\r\n    fs.setXAttr(file2, name1, value1);\r\n    fs.setXAttr(file2, name4, value4);\r\n    fs.setXAttr(file3, name3, value3);\r\n    fs.setXAttr(file3, name4, value4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdown()\n{\r\n    IOUtils.cleanupWithLogger(null, fs);\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveXAttrs",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testPreserveXAttrs() throws Exception\n{\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, rootedSrcName, \"/dstPreserveXAttrs\", \"-px\", conf);\r\n    Map<String, byte[]> xAttrs = Maps.newHashMap();\r\n    xAttrs.put(name1, value1);\r\n    xAttrs.put(name2, value2);\r\n    DistCpTestUtils.assertXAttrs(dstDir1, fs, xAttrs);\r\n    xAttrs.clear();\r\n    xAttrs.put(name1, value1);\r\n    xAttrs.put(name3, new byte[0]);\r\n    DistCpTestUtils.assertXAttrs(dstSubDir1, fs, xAttrs);\r\n    xAttrs.clear();\r\n    xAttrs.put(name1, value1);\r\n    xAttrs.put(name2, value2);\r\n    xAttrs.put(name3, new byte[0]);\r\n    DistCpTestUtils.assertXAttrs(dstFile1, fs, xAttrs);\r\n    xAttrs.clear();\r\n    xAttrs.put(name2, value2);\r\n    DistCpTestUtils.assertXAttrs(dstDir2, fs, xAttrs);\r\n    xAttrs.clear();\r\n    xAttrs.put(name1, value1);\r\n    xAttrs.put(name4, new byte[0]);\r\n    DistCpTestUtils.assertXAttrs(dstFile2, fs, xAttrs);\r\n    xAttrs.clear();\r\n    xAttrs.put(name3, new byte[0]);\r\n    xAttrs.put(name4, new byte[0]);\r\n    DistCpTestUtils.assertXAttrs(dstFile3, fs, xAttrs);\r\n    xAttrs.clear();\r\n    DistCpTestUtils.assertXAttrs(dstFile4, fs, xAttrs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testXAttrsNotEnabled",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testXAttrsNotEnabled() throws Exception\n{\r\n    try {\r\n        restart(false);\r\n        DistCpTestUtils.assertRunDistCp(DistCpConstants.XATTRS_NOT_SUPPORTED, rootedSrcName, \"/dstXAttrsNotEnabled\", \"-px\", conf);\r\n    } finally {\r\n        restart(true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testXAttrsNotImplemented",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testXAttrsNotImplemented() throws Exception\n{\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.XATTRS_NOT_SUPPORTED, rootedSrcName, \"stubfs://dstXAttrsNotImplemented\", \"-px\", conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initCluster",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initCluster(boolean format, boolean xAttrsEnabled) throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, xAttrsEnabled);\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, \"stubfs:///\");\r\n    conf.setClass(\"fs.stubfs.impl\", StubFileSystem.class, FileSystem.class);\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(format).build();\r\n    cluster.waitActive();\r\n    fs = cluster.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "restart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void restart(boolean xAttrsEnabled) throws Exception\n{\r\n    shutdown();\r\n    initCluster(false, xAttrsEnabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\contract",
  "methodName" : "createContract",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractFSContract createContract(Configuration conf)\n{\r\n    return new LocalFSContract(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initSourcePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initSourcePath()\n{\r\n    setSource(new Path(\"/source\"));\r\n    setSrcNotSameAsTgt(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    Configuration configuration = TestCopyMapper.getConfigurationForCluster();\r\n    configuration.set(HdfsClientConfigKeys.DFS_CHECKSUM_COMBINE_MODE_KEY, \"COMPOSITE_CRC\");\r\n    TestCopyMapper.setCluster(new MiniDFSCluster.Builder(configuration).numDataNodes(1).format(true).build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectDifferentBlockSizesMultipleBlocksToSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean expectDifferentBlockSizesMultipleBlocksToSucceed()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectDifferentBytesPerCrcToSucceed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean expectDifferentBytesPerCrcToSucceed()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    config.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);\r\n    cluster = new MiniDFSCluster.Builder(config).numDataNodes(2).format(true).build();\r\n    cluster.getFileSystem().enableErasureCodingPolicy(\"XOR-2-1-1024k\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroy()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testGetRelativePathRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetRelativePathRoot()\n{\r\n    Path root = new Path(\"/\");\r\n    Path child = new Path(\"/a\");\r\n    assertThat(DistCpUtils.getRelativePath(root, child)).isEqualTo(\"/a\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testGetRelativePath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetRelativePath()\n{\r\n    Path root = new Path(\"/tmp/abc\");\r\n    Path child = new Path(\"/tmp/abc/xyz/file\");\r\n    assertThat(DistCpUtils.getRelativePath(root, child)).isEqualTo(\"/xyz/file\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPackAttributes",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPackAttributes()\n{\r\n    EnumSet<FileAttribute> attributes = EnumSet.noneOf(FileAttribute.class);\r\n    assertThat(DistCpUtils.packAttributes(attributes)).isEqualTo(\"\");\r\n    attributes.add(FileAttribute.REPLICATION);\r\n    assertThat(DistCpUtils.packAttributes(attributes)).isEqualTo(\"R\");\r\n    attributes.add(FileAttribute.BLOCKSIZE);\r\n    assertThat(DistCpUtils.packAttributes(attributes)).isEqualTo(\"RB\");\r\n    attributes.add(FileAttribute.USER);\r\n    attributes.add(FileAttribute.CHECKSUMTYPE);\r\n    assertThat(DistCpUtils.packAttributes(attributes)).isEqualTo(\"RBUC\");\r\n    attributes.add(FileAttribute.GROUP);\r\n    assertThat(DistCpUtils.packAttributes(attributes)).isEqualTo(\"RBUGC\");\r\n    attributes.add(FileAttribute.PERMISSION);\r\n    assertThat(DistCpUtils.packAttributes(attributes)).isEqualTo(\"RBUGPC\");\r\n    attributes.add(FileAttribute.TIMES);\r\n    assertThat(DistCpUtils.packAttributes(attributes)).isEqualTo(\"RBUGPCT\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testUnpackAttributes",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testUnpackAttributes()\n{\r\n    EnumSet<FileAttribute> attributes = EnumSet.allOf(FileAttribute.class);\r\n    Assert.assertEquals(attributes, DistCpUtils.unpackAttributes(\"RCBUGPAXTE\"));\r\n    attributes.remove(FileAttribute.REPLICATION);\r\n    attributes.remove(FileAttribute.CHECKSUMTYPE);\r\n    attributes.remove(FileAttribute.ACL);\r\n    attributes.remove(FileAttribute.XATTR);\r\n    attributes.remove(FileAttribute.ERASURECODINGPOLICY);\r\n    Assert.assertEquals(attributes, DistCpUtils.unpackAttributes(\"BUGPT\"));\r\n    attributes.remove(FileAttribute.TIMES);\r\n    Assert.assertEquals(attributes, DistCpUtils.unpackAttributes(\"BUGP\"));\r\n    attributes.remove(FileAttribute.BLOCKSIZE);\r\n    Assert.assertEquals(attributes, DistCpUtils.unpackAttributes(\"UGP\"));\r\n    attributes.remove(FileAttribute.GROUP);\r\n    Assert.assertEquals(attributes, DistCpUtils.unpackAttributes(\"UP\"));\r\n    attributes.remove(FileAttribute.USER);\r\n    Assert.assertEquals(attributes, DistCpUtils.unpackAttributes(\"P\"));\r\n    attributes.remove(FileAttribute.PERMISSION);\r\n    Assert.assertEquals(attributes, DistCpUtils.unpackAttributes(\"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveDefaults",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPreserveDefaults() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = DistCpUtils.unpackAttributes(DistCpOptionSwitch.PRESERVE_STATUS_DEFAULT.substring(1));\r\n    Path dst = new Path(\"/tmp/dest2\");\r\n    Path src = new Path(\"/tmp/src2\");\r\n    createFile(fs, src);\r\n    createFile(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    assertStatusEqual(fs, dst, srcStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "assertStatusEqual",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void assertStatusEqual(final FileSystem fs, final Path dst, final CopyListingFileStatus srcStatus) throws IOException\n{\r\n    FileStatus destStatus = fs.getFileStatus(dst);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(destStatus);\r\n    String text = String.format(\"Source %s; dest %s: wrong \", srcStatus, destStatus);\r\n    assertEquals(text + \"permission\", srcStatus.getPermission(), dstStatus.getPermission());\r\n    assertEquals(text + \"owner\", srcStatus.getOwner(), dstStatus.getOwner());\r\n    assertEquals(text + \"group\", srcStatus.getGroup(), dstStatus.getGroup());\r\n    assertEquals(text + \"accessTime\", srcStatus.getAccessTime(), dstStatus.getAccessTime());\r\n    assertEquals(text + \"modificationTime\", srcStatus.getModificationTime(), dstStatus.getModificationTime());\r\n    assertEquals(text + \"replication\", srcStatus.getReplication(), dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "assertStatusNotEqual",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void assertStatusNotEqual(final FileSystem fs, final Path dst, final CopyListingFileStatus srcStatus) throws IOException\n{\r\n    FileStatus destStatus = fs.getFileStatus(dst);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(destStatus);\r\n    String text = String.format(\"Source %s; dest %s: wrong \", srcStatus, destStatus);\r\n    assertNotEquals(text + \"permission\", srcStatus.getPermission(), dstStatus.getPermission());\r\n    assertNotEquals(text + \"owner\", srcStatus.getOwner(), dstStatus.getOwner());\r\n    assertNotEquals(text + \"group\", srcStatus.getGroup(), dstStatus.getGroup());\r\n    assertNotEquals(text + \"accessTime\", srcStatus.getAccessTime(), dstStatus.getAccessTime());\r\n    assertNotEquals(text + \"modificationTime\", srcStatus.getModificationTime(), dstStatus.getModificationTime());\r\n    assertNotEquals(text + \"replication\", srcStatus.getReplication(), dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testSkipsNeedlessAttributes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSkipsNeedlessAttributes() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    Path src = new Path(\"/tmp/testSkipsNeedlessAttributes/source\");\r\n    Path dst = new Path(\"/tmp/testSkipsNeedlessAttributes/dest\");\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(new FileStatus(0, false, 1, 32, 0, src));\r\n    EnumSet<FileAttribute> attrs = EnumSet.of(FileAttribute.ACL, FileAttribute.GROUP, FileAttribute.PERMISSION, FileAttribute.TIMES, FileAttribute.XATTR);\r\n    for (FileAttribute attr : attrs) {\r\n        intercept(FileNotFoundException.class, () -> DistCpUtils.preserve(fs, dst, srcStatus, EnumSet.of(attr), false));\r\n    }\r\n    DistCpUtils.preserve(fs, dst, srcStatus, EnumSet.of(FileAttribute.BLOCKSIZE, FileAttribute.CHECKSUMTYPE), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveAclsforDefaultACL",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testPreserveAclsforDefaultACL() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.ACL, FileAttribute.PERMISSION, FileAttribute.XATTR, FileAttribute.GROUP, FileAttribute.USER, FileAttribute.REPLICATION, FileAttribute.XATTR, FileAttribute.TIMES);\r\n    Path dest = new Path(\"/tmpdest\");\r\n    Path src = new Path(\"/testsrc\");\r\n    fs.mkdirs(src);\r\n    fs.mkdirs(dest);\r\n    List<AclEntry> acls = Lists.newArrayList(aclEntry(DEFAULT, USER, \"foo\", READ_EXECUTE), aclEntry(ACCESS, USER, READ_WRITE), aclEntry(ACCESS, GROUP, READ), aclEntry(ACCESS, OTHER, READ), aclEntry(ACCESS, USER, \"bar\", ALL));\r\n    final List<AclEntry> acls1 = Lists.newArrayList(aclEntry(ACCESS, USER, ALL), aclEntry(ACCESS, USER, \"user1\", ALL), aclEntry(ACCESS, GROUP, READ_EXECUTE), aclEntry(ACCESS, OTHER, EXECUTE));\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setAcl(src, acls);\r\n    fs.setPermission(dest, noPerm);\r\n    fs.setOwner(dest, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dest, 100, 100);\r\n    fs.setReplication(dest, (short) 2);\r\n    fs.setAcl(dest, acls1);\r\n    List<AclEntry> en1 = fs.getAclStatus(src).getEntries();\r\n    List<AclEntry> dd2 = fs.getAclStatus(dest).getEntries();\r\n    Assert.assertNotEquals(en1, dd2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    en1 = srcStatus.getAclEntries();\r\n    DistCpUtils.preserve(fs, dest, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dest));\r\n    dd2 = dstStatus.getAclEntries();\r\n    en1 = srcStatus.getAclEntries();\r\n    assertStatusEqual(fs, dest, srcStatus);\r\n    Assert.assertArrayEquals(en1.toArray(), dd2.toArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveNothingOnDirectory",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testPreserveNothingOnDirectory() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.noneOf(FileAttribute.class);\r\n    Path dst = new Path(\"/tmp/abc\");\r\n    Path src = new Path(\"/tmp/src\");\r\n    createDirectory(fs, src);\r\n    createDirectory(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertTrue(dstStatus.getAccessTime() == 100);\r\n    Assert.assertTrue(dstStatus.getModificationTime() == 100);\r\n    Assert.assertTrue(dstStatus.getReplication() == 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreservePermissionOnDirectory",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testPreservePermissionOnDirectory() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.PERMISSION);\r\n    Path dst = new Path(\"/tmp/abc\");\r\n    Path src = new Path(\"/tmp/src\");\r\n    createDirectory(fs, src);\r\n    createDirectory(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertTrue(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveGroupOnDirectory",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testPreserveGroupOnDirectory() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.GROUP);\r\n    Path dst = new Path(\"/tmp/abc\");\r\n    Path src = new Path(\"/tmp/src\");\r\n    createDirectory(fs, src);\r\n    createDirectory(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertTrue(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveUserOnDirectory",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testPreserveUserOnDirectory() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.USER);\r\n    Path dst = new Path(\"/tmp/abc\");\r\n    Path src = new Path(\"/tmp/src\");\r\n    createDirectory(fs, src);\r\n    createDirectory(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertTrue(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveReplicationOnDirectory",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testPreserveReplicationOnDirectory() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.REPLICATION);\r\n    Path dst = new Path(\"/tmp/abc\");\r\n    Path src = new Path(\"/tmp/src\");\r\n    createDirectory(fs, src);\r\n    createDirectory(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertTrue(srcStatus.getReplication() == dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveTimestampOnDirectory",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testPreserveTimestampOnDirectory() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.TIMES);\r\n    Path dst = new Path(\"/tmp/abc\");\r\n    Path src = new Path(\"/tmp/src\");\r\n    createDirectory(fs, src);\r\n    createDirectory(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertTrue(srcStatus.getAccessTime() == dstStatus.getAccessTime());\r\n    Assert.assertTrue(srcStatus.getModificationTime() == dstStatus.getModificationTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveNothingOnFile",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testPreserveNothingOnFile() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.noneOf(FileAttribute.class);\r\n    Path dst = new Path(\"/tmp/dest2\");\r\n    Path src = new Path(\"/tmp/src2\");\r\n    createFile(fs, src);\r\n    createFile(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    assertStatusNotEqual(fs, dst, srcStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreservePermissionOnFile",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testPreservePermissionOnFile() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.PERMISSION);\r\n    Path dst = new Path(\"/tmp/dest2\");\r\n    Path src = new Path(\"/tmp/src2\");\r\n    createFile(fs, src);\r\n    createFile(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertTrue(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == dstStatus.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == dstStatus.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveGroupOnFile",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testPreserveGroupOnFile() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.GROUP);\r\n    Path dst = new Path(\"/tmp/dest2\");\r\n    Path src = new Path(\"/tmp/src2\");\r\n    createFile(fs, src);\r\n    createFile(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertTrue(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == dstStatus.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == dstStatus.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveUserOnFile",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testPreserveUserOnFile() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.USER);\r\n    Path dst = new Path(\"/tmp/dest2\");\r\n    Path src = new Path(\"/tmp/src2\");\r\n    createFile(fs, src);\r\n    createFile(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertTrue(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == dstStatus.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == dstStatus.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveReplicationOnFile",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testPreserveReplicationOnFile() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.REPLICATION);\r\n    Path dst = new Path(\"/tmp/dest2\");\r\n    Path src = new Path(\"/tmp/src2\");\r\n    createFile(fs, src);\r\n    createFile(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == dstStatus.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == dstStatus.getModificationTime());\r\n    Assert.assertTrue(srcStatus.getReplication() == dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testReplFactorNotPreservedOnErasureCodedFile",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testReplFactorNotPreservedOnErasureCodedFile() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    Path srcECDir = new Path(\"/tmp/srcECDir\");\r\n    Path srcECFile = new Path(srcECDir, \"srcECFile\");\r\n    Path dstReplDir = new Path(\"/tmp/dstReplDir\");\r\n    Path dstReplFile = new Path(dstReplDir, \"destReplFile\");\r\n    fs.mkdirs(srcECDir);\r\n    fs.mkdirs(dstReplDir);\r\n    String[] args = { \"-setPolicy\", \"-path\", \"/tmp/srcECDir\", \"-policy\", \"XOR-2-1-1024k\" };\r\n    int res = ToolRunner.run(config, new ECAdmin(config), args);\r\n    assertEquals(\"Setting EC policy should succeed!\", 0, res);\r\n    verifyReplFactorNotPreservedOnErasureCodedFile(srcECFile, true, dstReplFile, false);\r\n    Path srcReplDir = new Path(\"/tmp/srcReplDir\");\r\n    Path srcReplFile = new Path(srcReplDir, \"srcReplFile\");\r\n    Path dstECDir = new Path(\"/tmp/dstECDir\");\r\n    Path dstECFile = new Path(dstECDir, \"destECFile\");\r\n    fs.mkdirs(srcReplDir);\r\n    fs.mkdirs(dstECDir);\r\n    args = new String[] { \"-setPolicy\", \"-path\", \"/tmp/dstECDir\", \"-policy\", \"XOR-2-1-1024k\" };\r\n    res = ToolRunner.run(config, new ECAdmin(config), args);\r\n    assertEquals(\"Setting EC policy should succeed!\", 0, res);\r\n    verifyReplFactorNotPreservedOnErasureCodedFile(srcReplFile, false, dstECFile, true);\r\n    verifyReplFactorNotPreservedOnErasureCodedFile(srcECFile, true, dstECFile, true);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "verifyReplFactorNotPreservedOnErasureCodedFile",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void verifyReplFactorNotPreservedOnErasureCodedFile(Path srcFile, boolean isSrcEC, Path dstFile, boolean isDstEC) throws Exception\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    createFile(fs, srcFile);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(srcFile));\r\n    if (isSrcEC) {\r\n        assertTrue(srcFile + \"should be erasure coded!\", srcStatus.isErasureCoded());\r\n        assertEquals(INodeFile.DEFAULT_REPL_FOR_STRIPED_BLOCKS, srcStatus.getReplication());\r\n    } else {\r\n        assertEquals(\"Unexpected replication factor for \" + srcFile, fs.getDefaultReplication(srcFile), srcStatus.getReplication());\r\n    }\r\n    createFile(fs, dstFile);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dstFile));\r\n    if (isDstEC) {\r\n        assertTrue(dstFile + \"should be erasure coded!\", dstStatus.isErasureCoded());\r\n        assertEquals(\"Unexpected replication factor for erasure coded file!\", INodeFile.DEFAULT_REPL_FOR_STRIPED_BLOCKS, dstStatus.getReplication());\r\n    } else {\r\n        assertEquals(\"Unexpected replication factor for \" + dstFile, fs.getDefaultReplication(dstFile), dstStatus.getReplication());\r\n    }\r\n    fs.setPermission(srcFile, fullPerm);\r\n    fs.setOwner(srcFile, \"ec\", \"ec-group\");\r\n    fs.setTimes(srcFile, 0, 0);\r\n    fs.setPermission(dstFile, noPerm);\r\n    fs.setOwner(dstFile, \"normal\", \"normal-group\");\r\n    fs.setTimes(dstFile, 100, 100);\r\n    srcStatus = new CopyListingFileStatus(fs.getFileStatus(srcFile));\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.REPLICATION);\r\n    DistCpUtils.preserve(fs, dstFile, srcStatus, attributes, false);\r\n    dstStatus = new CopyListingFileStatus(fs.getFileStatus(dstFile));\r\n    assertFalse(\"Permission for \" + srcFile + \" and \" + dstFile + \" should not be same after preserve only for replication attr!\", srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    assertFalse(\"File ownership should not match!\", srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    assertFalse(srcStatus.getAccessTime() == dstStatus.getAccessTime());\r\n    assertFalse(srcStatus.getModificationTime() == dstStatus.getModificationTime());\r\n    if (isDstEC) {\r\n        assertEquals(\"Unexpected replication factor for erasure coded file!\", INodeFile.DEFAULT_REPL_FOR_STRIPED_BLOCKS, dstStatus.getReplication());\r\n    } else {\r\n        assertEquals(dstFile + \" replication factor should be same as dst \" + \"filesystem!\", fs.getDefaultReplication(dstFile), dstStatus.getReplication());\r\n    }\r\n    if (!isSrcEC || !isDstEC) {\r\n        assertFalse(dstFile + \" replication factor should not be \" + \"same as \" + srcFile, srcStatus.getReplication() == dstStatus.getReplication());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveTimestampOnFile",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testPreserveTimestampOnFile() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.TIMES);\r\n    Path dst = new Path(\"/tmp/dest2\");\r\n    Path src = new Path(\"/tmp/src2\");\r\n    createFile(fs, src);\r\n    createFile(fs, dst);\r\n    fs.setPermission(src, fullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(dst, noPerm);\r\n    fs.setOwner(dst, \"nobody\", \"nobody-group\");\r\n    fs.setTimes(dst, 100, 100);\r\n    fs.setReplication(dst, (short) 2);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);\r\n    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(dstStatus.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));\r\n    Assert.assertTrue(srcStatus.getAccessTime() == dstStatus.getAccessTime());\r\n    Assert.assertTrue(srcStatus.getModificationTime() == dstStatus.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == dstStatus.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveOnFileUpwardRecursion",
  "errType" : null,
  "containingMethodsNum" : 63,
  "sourceCodeText" : "void testPreserveOnFileUpwardRecursion() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.allOf(FileAttribute.class);\r\n    attributes.remove(FileAttribute.ACL);\r\n    Path src = new Path(\"/tmp/src2\");\r\n    Path f0 = new Path(\"/f0\");\r\n    Path f1 = new Path(\"/d1/f1\");\r\n    Path f2 = new Path(\"/d1/d2/f2\");\r\n    Path d1 = new Path(\"/d1/\");\r\n    Path d2 = new Path(\"/d1/d2/\");\r\n    createFile(fs, src);\r\n    createFile(fs, f0);\r\n    createFile(fs, f1);\r\n    createFile(fs, f2);\r\n    fs.setPermission(src, almostFullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(d1, fullPerm);\r\n    fs.setOwner(d1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d1, 400, 400);\r\n    fs.setReplication(d1, (short) 3);\r\n    fs.setPermission(d2, fullPerm);\r\n    fs.setOwner(d2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d2, 300, 300);\r\n    fs.setReplication(d2, (short) 3);\r\n    fs.setPermission(f0, fullPerm);\r\n    fs.setOwner(f0, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f0, 200, 200);\r\n    fs.setReplication(f0, (short) 3);\r\n    fs.setPermission(f1, fullPerm);\r\n    fs.setOwner(f1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f1, 200, 200);\r\n    fs.setReplication(f1, (short) 3);\r\n    fs.setPermission(f2, fullPerm);\r\n    fs.setOwner(f2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f2, 200, 200);\r\n    fs.setReplication(f2, (short) 3);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, f2, srcStatus, attributes, false);\r\n    cluster.triggerHeartbeats();\r\n    assertStatusEqual(fs, f2, srcStatus);\r\n    CopyListingFileStatus f1Status = new CopyListingFileStatus(fs.getFileStatus(f1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f1Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f1Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f1Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f1Status.getReplication());\r\n    CopyListingFileStatus f0Status = new CopyListingFileStatus(fs.getFileStatus(f0));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f0Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f0Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f0Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f0Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f0Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f0Status.getReplication());\r\n    CopyListingFileStatus d2Status = new CopyListingFileStatus(fs.getFileStatus(d2));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(d2Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(d2Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(d2Status.getGroup()));\r\n    Assert.assertTrue(d2Status.getAccessTime() == 300);\r\n    Assert.assertTrue(d2Status.getModificationTime() == 300);\r\n    Assert.assertFalse(srcStatus.getReplication() == d2Status.getReplication());\r\n    CopyListingFileStatus d1Status = new CopyListingFileStatus(fs.getFileStatus(d1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(d1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(d1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(d1Status.getGroup()));\r\n    Assert.assertTrue(d1Status.getAccessTime() == 400);\r\n    Assert.assertTrue(d1Status.getModificationTime() == 400);\r\n    Assert.assertFalse(srcStatus.getReplication() == d1Status.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveOnDirectoryUpwardRecursion",
  "errType" : null,
  "containingMethodsNum" : 69,
  "sourceCodeText" : "void testPreserveOnDirectoryUpwardRecursion() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.allOf(FileAttribute.class);\r\n    attributes.remove(FileAttribute.ACL);\r\n    Path src = new Path(\"/tmp/src2\");\r\n    Path f0 = new Path(\"/f0\");\r\n    Path f1 = new Path(\"/d1/f1\");\r\n    Path f2 = new Path(\"/d1/d2/f2\");\r\n    Path d1 = new Path(\"/d1/\");\r\n    Path d2 = new Path(\"/d1/d2/\");\r\n    createFile(fs, src);\r\n    createFile(fs, f0);\r\n    createFile(fs, f1);\r\n    createFile(fs, f2);\r\n    fs.setPermission(src, almostFullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(d1, fullPerm);\r\n    fs.setOwner(d1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d1, 400, 400);\r\n    fs.setReplication(d1, (short) 3);\r\n    fs.setPermission(d2, fullPerm);\r\n    fs.setOwner(d2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d2, 300, 300);\r\n    fs.setReplication(d2, (short) 3);\r\n    fs.setPermission(f0, fullPerm);\r\n    fs.setOwner(f0, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f0, 200, 200);\r\n    fs.setReplication(f0, (short) 3);\r\n    fs.setPermission(f1, fullPerm);\r\n    fs.setOwner(f1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f1, 200, 200);\r\n    fs.setReplication(f1, (short) 3);\r\n    fs.setPermission(f2, fullPerm);\r\n    fs.setOwner(f2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f2, 200, 200);\r\n    fs.setReplication(f2, (short) 3);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, d2, srcStatus, attributes, false);\r\n    cluster.triggerHeartbeats();\r\n    CopyListingFileStatus d2Status = new CopyListingFileStatus(fs.getFileStatus(d2));\r\n    Assert.assertTrue(srcStatus.getPermission().equals(d2Status.getPermission()));\r\n    Assert.assertTrue(srcStatus.getOwner().equals(d2Status.getOwner()));\r\n    Assert.assertTrue(srcStatus.getGroup().equals(d2Status.getGroup()));\r\n    Assert.assertTrue(srcStatus.getAccessTime() == d2Status.getAccessTime());\r\n    Assert.assertTrue(srcStatus.getModificationTime() == d2Status.getModificationTime());\r\n    Assert.assertTrue(srcStatus.getReplication() != d2Status.getReplication());\r\n    CopyListingFileStatus d1Status = new CopyListingFileStatus(fs.getFileStatus(d1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(d1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(d1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(d1Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == d1Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == d1Status.getModificationTime());\r\n    Assert.assertTrue(srcStatus.getReplication() != d1Status.getReplication());\r\n    CopyListingFileStatus f2Status = new CopyListingFileStatus(fs.getFileStatus(f2));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f2Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f2Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f2Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f2Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f2Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f2Status.getReplication());\r\n    CopyListingFileStatus f1Status = new CopyListingFileStatus(fs.getFileStatus(f1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f1Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f1Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f1Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f1Status.getReplication());\r\n    CopyListingFileStatus f0Status = new CopyListingFileStatus(fs.getFileStatus(f0));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f0Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f0Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f0Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f0Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f0Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f0Status.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveOnFileDownwardRecursion",
  "errType" : null,
  "containingMethodsNum" : 63,
  "sourceCodeText" : "void testPreserveOnFileDownwardRecursion() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.allOf(FileAttribute.class);\r\n    attributes.remove(FileAttribute.ACL);\r\n    Path src = new Path(\"/tmp/src2\");\r\n    Path f0 = new Path(\"/f0\");\r\n    Path f1 = new Path(\"/d1/f1\");\r\n    Path f2 = new Path(\"/d1/d2/f2\");\r\n    Path d1 = new Path(\"/d1/\");\r\n    Path d2 = new Path(\"/d1/d2/\");\r\n    createFile(fs, src);\r\n    createFile(fs, f0);\r\n    createFile(fs, f1);\r\n    createFile(fs, f2);\r\n    fs.setPermission(src, almostFullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(d1, fullPerm);\r\n    fs.setOwner(d1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d1, 400, 400);\r\n    fs.setReplication(d1, (short) 3);\r\n    fs.setPermission(d2, fullPerm);\r\n    fs.setOwner(d2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d2, 300, 300);\r\n    fs.setReplication(d2, (short) 3);\r\n    fs.setPermission(f0, fullPerm);\r\n    fs.setOwner(f0, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f0, 200, 200);\r\n    fs.setReplication(f0, (short) 3);\r\n    fs.setPermission(f1, fullPerm);\r\n    fs.setOwner(f1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f1, 200, 200);\r\n    fs.setReplication(f1, (short) 3);\r\n    fs.setPermission(f2, fullPerm);\r\n    fs.setOwner(f2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f2, 200, 200);\r\n    fs.setReplication(f2, (short) 3);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, f0, srcStatus, attributes, false);\r\n    cluster.triggerHeartbeats();\r\n    assertStatusEqual(fs, f0, srcStatus);\r\n    CopyListingFileStatus f1Status = new CopyListingFileStatus(fs.getFileStatus(f1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f1Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f1Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f1Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f1Status.getReplication());\r\n    CopyListingFileStatus f2Status = new CopyListingFileStatus(fs.getFileStatus(f2));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f2Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f2Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f2Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f2Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f2Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f2Status.getReplication());\r\n    CopyListingFileStatus d1Status = new CopyListingFileStatus(fs.getFileStatus(d1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(d1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(d1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(d1Status.getGroup()));\r\n    Assert.assertTrue(d1Status.getAccessTime() == 400);\r\n    Assert.assertTrue(d1Status.getModificationTime() == 400);\r\n    Assert.assertFalse(srcStatus.getReplication() == d1Status.getReplication());\r\n    CopyListingFileStatus d2Status = new CopyListingFileStatus(fs.getFileStatus(d2));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(d2Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(d2Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(d2Status.getGroup()));\r\n    Assert.assertTrue(d2Status.getAccessTime() == 300);\r\n    Assert.assertTrue(d2Status.getModificationTime() == 300);\r\n    Assert.assertFalse(srcStatus.getReplication() == d2Status.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testPreserveOnDirectoryDownwardRecursion",
  "errType" : null,
  "containingMethodsNum" : 80,
  "sourceCodeText" : "void testPreserveOnDirectoryDownwardRecursion() throws IOException\n{\r\n    FileSystem fs = FileSystem.get(config);\r\n    EnumSet<FileAttribute> attributes = EnumSet.allOf(FileAttribute.class);\r\n    attributes.remove(FileAttribute.ACL);\r\n    Path src = new Path(\"/tmp/src2\");\r\n    Path f0 = new Path(\"/f0\");\r\n    Path f1 = new Path(\"/d1/f1\");\r\n    Path f2 = new Path(\"/d1/d2/f2\");\r\n    Path d1 = new Path(\"/d1/\");\r\n    Path d2 = new Path(\"/d1/d2/\");\r\n    Path root = new Path(\"/\");\r\n    createFile(fs, src);\r\n    createFile(fs, f0);\r\n    createFile(fs, f1);\r\n    createFile(fs, f2);\r\n    fs.setPermission(src, almostFullPerm);\r\n    fs.setOwner(src, \"somebody\", \"somebody-group\");\r\n    fs.setTimes(src, 0, 0);\r\n    fs.setReplication(src, (short) 1);\r\n    fs.setPermission(root, fullPerm);\r\n    fs.setOwner(root, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(root, 400, 400);\r\n    fs.setReplication(root, (short) 3);\r\n    fs.setPermission(d1, fullPerm);\r\n    fs.setOwner(d1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d1, 400, 400);\r\n    fs.setReplication(d1, (short) 3);\r\n    fs.setPermission(d2, fullPerm);\r\n    fs.setOwner(d2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(d2, 300, 300);\r\n    fs.setReplication(d2, (short) 3);\r\n    fs.setPermission(f0, fullPerm);\r\n    fs.setOwner(f0, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f0, 200, 200);\r\n    fs.setReplication(f0, (short) 3);\r\n    fs.setPermission(f1, fullPerm);\r\n    fs.setOwner(f1, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f1, 200, 200);\r\n    fs.setReplication(f1, (short) 3);\r\n    fs.setPermission(f2, fullPerm);\r\n    fs.setOwner(f2, \"anybody\", \"anybody-group\");\r\n    fs.setTimes(f2, 200, 200);\r\n    fs.setReplication(f2, (short) 3);\r\n    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));\r\n    DistCpUtils.preserve(fs, root, srcStatus, attributes, false);\r\n    cluster.triggerHeartbeats();\r\n    CopyListingFileStatus rootStatus = new CopyListingFileStatus(fs.getFileStatus(root));\r\n    Assert.assertTrue(srcStatus.getPermission().equals(rootStatus.getPermission()));\r\n    Assert.assertTrue(srcStatus.getOwner().equals(rootStatus.getOwner()));\r\n    Assert.assertTrue(srcStatus.getGroup().equals(rootStatus.getGroup()));\r\n    Assert.assertTrue(srcStatus.getAccessTime() == rootStatus.getAccessTime());\r\n    Assert.assertTrue(srcStatus.getModificationTime() == rootStatus.getModificationTime());\r\n    Assert.assertTrue(srcStatus.getReplication() != rootStatus.getReplication());\r\n    CopyListingFileStatus d1Status = new CopyListingFileStatus(fs.getFileStatus(d1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(d1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(d1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(d1Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == d1Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == d1Status.getModificationTime());\r\n    Assert.assertTrue(srcStatus.getReplication() != d1Status.getReplication());\r\n    CopyListingFileStatus d2Status = new CopyListingFileStatus(fs.getFileStatus(d2));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(d2Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(d2Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(d2Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == d2Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == d2Status.getModificationTime());\r\n    Assert.assertTrue(srcStatus.getReplication() != d2Status.getReplication());\r\n    CopyListingFileStatus f0Status = new CopyListingFileStatus(fs.getFileStatus(f0));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f0Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f0Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f0Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f0Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f0Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f0Status.getReplication());\r\n    CopyListingFileStatus f1Status = new CopyListingFileStatus(fs.getFileStatus(f1));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f1Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f1Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f1Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f1Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f1Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f1Status.getReplication());\r\n    CopyListingFileStatus f2Status = new CopyListingFileStatus(fs.getFileStatus(f2));\r\n    Assert.assertFalse(srcStatus.getPermission().equals(f2Status.getPermission()));\r\n    Assert.assertFalse(srcStatus.getOwner().equals(f2Status.getOwner()));\r\n    Assert.assertFalse(srcStatus.getGroup().equals(f2Status.getGroup()));\r\n    Assert.assertFalse(srcStatus.getAccessTime() == f2Status.getAccessTime());\r\n    Assert.assertFalse(srcStatus.getModificationTime() == f2Status.getModificationTime());\r\n    Assert.assertFalse(srcStatus.getReplication() == f2Status.getReplication());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testCompareFileLengthsAndChecksums",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCompareFileLengthsAndChecksums() throws Throwable\n{\r\n    String base = \"/tmp/verify-checksum/\";\r\n    long srcSeed = System.currentTimeMillis();\r\n    long dstSeed = srcSeed + rand.nextLong();\r\n    short replFactor = 2;\r\n    FileSystem fs = FileSystem.get(config);\r\n    Path basePath = new Path(base);\r\n    fs.mkdirs(basePath);\r\n    Path srcWithLen0 = new Path(base + \"srcLen0\");\r\n    Path dstWithLen0 = new Path(base + \"dstLen0\");\r\n    fs.create(srcWithLen0).close();\r\n    fs.create(dstWithLen0).close();\r\n    DistCpUtils.compareFileLengthsAndChecksums(0, fs, srcWithLen0, null, fs, dstWithLen0, false, 0);\r\n    Path srcWithLen1 = new Path(base + \"srcLen1\");\r\n    Path dstWithLen2 = new Path(base + \"dstLen2\");\r\n    DFSTestUtil.createFile(fs, srcWithLen1, 1, replFactor, srcSeed);\r\n    DFSTestUtil.createFile(fs, dstWithLen2, 2, replFactor, srcSeed);\r\n    intercept(IOException.class, DistCpConstants.LENGTH_MISMATCH_ERROR_MSG, () -> DistCpUtils.compareFileLengthsAndChecksums(1, fs, srcWithLen1, null, fs, dstWithLen2, false, 2));\r\n    Path srcWithChecksum1 = new Path(base + \"srcChecksum1\");\r\n    Path dstWithChecksum1 = new Path(base + \"dstChecksum1\");\r\n    DFSTestUtil.createFile(fs, srcWithChecksum1, 1024, replFactor, srcSeed);\r\n    DFSTestUtil.createFile(fs, dstWithChecksum1, 1024, replFactor, srcSeed);\r\n    DistCpUtils.compareFileLengthsAndChecksums(1024, fs, srcWithChecksum1, null, fs, dstWithChecksum1, false, 1024);\r\n    DistCpUtils.compareFileLengthsAndChecksums(1024, fs, srcWithChecksum1, fs.getFileChecksum(srcWithChecksum1), fs, dstWithChecksum1, false, 1024);\r\n    Path dstWithChecksum2 = new Path(base + \"dstChecksum2\");\r\n    DFSTestUtil.createFile(fs, dstWithChecksum2, 1024, replFactor, dstSeed);\r\n    intercept(IOException.class, DistCpConstants.CHECKSUM_MISMATCH_ERROR_MSG, () -> DistCpUtils.compareFileLengthsAndChecksums(1024, fs, srcWithChecksum1, null, fs, dstWithChecksum2, false, 1024));\r\n    DistCpUtils.compareFileLengthsAndChecksums(1024, fs, srcWithChecksum1, null, fs, dstWithChecksum2, true, 1024);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createTestSetup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createTestSetup(FileSystem fs) throws IOException\n{\r\n    return createTestSetup(\"/tmp1\", fs, FsPermission.getDefault());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createTestSetup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createTestSetup(FileSystem fs, FsPermission perm) throws IOException\n{\r\n    return createTestSetup(\"/tmp1\", fs, perm);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createTestSetup",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "String createTestSetup(String baseDir, FileSystem fs, FsPermission perm) throws IOException\n{\r\n    String base = getBase(baseDir);\r\n    fs.mkdirs(new Path(base + \"/newTest/hello/world1\"));\r\n    fs.mkdirs(new Path(base + \"/newTest/hello/world2/newworld\"));\r\n    fs.mkdirs(new Path(base + \"/newTest/hello/world3/oldworld\"));\r\n    fs.setPermission(new Path(base + \"/newTest\"), perm);\r\n    fs.setPermission(new Path(base + \"/newTest/hello\"), perm);\r\n    fs.setPermission(new Path(base + \"/newTest/hello/world1\"), perm);\r\n    fs.setPermission(new Path(base + \"/newTest/hello/world2\"), perm);\r\n    fs.setPermission(new Path(base + \"/newTest/hello/world2/newworld\"), perm);\r\n    fs.setPermission(new Path(base + \"/newTest/hello/world3\"), perm);\r\n    fs.setPermission(new Path(base + \"/newTest/hello/world3/oldworld\"), perm);\r\n    createFile(fs, new Path(base, \"/newTest/1\"));\r\n    createFile(fs, new Path(base, \"/newTest/hello/2\"));\r\n    createFile(fs, new Path(base, \"/newTest/hello/world3/oldworld/3\"));\r\n    createFile(fs, new Path(base, \"/newTest/hello/world2/4\"));\r\n    return base;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getBase",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBase(String base)\n{\r\n    String location = String.valueOf(rand.nextLong());\r\n    return base + \"/\" + location;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createTestSetupWithOnlyFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String createTestSetupWithOnlyFile(FileSystem fs, FsPermission perm) throws IOException\n{\r\n    String location = String.valueOf(rand.nextLong());\r\n    fs.mkdirs(new Path(\"/tmp1/\" + location));\r\n    fs.setPermission(new Path(\"/tmp1/\" + location), perm);\r\n    createFile(fs, new Path(\"/tmp1/\" + location + \"/file\"));\r\n    return \"/tmp1/\" + location + \"/file\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "delete",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void delete(FileSystem fs, String path)\n{\r\n    try {\r\n        if (fs != null) {\r\n            if (path != null) {\r\n                fs.delete(new Path(path), true);\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.warn(\"Exception encountered \", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createFile(FileSystem fs, String filePath) throws IOException\n{\r\n    Path path = new Path(filePath);\r\n    createFile(fs, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createFile(FileSystem fs, Path filePath) throws IOException\n{\r\n    OutputStream out = fs.create(filePath, true);\r\n    IOUtils.closeStream(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "createDirectory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createDirectory(FileSystem fs, Path dirPath) throws IOException\n{\r\n    fs.delete(dirPath, true);\r\n    boolean created = fs.mkdirs(dirPath);\r\n    if (!created) {\r\n        LOG.warn(\"Could not create directory \" + dirPath + \" this might cause test failures.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "verifyFoldersAreInSync",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyFoldersAreInSync(FileSystem fs, String targetBase, String sourceBase) throws IOException\n{\r\n    Path base = new Path(targetBase);\r\n    Stack<Path> stack = new Stack<>();\r\n    stack.push(base);\r\n    while (!stack.isEmpty()) {\r\n        Path file = stack.pop();\r\n        if (!fs.exists(file)) {\r\n            continue;\r\n        }\r\n        FileStatus[] fStatus = fs.listStatus(file);\r\n        if (fStatus == null || fStatus.length == 0) {\r\n            continue;\r\n        }\r\n        for (FileStatus status : fStatus) {\r\n            if (status.isDirectory()) {\r\n                stack.push(status.getPath());\r\n            }\r\n            Path p = new Path(sourceBase + \"/\" + DistCpUtils.getRelativePath(new Path(targetBase), status.getPath()));\r\n            ContractTestUtils.assertPathExists(fs, \"path in sync with \" + status.getPath(), p);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testShouldCopy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testShouldCopy()\n{\r\n    Assert.assertTrue(new TrueCopyFilter().shouldCopy(new Path(\"fake\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testShouldCopyWithNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testShouldCopyWithNull()\n{\r\n    Assert.assertTrue(new TrueCopyFilter().shouldCopy(new Path(\"fake\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void create() throws IOException\n{\r\n    config = new Configuration();\r\n    if (testName.getMethodName().contains(\"WithCombineMode\")) {\r\n        config.set(\"dfs.checksum.combine.mode\", \"COMPOSITE_CRC\");\r\n    }\r\n    config.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 512);\r\n    cluster = new MiniDFSCluster.Builder(config).numDataNodes(2).format(true).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void destroy()\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testChecksumsComparisonWithCombineMode",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testChecksumsComparisonWithCombineMode() throws IOException\n{\r\n    try {\r\n        compareSameContentButDiffBlockSizes();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unexpected exception is found\", e);\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testChecksumsComparisonWithoutCombineMode",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testChecksumsComparisonWithoutCombineMode()\n{\r\n    try {\r\n        compareSameContentButDiffBlockSizes();\r\n        Assert.fail(\"Expected comparison to fail\");\r\n    } catch (IOException e) {\r\n        GenericTestUtils.assertExceptionContains(\"Checksum mismatch\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "compareSameContentButDiffBlockSizes",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void compareSameContentButDiffBlockSizes() throws IOException\n{\r\n    String base = \"/tmp/verify-checksum-\" + testName.getMethodName() + \"/\";\r\n    long seed = System.currentTimeMillis();\r\n    short rf = 2;\r\n    FileSystem fs = FileSystem.get(config);\r\n    Path basePath = new Path(base);\r\n    fs.mkdirs(basePath);\r\n    Path src = new Path(base + \"src\");\r\n    Path dst = new Path(base + \"dst\");\r\n    DFSTestUtil.createFile(fs, src, 256, 1024, 512, rf, seed);\r\n    DFSTestUtil.createFile(fs, dst, 256, 1024, 1024, rf, seed);\r\n    DistCpUtils.compareFileLengthsAndChecksums(1024, fs, src, null, fs, dst, false, 1024);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testSimpleProducerConsumer",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSimpleProducerConsumer()\n{\r\n    ProducerConsumer<Integer, Integer> worker = new ProducerConsumer<Integer, Integer>(1);\r\n    worker.addWorker(new CopyProcessor());\r\n    worker.put(new WorkRequest<Integer>(42));\r\n    try {\r\n        WorkReport<Integer> report = worker.take();\r\n        Assert.assertEquals(42, report.getItem().intValue());\r\n    } catch (InterruptedException ie) {\r\n        Assert.assertTrue(false);\r\n    }\r\n    worker.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testMultipleProducerConsumer",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMultipleProducerConsumer()\n{\r\n    ProducerConsumer<Integer, Integer> workers = new ProducerConsumer<Integer, Integer>(10);\r\n    for (int i = 0; i < 10; i++) {\r\n        workers.addWorker(new CopyProcessor());\r\n    }\r\n    int sum = 0;\r\n    int numRequests = 2000;\r\n    for (int i = 0; i < numRequests; i++) {\r\n        workers.put(new WorkRequest<Integer>(i + 42));\r\n        sum += i + 42;\r\n    }\r\n    int numReports = 0;\r\n    while (workers.getWorkCnt() > 0) {\r\n        WorkReport<Integer> report = workers.blockingTake();\r\n        sum -= report.getItem().intValue();\r\n        numReports++;\r\n    }\r\n    Assert.assertEquals(0, sum);\r\n    Assert.assertEquals(numRequests, numReports);\r\n    workers.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testExceptionProducerConsumer",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testExceptionProducerConsumer()\n{\r\n    ProducerConsumer<Integer, Integer> worker = new ProducerConsumer<Integer, Integer>(1);\r\n    worker.addWorker(new ExceptionProcessor());\r\n    worker.put(new WorkRequest<Integer>(42));\r\n    try {\r\n        WorkReport<Integer> report = worker.take();\r\n        Assert.assertEquals(42, report.getItem().intValue());\r\n        Assert.assertFalse(report.getSuccess());\r\n        Assert.assertNotNull(report.getException());\r\n    } catch (InterruptedException ie) {\r\n        Assert.assertTrue(false);\r\n    }\r\n    worker.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testSimpleProducerConsumerShutdown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSimpleProducerConsumerShutdown() throws InterruptedException, TimeoutException\n{\r\n    ProducerConsumer<Integer, Integer> worker = new ProducerConsumer<Integer, Integer>(1);\r\n    worker.addWorker(new CopyProcessor());\r\n    worker.shutdown();\r\n    GenericTestUtils.waitForThreadTermination(\"pool-.*-thread.*\", 100, 10000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "testMultipleProducerConsumerShutdown",
  "errType" : [ "InterruptedException", "InterruptedException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMultipleProducerConsumerShutdown() throws InterruptedException, TimeoutException\n{\r\n    int numWorkers = 10;\r\n    final ProducerConsumer<Integer, Integer> worker = new ProducerConsumer<Integer, Integer>(numWorkers);\r\n    for (int i = 0; i < numWorkers; i++) {\r\n        worker.addWorker(new CopyProcessor());\r\n    }\r\n    class SourceThread extends Thread {\r\n\r\n        public void run() {\r\n            while (true) {\r\n                try {\r\n                    worker.put(new WorkRequest<Integer>(42));\r\n                    Thread.sleep(1);\r\n                } catch (InterruptedException ie) {\r\n                    return;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    ;\r\n    SourceThread source = new SourceThread();\r\n    source.start();\r\n    class SinkThread extends Thread {\r\n\r\n        public void run() {\r\n            try {\r\n                while (true) {\r\n                    WorkReport<Integer> report = worker.take();\r\n                    Assert.assertEquals(42, report.getItem().intValue());\r\n                }\r\n            } catch (InterruptedException ie) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n    ;\r\n    SinkThread sink = new SinkThread();\r\n    sink.start();\r\n    Thread.sleep(1000);\r\n    source.interrupt();\r\n    while (worker.hasWork()) {\r\n        Thread.sleep(1);\r\n    }\r\n    worker.shutdown();\r\n    GenericTestUtils.waitForThreadTermination(\"pool-.*-thread.*\", 100, 10000);\r\n    sink.interrupt();\r\n    source.join();\r\n    sink.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    cluster = new MiniDFSCluster.Builder(getConfigurationForCluster()).numDataNodes(1).format(true).build();\r\n    for (int i = 0; i < N_FILES; ++i) createFile(\"/tmp/source/\" + String.valueOf(i));\r\n    FileSystem fileSystem = cluster.getFileSystem();\r\n    expectedFilePaths.add(fileSystem.listStatus(new Path(\"/tmp/source/0\"))[0].getPath().getParent().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getConfigurationForCluster",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration getConfigurationForCluster()\n{\r\n    Configuration configuration = new Configuration();\r\n    System.setProperty(\"test.build.data\", \"target/tmp/build/TEST_DYNAMIC_INPUT_FORMAT/data\");\r\n    configuration.set(\"hadoop.log.dir\", \"target/tmp\");\r\n    LOG.debug(\"fs.default.name  == \" + configuration.get(\"fs.default.name\"));\r\n    LOG.debug(\"dfs.http.address == \" + configuration.get(\"dfs.http.address\"));\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getOptions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DistCpOptions getOptions() throws Exception\n{\r\n    Path sourcePath = new Path(cluster.getFileSystem().getUri().toString() + \"/tmp/source\");\r\n    Path targetPath = new Path(cluster.getFileSystem().getUri().toString() + \"/tmp/target\");\r\n    List<Path> sourceList = new ArrayList<Path>();\r\n    sourceList.add(sourcePath);\r\n    return new DistCpOptions.Builder(sourceList, targetPath).maxMaps(NUM_SPLITS).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createFile(String path) throws Exception\n{\r\n    FileSystem fileSystem = null;\r\n    DataOutputStream outputStream = null;\r\n    try {\r\n        fileSystem = cluster.getFileSystem();\r\n        outputStream = fileSystem.create(new Path(path), true, 0);\r\n        expectedFilePaths.add(fileSystem.listStatus(new Path(path))[0].getPath().toString());\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(null, fileSystem, outputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "testGetSplits",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testGetSplits() throws Exception\n{\r\n    final DistCpContext context = new DistCpContext(getOptions());\r\n    Configuration configuration = new Configuration();\r\n    configuration.set(\"mapred.map.tasks\", String.valueOf(context.getMaxMaps()));\r\n    CopyListing.getCopyListing(configuration, CREDENTIALS, context).buildListing(new Path(cluster.getFileSystem().getUri().toString() + \"/tmp/testDynInputFormat/fileList.seq\"), context);\r\n    JobContext jobContext = new JobContextImpl(configuration, new JobID());\r\n    DynamicInputFormat<Text, CopyListingFileStatus> inputFormat = new DynamicInputFormat<Text, CopyListingFileStatus>();\r\n    List<InputSplit> splits = inputFormat.getSplits(jobContext);\r\n    int nFiles = 0;\r\n    int taskId = 0;\r\n    for (InputSplit split : splits) {\r\n        StubContext stubContext = new StubContext(jobContext.getConfiguration(), null, taskId);\r\n        final TaskAttemptContext taskAttemptContext = stubContext.getContext();\r\n        RecordReader<Text, CopyListingFileStatus> recordReader = inputFormat.createRecordReader(split, taskAttemptContext);\r\n        stubContext.setReader(recordReader);\r\n        recordReader.initialize(splits.get(0), taskAttemptContext);\r\n        float previousProgressValue = 0f;\r\n        while (recordReader.nextKeyValue()) {\r\n            CopyListingFileStatus fileStatus = recordReader.getCurrentValue();\r\n            String source = fileStatus.getPath().toString();\r\n            System.out.println(source);\r\n            Assert.assertTrue(expectedFilePaths.contains(source));\r\n            final float progress = recordReader.getProgress();\r\n            Assert.assertTrue(progress >= previousProgressValue);\r\n            Assert.assertTrue(progress >= 0.0f);\r\n            Assert.assertTrue(progress <= 1.0f);\r\n            previousProgressValue = progress;\r\n            ++nFiles;\r\n        }\r\n        Assert.assertTrue(recordReader.getProgress() == 1.0f);\r\n        ++taskId;\r\n    }\r\n    Assert.assertEquals(expectedFilePaths.size(), nFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "testGetSplitRatio",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testGetSplitRatio() throws Exception\n{\r\n    Assert.assertEquals(1, DynamicInputFormat.getSplitRatio(1, 1000000000));\r\n    Assert.assertEquals(2, DynamicInputFormat.getSplitRatio(11000000, 10));\r\n    Assert.assertEquals(4, DynamicInputFormat.getSplitRatio(30, 700));\r\n    Assert.assertEquals(2, DynamicInputFormat.getSplitRatio(30, 200));\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(DistCpConstants.CONF_LABEL_MAX_CHUNKS_TOLERABLE, -1);\r\n    conf.setInt(DistCpConstants.CONF_LABEL_MAX_CHUNKS_IDEAL, -1);\r\n    conf.setInt(DistCpConstants.CONF_LABEL_MIN_RECORDS_PER_CHUNK, -1);\r\n    conf.setInt(DistCpConstants.CONF_LABEL_SPLIT_RATIO, -1);\r\n    Assert.assertEquals(1, DynamicInputFormat.getSplitRatio(1, 1000000000, conf));\r\n    Assert.assertEquals(2, DynamicInputFormat.getSplitRatio(11000000, 10, conf));\r\n    Assert.assertEquals(4, DynamicInputFormat.getSplitRatio(30, 700, conf));\r\n    Assert.assertEquals(2, DynamicInputFormat.getSplitRatio(30, 200, conf));\r\n    conf.setInt(DistCpConstants.CONF_LABEL_MAX_CHUNKS_TOLERABLE, 100);\r\n    conf.setInt(DistCpConstants.CONF_LABEL_MAX_CHUNKS_IDEAL, 30);\r\n    conf.setInt(DistCpConstants.CONF_LABEL_MIN_RECORDS_PER_CHUNK, 10);\r\n    conf.setInt(DistCpConstants.CONF_LABEL_SPLIT_RATIO, 53);\r\n    Assert.assertEquals(53, DynamicInputFormat.getSplitRatio(3, 200, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "testDynamicInputChunkContext",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDynamicInputChunkContext() throws IOException\n{\r\n    Configuration configuration = new Configuration();\r\n    configuration.set(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, \"/tmp/test/file1.seq\");\r\n    DynamicInputFormat firstInputFormat = new DynamicInputFormat();\r\n    DynamicInputFormat secondInputFormat = new DynamicInputFormat();\r\n    DynamicInputChunkContext firstContext = firstInputFormat.getChunkContext(configuration);\r\n    DynamicInputChunkContext secondContext = firstInputFormat.getChunkContext(configuration);\r\n    DynamicInputChunkContext thirdContext = secondInputFormat.getChunkContext(configuration);\r\n    DynamicInputChunkContext fourthContext = secondInputFormat.getChunkContext(configuration);\r\n    Assert.assertTrue(\"Chunk contexts from the same DynamicInputFormat \" + \"object should be the same.\", firstContext.equals(secondContext));\r\n    Assert.assertTrue(\"Chunk contexts from the same DynamicInputFormat \" + \"object should be the same.\", thirdContext.equals(fourthContext));\r\n    Assert.assertTrue(\"Contexts from different DynamicInputFormat \" + \"objects should be different.\", !firstContext.equals(thirdContext));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void init() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, true);\r\n    conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, 2);\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();\r\n    cluster.waitActive();\r\n    fs = cluster.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdown()\n{\r\n    IOUtils.cleanupWithLogger(null, fs);\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveRawXAttrs1",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testPreserveRawXAttrs1() throws Exception\n{\r\n    final String relSrc = \"/./.reserved/../.reserved/raw/../raw/src/../src\";\r\n    final String relDst = \"/./.reserved/../.reserved/raw/../raw/dest/../dest\";\r\n    doTestPreserveRawXAttrs(relSrc, relDst, \"-px\", true, true, DistCpConstants.SUCCESS);\r\n    doTestStandardPreserveRawXAttrs(\"-px\", true);\r\n    final Path savedWd = fs.getWorkingDirectory();\r\n    try {\r\n        fs.setWorkingDirectory(new Path(\"/.reserved/raw\"));\r\n        doTestPreserveRawXAttrs(\"../..\" + rawSrcName, \"../..\" + rawDestName, \"-px\", true, true, DistCpConstants.SUCCESS);\r\n    } finally {\r\n        fs.setWorkingDirectory(savedWd);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveRawXAttrs2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPreserveRawXAttrs2() throws Exception\n{\r\n    doTestStandardPreserveRawXAttrs(\"-p\", false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveRawXAttrs3",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPreserveRawXAttrs3() throws Exception\n{\r\n    doTestStandardPreserveRawXAttrs(null, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveRawXAttrs4",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPreserveRawXAttrs4() throws Exception\n{\r\n    doTestStandardPreserveRawXAttrs(\"-update -delete\", false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "makeFilesAndDirs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void makeFilesAndDirs(FileSystem fs) throws Exception\n{\r\n    fs.delete(new Path(\"/src\"), true);\r\n    fs.delete(new Path(\"/dest\"), true);\r\n    fs.mkdirs(subDir1);\r\n    fs.create(file1).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initXAttrs() throws Exception\n{\r\n    makeFilesAndDirs(fs);\r\n    for (Path p : pathnames) {\r\n        fs.setXAttr(new Path(rawRootName + \"/src\", p), rawName1, rawValue1);\r\n        fs.setXAttr(new Path(rawRootName + \"/src\", p), userName1, userValue1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doTestStandardPreserveRawXAttrs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void doTestStandardPreserveRawXAttrs(String options, boolean expectUser) throws Exception\n{\r\n    doTestPreserveRawXAttrs(rootedSrcName, rootedDestName, options, false, expectUser, DistCpConstants.SUCCESS);\r\n    doTestPreserveRawXAttrs(rootedSrcName, rawDestName, options, false, expectUser, DistCpConstants.INVALID_ARGUMENT);\r\n    doTestPreserveRawXAttrs(rawSrcName, rootedDestName, options, false, expectUser, DistCpConstants.INVALID_ARGUMENT);\r\n    doTestPreserveRawXAttrs(rawSrcName, rawDestName, options, true, expectUser, DistCpConstants.SUCCESS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doTestPreserveRawXAttrs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void doTestPreserveRawXAttrs(String src, String dest, String preserveOpts, boolean expectRaw, boolean expectUser, int expectedExitCode) throws Exception\n{\r\n    initXAttrs();\r\n    DistCpTestUtils.assertRunDistCp(expectedExitCode, src, dest, preserveOpts, conf);\r\n    if (expectedExitCode == DistCpConstants.SUCCESS) {\r\n        Map<String, byte[]> xAttrs = Maps.newHashMap();\r\n        for (Path p : pathnames) {\r\n            xAttrs.clear();\r\n            if (expectRaw) {\r\n                xAttrs.put(rawName1, rawValue1);\r\n            }\r\n            if (expectUser) {\r\n                xAttrs.put(userName1, userValue1);\r\n            }\r\n            DistCpTestUtils.assertXAttrs(new Path(dest, p), fs, xAttrs);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserveEC",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testPreserveEC() throws Exception\n{\r\n    final String src = \"/src\";\r\n    final String dest = \"/dest\";\r\n    final Path destDir1 = new Path(\"/dest/dir1\");\r\n    final Path destSubDir1 = new Path(destDir1, \"subdir1\");\r\n    String[] args = { \"-setPolicy\", \"-path\", dir1.toString(), \"-policy\", \"XOR-2-1-1024k\" };\r\n    fs.delete(new Path(\"/dest\"), true);\r\n    fs.mkdirs(subDir1);\r\n    fs.create(file1).close();\r\n    DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n    dfs.enableErasureCodingPolicy(\"XOR-2-1-1024k\");\r\n    int res = ToolRunner.run(conf, new ECAdmin(conf), args);\r\n    assertEquals(\"Unable to set EC policy on \" + subDir1.toString(), res, 0);\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, src, dest, \"-pe\", conf);\r\n    FileStatus srcStatus = fs.getFileStatus(new Path(src));\r\n    FileStatus srcDir1Status = fs.getFileStatus(dir1);\r\n    FileStatus srcSubDir1Status = fs.getFileStatus(subDir1);\r\n    FileStatus destStatus = fs.getFileStatus(new Path(dest));\r\n    FileStatus destDir1Status = fs.getFileStatus(destDir1);\r\n    FileStatus destSubDir1Status = fs.getFileStatus(destSubDir1);\r\n    assertFalse(\"/src is erasure coded!\", srcStatus.isErasureCoded());\r\n    assertFalse(\"/dest is erasure coded!\", destStatus.isErasureCoded());\r\n    assertTrue(\"/src/dir1 is not erasure coded!\", srcDir1Status.isErasureCoded());\r\n    assertTrue(\"/dest/dir1 is not erasure coded!\", destDir1Status.isErasureCoded());\r\n    assertTrue(\"/src/dir1/subdir1 is not erasure coded!\", srcSubDir1Status.isErasureCoded());\r\n    assertTrue(\"/dest/dir1/subdir1 is not erasure coded!\", destSubDir1Status.isErasureCoded());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testUseIterator",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUseIterator() throws Exception\n{\r\n    Path source = new Path(\"/src\");\r\n    Path dest = new Path(\"/dest\");\r\n    fs.delete(source, true);\r\n    fs.delete(dest, true);\r\n    fs.mkdirs(source);\r\n    GenericTestUtils.createFiles(fs, source, 3, 10, 10);\r\n    DistCpTestUtils.assertRunDistCp(DistCpConstants.SUCCESS, source.toString(), dest.toString(), \"-useiterator\", conf);\r\n    Assertions.assertThat(RemoteIterators.toList(fs.listFiles(dest, true))).describedAs(\"files\").hasSize(1110);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    cluster = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(1).format(true).build();\r\n    totalFileSize = 0;\r\n    for (int i = 0; i < N_FILES; ++i) totalFileSize += createFile(\"/tmp/source/\" + String.valueOf(i), SIZEOF_EACH_FILE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getOptions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DistCpOptions getOptions(int nMaps) throws Exception\n{\r\n    Path sourcePath = new Path(cluster.getFileSystem().getUri().toString() + \"/tmp/source\");\r\n    Path targetPath = new Path(cluster.getFileSystem().getUri().toString() + \"/tmp/target\");\r\n    List<Path> sourceList = new ArrayList<Path>();\r\n    sourceList.add(sourcePath);\r\n    return new DistCpOptions.Builder(sourceList, targetPath).maxMaps(nMaps).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int createFile(String path, int fileSize) throws Exception\n{\r\n    FileSystem fileSystem = null;\r\n    DataOutputStream outputStream = null;\r\n    try {\r\n        fileSystem = cluster.getFileSystem();\r\n        outputStream = fileSystem.create(new Path(path), true, 0);\r\n        int size = (int) Math.ceil(fileSize + (1 - random.nextFloat()) * fileSize);\r\n        outputStream.write(new byte[size]);\r\n        return size;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(null, fileSystem, outputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testGetSplits",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testGetSplits(int nMaps) throws Exception\n{\r\n    DistCpContext context = new DistCpContext(getOptions(nMaps));\r\n    Configuration configuration = new Configuration();\r\n    configuration.set(\"mapred.map.tasks\", String.valueOf(context.getMaxMaps()));\r\n    Path listFile = new Path(cluster.getFileSystem().getUri().toString() + \"/tmp/testGetSplits_1/fileList.seq\");\r\n    CopyListing.getCopyListing(configuration, CREDENTIALS, context).buildListing(listFile, context);\r\n    JobContext jobContext = new JobContextImpl(configuration, new JobID());\r\n    UniformSizeInputFormat uniformSizeInputFormat = new UniformSizeInputFormat();\r\n    List<InputSplit> splits = uniformSizeInputFormat.getSplits(jobContext);\r\n    int sizePerMap = totalFileSize / nMaps;\r\n    checkSplits(listFile, splits);\r\n    int doubleCheckedTotalSize = 0;\r\n    int previousSplitSize = -1;\r\n    for (int i = 0; i < splits.size(); ++i) {\r\n        InputSplit split = splits.get(i);\r\n        int currentSplitSize = 0;\r\n        RecordReader<Text, CopyListingFileStatus> recordReader = uniformSizeInputFormat.createRecordReader(split, null);\r\n        StubContext stubContext = new StubContext(jobContext.getConfiguration(), recordReader, 0);\r\n        final TaskAttemptContext taskAttemptContext = stubContext.getContext();\r\n        recordReader.initialize(split, taskAttemptContext);\r\n        while (recordReader.nextKeyValue()) {\r\n            Path sourcePath = recordReader.getCurrentValue().getPath();\r\n            FileSystem fs = sourcePath.getFileSystem(configuration);\r\n            FileStatus[] fileStatus = fs.listStatus(sourcePath);\r\n            if (fileStatus.length > 1) {\r\n                continue;\r\n            }\r\n            currentSplitSize += fileStatus[0].getLen();\r\n        }\r\n        Assert.assertTrue(previousSplitSize == -1 || Math.abs(currentSplitSize - previousSplitSize) < 0.1 * sizePerMap || i == splits.size() - 1);\r\n        doubleCheckedTotalSize += currentSplitSize;\r\n    }\r\n    Assert.assertEquals(totalFileSize, doubleCheckedTotalSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "checkSplits",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void checkSplits(Path listFile, List<InputSplit> splits) throws IOException\n{\r\n    long lastEnd = 0;\r\n    for (InputSplit split : splits) {\r\n        FileSplit fileSplit = (FileSplit) split;\r\n        long start = fileSplit.getStart();\r\n        Assert.assertEquals(lastEnd, start);\r\n        lastEnd = start + fileSplit.getLength();\r\n    }\r\n    SequenceFile.Reader reader = new SequenceFile.Reader(cluster.getFileSystem().getConf(), SequenceFile.Reader.file(listFile));\r\n    try {\r\n        reader.seek(lastEnd);\r\n        CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\r\n        Text srcRelPath = new Text();\r\n        Assert.assertFalse(reader.next(srcRelPath, srcFileStatus));\r\n    } finally {\r\n        IOUtils.closeStream(reader);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testGetSplits",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetSplits() throws Exception\n{\r\n    testGetSplits(9);\r\n    for (int i = 1; i < N_FILES; ++i) testGetSplits(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseIgnoreFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testParseIgnoreFailure()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldIgnoreFailures());\r\n    options = OptionsParser.parse(new String[] { \"-i\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldIgnoreFailures());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseOverwrite",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testParseOverwrite()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldOverwrite());\r\n    options = OptionsParser.parse(new String[] { \"-overwrite\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldOverwrite());\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-update\", \"-overwrite\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Update and overwrite aren't allowed together\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testLogPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testLogPath()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertNull(options.getLogPath());\r\n    options = OptionsParser.parse(new String[] { \"-log\", \"hdfs://localhost:8020/logs\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(options.getLogPath(), new Path(\"hdfs://localhost:8020/logs\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseBlokcing",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testParseBlokcing()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldBlock());\r\n    options = OptionsParser.parse(new String[] { \"-async\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldBlock());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParsebandwidth",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testParsebandwidth()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getMapBandwidth()).isCloseTo(0f, within(DELTA));\r\n    options = OptionsParser.parse(new String[] { \"-bandwidth\", \"11.2\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getMapBandwidth()).isCloseTo(11.2f, within(DELTA));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseNonPositiveBandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testParseNonPositiveBandwidth()\n{\r\n    OptionsParser.parse(new String[] { \"-bandwidth\", \"-11\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseZeroBandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testParseZeroBandwidth()\n{\r\n    OptionsParser.parse(new String[] { \"-bandwidth\", \"0\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseSkipCRC",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testParseSkipCRC()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldSkipCRC());\r\n    options = OptionsParser.parse(new String[] { \"-update\", \"-skipcrccheck\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldSyncFolder());\r\n    Assert.assertTrue(options.shouldSkipCRC());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseAtomicCommit",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testParseAtomicCommit()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldAtomicCommit());\r\n    options = OptionsParser.parse(new String[] { \"-atomic\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldAtomicCommit());\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-atomic\", \"-update\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Atomic and sync folders were allowed\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseWorkPath",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testParseWorkPath()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertNull(options.getAtomicWorkPath());\r\n    options = OptionsParser.parse(new String[] { \"-atomic\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertNull(options.getAtomicWorkPath());\r\n    options = OptionsParser.parse(new String[] { \"-atomic\", \"-tmp\", \"hdfs://localhost:8020/work\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(options.getAtomicWorkPath(), new Path(\"hdfs://localhost:8020/work\"));\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-tmp\", \"hdfs://localhost:8020/work\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"work path was allowed without -atomic switch\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseSyncFolders",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testParseSyncFolders()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldSyncFolder());\r\n    options = OptionsParser.parse(new String[] { \"-update\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldSyncFolder());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseDeleteMissing",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testParseDeleteMissing()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldDeleteMissing());\r\n    options = OptionsParser.parse(new String[] { \"-update\", \"-delete\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldSyncFolder());\r\n    Assert.assertTrue(options.shouldDeleteMissing());\r\n    options = OptionsParser.parse(new String[] { \"-overwrite\", \"-delete\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldOverwrite());\r\n    Assert.assertTrue(options.shouldDeleteMissing());\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-atomic\", \"-delete\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Atomic and delete folders were allowed\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseMaps",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testParseMaps()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getMaxMaps()).isEqualTo(DistCpConstants.DEFAULT_MAPS);\r\n    options = OptionsParser.parse(new String[] { \"-m\", \"1\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getMaxMaps()).isEqualTo(1);\r\n    options = OptionsParser.parse(new String[] { \"-m\", \"0\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getMaxMaps()).isEqualTo(1);\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-m\", \"hello\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Non numberic map parsed\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-mapredXslConf\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Non numberic map parsed\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseNumListstatusThreads",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testParseNumListstatusThreads()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(0, options.getNumListstatusThreads());\r\n    options = OptionsParser.parse(new String[] { \"--numListstatusThreads\", \"12\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(12, options.getNumListstatusThreads());\r\n    options = OptionsParser.parse(new String[] { \"--numListstatusThreads\", \"0\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(0, options.getNumListstatusThreads());\r\n    try {\r\n        OptionsParser.parse(new String[] { \"--numListstatusThreads\", \"hello\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Non numberic numListstatusThreads parsed\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n    options = OptionsParser.parse(new String[] { \"--numListstatusThreads\", \"100\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(DistCpOptions.MAX_NUM_LISTSTATUS_THREADS, options.getNumListstatusThreads());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSourceListing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSourceListing()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(options.getSourceFileListing(), new Path(\"hdfs://localhost:8020/source/first\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSourceListingAndSourcePath",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSourceListingAndSourcePath()\n{\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Both source listing & source paths allowed\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMissingSourceInfo",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMissingSourceInfo()\n{\r\n    try {\r\n        OptionsParser.parse(new String[] { \"hdfs://localhost:8020/target/\" });\r\n        Assert.fail(\"Neither source listing not source paths present\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testMissingTarget",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMissingTarget()\n{\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-f\", \"hdfs://localhost:8020/source\" });\r\n        Assert.fail(\"Missing target allowed\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testInvalidArgs",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testInvalidArgs()\n{\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-m\", \"-f\", \"hdfs://localhost:8020/source\" });\r\n        Assert.fail(\"Missing map value\");\r\n    } catch (IllegalArgumentException ignore) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCopyStrategy",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCopyStrategy()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"-strategy\", \"dynamic\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getCopyStrategy()).isEqualTo(\"dynamic\");\r\n    options = OptionsParser.parse(new String[] { \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getCopyStrategy()).isEqualTo(DistCpConstants.UNIFORMSIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testTargetPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTargetPath()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(options.getTargetPath(), new Path(\"hdfs://localhost:8020/target/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPreserve",
  "errType" : [ "NoSuchElementException" ],
  "containingMethodsNum" : 71,
  "sourceCodeText" : "void testPreserve()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    options = OptionsParser.parse(new String[] { \"-p\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.ACL));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.XATTR));\r\n    options = OptionsParser.parse(new String[] { \"-p\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.ACL));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.XATTR));\r\n    options = OptionsParser.parse(new String[] { \"-pbr\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.ACL));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.XATTR));\r\n    options = OptionsParser.parse(new String[] { \"-pbrgup\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.ACL));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.XATTR));\r\n    options = OptionsParser.parse(new String[] { \"-pbrgupcaxt\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.ACL));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.XATTR));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.TIMES));\r\n    options = OptionsParser.parse(new String[] { \"-pc\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.BLOCKSIZE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.REPLICATION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.PERMISSION));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.USER));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.GROUP));\r\n    Assert.assertTrue(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.ACL));\r\n    Assert.assertFalse(options.shouldPreserve(FileAttribute.XATTR));\r\n    options = OptionsParser.parse(new String[] { \"-p\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertEquals(DistCpOptionSwitch.PRESERVE_STATUS_DEFAULT.length() - 2, options.getPreserveAttributes().size());\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-pabcd\", \"-f\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target\" });\r\n        Assert.fail(\"Invalid preserve attribute\");\r\n    } catch (NoSuchElementException ignore) {\r\n    }\r\n    Builder builder = new DistCpOptions.Builder(new Path(\"hdfs://localhost:8020/source/first\"), new Path(\"hdfs://localhost:8020/target/\"));\r\n    Assert.assertFalse(builder.build().shouldPreserve(FileAttribute.PERMISSION));\r\n    builder.preserve(FileAttribute.PERMISSION);\r\n    Assert.assertTrue(builder.build().shouldPreserve(FileAttribute.PERMISSION));\r\n    builder.preserve(FileAttribute.PERMISSION);\r\n    Assert.assertTrue(builder.build().shouldPreserve(FileAttribute.PERMISSION));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testOptionsSwitchAddToConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOptionsSwitchAddToConf()\n{\r\n    Configuration conf = new Configuration();\r\n    Assert.assertNull(conf.get(DistCpOptionSwitch.ATOMIC_COMMIT.getConfigLabel()));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.ATOMIC_COMMIT);\r\n    Assert.assertTrue(conf.getBoolean(DistCpOptionSwitch.ATOMIC_COMMIT.getConfigLabel(), false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testOptionsAppendToConf",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testOptionsAppendToConf()\n{\r\n    Configuration conf = new Configuration();\r\n    Assert.assertFalse(conf.getBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), false));\r\n    Assert.assertFalse(conf.getBoolean(DistCpOptionSwitch.ATOMIC_COMMIT.getConfigLabel(), false));\r\n    Assert.assertEquals(conf.getRaw(DistCpOptionSwitch.BANDWIDTH.getConfigLabel()), null);\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"-atomic\", \"-i\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertTrue(conf.getBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), false));\r\n    Assert.assertTrue(conf.getBoolean(DistCpOptionSwitch.ATOMIC_COMMIT.getConfigLabel(), false));\r\n    Assert.assertEquals(conf.getFloat(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), -1), -1.0, DELTA);\r\n    conf = new Configuration();\r\n    Assert.assertFalse(conf.getBoolean(DistCpOptionSwitch.SYNC_FOLDERS.getConfigLabel(), false));\r\n    Assert.assertFalse(conf.getBoolean(DistCpOptionSwitch.DELETE_MISSING.getConfigLabel(), false));\r\n    assertThat(conf.get(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel())).isNull();\r\n    options = OptionsParser.parse(new String[] { \"-update\", \"-delete\", \"-pu\", \"-bandwidth\", \"11.2\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertTrue(conf.getBoolean(DistCpOptionSwitch.SYNC_FOLDERS.getConfigLabel(), false));\r\n    Assert.assertTrue(conf.getBoolean(DistCpOptionSwitch.DELETE_MISSING.getConfigLabel(), false));\r\n    assertThat(conf.get(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel())).isEqualTo(\"U\");\r\n    assertThat(conf.getFloat(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), -1)).isCloseTo(11.2f, within(DELTA));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testOptionsAppendToConfDoesntOverwriteBandwidth",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testOptionsAppendToConfDoesntOverwriteBandwidth()\n{\r\n    Configuration conf = new Configuration();\r\n    Assert.assertEquals(conf.getRaw(DistCpOptionSwitch.BANDWIDTH.getConfigLabel()), null);\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    assertThat(conf.getFloat(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), -1)).isCloseTo(-1.0f, within(DELTA));\r\n    conf = new Configuration();\r\n    Assert.assertEquals(conf.getRaw(DistCpOptionSwitch.BANDWIDTH.getConfigLabel()), null);\r\n    options = OptionsParser.parse(new String[] { \"-update\", \"-delete\", \"-pu\", \"-bandwidth\", \"77\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertEquals(conf.getFloat(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), -1), 77.0, DELTA);\r\n    conf = new Configuration();\r\n    conf.set(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), \"88\");\r\n    Assert.assertEquals(conf.getRaw(DistCpOptionSwitch.BANDWIDTH.getConfigLabel()), \"88\");\r\n    options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertEquals(conf.getFloat(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), -1), 88.0, DELTA);\r\n    conf = new Configuration();\r\n    conf.set(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), \"88.0\");\r\n    Assert.assertEquals(conf.getRaw(DistCpOptionSwitch.BANDWIDTH.getConfigLabel()), \"88.0\");\r\n    options = OptionsParser.parse(new String[] { \"-bandwidth\", \"99\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertEquals(conf.getFloat(DistCpOptionSwitch.BANDWIDTH.getConfigLabel(), -1), 99.0, DELTA);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testAppendOption",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testAppendOption()\n{\r\n    Configuration conf = new Configuration();\r\n    Assert.assertFalse(conf.getBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), false));\r\n    Assert.assertFalse(conf.getBoolean(DistCpOptionSwitch.SYNC_FOLDERS.getConfigLabel(), false));\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"-update\", \"-append\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertTrue(conf.getBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), false));\r\n    Assert.assertTrue(conf.getBoolean(DistCpOptionSwitch.SYNC_FOLDERS.getConfigLabel(), false));\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-append\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(\"Append should fail if update option is not specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"Append is valid only with update options\", e);\r\n    }\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-append\", \"-update\", \"-skipcrccheck\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(\"Append should fail if skipCrc option is specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"Append is disallowed when skipping CRC\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSnapshotDiffOption",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testSnapshotDiffOption(boolean isDiff)\n{\r\n    final String optionStr = isDiff ? \"-diff\" : \"-rdiff\";\r\n    final String optionLabel = isDiff ? DistCpOptionSwitch.DIFF.getConfigLabel() : DistCpOptionSwitch.RDIFF.getConfigLabel();\r\n    Configuration conf = new Configuration();\r\n    Assert.assertFalse(conf.getBoolean(optionLabel, false));\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"-update\", optionStr, \"s1\", \"s2\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertTrue(conf.getBoolean(optionLabel, false));\r\n    Assert.assertTrue(isDiff ? options.shouldUseDiff() : options.shouldUseRdiff());\r\n    Assert.assertEquals(\"s1\", options.getFromSnapshot());\r\n    Assert.assertEquals(\"s2\", options.getToSnapshot());\r\n    options = OptionsParser.parse(new String[] { optionStr, \"s1\", \".\", \"-update\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    options.appendToConf(conf);\r\n    Assert.assertTrue(conf.getBoolean(optionLabel, false));\r\n    Assert.assertTrue(isDiff ? options.shouldUseDiff() : options.shouldUseRdiff());\r\n    Assert.assertEquals(\"s1\", options.getFromSnapshot());\r\n    Assert.assertEquals(\".\", options.getToSnapshot());\r\n    try {\r\n        OptionsParser.parse(new String[] { optionStr, \"s1\", \"-update\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(optionStr + \" should fail with only one snapshot name\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"Must provide both the starting and ending snapshot names\", e);\r\n    }\r\n    try {\r\n        OptionsParser.parse(new String[] { optionStr, \"s1\", \"s2\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(optionStr + \" should fail if -update option is not specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"-diff/-rdiff is valid only with -update option\", e);\r\n    }\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-diff\", \"s1\", \"s2\", \"-update\", \"-delete\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(\"Should fail as -delete and -diff/-rdiff are mutually exclusive\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-delete and -diff/-rdiff are mutually exclusive\", e);\r\n    }\r\n    try {\r\n        OptionsParser.parse(new String[] { \"-diff\", \"s1\", \"s2\", \"-delete\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(\"Should fail as -delete and -diff/-rdiff are mutually exclusive\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-delete and -diff/-rdiff are mutually exclusive\", e);\r\n    }\r\n    try {\r\n        OptionsParser.parse(new String[] { optionStr, \"s1\", \"s2\", \"-delete\", \"-overwrite\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(\"Should fail as -delete and -diff are mutually exclusive\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"-delete and -diff/-rdiff are mutually exclusive\", e);\r\n    }\r\n    final String optionStrOther = isDiff ? \"-rdiff\" : \"-diff\";\r\n    try {\r\n        OptionsParser.parse(new String[] { optionStr, \"s1\", \"s2\", optionStrOther, \"s2\", \"s1\", \"-update\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n        fail(optionStr + \" should fail if \" + optionStrOther + \" is also specified\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"-diff and -rdiff are mutually exclusive\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 6,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testDiffOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDiffOption()\n{\r\n    testSnapshotDiffOption(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testRdiffOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRdiffOption()\n{\r\n    testSnapshotDiffOption(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testExclusionsOption",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testExclusionsOption()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertNull(options.getFiltersFile());\r\n    options = OptionsParser.parse(new String[] { \"-filters\", \"/tmp/filters.txt\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    assertThat(options.getFiltersFile()).isEqualTo(\"/tmp/filters.txt\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testParseUpdateRoot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testParseUpdateRoot()\n{\r\n    DistCpOptions options = OptionsParser.parse(new String[] { \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertFalse(options.shouldUpdateRoot());\r\n    options = OptionsParser.parse(new String[] { \"-updateRoot\", \"hdfs://localhost:8020/source/first\", \"hdfs://localhost:8020/target/\" });\r\n    Assert.assertTrue(options.shouldUpdateRoot());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.default.name\", \"file:///\");\r\n    conf.set(\"mapred.job.tracker\", \"local\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup()\n{\r\n    securityManager = System.getSecurityManager();\r\n    System.setSecurityManager(new NoExitSecurityManager());\r\n    try {\r\n        fs = FileSystem.get(getConf());\r\n        root = new Path(\"target/tmp\").makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();\r\n        TestDistCpUtils.delete(fs, root);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    System.setSecurityManager(securityManager);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCleanup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCleanup() throws Exception\n{\r\n    Configuration conf = getConf();\r\n    Path stagingDir = JobSubmissionFiles.getStagingDir(new Cluster(conf), conf);\r\n    stagingDir.getFileSystem(conf).mkdirs(stagingDir);\r\n    Path soure = createFile(\"tmp.txt\");\r\n    Path target = createFile(\"target.txt\");\r\n    DistCp distcp = new DistCp(conf, null);\r\n    String[] arg = { soure.toString(), target.toString() };\r\n    distcp.run(arg);\r\n    Assert.assertTrue(fs.exists(target));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createFile(String fname) throws IOException\n{\r\n    Path result = new Path(root + \"/\" + fname);\r\n    OutputStream out = fs.create(result);\r\n    try {\r\n        out.write((root + \"/\" + fname).getBytes());\r\n        out.write(\"\\n\".getBytes());\r\n    } finally {\r\n        out.close();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCleanupTestViaToolRunner",
  "errType" : [ "ExitException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCleanupTestViaToolRunner() throws IOException, InterruptedException\n{\r\n    Configuration conf = getConf();\r\n    Path stagingDir = JobSubmissionFiles.getStagingDir(new Cluster(conf), conf);\r\n    stagingDir.getFileSystem(conf).mkdirs(stagingDir);\r\n    Path soure = createFile(\"tmp.txt\");\r\n    Path target = createFile(\"target.txt\");\r\n    try {\r\n        String[] arg = { target.toString(), soure.toString() };\r\n        DistCp.main(arg);\r\n        Assert.fail();\r\n    } catch (ExitException t) {\r\n        Assert.assertTrue(fs.exists(target));\r\n        Assert.assertEquals(t.status, 0);\r\n        Assert.assertEquals(stagingDir.getFileSystem(conf).listStatus(stagingDir).length, 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCleanupOfJob",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testCleanupOfJob() throws Exception\n{\r\n    Configuration conf = getConf();\r\n    Path stagingDir = JobSubmissionFiles.getStagingDir(new Cluster(conf), conf);\r\n    stagingDir.getFileSystem(conf).mkdirs(stagingDir);\r\n    Path soure = createFile(\"tmp.txt\");\r\n    Path target = createFile(\"target.txt\");\r\n    DistCp distcp = mock(DistCp.class);\r\n    Job job = spy(Job.class);\r\n    Mockito.when(distcp.getConf()).thenReturn(conf);\r\n    Mockito.when(distcp.execute()).thenReturn(job);\r\n    Mockito.when(distcp.run(Mockito.any())).thenCallRealMethod();\r\n    String[] arg = { soure.toString(), target.toString() };\r\n    distcp.run(arg);\r\n    Mockito.verify(job, times(1)).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    cluster = new MiniDFSCluster.Builder(new Configuration()).build();\r\n    createSourceData();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createSourceData",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void createSourceData() throws Exception\n{\r\n    mkdirs(\"/tmp/source/1\");\r\n    mkdirs(\"/tmp/source/2\");\r\n    mkdirs(\"/tmp/source/2/3\");\r\n    mkdirs(\"/tmp/source/2/3/4\");\r\n    mkdirs(\"/tmp/source/5\");\r\n    touchFile(\"/tmp/source/5/6\");\r\n    mkdirs(\"/tmp/source/7\");\r\n    mkdirs(\"/tmp/source/7/8\");\r\n    touchFile(\"/tmp/source/7/8/9\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void mkdirs(String path) throws Exception\n{\r\n    FileSystem fileSystem = null;\r\n    try {\r\n        fileSystem = cluster.getFileSystem();\r\n        fileSystem.mkdirs(new Path(path));\r\n        recordInExpectedValues(path);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(null, fileSystem);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "touchFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void touchFile(String path) throws Exception\n{\r\n    FileSystem fileSystem = null;\r\n    DataOutputStream outputStream = null;\r\n    try {\r\n        fileSystem = cluster.getFileSystem();\r\n        outputStream = fileSystem.create(new Path(path), true, 0);\r\n        recordInExpectedValues(path);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(null, fileSystem, outputStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "recordInExpectedValues",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void recordInExpectedValues(String path) throws Exception\n{\r\n    FileSystem fileSystem = cluster.getFileSystem();\r\n    Path sourcePath = new Path(fileSystem.getUri().toString() + path);\r\n    expectedValues.put(sourcePath.toString(), DistCpUtils.getRelativePath(new Path(\"/tmp/source\"), sourcePath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testRun",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRun() throws Exception\n{\r\n    final URI uri = cluster.getFileSystem().getUri();\r\n    final String pathString = uri.toString();\r\n    Path fileSystemPath = new Path(pathString);\r\n    Path source = new Path(fileSystemPath.toString() + \"/tmp/source\");\r\n    Path target = new Path(fileSystemPath.toString() + \"/tmp/target\");\r\n    Path listingPath = new Path(fileSystemPath.toString() + \"/tmp/META/fileList.seq\");\r\n    DistCpOptions options = new DistCpOptions.Builder(Collections.singletonList(source), target).build();\r\n    DistCpContext context = new DistCpContext(options);\r\n    context.setTargetPathExists(false);\r\n    new GlobbedCopyListing(new Configuration(), CREDENTIALS).buildListing(listingPath, context);\r\n    verifyContents(listingPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "verifyContents",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void verifyContents(Path listingPath) throws Exception\n{\r\n    SequenceFile.Reader reader = new SequenceFile.Reader(cluster.getFileSystem(), listingPath, new Configuration());\r\n    Text key = new Text();\r\n    CopyListingFileStatus value = new CopyListingFileStatus();\r\n    Map<String, String> actualValues = new HashMap<String, String>();\r\n    while (reader.next(key, value)) {\r\n        if (value.isDirectory() && key.toString().equals(\"\")) {\r\n            continue;\r\n        }\r\n        actualValues.put(value.getPath().toString(), key.toString());\r\n    }\r\n    Assert.assertEquals(expectedValues.size(), actualValues.size());\r\n    for (Map.Entry<String, String> entry : actualValues.entrySet()) {\r\n        Assert.assertEquals(entry.getValue(), expectedValues.get(entry.getKey()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testSetCommitDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSetCommitDirectory()\n{\r\n    try {\r\n        Job job = Job.getInstance(new Configuration());\r\n        Assert.assertEquals(null, CopyOutputFormat.getCommitDirectory(job));\r\n        job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, \"\");\r\n        Assert.assertEquals(null, CopyOutputFormat.getCommitDirectory(job));\r\n        Path directory = new Path(\"/tmp/test\");\r\n        CopyOutputFormat.setCommitDirectory(job, directory);\r\n        Assert.assertEquals(directory, CopyOutputFormat.getCommitDirectory(job));\r\n        Assert.assertEquals(directory.toString(), job.getConfiguration().get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while running test\", e);\r\n        Assert.fail(\"Failed while testing for set Commit Directory\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testSetWorkingDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSetWorkingDirectory()\n{\r\n    try {\r\n        Job job = Job.getInstance(new Configuration());\r\n        Assert.assertEquals(null, CopyOutputFormat.getWorkingDirectory(job));\r\n        job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, \"\");\r\n        Assert.assertEquals(null, CopyOutputFormat.getWorkingDirectory(job));\r\n        Path directory = new Path(\"/tmp/test\");\r\n        CopyOutputFormat.setWorkingDirectory(job, directory);\r\n        Assert.assertEquals(directory, CopyOutputFormat.getWorkingDirectory(job));\r\n        Assert.assertEquals(directory.toString(), job.getConfiguration().get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while running test\", e);\r\n        Assert.fail(\"Failed while testing for set Working Directory\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testGetOutputCommitter",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetOutputCommitter()\n{\r\n    try {\r\n        TaskAttemptContext context = new TaskAttemptContextImpl(new Configuration(), new TaskAttemptID(\"200707121733\", 1, TaskType.MAP, 1, 1));\r\n        context.getConfiguration().set(\"mapred.output.dir\", \"/out\");\r\n        Assert.assertTrue(new CopyOutputFormat().getOutputCommitter(context) instanceof CopyCommitter);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        Assert.fail(\"Unable to get output committer\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testCheckOutputSpecs",
  "errType" : [ "IOException", "InterruptedException", "IllegalStateException", "IllegalStateException", "IllegalStateException", "IllegalStateException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testCheckOutputSpecs()\n{\r\n    try {\r\n        OutputFormat outputFormat = new CopyOutputFormat();\r\n        Job job = Job.getInstance(new Configuration());\r\n        JobID jobID = new JobID(\"200707121733\", 1);\r\n        try {\r\n            JobContext context = new JobContextImpl(job.getConfiguration(), jobID);\r\n            outputFormat.checkOutputSpecs(context);\r\n            Assert.fail(\"No checking for invalid work/commit path\");\r\n        } catch (IllegalStateException ignore) {\r\n        }\r\n        CopyOutputFormat.setWorkingDirectory(job, new Path(\"/tmp/work\"));\r\n        try {\r\n            JobContext context = new JobContextImpl(job.getConfiguration(), jobID);\r\n            outputFormat.checkOutputSpecs(context);\r\n            Assert.fail(\"No checking for invalid commit path\");\r\n        } catch (IllegalStateException ignore) {\r\n        }\r\n        job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, \"\");\r\n        CopyOutputFormat.setCommitDirectory(job, new Path(\"/tmp/commit\"));\r\n        try {\r\n            JobContext context = new JobContextImpl(job.getConfiguration(), jobID);\r\n            outputFormat.checkOutputSpecs(context);\r\n            Assert.fail(\"No checking for invalid work path\");\r\n        } catch (IllegalStateException ignore) {\r\n        }\r\n        CopyOutputFormat.setWorkingDirectory(job, new Path(\"/tmp/work\"));\r\n        CopyOutputFormat.setCommitDirectory(job, new Path(\"/tmp/commit\"));\r\n        try {\r\n            JobContext context = new JobContextImpl(job.getConfiguration(), jobID);\r\n            outputFormat.checkOutputSpecs(context);\r\n        } catch (IllegalStateException ignore) {\r\n            Assert.fail(\"Output spec check failed.\");\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Exception encountered while testing checkoutput specs\", e);\r\n        Assert.fail(\"Checkoutput Spec failure\");\r\n    } catch (InterruptedException e) {\r\n        LOG.error(\"Exception encountered while testing checkoutput specs\", e);\r\n        Assert.fail(\"Checkoutput Spec failure\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup()\n{\r\n    tracker = new DeletedDirTracker(1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown()\n{\r\n    LOG.info(tracker.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testNoRootDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoRootDir() throws Throwable\n{\r\n    shouldDelete(ROOT, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testNoRootFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNoRootFile() throws Throwable\n{\r\n    shouldDelete(dirStatus(ROOT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testFileInRootDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFileInRootDir() throws Throwable\n{\r\n    expectShouldDelete(FILE0, false);\r\n    expectShouldDelete(FILE0, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteDir1",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDeleteDir1() throws Throwable\n{\r\n    expectShouldDelete(DIR1, true);\r\n    expectShouldNotDelete(DIR1, true);\r\n    expectShouldNotDelete(DIR1_FILE1, false);\r\n    expectNotCached(DIR1_FILE1);\r\n    expectShouldNotDelete(DIR1_DIR3, true);\r\n    expectCached(DIR1_DIR3);\r\n    expectShouldNotDelete(DIR1_FILE2, false);\r\n    expectShouldNotDelete(DIR1_DIR3_DIR4_FILE_3, false);\r\n    expectShouldNotDelete(DIR1_DIR3_DIR4, true);\r\n    expectShouldNotDelete(DIR1_DIR3_DIR4, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteDirDeep",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDeleteDirDeep() throws Throwable\n{\r\n    expectShouldDelete(DIR1, true);\r\n    expectShouldNotDelete(DIR1_DIR3_DIR4_FILE_3, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeletePerfectCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDeletePerfectCache() throws Throwable\n{\r\n    List<CopyListingFileStatus> statusList = buildStatusList();\r\n    tracker = new DeletedDirTracker(statusList.size());\r\n    AtomicInteger deletedFiles = new AtomicInteger(0);\r\n    AtomicInteger deletedDirs = new AtomicInteger(0);\r\n    deletePaths(statusList, deletedFiles, deletedDirs);\r\n    assertEquals(0, deletedFiles.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteFullCache",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDeleteFullCache() throws Throwable\n{\r\n    AtomicInteger deletedFiles = new AtomicInteger(0);\r\n    AtomicInteger deletedDirs = new AtomicInteger(0);\r\n    deletePaths(buildStatusList(), deletedFiles, deletedDirs);\r\n    assertEquals(0, deletedFiles.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteMediumCache",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDeleteMediumCache() throws Throwable\n{\r\n    tracker = new DeletedDirTracker(100);\r\n    AtomicInteger deletedFiles = new AtomicInteger(0);\r\n    AtomicInteger deletedDirs = new AtomicInteger(0);\r\n    deletePaths(buildStatusList(), deletedFiles, deletedDirs);\r\n    assertEquals(0, deletedFiles.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "testDeleteFullSmallCache",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDeleteFullSmallCache() throws Throwable\n{\r\n    tracker = new DeletedDirTracker(10);\r\n    AtomicInteger deletedFiles = new AtomicInteger(0);\r\n    AtomicInteger deletedDirs = new AtomicInteger(0);\r\n    deletePaths(buildStatusList(), deletedFiles, deletedDirs);\r\n    assertEquals(0, deletedFiles.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "deletePaths",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void deletePaths(final List<CopyListingFileStatus> statusList, final AtomicInteger deletedFiles, final AtomicInteger deletedDirs)\n{\r\n    for (CopyListingFileStatus status : statusList) {\r\n        if (shouldDelete(status)) {\r\n            AtomicInteger r = status.isDirectory() ? deletedDirs : deletedFiles;\r\n            r.incrementAndGet();\r\n            LOG.info(\"Delete {}\", status.getPath());\r\n        }\r\n    }\r\n    LOG.info(\"After proposing to delete {} paths, {} directories and {} files\" + \" were explicitly deleted from a cache {}\", statusList.size(), deletedDirs, deletedFiles, tracker);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "buildStatusList",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<CopyListingFileStatus> buildStatusList()\n{\r\n    List<CopyListingFileStatus> statusList = new ArrayList<>();\r\n    for (int y = 0; y <= 20; y++) {\r\n        Path yp = new Path(String.format(\"YEAR=%d\", y));\r\n        statusList.add(dirStatus(yp));\r\n        for (int m = 1; m <= 12; m++) {\r\n            Path ymp = new Path(yp, String.format(\"MONTH=%d\", m));\r\n            statusList.add(dirStatus(ymp));\r\n            for (int d = 1; d < 30; d++) {\r\n                Path dir = new Path(ymp, String.format(\"DAY=%02d\", d));\r\n                statusList.add(dirStatus(dir));\r\n                for (int h = 0; h < 24; h++) {\r\n                    statusList.add(fileStatus(new Path(dir, String.format(\"%02d00.avro\", h))));\r\n                }\r\n            }\r\n        }\r\n        Collections.sort(statusList, (l, r) -> l.getPath().compareTo(r.getPath()));\r\n    }\r\n    return statusList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectShouldDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectShouldDelete(final Path path, boolean isDir)\n{\r\n    expectShouldDelete(newStatus(path, isDir));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectShouldDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectShouldDelete(CopyListingFileStatus status)\n{\r\n    assertTrue(\"Expected shouldDelete of \" + status.getPath(), shouldDelete(status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "shouldDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldDelete(final Path path, final boolean isDir)\n{\r\n    return shouldDelete(newStatus(path, isDir));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "shouldDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldDelete(final CopyListingFileStatus status)\n{\r\n    return tracker.shouldDelete(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectShouldNotDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectShouldNotDelete(final Path path, boolean isDir)\n{\r\n    expectShouldNotDelete(newStatus(path, isDir));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectShouldNotDelete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectShouldNotDelete(CopyListingFileStatus status)\n{\r\n    assertFalse(\"Expected !shouldDelete of \" + status.getPath() + \" but got true\", shouldDelete(status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "newStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CopyListingFileStatus newStatus(final Path path, final boolean isDir)\n{\r\n    return new CopyListingFileStatus(new FileStatus(0, isDir, 0, 0, 0, path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "dirStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CopyListingFileStatus dirStatus(final Path path)\n{\r\n    return newStatus(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "fileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CopyListingFileStatus fileStatus(final Path path)\n{\r\n    return newStatus(path, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectCached",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectCached(final Path path)\n{\r\n    assertTrue(\"Path \" + path + \" is not in the cache of \" + tracker, tracker.isContained(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "expectNotCached",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectNotCached(final Path path)\n{\r\n    assertFalse(\"Path \" + path + \" is in the cache of \" + tracker, tracker.isContained(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initSourcePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initSourcePath()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "lsr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> lsr(final String prefix, final FsShell shell, Path rootDir) throws Exception\n{\r\n    return lsr(prefix, shell, rootDir.toString(), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "lsrSource",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> lsrSource(final String prefix, final FsShell shell, Path rootDir) throws Exception\n{\r\n    final Path spath = isSrcNotSameAsTgt ? rootDir : new Path(rootDir.toString(), HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s1\");\r\n    return lsr(prefix, shell, spath.toString(), null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "lsr",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "List<String> lsr(final String prefix, final FsShell shell, String rootDir, String glob) throws Exception\n{\r\n    final String dir = glob == null ? rootDir : glob;\r\n    System.out.println(prefix + \" lsr root=\" + rootDir);\r\n    final ByteArrayOutputStream bytes = new ByteArrayOutputStream();\r\n    final PrintStream out = new PrintStream(bytes);\r\n    final PrintStream oldOut = System.out;\r\n    final PrintStream oldErr = System.err;\r\n    System.setOut(out);\r\n    System.setErr(out);\r\n    final String results;\r\n    try {\r\n        Assert.assertEquals(0, shell.run(new String[] { \"-lsr\", dir }));\r\n        results = bytes.toString();\r\n    } finally {\r\n        IOUtils.closeStream(out);\r\n        System.setOut(oldOut);\r\n        System.setErr(oldErr);\r\n    }\r\n    System.out.println(\"lsr results:\\n\" + results);\r\n    String dirname = rootDir;\r\n    if (rootDir.lastIndexOf(Path.SEPARATOR) != -1) {\r\n        dirname = rootDir.substring(rootDir.lastIndexOf(Path.SEPARATOR));\r\n    }\r\n    final List<String> paths = new ArrayList<String>();\r\n    for (StringTokenizer t = new StringTokenizer(results, \"\\n\"); t.hasMoreTokens(); ) {\r\n        final String s = t.nextToken();\r\n        final int i = s.indexOf(dirname);\r\n        if (i >= 0) {\r\n            paths.add(s.substring(i + dirname.length()));\r\n        }\r\n    }\r\n    Collections.sort(paths);\r\n    System.out.println(\"lsr paths = \" + paths.toString().replace(\", \", \",\\n  \"));\r\n    return paths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSource(final Path src)\n{\r\n    this.source = src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setSrcNotSameAsTgt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSrcNotSameAsTgt(final boolean srcNotSameAsTgt)\n{\r\n    isSrcNotSameAsTgt = srcNotSameAsTgt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    initSourcePath();\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNum).build();\r\n    cluster.waitActive();\r\n    dfs = cluster.getFileSystem();\r\n    if (isSrcNotSameAsTgt) {\r\n        dfs.mkdirs(source);\r\n    }\r\n    dfs.mkdirs(target);\r\n    optionsBuilder = new DistCpOptions.Builder(Arrays.asList(source), target).withSyncFolder(true).withUseRdiff(\"s2\", \"s1\");\r\n    final DistCpOptions options = optionsBuilder.build();\r\n    options.appendToConf(conf);\r\n    distCpContext = new DistCpContext(options);\r\n    conf.set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, target.toString());\r\n    conf.set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, target.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    IOUtils.cleanupWithLogger(null, dfs);\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testFallback",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testFallback() throws Exception\n{\r\n    Assert.assertFalse(sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s1\");\r\n    Assert.assertEquals(spath, distCpContext.getSourcePaths().get(0));\r\n    optionsBuilder.withSourcePaths(Arrays.asList(source));\r\n    dfs.allowSnapshot(source);\r\n    dfs.allowSnapshot(target);\r\n    Assert.assertFalse(sync());\r\n    Assert.assertEquals(spath, distCpContext.getSourcePaths().get(0));\r\n    optionsBuilder.withSourcePaths(Arrays.asList(source));\r\n    this.enableAndCreateFirstSnapshot();\r\n    dfs.createSnapshot(target, \"s2\");\r\n    Assert.assertTrue(sync());\r\n    optionsBuilder.withSourcePaths(Arrays.asList(source));\r\n    final Path subTarget = new Path(target, \"sub\");\r\n    dfs.mkdirs(subTarget);\r\n    Assert.assertFalse(sync());\r\n    Assert.assertEquals(spath, distCpContext.getSourcePaths().get(0));\r\n    optionsBuilder.withSourcePaths(Arrays.asList(source));\r\n    dfs.delete(subTarget, true);\r\n    Assert.assertTrue(sync());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "syncAndVerify",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void syncAndVerify() throws Exception\n{\r\n    final FsShell shell = new FsShell(conf);\r\n    lsrSource(\"Before sync source: \", shell, source);\r\n    lsr(\"Before sync target: \", shell, target);\r\n    Assert.assertTrue(sync());\r\n    lsrSource(\"After sync source: \", shell, source);\r\n    lsr(\"After sync target: \", shell, target);\r\n    verifyCopy(dfs.getFileStatus(source), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "sync",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean sync() throws Exception\n{\r\n    distCpContext = new DistCpContext(optionsBuilder.build());\r\n    final DistCpSync distCpSync = new DistCpSync(distCpContext, conf);\r\n    return distCpSync.sync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "enableAndCreateFirstSnapshot",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void enableAndCreateFirstSnapshot() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        dfs.allowSnapshot(source);\r\n        dfs.createSnapshot(source, \"s1\");\r\n    }\r\n    dfs.allowSnapshot(target);\r\n    dfs.createSnapshot(target, \"s1\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createSecondSnapshotAtTarget",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createSecondSnapshotAtTarget() throws Exception\n{\r\n    dfs.createSnapshot(target, \"s2\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createMiddleSnapshotAtTarget",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createMiddleSnapshotAtTarget() throws Exception\n{\r\n    dfs.createSnapshot(target, \"s1.5\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void initData(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path d1 = new Path(foo, \"d1\");\r\n    final Path f1 = new Path(foo, \"f1\");\r\n    final Path d2 = new Path(bar, \"d2\");\r\n    final Path f2 = new Path(bar, \"f2\");\r\n    final Path f3 = new Path(d1, \"f3\");\r\n    final Path f4 = new Path(d2, \"f4\");\r\n    DFSTestUtil.createFile(dfs, f1, blockSize, dataNum, 0);\r\n    DFSTestUtil.createFile(dfs, f2, blockSize, dataNum, 0);\r\n    DFSTestUtil.createFile(dfs, f3, blockSize, dataNum, 0);\r\n    DFSTestUtil.createFile(dfs, f4, blockSize, dataNum, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int changeData(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path d1 = new Path(foo, \"d1\");\r\n    final Path f2 = new Path(bar, \"f2\");\r\n    final Path bar_d1 = new Path(bar, \"d1\");\r\n    int numDeletedModified = 0;\r\n    dfs.rename(d1, bar_d1);\r\n    numDeletedModified += 1;\r\n    numDeletedModified += 1;\r\n    final Path f3 = new Path(bar_d1, \"f3\");\r\n    dfs.delete(f3, true);\r\n    numDeletedModified += 1;\r\n    final Path newfoo = new Path(bar_d1, \"foo\");\r\n    dfs.rename(foo, newfoo);\r\n    numDeletedModified += 1;\r\n    final Path f1 = new Path(newfoo, \"f1\");\r\n    dfs.delete(f1, true);\r\n    numDeletedModified += 1;\r\n    DFSTestUtil.createFile(dfs, f1, 2 * blockSize, dataNum, 0);\r\n    DFSTestUtil.appendFile(dfs, f2, (int) blockSize);\r\n    numDeletedModified += 1;\r\n    dfs.rename(bar, new Path(dir, \"foo\"));\r\n    return numDeletedModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testSync() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData(source);\r\n    }\r\n    initData(target);\r\n    enableAndCreateFirstSnapshot();\r\n    final FsShell shell = new FsShell(conf);\r\n    lsrSource(\"Before source: \", shell, source);\r\n    lsr(\"Before target: \", shell, target);\r\n    int numDeletedModified = changeData(target);\r\n    createSecondSnapshotAtTarget();\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(target, \"s2\", \"s1\");\r\n    System.out.println(report);\r\n    final DistCpSync distCpSync = new DistCpSync(distCpContext, conf);\r\n    lsr(\"Before sync target: \", shell, target);\r\n    Assert.assertTrue(distCpSync.sync());\r\n    lsr(\"After sync target: \", shell, target);\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s1\");\r\n    Assert.assertEquals(spath, distCpContext.getSourcePaths().get(0));\r\n    final Path listingPath = new Path(\"/tmp/META/fileList.seq\");\r\n    CopyListing listing = new SimpleCopyListing(conf, new Credentials(), distCpSync);\r\n    listing.buildListing(listingPath, distCpContext);\r\n    Map<Text, CopyListingFileStatus> copyListing = getListing(listingPath);\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n    context.getConfiguration().setBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), true);\r\n    copyMapper.setup(context);\r\n    for (Map.Entry<Text, CopyListingFileStatus> entry : copyListing.entrySet()) {\r\n        copyMapper.map(entry.getKey(), entry.getValue(), context);\r\n    }\r\n    lsrSource(\"After mapper source: \", shell, source);\r\n    lsr(\"After mapper target: \", shell, target);\r\n    Assert.assertEquals(numDeletedModified, copyListing.size());\r\n    Assert.assertEquals(blockSize * 3, stubContext.getReporter().getCounter(CopyMapper.Counter.BYTESCOPIED).getValue());\r\n    verifyCopy(dfs.getFileStatus(spath), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getListing",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<Text, CopyListingFileStatus> getListing(Path listingPath) throws Exception\n{\r\n    SequenceFile.Reader reader = null;\r\n    Map<Text, CopyListingFileStatus> values = new HashMap<>();\r\n    try {\r\n        reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(listingPath));\r\n        Text key = new Text();\r\n        CopyListingFileStatus value = new CopyListingFileStatus();\r\n        while (reader.next(key, value)) {\r\n            values.put(key, value);\r\n            key = new Text();\r\n            value = new CopyListingFileStatus();\r\n        }\r\n    } finally {\r\n        if (reader != null) {\r\n            reader.close();\r\n        }\r\n    }\r\n    return values;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "verifyCopy",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyCopy(FileStatus s, FileStatus t, boolean compareName) throws Exception\n{\r\n    Assert.assertEquals(s.isDirectory(), t.isDirectory());\r\n    if (compareName) {\r\n        Assert.assertEquals(s.getPath().getName(), t.getPath().getName());\r\n    }\r\n    if (!s.isDirectory()) {\r\n        byte[] sbytes = DFSTestUtil.readFileBuffer(dfs, s.getPath());\r\n        byte[] tbytes = DFSTestUtil.readFileBuffer(dfs, t.getPath());\r\n        Assert.assertArrayEquals(sbytes, tbytes);\r\n    } else {\r\n        FileStatus[] slist = dfs.listStatus(s.getPath());\r\n        FileStatus[] tlist = dfs.listStatus(t.getPath());\r\n        Assert.assertEquals(slist.length, tlist.length);\r\n        for (int i = 0; i < slist.length; i++) {\r\n            verifyCopy(slist[i], tlist[i], true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSyncWithCurrent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSyncWithCurrent() throws Exception\n{\r\n    optionsBuilder.withUseRdiff(\".\", \"s1\");\r\n    if (isSrcNotSameAsTgt) {\r\n        initData(source);\r\n    }\r\n    initData(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData(target);\r\n    Assert.assertTrue(sync());\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s1\");\r\n    Assert.assertEquals(spath, distCpContext.getSourcePaths().get(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData2",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initData2(Path dir) throws Exception\n{\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path f1 = new Path(test, \"f1\");\r\n    final Path f2 = new Path(foo, \"f2\");\r\n    final Path f3 = new Path(bar, \"f3\");\r\n    DFSTestUtil.createFile(dfs, f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, f2, blockSize, dataNum, 1L);\r\n    DFSTestUtil.createFile(dfs, f3, blockSize, dataNum, 2L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData2",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void changeData2(Path dir) throws Exception\n{\r\n    final Path tmpFoo = new Path(dir, \"tmpFoo\");\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    dfs.rename(test, tmpFoo);\r\n    dfs.rename(foo, test);\r\n    dfs.rename(bar, foo);\r\n    dfs.rename(tmpFoo, bar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync2",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSync2() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData2(source);\r\n    }\r\n    initData2(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData2(target);\r\n    createSecondSnapshotAtTarget();\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(target, \"s2\", \"s1\");\r\n    System.out.println(report);\r\n    syncAndVerify();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initData3(Path dir) throws Exception\n{\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path f1 = new Path(test, \"file\");\r\n    final Path f2 = new Path(foo, \"file\");\r\n    final Path f3 = new Path(bar, \"file\");\r\n    DFSTestUtil.createFile(dfs, f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, f2, blockSize * 2, dataNum, 1L);\r\n    DFSTestUtil.createFile(dfs, f3, blockSize * 3, dataNum, 2L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void changeData3(Path dir) throws Exception\n{\r\n    final Path test = new Path(dir, \"test\");\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path f1 = new Path(test, \"file\");\r\n    final Path f2 = new Path(foo, \"file\");\r\n    final Path f3 = new Path(bar, \"file\");\r\n    final Path newf1 = new Path(test, \"newfile\");\r\n    final Path newf2 = new Path(foo, \"newfile\");\r\n    final Path newf3 = new Path(bar, \"newfile\");\r\n    dfs.rename(f1, newf1);\r\n    dfs.rename(f2, newf2);\r\n    dfs.rename(f3, newf3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync3",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSync3() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData3(source);\r\n    }\r\n    initData3(target);\r\n    enableAndCreateFirstSnapshot();\r\n    changeData3(target);\r\n    createSecondSnapshotAtTarget();\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(target, \"s2\", \"s1\");\r\n    System.out.println(report);\r\n    syncAndVerify();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData4",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initData4(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d2 = new Path(d1, \"d2\");\r\n    final Path f1 = new Path(d2, \"f1\");\r\n    DFSTestUtil.createFile(dfs, f1, blockSize, dataNum, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData4",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int changeData4(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d11 = new Path(dir, \"d11\");\r\n    final Path d2 = new Path(d1, \"d2\");\r\n    final Path d21 = new Path(d1, \"d21\");\r\n    final Path f1 = new Path(d2, \"f1\");\r\n    int numDeletedAndModified = 0;\r\n    dfs.delete(f1, false);\r\n    numDeletedAndModified += 1;\r\n    dfs.rename(d2, d21);\r\n    numDeletedAndModified += 1;\r\n    dfs.rename(d1, d11);\r\n    numDeletedAndModified += 1;\r\n    return numDeletedAndModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync4",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSync4() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData4(source);\r\n    }\r\n    initData4(target);\r\n    enableAndCreateFirstSnapshot();\r\n    final FsShell shell = new FsShell(conf);\r\n    lsr(\"Before change target: \", shell, target);\r\n    int numDeletedAndModified = changeData4(target);\r\n    createSecondSnapshotAtTarget();\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(target, \"s2\", \"s1\");\r\n    System.out.println(report);\r\n    testAndVerify(numDeletedAndModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData5",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData5(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d2 = new Path(dir, \"d2\");\r\n    final Path f1 = new Path(d1, \"f1\");\r\n    final Path f2 = new Path(d2, \"f2\");\r\n    DFSTestUtil.createFile(dfs, f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, f2, blockSize, dataNum, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData5",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int changeData5(Path dir) throws Exception\n{\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d2 = new Path(dir, \"d2\");\r\n    final Path f1 = new Path(d1, \"f1\");\r\n    final Path tmp = new Path(dir, \"tmp\");\r\n    int numDeletedAndModified = 0;\r\n    dfs.delete(f1, false);\r\n    numDeletedAndModified += 1;\r\n    dfs.rename(d1, tmp);\r\n    numDeletedAndModified += 1;\r\n    dfs.rename(d2, d1);\r\n    numDeletedAndModified += 1;\r\n    final Path f2 = new Path(d1, \"f2\");\r\n    dfs.delete(f2, false);\r\n    numDeletedAndModified += 1;\r\n    return numDeletedAndModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync5",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSync5() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData5(source);\r\n    }\r\n    initData5(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numDeletedAndModified = changeData5(target);\r\n    createSecondSnapshotAtTarget();\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(target, \"s2\", \"s1\");\r\n    System.out.println(report);\r\n    testAndVerify(numDeletedAndModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testAndVerify",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testAndVerify(int numDeletedAndModified) throws Exception\n{\r\n    SnapshotDiffReport report = dfs.getSnapshotDiffReport(target, \"s2\", \"s1\");\r\n    System.out.println(report);\r\n    final FsShell shell = new FsShell(conf);\r\n    lsrSource(\"Before sync source: \", shell, source);\r\n    lsr(\"Before sync target: \", shell, target);\r\n    DistCpSync distCpSync = new DistCpSync(distCpContext, conf);\r\n    distCpSync.sync();\r\n    lsr(\"After sync target: \", shell, target);\r\n    final Path spath = new Path(source, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + \"s1\");\r\n    Assert.assertEquals(spath, distCpContext.getSourcePaths().get(0));\r\n    final Path listingPath = new Path(\"/tmp/META/fileList.seq\");\r\n    CopyListing listing = new SimpleCopyListing(conf, new Credentials(), distCpSync);\r\n    listing.buildListing(listingPath, distCpContext);\r\n    Map<Text, CopyListingFileStatus> copyListing = getListing(listingPath);\r\n    CopyMapper copyMapper = new CopyMapper();\r\n    StubContext stubContext = new StubContext(conf, null, 0);\r\n    Mapper<Text, CopyListingFileStatus, Text, Text>.Context context = stubContext.getContext();\r\n    context.getConfiguration().setBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), true);\r\n    copyMapper.setup(context);\r\n    for (Map.Entry<Text, CopyListingFileStatus> entry : copyListing.entrySet()) {\r\n        copyMapper.map(entry.getKey(), entry.getValue(), context);\r\n    }\r\n    Assert.assertEquals(numDeletedAndModified, copyListing.size());\r\n    lsr(\"After Copy target: \", shell, target);\r\n    verifyCopy(dfs.getFileStatus(spath), dfs.getFileStatus(target), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData6",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData6(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path bar_f1 = new Path(bar, \"f1\");\r\n    DFSTestUtil.createFile(dfs, foo_f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, bar_f1, blockSize, dataNum, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData6",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int changeData6(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path foo2 = new Path(dir, \"foo2\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    int numDeletedModified = 0;\r\n    dfs.rename(foo, foo2);\r\n    dfs.rename(bar, foo);\r\n    dfs.rename(foo2, bar);\r\n    DFSTestUtil.appendFile(dfs, foo_f1, (int) blockSize);\r\n    numDeletedModified += 1;\r\n    return numDeletedModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync6",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSync6() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData6(source);\r\n    }\r\n    initData6(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numDeletedModified = changeData6(target);\r\n    createSecondSnapshotAtTarget();\r\n    testAndVerify(numDeletedModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData7",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initData7(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path bar_f1 = new Path(bar, \"f1\");\r\n    DFSTestUtil.createFile(dfs, foo_f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, bar_f1, blockSize, dataNum, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData7",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int changeData7(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path foo2 = new Path(dir, \"foo2\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path foo2_f2 = new Path(foo2, \"f2\");\r\n    final Path foo_d1 = new Path(foo, \"d1\");\r\n    final Path foo_d1_f3 = new Path(foo_d1, \"f3\");\r\n    int numDeletedAndModified = 0;\r\n    dfs.rename(foo, foo2);\r\n    DFSTestUtil.createFile(dfs, foo_f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.appendFile(dfs, foo_f1, (int) blockSize);\r\n    dfs.rename(foo_f1, foo2_f2);\r\n    numDeletedAndModified += 1;\r\n    DFSTestUtil.createFile(dfs, foo_d1_f3, blockSize, dataNum, 0L);\r\n    return numDeletedAndModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync7",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSync7() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData7(source);\r\n    }\r\n    initData7(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numDeletedAndModified = changeData7(target);\r\n    createSecondSnapshotAtTarget();\r\n    testAndVerify(numDeletedAndModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initData8",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initData8(Path dir) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path foo_f1 = new Path(foo, \"f1\");\r\n    final Path bar_f1 = new Path(bar, \"f1\");\r\n    final Path d1_f1 = new Path(d1, \"f1\");\r\n    DFSTestUtil.createFile(dfs, foo_f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, bar_f1, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, d1_f1, blockSize, dataNum, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "changeData8",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int changeData8(Path dir, boolean createMiddleSnapshot) throws Exception\n{\r\n    final Path foo = new Path(dir, \"foo\");\r\n    final Path createdDir = new Path(dir, \"c\");\r\n    final Path d1 = new Path(dir, \"d1\");\r\n    final Path d1_f1 = new Path(d1, \"f1\");\r\n    final Path createdDir_f1 = new Path(createdDir, \"f1\");\r\n    final Path foo_f3 = new Path(foo, \"f3\");\r\n    final Path new_foo = new Path(createdDir, \"foo\");\r\n    final Path foo_f4 = new Path(foo, \"f4\");\r\n    final Path foo_d1 = new Path(foo, \"d1\");\r\n    final Path bar = new Path(dir, \"bar\");\r\n    final Path bar1 = new Path(dir, \"bar1\");\r\n    int numDeletedAndModified = 0;\r\n    DFSTestUtil.createFile(dfs, foo_f3, blockSize, dataNum, 0L);\r\n    DFSTestUtil.createFile(dfs, createdDir_f1, blockSize, dataNum, 0L);\r\n    dfs.rename(createdDir_f1, foo_f4);\r\n    dfs.rename(d1_f1, createdDir_f1);\r\n    numDeletedAndModified += 1;\r\n    if (createMiddleSnapshot) {\r\n        this.createMiddleSnapshotAtTarget();\r\n    }\r\n    dfs.rename(d1, foo_d1);\r\n    numDeletedAndModified += 1;\r\n    dfs.rename(foo, new_foo);\r\n    dfs.rename(bar, bar1);\r\n    return numDeletedAndModified;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync8",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSync8() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData8(source);\r\n    }\r\n    initData8(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numDeletedModified = changeData8(target, false);\r\n    createSecondSnapshotAtTarget();\r\n    testAndVerify(numDeletedModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSync9",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSync9() throws Exception\n{\r\n    if (isSrcNotSameAsTgt) {\r\n        initData8(source);\r\n    }\r\n    initData8(target);\r\n    enableAndCreateFirstSnapshot();\r\n    int numDeletedModified = changeData8(target, true);\r\n    createSecondSnapshotAtTarget();\r\n    testAndVerify(numDeletedModified);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
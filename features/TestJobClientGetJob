[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTempFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path createTempFile(String filename, String contents) throws IOException\n{\r\n    Path path = new Path(TEST_ROOT_DIR, filename);\r\n    Configuration conf = new Configuration();\r\n    FSDataOutputStream os = FileSystem.getLocal(conf).create(path);\r\n    os.writeBytes(contents);\r\n    os.close();\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetRunningJobFromJobClient",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetRunningJobFromJobClient() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    conf.set(\"mapreduce.framework.name\", \"local\");\r\n    FileInputFormat.addInputPath(conf, createTempFile(\"in\", \"hello\"));\r\n    Path outputDir = new Path(TEST_ROOT_DIR, getClass().getSimpleName());\r\n    outputDir.getFileSystem(conf).delete(outputDir, true);\r\n    FileOutputFormat.setOutputPath(conf, outputDir);\r\n    JobClient jc = new JobClient(conf);\r\n    RunningJob runningJob = jc.submitJob(conf);\r\n    assertNotNull(\"Running job\", runningJob);\r\n    RunningJob newRunningJob = jc.getJob(runningJob.getID());\r\n    assertNotNull(\"New running job\", newRunningJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testJobId",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testJobId()\n{\r\n    long ts1 = 1315890136000l;\r\n    long ts2 = 1315890136001l;\r\n    JobId j1 = createJobId(ts1, 2);\r\n    JobId j2 = createJobId(ts1, 1);\r\n    JobId j3 = createJobId(ts2, 1);\r\n    JobId j4 = createJobId(ts1, 2);\r\n    assertTrue(j1.equals(j4));\r\n    assertFalse(j1.equals(j2));\r\n    assertFalse(j1.equals(j3));\r\n    assertTrue(j1.compareTo(j4) == 0);\r\n    assertTrue(j1.compareTo(j2) > 0);\r\n    assertTrue(j1.compareTo(j3) < 0);\r\n    assertTrue(j1.hashCode() == j4.hashCode());\r\n    assertFalse(j1.hashCode() == j2.hashCode());\r\n    assertFalse(j1.hashCode() == j3.hashCode());\r\n    JobId j5 = createJobId(ts1, 231415);\r\n    assertEquals(\"job_\" + ts1 + \"_0002\", j1.toString());\r\n    assertEquals(\"job_\" + ts1 + \"_231415\", j5.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testTaskId",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTaskId()\n{\r\n    long ts1 = 1315890136000l;\r\n    long ts2 = 1315890136001l;\r\n    TaskId t1 = createTaskId(ts1, 1, 2, TaskType.MAP);\r\n    TaskId t2 = createTaskId(ts1, 1, 2, TaskType.REDUCE);\r\n    TaskId t3 = createTaskId(ts1, 1, 1, TaskType.MAP);\r\n    TaskId t4 = createTaskId(ts1, 1, 2, TaskType.MAP);\r\n    TaskId t5 = createTaskId(ts2, 1, 1, TaskType.MAP);\r\n    assertTrue(t1.equals(t4));\r\n    assertFalse(t1.equals(t2));\r\n    assertFalse(t1.equals(t3));\r\n    assertFalse(t1.equals(t5));\r\n    assertTrue(t1.compareTo(t4) == 0);\r\n    assertTrue(t1.compareTo(t2) < 0);\r\n    assertTrue(t1.compareTo(t3) > 0);\r\n    assertTrue(t1.compareTo(t5) < 0);\r\n    assertTrue(t1.hashCode() == t4.hashCode());\r\n    assertFalse(t1.hashCode() == t2.hashCode());\r\n    assertFalse(t1.hashCode() == t3.hashCode());\r\n    assertFalse(t1.hashCode() == t5.hashCode());\r\n    TaskId t6 = createTaskId(ts1, 324151, 54643747, TaskType.REDUCE);\r\n    assertEquals(\"task_\" + ts1 + \"_0001_m_000002\", t1.toString());\r\n    assertEquals(\"task_\" + ts1 + \"_324151_r_54643747\", t6.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testTaskAttemptId()\n{\r\n    long ts1 = 1315890136000l;\r\n    long ts2 = 1315890136001l;\r\n    TaskAttemptId t1 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 2);\r\n    TaskAttemptId t2 = createTaskAttemptId(ts1, 2, 2, TaskType.REDUCE, 2);\r\n    TaskAttemptId t3 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 3);\r\n    TaskAttemptId t4 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 1);\r\n    TaskAttemptId t5 = createTaskAttemptId(ts1, 2, 1, TaskType.MAP, 3);\r\n    TaskAttemptId t6 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 2);\r\n    assertTrue(t1.equals(t6));\r\n    assertFalse(t1.equals(t2));\r\n    assertFalse(t1.equals(t3));\r\n    assertFalse(t1.equals(t5));\r\n    assertTrue(t1.compareTo(t6) == 0);\r\n    assertTrue(t1.compareTo(t2) < 0);\r\n    assertTrue(t1.compareTo(t3) < 0);\r\n    assertTrue(t1.compareTo(t4) > 0);\r\n    assertTrue(t1.compareTo(t5) > 0);\r\n    assertTrue(t1.hashCode() == t6.hashCode());\r\n    assertFalse(t1.hashCode() == t2.hashCode());\r\n    assertFalse(t1.hashCode() == t3.hashCode());\r\n    assertFalse(t1.hashCode() == t5.hashCode());\r\n    TaskAttemptId t7 = createTaskAttemptId(ts2, 5463346, 4326575, TaskType.REDUCE, 54375);\r\n    assertEquals(\"attempt_\" + ts1 + \"_0002_m_000002_2\", t1.toString());\r\n    assertEquals(\"attempt_\" + ts2 + \"_5463346_r_4326575_54375\", t7.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "createJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobId createJobId(long clusterTimestamp, int idInt)\n{\r\n    return MRBuilderUtils.newJobId(ApplicationId.newInstance(clusterTimestamp, idInt), idInt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "createTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskId createTaskId(long clusterTimestamp, int jobIdInt, int taskIdInt, TaskType taskType)\n{\r\n    return MRBuilderUtils.newTaskId(createJobId(clusterTimestamp, jobIdInt), taskIdInt, taskType);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "createTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptId createTaskAttemptId(long clusterTimestamp, int jobIdInt, int taskIdInt, TaskType taskType, int taskAttemptIdInt)\n{\r\n    return MRBuilderUtils.newTaskAttemptId(createTaskId(clusterTimestamp, jobIdInt, taskIdInt, taskType), taskAttemptIdInt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNewApis",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testNewApis() throws Exception\n{\r\n    Random r = new Random(System.currentTimeMillis());\r\n    Path tmpBaseDir = new Path(\"/tmp/wc-\" + r.nextInt());\r\n    final Path inDir = new Path(tmpBaseDir, \"input\");\r\n    final Path outDir = new Path(tmpBaseDir, \"output\");\r\n    String input = \"The quick brown fox\\nhas many silly\\nred fox sox\\n\";\r\n    FileSystem inFs = inDir.getFileSystem(conf);\r\n    FileSystem outFs = outDir.getFileSystem(conf);\r\n    outFs.delete(outDir, true);\r\n    if (!inFs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    {\r\n        DataOutputStream file = inFs.create(new Path(inDir, \"part-0\"));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    Job job = Job.getInstance(conf, \"word count\");\r\n    job.setJarByClass(TestLocalModeWithNewApis.class);\r\n    job.setMapperClass(TokenizerMapper.class);\r\n    job.setCombinerClass(IntSumReducer.class);\r\n    job.setReducerClass(IntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    FileInputFormat.addInputPath(job, inDir);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    assertTrue(job.waitForCompletion(true));\r\n    String output = readOutput(outDir, conf);\r\n    assertEquals(\"The\\t1\\nbrown\\t1\\nfox\\t2\\nhas\\t1\\nmany\\t1\\n\" + \"quick\\t1\\nred\\t1\\nsilly\\t1\\nsox\\t1\\n\", output);\r\n    outFs.delete(tmpBaseDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String readOutput(Path outDir, Configuration conf) throws IOException\n{\r\n    FileSystem fs = outDir.getFileSystem(conf);\r\n    StringBuffer result = new StringBuffer();\r\n    Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    for (Path outputFile : fileList) {\r\n        LOG.info(\"Path\" + \": \" + outputFile);\r\n        BufferedReader file = new BufferedReader(new InputStreamReader(fs.open(outputFile)));\r\n        String line = file.readLine();\r\n        while (line != null) {\r\n            result.append(line);\r\n            result.append(\"\\n\");\r\n            line = file.readLine();\r\n        }\r\n        file.close();\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testEnums",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testEnums() throws Exception\n{\r\n    for (YarnApplicationState applicationState : YarnApplicationState.values()) {\r\n        TypeConverter.fromYarn(applicationState, FinalApplicationStatus.FAILED);\r\n    }\r\n    Assert.assertEquals(State.PREP, TypeConverter.fromYarn(YarnApplicationState.NEW_SAVING, FinalApplicationStatus.FAILED));\r\n    for (TaskType taskType : TaskType.values()) {\r\n        TypeConverter.fromYarn(taskType);\r\n    }\r\n    for (JobState jobState : JobState.values()) {\r\n        TypeConverter.fromYarn(jobState);\r\n    }\r\n    for (QueueState queueState : QueueState.values()) {\r\n        TypeConverter.fromYarn(queueState);\r\n    }\r\n    for (TaskState taskState : TaskState.values()) {\r\n        TypeConverter.fromYarn(taskState);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testFromYarn",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testFromYarn() throws Exception\n{\r\n    int appStartTime = 612354;\r\n    int appFinishTime = 612355;\r\n    YarnApplicationState state = YarnApplicationState.RUNNING;\r\n    ApplicationId applicationId = ApplicationId.newInstance(0, 0);\r\n    ApplicationReport applicationReport = Records.newRecord(ApplicationReport.class);\r\n    applicationReport.setApplicationId(applicationId);\r\n    applicationReport.setYarnApplicationState(state);\r\n    applicationReport.setStartTime(appStartTime);\r\n    applicationReport.setFinishTime(appFinishTime);\r\n    applicationReport.setUser(\"TestTypeConverter-user\");\r\n    applicationReport.setPriority(Priority.newInstance(3));\r\n    ApplicationResourceUsageReport appUsageRpt = Records.newRecord(ApplicationResourceUsageReport.class);\r\n    Resource r = Records.newRecord(Resource.class);\r\n    r.setMemorySize(2048);\r\n    appUsageRpt.setNeededResources(r);\r\n    appUsageRpt.setNumReservedContainers(1);\r\n    appUsageRpt.setNumUsedContainers(3);\r\n    appUsageRpt.setReservedResources(r);\r\n    appUsageRpt.setUsedResources(r);\r\n    applicationReport.setApplicationResourceUsageReport(appUsageRpt);\r\n    JobStatus jobStatus = TypeConverter.fromYarn(applicationReport, \"dummy-jobfile\");\r\n    Assert.assertEquals(appStartTime, jobStatus.getStartTime());\r\n    Assert.assertEquals(appFinishTime, jobStatus.getFinishTime());\r\n    Assert.assertEquals(state.toString(), jobStatus.getState().toString());\r\n    Assert.assertEquals(JobPriority.NORMAL, jobStatus.getPriority());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testFromYarnApplicationReport",
  "errType" : [ "NullPointerException" ],
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testFromYarnApplicationReport()\n{\r\n    ApplicationId mockAppId = mock(ApplicationId.class);\r\n    when(mockAppId.getClusterTimestamp()).thenReturn(12345L);\r\n    when(mockAppId.getId()).thenReturn(6789);\r\n    ApplicationReport mockReport = mock(ApplicationReport.class);\r\n    when(mockReport.getTrackingUrl()).thenReturn(\"dummy-tracking-url\");\r\n    when(mockReport.getApplicationId()).thenReturn(mockAppId);\r\n    when(mockReport.getYarnApplicationState()).thenReturn(YarnApplicationState.KILLED);\r\n    when(mockReport.getUser()).thenReturn(\"dummy-user\");\r\n    when(mockReport.getQueue()).thenReturn(\"dummy-queue\");\r\n    when(mockReport.getPriority()).thenReturn(Priority.newInstance(4));\r\n    String jobFile = \"dummy-path/job.xml\";\r\n    try {\r\n        JobStatus status = TypeConverter.fromYarn(mockReport, jobFile);\r\n    } catch (NullPointerException npe) {\r\n        Assert.fail(\"Type converstion from YARN fails for jobs without \" + \"ApplicationUsageReport\");\r\n    }\r\n    ApplicationResourceUsageReport appUsageRpt = Records.newRecord(ApplicationResourceUsageReport.class);\r\n    Resource r = Records.newRecord(Resource.class);\r\n    r.setMemorySize(2048);\r\n    appUsageRpt.setNeededResources(r);\r\n    appUsageRpt.setNumReservedContainers(1);\r\n    appUsageRpt.setNumUsedContainers(3);\r\n    appUsageRpt.setReservedResources(r);\r\n    appUsageRpt.setUsedResources(r);\r\n    when(mockReport.getApplicationResourceUsageReport()).thenReturn(appUsageRpt);\r\n    JobStatus status = TypeConverter.fromYarn(mockReport, jobFile);\r\n    Assert.assertNotNull(\"fromYarn returned null status\", status);\r\n    Assert.assertEquals(\"jobFile set incorrectly\", \"dummy-path/job.xml\", status.getJobFile());\r\n    Assert.assertEquals(\"queue set incorrectly\", \"dummy-queue\", status.getQueue());\r\n    Assert.assertEquals(\"trackingUrl set incorrectly\", \"dummy-tracking-url\", status.getTrackingUrl());\r\n    Assert.assertEquals(\"user set incorrectly\", \"dummy-user\", status.getUsername());\r\n    Assert.assertEquals(\"schedulingInfo set incorrectly\", \"dummy-tracking-url\", status.getSchedulingInfo());\r\n    Assert.assertEquals(\"jobId set incorrectly\", 6789, status.getJobID().getId());\r\n    Assert.assertEquals(\"state set incorrectly\", JobStatus.State.KILLED, status.getState());\r\n    Assert.assertEquals(\"needed mem info set incorrectly\", 2048, status.getNeededMem());\r\n    Assert.assertEquals(\"num rsvd slots info set incorrectly\", 1, status.getNumReservedSlots());\r\n    Assert.assertEquals(\"num used slots info set incorrectly\", 3, status.getNumUsedSlots());\r\n    Assert.assertEquals(\"rsvd mem info set incorrectly\", 2048, status.getReservedMem());\r\n    Assert.assertEquals(\"used mem info set incorrectly\", 2048, status.getUsedMem());\r\n    Assert.assertEquals(\"priority set incorrectly\", JobPriority.HIGH, status.getPriority());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testFromYarnQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFromYarnQueueInfo()\n{\r\n    org.apache.hadoop.yarn.api.records.QueueInfo queueInfo = Records.newRecord(org.apache.hadoop.yarn.api.records.QueueInfo.class);\r\n    queueInfo.setQueueState(org.apache.hadoop.yarn.api.records.QueueState.STOPPED);\r\n    org.apache.hadoop.mapreduce.QueueInfo returned = TypeConverter.fromYarn(queueInfo, new Configuration());\r\n    Assert.assertEquals(\"queueInfo translation didn't work.\", returned.getState().toString(), StringUtils.toLowerCase(queueInfo.getQueueState().toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testFromYarnQueue",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFromYarnQueue()\n{\r\n    org.apache.hadoop.yarn.api.records.QueueInfo child = Mockito.mock(org.apache.hadoop.yarn.api.records.QueueInfo.class);\r\n    Mockito.when(child.getQueueState()).thenReturn(QueueState.RUNNING);\r\n    org.apache.hadoop.yarn.api.records.QueueInfo queueInfo = Mockito.mock(org.apache.hadoop.yarn.api.records.QueueInfo.class);\r\n    List<org.apache.hadoop.yarn.api.records.QueueInfo> children = new ArrayList<org.apache.hadoop.yarn.api.records.QueueInfo>();\r\n    children.add(child);\r\n    Mockito.when(queueInfo.getChildQueues()).thenReturn(children);\r\n    Mockito.when(queueInfo.getQueueState()).thenReturn(QueueState.RUNNING);\r\n    org.apache.hadoop.mapreduce.QueueInfo returned = TypeConverter.fromYarn(queueInfo, new Configuration());\r\n    assertThat(returned.getQueueChildren().size()).withFailMessage(\"QueueInfo children weren't properly converted\").isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testFromYarnJobReport",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testFromYarnJobReport() throws Exception\n{\r\n    int jobStartTime = 612354;\r\n    int jobFinishTime = 612355;\r\n    JobState state = JobState.RUNNING;\r\n    JobId jobId = Records.newRecord(JobId.class);\r\n    JobReport jobReport = Records.newRecord(JobReport.class);\r\n    ApplicationId applicationId = ApplicationId.newInstance(0, 0);\r\n    jobId.setAppId(applicationId);\r\n    jobId.setId(0);\r\n    jobReport.setJobId(jobId);\r\n    jobReport.setJobState(state);\r\n    jobReport.setStartTime(jobStartTime);\r\n    jobReport.setFinishTime(jobFinishTime);\r\n    jobReport.setUser(\"TestTypeConverter-user\");\r\n    jobReport.setJobPriority(Priority.newInstance(0));\r\n    JobStatus jobStatus = TypeConverter.fromYarn(jobReport, \"dummy-jobfile\");\r\n    Assert.assertEquals(jobStartTime, jobStatus.getStartTime());\r\n    Assert.assertEquals(jobFinishTime, jobStatus.getFinishTime());\r\n    Assert.assertEquals(state.toString(), jobStatus.getState().toString());\r\n    Assert.assertEquals(JobPriority.DEFAULT, jobStatus.getPriority());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testEncodingDecodingEquivalence",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testEncodingDecodingEquivalence() throws IOException\n{\r\n    JobIndexInfo info = new JobIndexInfo();\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    info.setJobId(jobId);\r\n    info.setSubmitTime(Long.parseLong(SUBMIT_TIME));\r\n    info.setUser(USER_NAME);\r\n    info.setJobName(JOB_NAME);\r\n    info.setFinishTime(Long.parseLong(FINISH_TIME));\r\n    info.setNumMaps(Integer.parseInt(NUM_MAPS));\r\n    info.setNumReduces(Integer.parseInt(NUM_REDUCES));\r\n    info.setJobStatus(JOB_STATUS);\r\n    info.setQueueName(QUEUE_NAME);\r\n    info.setJobStartTime(Long.parseLong(JOB_START_TIME));\r\n    String jobHistoryFile = FileNameIndexUtils.getDoneFileName(info);\r\n    JobIndexInfo parsedInfo = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(\"Job id different after encoding and decoding\", info.getJobId(), parsedInfo.getJobId());\r\n    Assert.assertEquals(\"Submit time different after encoding and decoding\", info.getSubmitTime(), parsedInfo.getSubmitTime());\r\n    Assert.assertEquals(\"User different after encoding and decoding\", info.getUser(), parsedInfo.getUser());\r\n    Assert.assertEquals(\"Job name different after encoding and decoding\", info.getJobName(), parsedInfo.getJobName());\r\n    Assert.assertEquals(\"Finish time different after encoding and decoding\", info.getFinishTime(), parsedInfo.getFinishTime());\r\n    Assert.assertEquals(\"Num maps different after encoding and decoding\", info.getNumMaps(), parsedInfo.getNumMaps());\r\n    Assert.assertEquals(\"Num reduces different after encoding and decoding\", info.getNumReduces(), parsedInfo.getNumReduces());\r\n    Assert.assertEquals(\"Job status different after encoding and decoding\", info.getJobStatus(), parsedInfo.getJobStatus());\r\n    Assert.assertEquals(\"Queue name different after encoding and decoding\", info.getQueueName(), parsedInfo.getQueueName());\r\n    Assert.assertEquals(\"Job start time different after encoding and decoding\", info.getJobStartTime(), parsedInfo.getJobStartTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testUserNamePercentEncoding",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testUserNamePercentEncoding() throws IOException\n{\r\n    JobIndexInfo info = new JobIndexInfo();\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    info.setJobId(jobId);\r\n    info.setSubmitTime(Long.parseLong(SUBMIT_TIME));\r\n    info.setUser(USER_NAME_WITH_DELIMITER);\r\n    info.setJobName(JOB_NAME);\r\n    info.setFinishTime(Long.parseLong(FINISH_TIME));\r\n    info.setNumMaps(Integer.parseInt(NUM_MAPS));\r\n    info.setNumReduces(Integer.parseInt(NUM_REDUCES));\r\n    info.setJobStatus(JOB_STATUS);\r\n    info.setQueueName(QUEUE_NAME);\r\n    info.setJobStartTime(Long.parseLong(JOB_START_TIME));\r\n    String jobHistoryFile = FileNameIndexUtils.getDoneFileName(info);\r\n    Assert.assertTrue(\"User name not encoded correctly into job history file\", jobHistoryFile.contains(USER_NAME_WITH_DELIMITER_ESCAPE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testTrimJobName",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTrimJobName() throws IOException\n{\r\n    int jobNameTrimLength = 5;\r\n    JobIndexInfo info = new JobIndexInfo();\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    info.setJobId(jobId);\r\n    info.setSubmitTime(Long.parseLong(SUBMIT_TIME));\r\n    info.setUser(USER_NAME);\r\n    info.setJobName(JOB_NAME);\r\n    info.setFinishTime(Long.parseLong(FINISH_TIME));\r\n    info.setNumMaps(Integer.parseInt(NUM_MAPS));\r\n    info.setNumReduces(Integer.parseInt(NUM_REDUCES));\r\n    info.setJobStatus(JOB_STATUS);\r\n    info.setQueueName(QUEUE_NAME);\r\n    info.setJobStartTime(Long.parseLong(JOB_START_TIME));\r\n    String jobHistoryFile = FileNameIndexUtils.getDoneFileName(info, jobNameTrimLength);\r\n    JobIndexInfo parsedInfo = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(\"Job name did not get trimmed correctly\", info.getJobName().substring(0, jobNameTrimLength), parsedInfo.getJobName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testJobNameWithMultibyteChars",
  "errType" : null,
  "containingMethodsNum" : 60,
  "sourceCodeText" : "void testJobNameWithMultibyteChars() throws IOException\n{\r\n    JobIndexInfo info = new JobIndexInfo();\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    info.setJobId(jobId);\r\n    info.setSubmitTime(Long.parseLong(SUBMIT_TIME));\r\n    info.setUser(USER_NAME);\r\n    StringBuilder sb = new StringBuilder();\r\n    info.setFinishTime(Long.parseLong(FINISH_TIME));\r\n    info.setNumMaps(Integer.parseInt(NUM_MAPS));\r\n    info.setNumReduces(Integer.parseInt(NUM_REDUCES));\r\n    info.setJobStatus(JOB_STATUS);\r\n    info.setQueueName(QUEUE_NAME);\r\n    info.setJobStartTime(Long.parseLong(JOB_START_TIME));\r\n    for (int i = 0; i < 100; i++) {\r\n        sb.append('%');\r\n    }\r\n    String longJobName = sb.toString();\r\n    info.setJobName(longJobName);\r\n    String jobHistoryFile = FileNameIndexUtils.getDoneFileName(info, 50);\r\n    Assert.assertTrue(jobHistoryFile.length() <= 255);\r\n    String trimedJobName = jobHistoryFile.split(FileNameIndexUtils.DELIMITER)[3];\r\n    Assert.assertEquals(48, trimedJobName.getBytes(UTF_8).length);\r\n    byte[] trimedJobNameInByte = trimedJobName.getBytes(UTF_8);\r\n    String reEncodedTrimedJobName = new String(trimedJobNameInByte, UTF_8);\r\n    Assert.assertArrayEquals(trimedJobNameInByte, reEncodedTrimedJobName.getBytes(UTF_8));\r\n    sb.setLength(0);\r\n    for (int i = 0; i < 100; i++) {\r\n        sb.append('\\u03A9');\r\n    }\r\n    longJobName = sb.toString();\r\n    info.setJobName(longJobName);\r\n    jobHistoryFile = FileNameIndexUtils.getDoneFileName(info, 27);\r\n    Assert.assertTrue(jobHistoryFile.length() <= 255);\r\n    trimedJobName = jobHistoryFile.split(FileNameIndexUtils.DELIMITER)[3];\r\n    Assert.assertEquals(24, trimedJobName.getBytes(UTF_8).length);\r\n    trimedJobNameInByte = trimedJobName.getBytes(UTF_8);\r\n    reEncodedTrimedJobName = new String(trimedJobNameInByte, UTF_8);\r\n    Assert.assertArrayEquals(trimedJobNameInByte, reEncodedTrimedJobName.getBytes(UTF_8));\r\n    sb.setLength(0);\r\n    for (int i = 0; i < 100; i++) {\r\n        sb.append('\\u2192');\r\n    }\r\n    longJobName = sb.toString();\r\n    info.setJobName(longJobName);\r\n    jobHistoryFile = FileNameIndexUtils.getDoneFileName(info, 40);\r\n    Assert.assertTrue(jobHistoryFile.length() <= 255);\r\n    trimedJobName = jobHistoryFile.split(FileNameIndexUtils.DELIMITER)[3];\r\n    Assert.assertEquals(36, trimedJobName.getBytes(UTF_8).length);\r\n    trimedJobNameInByte = trimedJobName.getBytes(UTF_8);\r\n    reEncodedTrimedJobName = new String(trimedJobNameInByte, UTF_8);\r\n    Assert.assertArrayEquals(trimedJobNameInByte, reEncodedTrimedJobName.getBytes(UTF_8));\r\n    sb.setLength(0);\r\n    for (int i = 0; i < 100; i++) {\r\n        sb.append(\"\\uD867\\uDE3D\");\r\n    }\r\n    longJobName = sb.toString();\r\n    info.setJobName(longJobName);\r\n    jobHistoryFile = FileNameIndexUtils.getDoneFileName(info, 49);\r\n    Assert.assertTrue(jobHistoryFile.length() <= 255);\r\n    trimedJobName = jobHistoryFile.split(FileNameIndexUtils.DELIMITER)[3];\r\n    Assert.assertEquals(48, trimedJobName.getBytes(UTF_8).length);\r\n    trimedJobNameInByte = trimedJobName.getBytes(UTF_8);\r\n    reEncodedTrimedJobName = new String(trimedJobNameInByte, UTF_8);\r\n    Assert.assertArrayEquals(trimedJobNameInByte, reEncodedTrimedJobName.getBytes(UTF_8));\r\n    sb.setLength(0);\r\n    sb.append('\\u732B').append(\"[\").append('\\u03BB').append('/').append('A').append(\"\\ud867\\ude49\").append('\\u72AC');\r\n    longJobName = sb.toString();\r\n    info.setJobName(longJobName);\r\n    jobHistoryFile = FileNameIndexUtils.getDoneFileName(info, 23);\r\n    Assert.assertTrue(jobHistoryFile.length() <= 255);\r\n    trimedJobName = jobHistoryFile.split(FileNameIndexUtils.DELIMITER)[3];\r\n    Assert.assertEquals(22, trimedJobName.getBytes(UTF_8).length);\r\n    trimedJobNameInByte = trimedJobName.getBytes(UTF_8);\r\n    reEncodedTrimedJobName = new String(trimedJobNameInByte, UTF_8);\r\n    Assert.assertArrayEquals(trimedJobNameInByte, reEncodedTrimedJobName.getBytes(UTF_8));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testUserNamePercentDecoding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUserNamePercentDecoding() throws IOException\n{\r\n    String jobHistoryFile = String.format(JOB_HISTORY_FILE_FORMATTER, JOB_ID, SUBMIT_TIME, USER_NAME_WITH_DELIMITER_ESCAPE, JOB_NAME, FINISH_TIME, NUM_MAPS, NUM_REDUCES, JOB_STATUS, QUEUE_NAME, JOB_START_TIME);\r\n    JobIndexInfo info = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(\"User name doesn't match\", USER_NAME_WITH_DELIMITER, info.getUser());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testJobNamePercentEncoding",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testJobNamePercentEncoding() throws IOException\n{\r\n    JobIndexInfo info = new JobIndexInfo();\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    info.setJobId(jobId);\r\n    info.setSubmitTime(Long.parseLong(SUBMIT_TIME));\r\n    info.setUser(USER_NAME);\r\n    info.setJobName(JOB_NAME_WITH_DELIMITER);\r\n    info.setFinishTime(Long.parseLong(FINISH_TIME));\r\n    info.setNumMaps(Integer.parseInt(NUM_MAPS));\r\n    info.setNumReduces(Integer.parseInt(NUM_REDUCES));\r\n    info.setJobStatus(JOB_STATUS);\r\n    info.setQueueName(QUEUE_NAME);\r\n    info.setJobStartTime(Long.parseLong(JOB_START_TIME));\r\n    String jobHistoryFile = FileNameIndexUtils.getDoneFileName(info);\r\n    Assert.assertTrue(\"Job name not encoded correctly into job history file\", jobHistoryFile.contains(JOB_NAME_WITH_DELIMITER_ESCAPE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testJobNamePercentDecoding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJobNamePercentDecoding() throws IOException\n{\r\n    String jobHistoryFile = String.format(JOB_HISTORY_FILE_FORMATTER, JOB_ID, SUBMIT_TIME, USER_NAME, JOB_NAME_WITH_DELIMITER_ESCAPE, FINISH_TIME, NUM_MAPS, NUM_REDUCES, JOB_STATUS, QUEUE_NAME, JOB_START_TIME);\r\n    JobIndexInfo info = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(\"Job name doesn't match\", JOB_NAME_WITH_DELIMITER, info.getJobName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testQueueNamePercentEncoding",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testQueueNamePercentEncoding() throws IOException\n{\r\n    JobIndexInfo info = new JobIndexInfo();\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    info.setJobId(jobId);\r\n    info.setSubmitTime(Long.parseLong(SUBMIT_TIME));\r\n    info.setUser(USER_NAME);\r\n    info.setJobName(JOB_NAME);\r\n    info.setFinishTime(Long.parseLong(FINISH_TIME));\r\n    info.setNumMaps(Integer.parseInt(NUM_MAPS));\r\n    info.setNumReduces(Integer.parseInt(NUM_REDUCES));\r\n    info.setJobStatus(JOB_STATUS);\r\n    info.setQueueName(QUEUE_NAME_WITH_DELIMITER);\r\n    info.setJobStartTime(Long.parseLong(JOB_START_TIME));\r\n    String jobHistoryFile = FileNameIndexUtils.getDoneFileName(info);\r\n    Assert.assertTrue(\"Queue name not encoded correctly into job history file\", jobHistoryFile.contains(QUEUE_NAME_WITH_DELIMITER_ESCAPE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testQueueNamePercentDecoding",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testQueueNamePercentDecoding() throws IOException\n{\r\n    String jobHistoryFile = String.format(JOB_HISTORY_FILE_FORMATTER, JOB_ID, SUBMIT_TIME, USER_NAME, JOB_NAME, FINISH_TIME, NUM_MAPS, NUM_REDUCES, JOB_STATUS, QUEUE_NAME_WITH_DELIMITER_ESCAPE, JOB_START_TIME);\r\n    JobIndexInfo info = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(\"Queue name doesn't match\", QUEUE_NAME_WITH_DELIMITER, info.getQueueName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testJobStartTimeBackwardsCompatible",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJobStartTimeBackwardsCompatible() throws IOException\n{\r\n    String jobHistoryFile = String.format(OLD_FORMAT_BEFORE_ADD_START_TIME, JOB_ID, SUBMIT_TIME, USER_NAME, JOB_NAME_WITH_DELIMITER_ESCAPE, FINISH_TIME, NUM_MAPS, NUM_REDUCES, JOB_STATUS, QUEUE_NAME);\r\n    JobIndexInfo info = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(info.getJobStartTime(), info.getSubmitTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testJobHistoryFileNameBackwardsCompatible",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testJobHistoryFileNameBackwardsCompatible() throws IOException\n{\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    long submitTime = Long.parseLong(SUBMIT_TIME);\r\n    long finishTime = Long.parseLong(FINISH_TIME);\r\n    int numMaps = Integer.parseInt(NUM_MAPS);\r\n    int numReduces = Integer.parseInt(NUM_REDUCES);\r\n    String jobHistoryFile = String.format(OLD_JOB_HISTORY_FILE_FORMATTER, JOB_ID, SUBMIT_TIME, USER_NAME, JOB_NAME, FINISH_TIME, NUM_MAPS, NUM_REDUCES, JOB_STATUS);\r\n    JobIndexInfo info = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(\"Job id incorrect after decoding old history file\", jobId, info.getJobId());\r\n    Assert.assertEquals(\"Submit time incorrect after decoding old history file\", submitTime, info.getSubmitTime());\r\n    Assert.assertEquals(\"User incorrect after decoding old history file\", USER_NAME, info.getUser());\r\n    Assert.assertEquals(\"Job name incorrect after decoding old history file\", JOB_NAME, info.getJobName());\r\n    Assert.assertEquals(\"Finish time incorrect after decoding old history file\", finishTime, info.getFinishTime());\r\n    Assert.assertEquals(\"Num maps incorrect after decoding old history file\", numMaps, info.getNumMaps());\r\n    Assert.assertEquals(\"Num reduces incorrect after decoding old history file\", numReduces, info.getNumReduces());\r\n    Assert.assertEquals(\"Job status incorrect after decoding old history file\", JOB_STATUS, info.getJobStatus());\r\n    Assert.assertNull(\"Queue name incorrect after decoding old history file\", info.getQueueName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testTrimJobNameEqualsLimitLength",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTrimJobNameEqualsLimitLength() throws IOException\n{\r\n    int jobNameTrimLength = 9;\r\n    JobIndexInfo info = new JobIndexInfo();\r\n    JobID oldJobId = JobID.forName(JOB_ID);\r\n    JobId jobId = TypeConverter.toYarn(oldJobId);\r\n    info.setJobId(jobId);\r\n    info.setSubmitTime(Long.parseLong(SUBMIT_TIME));\r\n    info.setUser(USER_NAME);\r\n    info.setJobName(JOB_NAME);\r\n    info.setFinishTime(Long.parseLong(FINISH_TIME));\r\n    info.setNumMaps(Integer.parseInt(NUM_MAPS));\r\n    info.setNumReduces(Integer.parseInt(NUM_REDUCES));\r\n    info.setJobStatus(JOB_STATUS);\r\n    info.setQueueName(QUEUE_NAME);\r\n    info.setJobStartTime(Long.parseLong(JOB_START_TIME));\r\n    String jobHistoryFile = FileNameIndexUtils.getDoneFileName(info, jobNameTrimLength);\r\n    JobIndexInfo parsedInfo = FileNameIndexUtils.getIndexInfo(jobHistoryFile);\r\n    Assert.assertEquals(\"Job name did not get trimmed correctly\", info.getJobName().substring(0, jobNameTrimLength), parsedInfo.getJobName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void delete(File file) throws IOException\n{\r\n    if (file.getAbsolutePath().length() < 5) {\r\n        throw new IllegalArgumentException(\"Path [\" + file + \"] is too short, not deleting\");\r\n    }\r\n    if (file.exists()) {\r\n        if (file.isDirectory()) {\r\n            File[] children = file.listFiles();\r\n            if (children != null) {\r\n                for (File child : children) {\r\n                    delete(child);\r\n                }\r\n            }\r\n        }\r\n        if (!file.delete()) {\r\n            throw new RuntimeException(\"Could not delete path [\" + file + \"]\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    mockfs = mock(FileSystem.class);\r\n    localDir = new File(System.getProperty(\"test.build.dir\", \"target/test-dir\"), TestLocalDistributedCacheManager.class.getName());\r\n    delete(localDir);\r\n    localDir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    delete(localDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDownload",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testDownload() throws Exception\n{\r\n    JobID jobId = new JobID();\r\n    JobConf conf = new JobConf();\r\n    conf.setClass(\"fs.mock.impl\", MockFileSystem.class, FileSystem.class);\r\n    URI mockBase = new URI(\"mock://test-nn1/\");\r\n    when(mockfs.getUri()).thenReturn(mockBase);\r\n    Path working = new Path(\"mock://test-nn1/user/me/\");\r\n    when(mockfs.getWorkingDirectory()).thenReturn(working);\r\n    when(mockfs.resolvePath(any(Path.class))).thenAnswer(new Answer<Path>() {\r\n\r\n        @Override\r\n        public Path answer(InvocationOnMock args) throws Throwable {\r\n            return (Path) args.getArguments()[0];\r\n        }\r\n    });\r\n    final URI file = new URI(\"mock://test-nn1/user/me/file.txt#link\");\r\n    final Path filePath = new Path(file);\r\n    File link = new File(\"link\");\r\n    when(mockfs.getFileStatus(any(Path.class))).thenAnswer(new Answer<FileStatus>() {\r\n\r\n        @Override\r\n        public FileStatus answer(InvocationOnMock args) throws Throwable {\r\n            Path p = (Path) args.getArguments()[0];\r\n            if (\"file.txt\".equals(p.getName())) {\r\n                return new FileStatus(201, false, 1, 500, 101, 101, FsPermission.getDefault(), \"me\", \"me\", filePath);\r\n            } else {\r\n                throw new FileNotFoundException(p + \" not supported by mocking\");\r\n            }\r\n        }\r\n    });\r\n    when(mockfs.getConf()).thenReturn(conf);\r\n    final FSDataInputStream in = new FSDataInputStream(new MockInputStream(\"This is a test file\\n\".getBytes()));\r\n    when(mockfs.open(any(Path.class), anyInt())).thenAnswer(new Answer<FSDataInputStream>() {\r\n\r\n        @Override\r\n        public FSDataInputStream answer(InvocationOnMock args) throws Throwable {\r\n            Path src = (Path) args.getArguments()[0];\r\n            if (\"file.txt\".equals(src.getName())) {\r\n                return in;\r\n            } else {\r\n                throw new FileNotFoundException(src + \" not supported by mocking\");\r\n            }\r\n        }\r\n    });\r\n    Job.addCacheFile(file, conf);\r\n    Map<String, Boolean> policies = new HashMap<String, Boolean>();\r\n    policies.put(file.toString(), true);\r\n    Job.setFileSharedCacheUploadPolicies(conf, policies);\r\n    conf.set(MRJobConfig.CACHE_FILE_TIMESTAMPS, \"101\");\r\n    conf.set(MRJobConfig.CACHE_FILES_SIZES, \"201\");\r\n    conf.set(MRJobConfig.CACHE_FILE_VISIBILITIES, \"false\");\r\n    conf.set(MRConfig.LOCAL_DIR, localDir.getAbsolutePath());\r\n    LocalDistributedCacheManager manager = new LocalDistributedCacheManager();\r\n    try {\r\n        manager.setup(conf, jobId);\r\n        assertTrue(link.exists());\r\n    } finally {\r\n        manager.close();\r\n    }\r\n    assertFalse(link.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testEmptyDownload",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testEmptyDownload() throws Exception\n{\r\n    JobID jobId = new JobID();\r\n    JobConf conf = new JobConf();\r\n    conf.setClass(\"fs.mock.impl\", MockFileSystem.class, FileSystem.class);\r\n    URI mockBase = new URI(\"mock://test-nn1/\");\r\n    when(mockfs.getUri()).thenReturn(mockBase);\r\n    Path working = new Path(\"mock://test-nn1/user/me/\");\r\n    when(mockfs.getWorkingDirectory()).thenReturn(working);\r\n    when(mockfs.resolvePath(any(Path.class))).thenAnswer(new Answer<Path>() {\r\n\r\n        @Override\r\n        public Path answer(InvocationOnMock args) throws Throwable {\r\n            return (Path) args.getArguments()[0];\r\n        }\r\n    });\r\n    when(mockfs.getFileStatus(any(Path.class))).thenAnswer(new Answer<FileStatus>() {\r\n\r\n        @Override\r\n        public FileStatus answer(InvocationOnMock args) throws Throwable {\r\n            Path p = (Path) args.getArguments()[0];\r\n            throw new FileNotFoundException(p + \" not supported by mocking\");\r\n        }\r\n    });\r\n    when(mockfs.getConf()).thenReturn(conf);\r\n    when(mockfs.open(any(Path.class), anyInt())).thenAnswer(new Answer<FSDataInputStream>() {\r\n\r\n        @Override\r\n        public FSDataInputStream answer(InvocationOnMock args) throws Throwable {\r\n            Path src = (Path) args.getArguments()[0];\r\n            throw new FileNotFoundException(src + \" not supported by mocking\");\r\n        }\r\n    });\r\n    conf.set(MRJobConfig.CACHE_FILES, \"\");\r\n    conf.set(MRConfig.LOCAL_DIR, localDir.getAbsolutePath());\r\n    LocalDistributedCacheManager manager = new LocalDistributedCacheManager();\r\n    try {\r\n        manager.setup(conf, jobId);\r\n    } finally {\r\n        manager.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDuplicateDownload",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testDuplicateDownload() throws Exception\n{\r\n    JobID jobId = new JobID();\r\n    JobConf conf = new JobConf();\r\n    conf.setClass(\"fs.mock.impl\", MockFileSystem.class, FileSystem.class);\r\n    URI mockBase = new URI(\"mock://test-nn1/\");\r\n    when(mockfs.getUri()).thenReturn(mockBase);\r\n    Path working = new Path(\"mock://test-nn1/user/me/\");\r\n    when(mockfs.getWorkingDirectory()).thenReturn(working);\r\n    when(mockfs.resolvePath(any(Path.class))).thenAnswer(new Answer<Path>() {\r\n\r\n        @Override\r\n        public Path answer(InvocationOnMock args) throws Throwable {\r\n            return (Path) args.getArguments()[0];\r\n        }\r\n    });\r\n    final URI file = new URI(\"mock://test-nn1/user/me/file.txt#link\");\r\n    final Path filePath = new Path(file);\r\n    File link = new File(\"link\");\r\n    when(mockfs.getFileStatus(any(Path.class))).thenAnswer(new Answer<FileStatus>() {\r\n\r\n        @Override\r\n        public FileStatus answer(InvocationOnMock args) throws Throwable {\r\n            Path p = (Path) args.getArguments()[0];\r\n            if (\"file.txt\".equals(p.getName())) {\r\n                return new FileStatus(201, false, 1, 500, 101, 101, FsPermission.getDefault(), \"me\", \"me\", filePath);\r\n            } else {\r\n                throw new FileNotFoundException(p + \" not supported by mocking\");\r\n            }\r\n        }\r\n    });\r\n    when(mockfs.getConf()).thenReturn(conf);\r\n    final FSDataInputStream in = new FSDataInputStream(new MockInputStream(\"This is a test file\\n\".getBytes()));\r\n    when(mockfs.open(any(Path.class), anyInt())).thenAnswer(new Answer<FSDataInputStream>() {\r\n\r\n        @Override\r\n        public FSDataInputStream answer(InvocationOnMock args) throws Throwable {\r\n            Path src = (Path) args.getArguments()[0];\r\n            if (\"file.txt\".equals(src.getName())) {\r\n                return in;\r\n            } else {\r\n                throw new FileNotFoundException(src + \" not supported by mocking\");\r\n            }\r\n        }\r\n    });\r\n    Job.addCacheFile(file, conf);\r\n    Job.addCacheFile(file, conf);\r\n    Map<String, Boolean> policies = new HashMap<String, Boolean>();\r\n    policies.put(file.toString(), true);\r\n    Job.setFileSharedCacheUploadPolicies(conf, policies);\r\n    conf.set(MRJobConfig.CACHE_FILE_TIMESTAMPS, \"101,101\");\r\n    conf.set(MRJobConfig.CACHE_FILES_SIZES, \"201,201\");\r\n    conf.set(MRJobConfig.CACHE_FILE_VISIBILITIES, \"false,false\");\r\n    conf.set(MRConfig.LOCAL_DIR, localDir.getAbsolutePath());\r\n    LocalDistributedCacheManager manager = new LocalDistributedCacheManager();\r\n    try {\r\n        manager.setup(conf, jobId);\r\n        assertTrue(link.exists());\r\n    } finally {\r\n        manager.close();\r\n    }\r\n    assertFalse(link.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMultipleCacheSetup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultipleCacheSetup() throws Exception\n{\r\n    JobID jobId = new JobID();\r\n    JobConf conf = new JobConf();\r\n    LocalDistributedCacheManager manager = new LocalDistributedCacheManager();\r\n    final int threadCount = 10;\r\n    final CyclicBarrier barrier = new CyclicBarrier(threadCount);\r\n    ArrayList<Callable<Void>> setupCallable = new ArrayList<>();\r\n    for (int i = 0; i < threadCount; ++i) {\r\n        setupCallable.add(() -> {\r\n            barrier.await();\r\n            manager.setup(conf, jobId);\r\n            return null;\r\n        });\r\n    }\r\n    ExecutorService ePool = Executors.newFixedThreadPool(threadCount);\r\n    try {\r\n        for (Future<Void> future : ePool.invokeAll(setupCallable)) {\r\n            future.get();\r\n        }\r\n    } finally {\r\n        ePool.shutdown();\r\n        manager.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    FileUtil.fullyDelete(new File(TEST_DIR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetClusterStatusWithLocalJobRunner",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetClusterStatusWithLocalJobRunner() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(JTConfig.JT_IPC_ADDRESS, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    JobClient client = new JobClient(conf);\r\n    ClusterStatus clusterStatus = client.getClusterStatus(true);\r\n    Collection<String> activeTrackerNames = clusterStatus.getActiveTrackerNames();\r\n    Assert.assertEquals(0, activeTrackerNames.size());\r\n    int blacklistedTrackers = clusterStatus.getBlacklistedTrackers();\r\n    Assert.assertEquals(0, blacklistedTrackers);\r\n    Collection<BlackListInfo> blackListedTrackersInfo = clusterStatus.getBlackListedTrackersInfo();\r\n    Assert.assertEquals(0, blackListedTrackersInfo.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testIsJobDirValid",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testIsJobDirValid() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path testDir = new Path(TEST_DIR);\r\n    fs.mkdirs(testDir);\r\n    Assert.assertFalse(JobClient.isJobDirValid(testDir, fs));\r\n    Path jobconf = new Path(testDir, \"job.xml\");\r\n    Path jobsplit = new Path(testDir, \"job.split\");\r\n    fs.create(jobconf);\r\n    fs.create(jobsplit);\r\n    Assert.assertTrue(JobClient.isJobDirValid(testDir, fs));\r\n    fs.delete(jobconf, true);\r\n    fs.delete(jobsplit, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetStagingAreaDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testGetStagingAreaDir() throws IOException, InterruptedException\n{\r\n    Configuration conf = new Configuration();\r\n    JobClient client = new JobClient(conf);\r\n    Assert.assertTrue(\"Mismatch in paths\", client.getClusterHandle().getStagingAreaDir().toString().equals(client.getStagingAreaDir().toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAutoClosable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testAutoClosable() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    try (JobClient jobClient = new JobClient(conf)) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWithConf",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testWithConf(Configuration conf) throws IOException, InterruptedException, ClassNotFoundException, URISyntaxException\n{\r\n    Path first = createTempFile(\"distributed.first\", \"x\");\r\n    Path second = makeJar(new Path(TEST_ROOT_DIR, \"distributed.second.jar\"), 2);\r\n    Path third = makeJar(new Path(TEST_ROOT_DIR, \"distributed.third.jar\"), 3);\r\n    Path fourth = makeJar(new Path(TEST_ROOT_DIR, \"distributed.fourth.jar\"), 4);\r\n    Job job = Job.getInstance(conf);\r\n    job.setMapperClass(DistributedCacheCheckerMapper.class);\r\n    job.setReducerClass(DistributedCacheCheckerReducer.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    FileInputFormat.setInputPaths(job, first);\r\n    job.addCacheFile(new URI(first.toUri().toString() + \"#distributed.first.symlink\"));\r\n    job.addFileToClassPath(second);\r\n    job.addArchiveToClassPath(third);\r\n    job.addCacheArchive(fourth.toUri());\r\n    job.setMaxMapAttempts(1);\r\n    job.submit();\r\n    assertTrue(job.waitForCompletion(false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobRunner",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLocalJobRunner() throws Exception\n{\r\n    symlinkFile.delete();\r\n    Configuration c = new Configuration();\r\n    c.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n    c.set(\"fs.defaultFS\", \"file:///\");\r\n    testWithConf(c);\r\n    assertFalse(\"Symlink not removed by local job runner\", Arrays.asList(new File(\".\").list()).contains(symlinkFile.getName()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTempFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path createTempFile(String filename, String contents) throws IOException\n{\r\n    Path path = new Path(TEST_ROOT_DIR, filename);\r\n    FSDataOutputStream os = localFs.create(path);\r\n    os.writeBytes(contents);\r\n    os.close();\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeJar",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path makeJar(Path p, int index) throws FileNotFoundException, IOException\n{\r\n    FileOutputStream fos = new FileOutputStream(new File(p.toString()));\r\n    JarOutputStream jos = new JarOutputStream(fos);\r\n    ZipEntry ze = new ZipEntry(\"distributed.jar.inside\" + index);\r\n    jos.putNextEntry(ze);\r\n    jos.write((\"inside the jar!\" + index).getBytes());\r\n    jos.closeEntry();\r\n    jos.close();\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDeprecatedFunctions",
  "errType" : null,
  "containingMethodsNum" : 48,
  "sourceCodeText" : "void testDeprecatedFunctions() throws Exception\n{\r\n    DistributedCache.addLocalArchives(conf, \"Test Local Archives 1\");\r\n    Assert.assertEquals(\"Test Local Archives 1\", conf.get(DistributedCache.CACHE_LOCALARCHIVES));\r\n    Assert.assertEquals(1, JobContextImpl.getLocalCacheArchives(conf).length);\r\n    Assert.assertEquals(\"Test Local Archives 1\", JobContextImpl.getLocalCacheArchives(conf)[0].getName());\r\n    DistributedCache.addLocalArchives(conf, \"Test Local Archives 2\");\r\n    Assert.assertEquals(\"Test Local Archives 1,Test Local Archives 2\", conf.get(DistributedCache.CACHE_LOCALARCHIVES));\r\n    Assert.assertEquals(2, JobContextImpl.getLocalCacheArchives(conf).length);\r\n    Assert.assertEquals(\"Test Local Archives 2\", JobContextImpl.getLocalCacheArchives(conf)[1].getName());\r\n    DistributedCache.setLocalArchives(conf, \"Test Local Archives 3\");\r\n    Assert.assertEquals(\"Test Local Archives 3\", conf.get(DistributedCache.CACHE_LOCALARCHIVES));\r\n    Assert.assertEquals(1, JobContextImpl.getLocalCacheArchives(conf).length);\r\n    Assert.assertEquals(\"Test Local Archives 3\", JobContextImpl.getLocalCacheArchives(conf)[0].getName());\r\n    DistributedCache.addLocalFiles(conf, \"Test Local Files 1\");\r\n    Assert.assertEquals(\"Test Local Files 1\", conf.get(DistributedCache.CACHE_LOCALFILES));\r\n    Assert.assertEquals(1, JobContextImpl.getLocalCacheFiles(conf).length);\r\n    Assert.assertEquals(\"Test Local Files 1\", JobContextImpl.getLocalCacheFiles(conf)[0].getName());\r\n    DistributedCache.addLocalFiles(conf, \"Test Local Files 2\");\r\n    Assert.assertEquals(\"Test Local Files 1,Test Local Files 2\", conf.get(DistributedCache.CACHE_LOCALFILES));\r\n    Assert.assertEquals(2, JobContextImpl.getLocalCacheFiles(conf).length);\r\n    Assert.assertEquals(\"Test Local Files 2\", JobContextImpl.getLocalCacheFiles(conf)[1].getName());\r\n    DistributedCache.setLocalFiles(conf, \"Test Local Files 3\");\r\n    Assert.assertEquals(\"Test Local Files 3\", conf.get(DistributedCache.CACHE_LOCALFILES));\r\n    Assert.assertEquals(1, JobContextImpl.getLocalCacheFiles(conf).length);\r\n    Assert.assertEquals(\"Test Local Files 3\", JobContextImpl.getLocalCacheFiles(conf)[0].getName());\r\n    DistributedCache.setArchiveTimestamps(conf, \"1234567890\");\r\n    Assert.assertEquals(1234567890, conf.getLong(DistributedCache.CACHE_ARCHIVES_TIMESTAMPS, 0));\r\n    Assert.assertEquals(1, JobContextImpl.getArchiveTimestamps(conf).length);\r\n    Assert.assertEquals(1234567890, JobContextImpl.getArchiveTimestamps(conf)[0]);\r\n    DistributedCache.setFileTimestamps(conf, \"1234567890\");\r\n    Assert.assertEquals(1234567890, conf.getLong(DistributedCache.CACHE_FILES_TIMESTAMPS, 0));\r\n    Assert.assertEquals(1, JobContextImpl.getFileTimestamps(conf).length);\r\n    Assert.assertEquals(1234567890, JobContextImpl.getFileTimestamps(conf)[0]);\r\n    DistributedCache.createAllSymlink(conf, new File(\"Test Job Cache Dir\"), new File(\"Test Work Dir\"));\r\n    Assert.assertNull(conf.get(DistributedCache.CACHE_SYMLINK));\r\n    Assert.assertTrue(DistributedCache.getSymlink(conf));\r\n    Assert.assertTrue(symlinkFile.createNewFile());\r\n    FileStatus fileStatus = DistributedCache.getFileStatus(conf, symlinkFile.toURI());\r\n    Assert.assertNotNull(fileStatus);\r\n    Assert.assertEquals(fileStatus.getModificationTime(), DistributedCache.getTimestamp(conf, symlinkFile.toURI()));\r\n    Assert.assertTrue(symlinkFile.delete());\r\n    Job.addCacheArchive(symlinkFile.toURI(), conf);\r\n    Assert.assertEquals(symlinkFile.toURI().toString(), conf.get(DistributedCache.CACHE_ARCHIVES));\r\n    Assert.assertEquals(1, JobContextImpl.getCacheArchives(conf).length);\r\n    Assert.assertEquals(symlinkFile.toURI(), JobContextImpl.getCacheArchives(conf)[0]);\r\n    Job.addCacheFile(symlinkFile.toURI(), conf);\r\n    Assert.assertEquals(symlinkFile.toURI().toString(), conf.get(DistributedCache.CACHE_FILES));\r\n    Assert.assertEquals(1, JobContextImpl.getCacheFiles(conf).length);\r\n    Assert.assertEquals(symlinkFile.toURI(), JobContextImpl.getCacheFiles(conf)[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testPbRecordFactory",
  "errType" : [ "YarnRuntimeException", "YarnRuntimeException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPbRecordFactory()\n{\r\n    RecordFactory pbRecordFactory = RecordFactoryPBImpl.get();\r\n    try {\r\n        CounterGroup response = pbRecordFactory.newRecordInstance(CounterGroup.class);\r\n        Assert.assertEquals(CounterGroupPBImpl.class, response.getClass());\r\n    } catch (YarnRuntimeException e) {\r\n        e.printStackTrace();\r\n        Assert.fail(\"Failed to crete record\");\r\n    }\r\n    try {\r\n        GetCountersRequest response = pbRecordFactory.newRecordInstance(GetCountersRequest.class);\r\n        Assert.assertEquals(GetCountersRequestPBImpl.class, response.getClass());\r\n    } catch (YarnRuntimeException e) {\r\n        e.printStackTrace();\r\n        Assert.fail(\"Failed to crete record\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testGetHistoryDirsForCleaning",
  "errType" : null,
  "containingMethodsNum" : 52,
  "sourceCodeText" : "void testGetHistoryDirsForCleaning() throws IOException\n{\r\n    Path pRoot = new Path(TEST_DIR, \"org.apache.hadoop.mapreduce.v2.jobhistory.\" + \"TestJobHistoryUtils.testGetHistoryDirsForCleaning\");\r\n    FileContext fc = FileContext.getFileContext();\r\n    Calendar cCal = Calendar.getInstance();\r\n    int year = 2013;\r\n    int month = 7;\r\n    int day = 21;\r\n    cCal.set(year, month - 1, day, 1, 0);\r\n    long cutoff = cCal.getTimeInMillis();\r\n    clearDir(fc, pRoot);\r\n    Path pId00 = createPath(fc, pRoot, year, month, day, \"000000\");\r\n    Path pId01 = createPath(fc, pRoot, year, month, day + 1, \"000001\");\r\n    Path pId02 = createPath(fc, pRoot, year, month, day - 1, \"000002\");\r\n    Path pId03 = createPath(fc, pRoot, year, month + 1, day, \"000003\");\r\n    Path pId04 = createPath(fc, pRoot, year, month + 1, day + 1, \"000004\");\r\n    Path pId05 = createPath(fc, pRoot, year, month + 1, day - 1, \"000005\");\r\n    Path pId06 = createPath(fc, pRoot, year, month - 1, day, \"000006\");\r\n    Path pId07 = createPath(fc, pRoot, year, month - 1, day + 1, \"000007\");\r\n    Path pId08 = createPath(fc, pRoot, year, month - 1, day - 1, \"000008\");\r\n    Path pId09 = createPath(fc, pRoot, year + 1, month, day, \"000009\");\r\n    Path pId10 = createPath(fc, pRoot, year + 1, month, day + 1, \"000010\");\r\n    Path pId11 = createPath(fc, pRoot, year + 1, month, day - 1, \"000011\");\r\n    Path pId12 = createPath(fc, pRoot, year + 1, month + 1, day, \"000012\");\r\n    Path pId13 = createPath(fc, pRoot, year + 1, month + 1, day + 1, \"000013\");\r\n    Path pId14 = createPath(fc, pRoot, year + 1, month + 1, day - 1, \"000014\");\r\n    Path pId15 = createPath(fc, pRoot, year + 1, month - 1, day, \"000015\");\r\n    Path pId16 = createPath(fc, pRoot, year + 1, month - 1, day + 1, \"000016\");\r\n    Path pId17 = createPath(fc, pRoot, year + 1, month - 1, day - 1, \"000017\");\r\n    Path pId18 = createPath(fc, pRoot, year - 1, month, day, \"000018\");\r\n    Path pId19 = createPath(fc, pRoot, year - 1, month, day + 1, \"000019\");\r\n    Path pId20 = createPath(fc, pRoot, year - 1, month, day - 1, \"000020\");\r\n    Path pId21 = createPath(fc, pRoot, year - 1, month + 1, day, \"000021\");\r\n    Path pId22 = createPath(fc, pRoot, year - 1, month + 1, day + 1, \"000022\");\r\n    Path pId23 = createPath(fc, pRoot, year - 1, month + 1, day - 1, \"000023\");\r\n    Path pId24 = createPath(fc, pRoot, year - 1, month - 1, day, \"000024\");\r\n    Path pId25 = createPath(fc, pRoot, year - 1, month - 1, day + 1, \"000025\");\r\n    Path pId26 = createPath(fc, pRoot, year - 1, month - 1, day - 1, \"000026\");\r\n    Path pId27 = createPath(fc, pRoot, \"foo\", \"\" + month, \"\" + day, \"000027\");\r\n    Path pId28 = createPath(fc, pRoot, \"\" + year, \"foo\", \"\" + day, \"000028\");\r\n    Path pId29 = createPath(fc, pRoot, \"\" + year, \"\" + month, \"foo\", \"000029\");\r\n    List<FileStatus> dirs = JobHistoryUtils.getHistoryDirsForCleaning(fc, pRoot, cutoff);\r\n    Collections.sort(dirs);\r\n    Assert.assertEquals(14, dirs.size());\r\n    Assert.assertEquals(pId26.toUri().getPath(), dirs.get(0).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId24.toUri().getPath(), dirs.get(1).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId25.toUri().getPath(), dirs.get(2).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId20.toUri().getPath(), dirs.get(3).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId18.toUri().getPath(), dirs.get(4).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId19.toUri().getPath(), dirs.get(5).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId23.toUri().getPath(), dirs.get(6).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId21.toUri().getPath(), dirs.get(7).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId22.toUri().getPath(), dirs.get(8).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId08.toUri().getPath(), dirs.get(9).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId06.toUri().getPath(), dirs.get(10).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId07.toUri().getPath(), dirs.get(11).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId02.toUri().getPath(), dirs.get(12).getPath().toUri().getPath());\r\n    Assert.assertEquals(pId00.toUri().getPath(), dirs.get(13).getPath().toUri().getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "clearDir",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void clearDir(FileContext fc, Path p) throws IOException\n{\r\n    try {\r\n        fc.delete(p, true);\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n    fc.mkdir(p, FsPermission.getDirDefault(), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "createPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path createPath(FileContext fc, Path root, int year, int month, int day, String id) throws IOException\n{\r\n    Path path = new Path(root, year + Path.SEPARATOR + month + Path.SEPARATOR + day + Path.SEPARATOR + id);\r\n    fc.mkdir(path, FsPermission.getDirDefault(), true);\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "createPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path createPath(FileContext fc, Path root, String year, String month, String day, String id) throws IOException\n{\r\n    Path path = new Path(root, year + Path.SEPARATOR + month + Path.SEPARATOR + day + Path.SEPARATOR + id);\r\n    fc.mkdir(path, FsPermission.getDirDefault(), true);\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\jobhistory",
  "methodName" : "testGetConfiguredHistoryIntermediateUserDoneDirPermissions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetConfiguredHistoryIntermediateUserDoneDirPermissions()\n{\r\n    Configuration conf = new Configuration();\r\n    Map<String, FsPermission> parameters = ImmutableMap.of(\"775\", new FsPermission(0775), \"123\", new FsPermission(0773), \"-rwx\", new FsPermission(0770), \"+rwx\", new FsPermission(0777));\r\n    for (Map.Entry<String, FsPermission> entry : parameters.entrySet()) {\r\n        conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_USER_DONE_DIR_PERMISSIONS, entry.getKey());\r\n        Assert.assertEquals(entry.getValue(), getConfiguredHistoryIntermediateUserDoneDirPermissions(conf));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "setupTestDirs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupTestDirs() throws IOException\n{\r\n    testWorkDir = new File(\"target\", TestMRApps.class.getCanonicalName());\r\n    delete(testWorkDir);\r\n    testWorkDir.mkdirs();\r\n    testWorkDir = testWorkDir.getAbsoluteFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "cleanupTestDirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupTestDirs() throws IOException\n{\r\n    if (testWorkDir != null) {\r\n        delete(testWorkDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void delete(File dir) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path p = fs.makeQualified(new Path(dir.getAbsolutePath()));\r\n    fs.delete(p, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testJobIDtoString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJobIDtoString()\n{\r\n    JobId jid = RecordFactoryProvider.getRecordFactory(null).newRecordInstance(JobId.class);\r\n    jid.setAppId(ApplicationId.newInstance(0, 0));\r\n    assertEquals(\"job_0_0000\", MRApps.toString(jid));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testToJobID",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testToJobID()\n{\r\n    JobId jid = MRApps.toJobID(\"job_1_1\");\r\n    assertEquals(1, jid.getAppId().getClusterTimestamp());\r\n    assertEquals(1, jid.getAppId().getId());\r\n    assertEquals(1, jid.getId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testJobIDShort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJobIDShort()\n{\r\n    MRApps.toJobID(\"job_0_0_0\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testTaskIDtoString",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testTaskIDtoString()\n{\r\n    TaskId tid = RecordFactoryProvider.getRecordFactory(null).newRecordInstance(TaskId.class);\r\n    tid.setJobId(RecordFactoryProvider.getRecordFactory(null).newRecordInstance(JobId.class));\r\n    tid.getJobId().setAppId(ApplicationId.newInstance(0, 0));\r\n    tid.setTaskType(TaskType.MAP);\r\n    TaskType type = tid.getTaskType();\r\n    System.err.println(type);\r\n    type = TaskType.REDUCE;\r\n    System.err.println(type);\r\n    System.err.println(tid.getTaskType());\r\n    assertEquals(\"task_0_0000_m_000000\", MRApps.toString(tid));\r\n    tid.setTaskType(TaskType.REDUCE);\r\n    assertEquals(\"task_0_0000_r_000000\", MRApps.toString(tid));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testToTaskID",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testToTaskID()\n{\r\n    TaskId tid = MRApps.toTaskID(\"task_1_2_r_3\");\r\n    assertEquals(1, tid.getJobId().getAppId().getClusterTimestamp());\r\n    assertEquals(2, tid.getJobId().getAppId().getId());\r\n    assertEquals(2, tid.getJobId().getId());\r\n    assertEquals(TaskType.REDUCE, tid.getTaskType());\r\n    assertEquals(3, tid.getId());\r\n    tid = MRApps.toTaskID(\"task_1_2_m_3\");\r\n    assertEquals(TaskType.MAP, tid.getTaskType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testTaskIDShort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskIDShort()\n{\r\n    MRApps.toTaskID(\"task_0_0000_m\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testTaskIDBadType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskIDBadType()\n{\r\n    MRApps.toTaskID(\"task_0_0000_x_000000\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testTaskAttemptIDtoString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testTaskAttemptIDtoString()\n{\r\n    TaskAttemptId taid = RecordFactoryProvider.getRecordFactory(null).newRecordInstance(TaskAttemptId.class);\r\n    taid.setTaskId(RecordFactoryProvider.getRecordFactory(null).newRecordInstance(TaskId.class));\r\n    taid.getTaskId().setTaskType(TaskType.MAP);\r\n    taid.getTaskId().setJobId(RecordFactoryProvider.getRecordFactory(null).newRecordInstance(JobId.class));\r\n    taid.getTaskId().getJobId().setAppId(ApplicationId.newInstance(0, 0));\r\n    assertEquals(\"attempt_0_0000_m_000000_0\", MRApps.toString(taid));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testToTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testToTaskAttemptID()\n{\r\n    TaskAttemptId taid = MRApps.toTaskAttemptID(\"attempt_0_1_m_2_3\");\r\n    assertEquals(0, taid.getTaskId().getJobId().getAppId().getClusterTimestamp());\r\n    assertEquals(1, taid.getTaskId().getJobId().getAppId().getId());\r\n    assertEquals(1, taid.getTaskId().getJobId().getId());\r\n    assertEquals(2, taid.getTaskId().getId());\r\n    assertEquals(3, taid.getId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testTaskAttemptIDShort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIDShort()\n{\r\n    MRApps.toTaskAttemptID(\"attempt_0_0_0_m_0\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testGetJobFileWithUser",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetJobFileWithUser()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, \"/my/path/to/staging\");\r\n    String jobFile = MRApps.getJobFile(conf, \"dummy-user\", new JobID(\"dummy-job\", 12345));\r\n    assertNotNull(\"getJobFile results in null.\", jobFile);\r\n    assertEquals(\"jobFile with specified user is not as expected.\", \"/my/path/to/staging/dummy-user/.staging/job_dummy-job_12345/job.xml\", jobFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetClasspath",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSetClasspath() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    Job job = Job.getInstance(conf);\r\n    Map<String, String> environment = new HashMap<String, String>();\r\n    MRApps.setClasspath(environment, job.getConfiguration());\r\n    assertTrue(environment.get(\"CLASSPATH\").startsWith(ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));\r\n    String yarnAppClasspath = job.getConfiguration().get(YarnConfiguration.YARN_APPLICATION_CLASSPATH, StringUtils.join(\",\", YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH));\r\n    if (yarnAppClasspath != null) {\r\n        yarnAppClasspath = yarnAppClasspath.replaceAll(\",\\\\s*\", ApplicationConstants.CLASS_PATH_SEPARATOR).trim();\r\n    }\r\n    assertTrue(environment.get(\"CLASSPATH\").contains(yarnAppClasspath));\r\n    String mrAppClasspath = job.getConfiguration().get(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH, MRJobConfig.DEFAULT_MAPREDUCE_CROSS_PLATFORM_APPLICATION_CLASSPATH);\r\n    if (mrAppClasspath != null) {\r\n        mrAppClasspath = mrAppClasspath.replaceAll(\",\\\\s*\", ApplicationConstants.CLASS_PATH_SEPARATOR).trim();\r\n    }\r\n    assertTrue(environment.get(\"CLASSPATH\").contains(mrAppClasspath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetClasspathWithArchives",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSetClasspathWithArchives() throws IOException\n{\r\n    File testTGZ = new File(testWorkDir, \"test.tgz\");\r\n    FileOutputStream out = new FileOutputStream(testTGZ);\r\n    out.write(0);\r\n    out.close();\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    Job job = Job.getInstance(conf);\r\n    conf = job.getConfiguration();\r\n    String testTGZQualifiedPath = FileSystem.getLocal(conf).makeQualified(new Path(testTGZ.getAbsolutePath())).toString();\r\n    conf.set(MRJobConfig.CLASSPATH_ARCHIVES, testTGZQualifiedPath);\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES, testTGZQualifiedPath + \"#testTGZ\");\r\n    Map<String, String> environment = new HashMap<String, String>();\r\n    MRApps.setClasspath(environment, conf);\r\n    assertTrue(environment.get(\"CLASSPATH\").startsWith(ApplicationConstants.Environment.PWD.$$() + ApplicationConstants.CLASS_PATH_SEPARATOR));\r\n    String confClasspath = job.getConfiguration().get(YarnConfiguration.YARN_APPLICATION_CLASSPATH, StringUtils.join(\",\", YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH));\r\n    if (confClasspath != null) {\r\n        confClasspath = confClasspath.replaceAll(\",\\\\s*\", ApplicationConstants.CLASS_PATH_SEPARATOR).trim();\r\n    }\r\n    assertTrue(environment.get(\"CLASSPATH\").contains(confClasspath));\r\n    assertTrue(environment.get(\"CLASSPATH\").contains(\"testTGZ\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetClasspathWithUserPrecendence",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSetClasspathWithUserPrecendence()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, true);\r\n    Map<String, String> env = new HashMap<String, String>();\r\n    try {\r\n        MRApps.setClasspath(env, conf);\r\n    } catch (Exception e) {\r\n        fail(\"Got exception while setting classpath\");\r\n    }\r\n    String env_str = env.get(\"CLASSPATH\");\r\n    String expectedClasspath = StringUtils.join(ApplicationConstants.CLASS_PATH_SEPARATOR, Arrays.asList(ApplicationConstants.Environment.PWD.$$(), \"job.jar/*\", \"job.jar/classes/\", \"job.jar/lib/*\", ApplicationConstants.Environment.PWD.$$() + \"/*\"));\r\n    assertTrue(\"MAPREDUCE_JOB_USER_CLASSPATH_FIRST set, but not taking effect!\", env_str.startsWith(expectedClasspath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetClasspathWithNoUserPrecendence",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetClasspathWithNoUserPrecendence()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, false);\r\n    Map<String, String> env = new HashMap<String, String>();\r\n    try {\r\n        MRApps.setClasspath(env, conf);\r\n    } catch (Exception e) {\r\n        fail(\"Got exception while setting classpath\");\r\n    }\r\n    String env_str = env.get(\"CLASSPATH\");\r\n    String expectedClasspath = StringUtils.join(ApplicationConstants.CLASS_PATH_SEPARATOR, Arrays.asList(\"job.jar/*\", \"job.jar/classes/\", \"job.jar/lib/*\", ApplicationConstants.Environment.PWD.$$() + \"/*\"));\r\n    assertTrue(\"MAPREDUCE_JOB_USER_CLASSPATH_FIRST false, and job.jar is not in\" + \" the classpath!\", env_str.contains(expectedClasspath));\r\n    assertFalse(\"MAPREDUCE_JOB_USER_CLASSPATH_FIRST false, but taking effect!\", env_str.startsWith(expectedClasspath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetClasspathWithJobClassloader",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSetClasspathWithJobClassloader() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER, true);\r\n    Map<String, String> env = new HashMap<String, String>();\r\n    MRApps.setClasspath(env, conf);\r\n    String cp = env.get(\"CLASSPATH\");\r\n    String appCp = env.get(\"APP_CLASSPATH\");\r\n    assertFalse(\"MAPREDUCE_JOB_CLASSLOADER true, but job.jar is in the\" + \" classpath!\", cp.contains(\"jar\" + ApplicationConstants.CLASS_PATH_SEPARATOR + \"job\"));\r\n    assertFalse(\"MAPREDUCE_JOB_CLASSLOADER true, but PWD is in the classpath!\", cp.contains(\"PWD\"));\r\n    String expectedAppClasspath = StringUtils.join(ApplicationConstants.CLASS_PATH_SEPARATOR, Arrays.asList(ApplicationConstants.Environment.PWD.$$(), \"job.jar/*\", \"job.jar/classes/\", \"job.jar/lib/*\", ApplicationConstants.Environment.PWD.$$() + \"/*\"));\r\n    assertEquals(\"MAPREDUCE_JOB_CLASSLOADER true, but job.jar is not in the app\" + \" classpath!\", expectedAppClasspath, appCp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetClasspathWithFramework",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSetClasspathWithFramework() throws IOException\n{\r\n    final String FRAMEWORK_NAME = \"some-framework-name\";\r\n    final String FRAMEWORK_PATH = \"some-framework-path#\" + FRAMEWORK_NAME;\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    conf.set(MRJobConfig.MAPREDUCE_APPLICATION_FRAMEWORK_PATH, FRAMEWORK_PATH);\r\n    Map<String, String> env = new HashMap<String, String>();\r\n    try {\r\n        MRApps.setClasspath(env, conf);\r\n        fail(\"Failed to catch framework path set without classpath change\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertTrue(\"Unexpected IllegalArgumentException\", e.getMessage().contains(\"Could not locate MapReduce framework name '\" + FRAMEWORK_NAME + \"'\"));\r\n    }\r\n    env.clear();\r\n    final String FRAMEWORK_CLASSPATH = FRAMEWORK_NAME + \"/*.jar\";\r\n    conf.set(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH, FRAMEWORK_CLASSPATH);\r\n    MRApps.setClasspath(env, conf);\r\n    final String stdClasspath = StringUtils.join(ApplicationConstants.CLASS_PATH_SEPARATOR, Arrays.asList(\"job.jar/*\", \"job.jar/classes/\", \"job.jar/lib/*\", ApplicationConstants.Environment.PWD.$$() + \"/*\"));\r\n    String expectedClasspath = StringUtils.join(ApplicationConstants.CLASS_PATH_SEPARATOR, Arrays.asList(ApplicationConstants.Environment.PWD.$$(), FRAMEWORK_CLASSPATH, stdClasspath));\r\n    assertEquals(\"Incorrect classpath with framework and no user precedence\", expectedClasspath, env.get(\"CLASSPATH\"));\r\n    env.clear();\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, true);\r\n    MRApps.setClasspath(env, conf);\r\n    expectedClasspath = StringUtils.join(ApplicationConstants.CLASS_PATH_SEPARATOR, Arrays.asList(ApplicationConstants.Environment.PWD.$$(), stdClasspath, FRAMEWORK_CLASSPATH));\r\n    assertEquals(\"Incorrect classpath with framework and user precedence\", expectedClasspath, env.get(\"CLASSPATH\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetupDistributedCacheEmpty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSetupDistributedCacheEmpty() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();\r\n    MRApps.setupDistributedCache(conf, localResources);\r\n    assertTrue(\"Empty Config did not produce an empty list of resources\", localResources.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetupDistributedCacheConflicts",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testSetupDistributedCacheConflicts() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(\"fs.mockfs.impl\", MockFileSystem.class, FileSystem.class);\r\n    URI mockUri = URI.create(\"mockfs://mock/\");\r\n    FileSystem mockFs = ((FilterFileSystem) FileSystem.get(mockUri, conf)).getRawFileSystem();\r\n    URI archive = new URI(\"mockfs://mock/tmp/something.zip#something\");\r\n    Path archivePath = new Path(archive);\r\n    URI file = new URI(\"mockfs://mock/tmp/something.txt#something\");\r\n    Path filePath = new Path(file);\r\n    when(mockFs.resolvePath(archivePath)).thenReturn(archivePath);\r\n    when(mockFs.resolvePath(filePath)).thenReturn(filePath);\r\n    Job.addCacheArchive(archive, conf);\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_TIMESTAMPS, \"10\");\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_SIZES, \"10\");\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_VISIBILITIES, \"true\");\r\n    Job.addCacheFile(file, conf);\r\n    conf.set(MRJobConfig.CACHE_FILE_TIMESTAMPS, \"11\");\r\n    conf.set(MRJobConfig.CACHE_FILES_SIZES, \"11\");\r\n    conf.set(MRJobConfig.CACHE_FILE_VISIBILITIES, \"true\");\r\n    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();\r\n    MRApps.setupDistributedCache(conf, localResources);\r\n    assertEquals(1, localResources.size());\r\n    LocalResource lr = localResources.get(\"something\");\r\n    assertNotNull(lr);\r\n    assertEquals(10l, lr.getSize());\r\n    assertEquals(10l, lr.getTimestamp());\r\n    assertEquals(LocalResourceType.ARCHIVE, lr.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetupDistributedCacheConflictsFiles",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testSetupDistributedCacheConflictsFiles() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(\"fs.mockfs.impl\", MockFileSystem.class, FileSystem.class);\r\n    URI mockUri = URI.create(\"mockfs://mock/\");\r\n    FileSystem mockFs = ((FilterFileSystem) FileSystem.get(mockUri, conf)).getRawFileSystem();\r\n    URI file = new URI(\"mockfs://mock/tmp/something.zip#something\");\r\n    Path filePath = new Path(file);\r\n    URI file2 = new URI(\"mockfs://mock/tmp/something.txt#something\");\r\n    Path file2Path = new Path(file2);\r\n    when(mockFs.resolvePath(filePath)).thenReturn(filePath);\r\n    when(mockFs.resolvePath(file2Path)).thenReturn(file2Path);\r\n    Job.addCacheFile(file, conf);\r\n    Job.addCacheFile(file2, conf);\r\n    conf.set(MRJobConfig.CACHE_FILE_TIMESTAMPS, \"10,11\");\r\n    conf.set(MRJobConfig.CACHE_FILES_SIZES, \"10,11\");\r\n    conf.set(MRJobConfig.CACHE_FILE_VISIBILITIES, \"true,true\");\r\n    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();\r\n    MRApps.setupDistributedCache(conf, localResources);\r\n    assertEquals(1, localResources.size());\r\n    LocalResource lr = localResources.get(\"something\");\r\n    assertNotNull(lr);\r\n    assertEquals(10l, lr.getSize());\r\n    assertEquals(10l, lr.getTimestamp());\r\n    assertEquals(LocalResourceType.FILE, lr.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSetupDistributedCache",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testSetupDistributedCache() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(\"fs.mockfs.impl\", MockFileSystem.class, FileSystem.class);\r\n    URI mockUri = URI.create(\"mockfs://mock/\");\r\n    FileSystem mockFs = ((FilterFileSystem) FileSystem.get(mockUri, conf)).getRawFileSystem();\r\n    URI archive = new URI(\"mockfs://mock/tmp/something.zip\");\r\n    Path archivePath = new Path(archive);\r\n    URI file = new URI(\"mockfs://mock/tmp/something.txt#something\");\r\n    Path filePath = new Path(file);\r\n    when(mockFs.resolvePath(archivePath)).thenReturn(archivePath);\r\n    when(mockFs.resolvePath(filePath)).thenReturn(filePath);\r\n    Job.addCacheArchive(archive, conf);\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_TIMESTAMPS, \"10\");\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_SIZES, \"10\");\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_VISIBILITIES, \"true\");\r\n    Job.addCacheFile(file, conf);\r\n    conf.set(MRJobConfig.CACHE_FILE_TIMESTAMPS, \"11\");\r\n    conf.set(MRJobConfig.CACHE_FILES_SIZES, \"11\");\r\n    conf.set(MRJobConfig.CACHE_FILE_VISIBILITIES, \"true\");\r\n    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();\r\n    MRApps.setupDistributedCache(conf, localResources);\r\n    assertEquals(2, localResources.size());\r\n    LocalResource lr = localResources.get(\"something.zip\");\r\n    assertNotNull(lr);\r\n    assertEquals(10l, lr.getSize());\r\n    assertEquals(10l, lr.getTimestamp());\r\n    assertEquals(LocalResourceType.ARCHIVE, lr.getType());\r\n    lr = localResources.get(\"something\");\r\n    assertNotNull(lr);\r\n    assertEquals(11l, lr.getSize());\r\n    assertEquals(11l, lr.getTimestamp());\r\n    assertEquals(LocalResourceType.FILE, lr.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testLogSystemProperties",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testLogSystemProperties() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MAPREDUCE_JVM_SYSTEM_PROPERTIES_TO_LOG, \" \");\r\n    String value = MRApps.getSystemPropertiesToLog(conf);\r\n    assertNull(value);\r\n    String classpath = \"java.class.path\";\r\n    String os = \"os.name\";\r\n    String version = \"java.version\";\r\n    conf.set(MRJobConfig.MAPREDUCE_JVM_SYSTEM_PROPERTIES_TO_LOG, classpath + \", \" + os);\r\n    value = MRApps.getSystemPropertiesToLog(conf);\r\n    assertNotNull(value);\r\n    assertTrue(value.contains(classpath));\r\n    assertTrue(value.contains(os));\r\n    assertFalse(value.contains(version));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testTaskStateUI",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testTaskStateUI()\n{\r\n    assertTrue(MRApps.TaskStateUI.PENDING.correspondsTo(TaskState.SCHEDULED));\r\n    assertTrue(MRApps.TaskStateUI.COMPLETED.correspondsTo(TaskState.SUCCEEDED));\r\n    assertTrue(MRApps.TaskStateUI.COMPLETED.correspondsTo(TaskState.FAILED));\r\n    assertTrue(MRApps.TaskStateUI.COMPLETED.correspondsTo(TaskState.KILLED));\r\n    assertTrue(MRApps.TaskStateUI.RUNNING.correspondsTo(TaskState.RUNNING));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testSystemClasses",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSystemClasses()\n{\r\n    final List<String> systemClasses = Arrays.asList(StringUtils.getTrimmedStrings(ApplicationClassLoader.SYSTEM_CLASSES_DEFAULT));\r\n    for (String defaultXml : DEFAULT_XMLS) {\r\n        assertTrue(defaultXml + \" must be system resource\", ApplicationClassLoader.isSystemClass(defaultXml, systemClasses));\r\n    }\r\n    for (String klass : SYS_CLASSES) {\r\n        assertTrue(klass + \" must be system class\", ApplicationClassLoader.isSystemClass(klass, systemClasses));\r\n    }\r\n    assertFalse(\"/fake/Klass must not be a system class\", ApplicationClassLoader.isSystemClass(\"/fake/Klass\", systemClasses));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\util",
  "methodName" : "testInvalidWebappAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testInvalidWebappAddress() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(JHAdminConfig.MR_HISTORY_WEBAPP_ADDRESS, \"19888\");\r\n    MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(conf, ApplicationId.newInstance(0, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "test",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void test()\n{\r\n    testPbServerFactory();\r\n    testPbClientFactory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testPbServerFactory",
  "errType" : [ "YarnRuntimeException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPbServerFactory()\n{\r\n    InetSocketAddress addr = new InetSocketAddress(0);\r\n    Configuration conf = new Configuration();\r\n    MRClientProtocol instance = new MRClientProtocolTestImpl();\r\n    Server server = null;\r\n    try {\r\n        server = RpcServerFactoryPBImpl.get().getServer(MRClientProtocol.class, instance, addr, conf, null, 1);\r\n        server.start();\r\n    } catch (YarnRuntimeException e) {\r\n        e.printStackTrace();\r\n        Assert.fail(\"Failed to crete server\");\r\n    } finally {\r\n        server.stop();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-common\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testPbClientFactory",
  "errType" : [ "YarnRuntimeException", "YarnRuntimeException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testPbClientFactory()\n{\r\n    InetSocketAddress addr = new InetSocketAddress(0);\r\n    System.err.println(addr.getHostName() + addr.getPort());\r\n    Configuration conf = new Configuration();\r\n    MRClientProtocol instance = new MRClientProtocolTestImpl();\r\n    Server server = null;\r\n    try {\r\n        server = RpcServerFactoryPBImpl.get().getServer(MRClientProtocol.class, instance, addr, conf, null, 1);\r\n        server.start();\r\n        System.err.println(server.getListenerAddress());\r\n        System.err.println(NetUtils.getConnectAddress(server));\r\n        MRClientProtocol client = null;\r\n        try {\r\n            client = (MRClientProtocol) RpcClientFactoryPBImpl.get().getClient(MRClientProtocol.class, 1, NetUtils.getConnectAddress(server), conf);\r\n        } catch (YarnRuntimeException e) {\r\n            e.printStackTrace();\r\n            Assert.fail(\"Failed to crete client\");\r\n        }\r\n    } catch (YarnRuntimeException e) {\r\n        e.printStackTrace();\r\n        Assert.fail(\"Failed to crete server\");\r\n    } finally {\r\n        server.stop();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
} ]
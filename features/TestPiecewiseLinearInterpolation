[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "makeRR",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LoggedSingleRelativeRanking makeRR(double ranking, long datum)\n{\r\n    LoggedSingleRelativeRanking result = new LoggedSingleRelativeRanking();\r\n    result.setDatum(datum);\r\n    result.setRelativeRanking(ranking);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "testOneRun",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testOneRun()\n{\r\n    LoggedDiscreteCDF input = new LoggedDiscreteCDF();\r\n    input.setMinimum(100000L);\r\n    input.setMaximum(1100000L);\r\n    ArrayList<LoggedSingleRelativeRanking> rankings = new ArrayList<LoggedSingleRelativeRanking>();\r\n    rankings.add(makeRR(0.1, 200000L));\r\n    rankings.add(makeRR(0.5, 800000L));\r\n    rankings.add(makeRR(0.9, 1000000L));\r\n    input.setRankings(rankings);\r\n    input.setNumberValues(3);\r\n    CDFRandomGenerator gen = new CDFPiecewiseLinearRandomGenerator(input);\r\n    Histogram values = new Histogram();\r\n    for (int i = 0; i < 1000000; ++i) {\r\n        long value = gen.randomValue();\r\n        values.enter(value);\r\n    }\r\n    int[] percentiles = new int[99];\r\n    for (int i = 0; i < 99; ++i) {\r\n        percentiles[i] = i + 1;\r\n    }\r\n    long[] result = values.getCDF(100, percentiles);\r\n    long sumErrorSquares = 0L;\r\n    for (int i = 0; i < 10; ++i) {\r\n        long error = result[i] - (10000L * i + 100000L);\r\n        System.out.println(\"element \" + i + \", got \" + result[i] + \", expected \" + (10000L * i + 100000L) + \", error = \" + error);\r\n        sumErrorSquares += error * error;\r\n    }\r\n    for (int i = 10; i < 50; ++i) {\r\n        long error = result[i] - (15000L * i + 50000L);\r\n        System.out.println(\"element \" + i + \", got \" + result[i] + \", expected \" + (15000L * i + 50000L) + \", error = \" + error);\r\n        sumErrorSquares += error * error;\r\n    }\r\n    for (int i = 50; i < 90; ++i) {\r\n        long error = result[i] - (5000L * i + 550000L);\r\n        System.out.println(\"element \" + i + \", got \" + result[i] + \", expected \" + (5000L * i + 550000L) + \", error = \" + error);\r\n        sumErrorSquares += error * error;\r\n    }\r\n    for (int i = 90; i <= 100; ++i) {\r\n        long error = result[i] - (10000L * i + 100000L);\r\n        System.out.println(\"element \" + i + \", got \" + result[i] + \", expected \" + (10000L * i + 100000L) + \", error = \" + error);\r\n        sumErrorSquares += error * error;\r\n    }\r\n    double realSumErrorSquares = (double) sumErrorSquares;\r\n    double normalizedError = realSumErrorSquares / 100 / rankings.get(1).getDatum() / rankings.get(1).getDatum();\r\n    double RMSNormalizedError = Math.sqrt(normalizedError);\r\n    System.out.println(\"sumErrorSquares = \" + sumErrorSquares);\r\n    System.out.println(\"normalizedError: \" + normalizedError + \", RMSNormalizedError: \" + RMSNormalizedError);\r\n    System.out.println(\"Cumulative error is \" + RMSNormalizedError);\r\n    assertTrue(\"The RMS relative error per bucket, \" + RMSNormalizedError + \", exceeds our tolerance of \" + maximumRelativeError, RMSNormalizedError <= maximumRelativeError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "testSeedGeneration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSeedGeneration()\n{\r\n    long masterSeed1 = 42;\r\n    long masterSeed2 = 43;\r\n    assertTrue(\"Deterministic seeding\", getSeed(\"stream1\", masterSeed1) == getSeed(\"stream1\", masterSeed1));\r\n    assertTrue(\"Deterministic seeding\", getSeed(\"stream2\", masterSeed2) == getSeed(\"stream2\", masterSeed2));\r\n    assertTrue(\"Different streams\", getSeed(\"stream1\", masterSeed1) != getSeed(\"stream2\", masterSeed1));\r\n    assertTrue(\"Different master seeds\", getSeed(\"stream1\", masterSeed1) != getSeed(\"stream1\", masterSeed2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getPercentiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Integer> getPercentiles()\n{\r\n    return percentiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setPercentiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPercentiles(List<Integer> percentiles)\n{\r\n    this.percentiles = percentiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getScale",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getScale()\n{\r\n    return scale;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setScale",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setScale(int scale)\n{\r\n    this.scale = scale;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Long> getData()\n{\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "setData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setData(List<Long> data)\n{\r\n    this.data = data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "testHistograms",
  "errType" : [ "DeepInequalityException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testHistograms() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem lfs = FileSystem.getLocal(conf);\r\n    final Path rootInputDir = lfs.makeQualified(new Path(System.getProperty(\"test.tools.input.dir\", \"target/input\")));\r\n    final Path rootInputFile = new Path(rootInputDir, \"rumen/histogram-tests\");\r\n    FileStatus[] tests = lfs.listStatus(rootInputFile);\r\n    for (int i = 0; i < tests.length; ++i) {\r\n        Path filePath = tests[i].getPath();\r\n        String fileName = filePath.getName();\r\n        if (fileName.startsWith(\"input\")) {\r\n            String testName = fileName.substring(\"input\".length());\r\n            Path goldFilePath = new Path(rootInputFile, \"gold\" + testName);\r\n            assertTrue(\"Gold file dies not exist\", lfs.exists(goldFilePath));\r\n            LoggedDiscreteCDF newResult = histogramFileToCDF(filePath, lfs);\r\n            System.out.println(\"Testing a Histogram for \" + fileName);\r\n            FSDataInputStream goldStream = lfs.open(goldFilePath);\r\n            JsonObjectMapperParser<LoggedDiscreteCDF> parser = new JsonObjectMapperParser<LoggedDiscreteCDF>(goldStream, LoggedDiscreteCDF.class);\r\n            try {\r\n                LoggedDiscreteCDF dcdf = parser.getNext();\r\n                dcdf.deepCompare(newResult, new TreePath(null, \"<root>\"));\r\n            } catch (DeepInequalityException e) {\r\n                fail(e.path.toString());\r\n            } finally {\r\n                parser.close();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "histogramFileToCDF",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "LoggedDiscreteCDF histogramFileToCDF(Path path, FileSystem fs) throws IOException\n{\r\n    FSDataInputStream dataStream = fs.open(path);\r\n    JsonObjectMapperParser<HistogramRawTestData> parser = new JsonObjectMapperParser<HistogramRawTestData>(dataStream, HistogramRawTestData.class);\r\n    HistogramRawTestData data;\r\n    try {\r\n        data = parser.getNext();\r\n    } finally {\r\n        parser.close();\r\n    }\r\n    Histogram hist = new Histogram();\r\n    List<Long> measurements = data.getData();\r\n    List<Long> typeProbeData = new HistogramRawTestData().getData();\r\n    assertTrue(\"The data attribute of a jackson-reconstructed HistogramRawTestData \" + \" should be a \" + typeProbeData.getClass().getName() + \", like a virgin HistogramRawTestData, but it's a \" + measurements.getClass().getName(), measurements.getClass() == typeProbeData.getClass());\r\n    for (int j = 0; j < measurements.size(); ++j) {\r\n        hist.enter(measurements.get(j));\r\n    }\r\n    LoggedDiscreteCDF result = new LoggedDiscreteCDF();\r\n    int[] percentiles = new int[data.getPercentiles().size()];\r\n    for (int j = 0; j < data.getPercentiles().size(); ++j) {\r\n        percentiles[j] = data.getPercentiles().get(j);\r\n    }\r\n    result.setCDF(hist, percentiles, data.getScale());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem lfs = FileSystem.getLocal(conf);\r\n    for (String arg : args) {\r\n        Path filePath = lfs.makeQualified(new Path(arg));\r\n        String fileName = filePath.getName();\r\n        if (fileName.startsWith(\"input\")) {\r\n            LoggedDiscreteCDF newResult = histogramFileToCDF(filePath, lfs);\r\n            String testName = fileName.substring(\"input\".length());\r\n            Path goldFilePath = new Path(filePath.getParent(), \"gold\" + testName);\r\n            ObjectMapper mapper = new ObjectMapper();\r\n            JsonFactory factory = mapper.getFactory();\r\n            FSDataOutputStream ostream = lfs.create(goldFilePath, true);\r\n            JsonGenerator gen = factory.createGenerator((OutputStream) ostream, JsonEncoding.UTF8);\r\n            gen.useDefaultPrettyPrinter();\r\n            gen.writeObject(newResult);\r\n            gen.close();\r\n        } else {\r\n            System.err.println(\"Input file not started with \\\"input\\\". File \" + fileName + \" skipped.\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "bindTo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void bindTo(Path path, Configuration conf) throws IOException\n{\r\n    InputStream underlyingInput = null;\r\n    if (name != null) {\r\n        close();\r\n    }\r\n    name = path.getName();\r\n    underlyingInput = new PossiblyDecompressedInputStream(path, conf);\r\n    input = new DelimitedInputStream(new BufferedInputStream(underlyingInput), \"\\f!!FILE=\", \"!!\\n\");\r\n    knownNextFileName = input.nextFileName();\r\n    if (knownNextFileName == null) {\r\n        close();\r\n        return;\r\n    }\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "getNext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Pair<String, InputStream> getNext() throws IOException\n{\r\n    if (knownNextFileName != null) {\r\n        Pair<String, InputStream> result = new Pair<String, InputStream>(knownNextFileName, input);\r\n        knownNextFileName = null;\r\n        return result;\r\n    }\r\n    String nextFileName = input.nextFileName();\r\n    if (nextFileName == null) {\r\n        return null;\r\n    }\r\n    return new Pair<String, InputStream>(nextFileName, input);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-rumen\\src\\test\\java\\org\\apache\\hadoop\\tools\\rumen",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (input != null) {\r\n        input.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp()\n{\r\n    String testRootDir = new File(System.getProperty(\"test.build.data\", \"/tmp\")).getAbsolutePath().replace(' ', '+');\r\n    Random random = new Random(System.currentTimeMillis());\r\n    testDir = testRootDir + File.separatorChar + Long.toString(random.nextLong());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testHelp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testHelp() throws IOException\n{\r\n    String[] args = new String[] { \"-help\" };\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    boolean success = uploader.parseArguments(args);\r\n    Assert.assertFalse(\"Expected to print help\", success);\r\n    assertThat(uploader.input).withFailMessage(\"Expected ignore run\").isNull();\r\n    assertThat(uploader.whitelist).withFailMessage(\"Expected ignore run\").isNull();\r\n    assertThat(uploader.target).withFailMessage(\"Expected ignore run\").isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testWrongArgument",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testWrongArgument() throws IOException\n{\r\n    String[] args = new String[] { \"-unexpected\" };\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    boolean success = uploader.parseArguments(args);\r\n    Assert.assertFalse(\"Expected to print help\", success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testArguments",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testArguments() throws IOException\n{\r\n    String[] args = new String[] { \"-input\", \"A\", \"-whitelist\", \"B\", \"-blacklist\", \"C\", \"-fs\", \"hdfs://C:8020\", \"-target\", \"D\", \"-initialReplication\", \"100\", \"-acceptableReplication\", \"120\", \"-finalReplication\", \"140\", \"-timeout\", \"10\" };\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    boolean success = uploader.parseArguments(args);\r\n    Assert.assertTrue(\"Expected to print help\", success);\r\n    Assert.assertEquals(\"Input mismatch\", \"A\", uploader.input);\r\n    Assert.assertEquals(\"Whitelist mismatch\", \"B\", uploader.whitelist);\r\n    Assert.assertEquals(\"Blacklist mismatch\", \"C\", uploader.blacklist);\r\n    Assert.assertEquals(\"Target mismatch\", \"hdfs://C:8020/D\", uploader.target);\r\n    Assert.assertEquals(\"Initial replication mismatch\", 100, uploader.initialReplication);\r\n    Assert.assertEquals(\"Acceptable replication mismatch\", 120, uploader.acceptableReplication);\r\n    Assert.assertEquals(\"Final replication mismatch\", 140, uploader.finalReplication);\r\n    Assert.assertEquals(\"Timeout mismatch\", 10, uploader.timeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testNoFilesystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testNoFilesystem() throws IOException\n{\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    boolean success = uploader.parseArguments(new String[] {});\r\n    Assert.assertTrue(\"Expected to parse arguments\", success);\r\n    Assert.assertEquals(\"Expected\", \"file:////usr/lib/mr-framework.tar.gz#mr-framework\", uploader.target);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testDefaultFilesystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDefaultFilesystem() throws IOException\n{\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    Configuration conf = new Configuration();\r\n    conf.set(FS_DEFAULT_NAME_KEY, \"hdfs://namenode:555\");\r\n    uploader.setConf(conf);\r\n    boolean success = uploader.parseArguments(new String[] {});\r\n    Assert.assertTrue(\"Expected to parse arguments\", success);\r\n    Assert.assertEquals(\"Expected\", \"hdfs://namenode:555/usr/lib/mr-framework.tar.gz#mr-framework\", uploader.target);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testExplicitFilesystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testExplicitFilesystem() throws IOException\n{\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    Configuration conf = new Configuration();\r\n    uploader.setConf(conf);\r\n    boolean success = uploader.parseArguments(new String[] { \"-target\", \"hdfs://namenode:555/usr/lib/mr-framework.tar.gz#mr-framework\" });\r\n    Assert.assertTrue(\"Expected to parse arguments\", success);\r\n    Assert.assertEquals(\"Expected\", \"hdfs://namenode:555/usr/lib/mr-framework.tar.gz#mr-framework\", uploader.target);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testConflictingFilesystem",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testConflictingFilesystem() throws IOException\n{\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    Configuration conf = new Configuration();\r\n    conf.set(FS_DEFAULT_NAME_KEY, \"hdfs://namenode:555\");\r\n    uploader.setConf(conf);\r\n    boolean success = uploader.parseArguments(new String[] { \"-target\", \"file:///usr/lib/mr-framework.tar.gz#mr-framework\" });\r\n    Assert.assertTrue(\"Expected to parse arguments\", success);\r\n    Assert.assertEquals(\"Expected\", \"file:///usr/lib/mr-framework.tar.gz#mr-framework\", uploader.target);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testCollectPackages",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testCollectPackages() throws IOException, UploaderException\n{\r\n    File parent = new File(testDir);\r\n    try {\r\n        parent.deleteOnExit();\r\n        Assert.assertTrue(\"Directory creation failed\", parent.mkdirs());\r\n        File dirA = new File(parent, \"A\");\r\n        Assert.assertTrue(dirA.mkdirs());\r\n        File dirB = new File(parent, \"B\");\r\n        Assert.assertTrue(dirB.mkdirs());\r\n        File jarA = new File(dirA, \"a.jar\");\r\n        Assert.assertTrue(jarA.createNewFile());\r\n        File jarB = new File(dirA, \"b.jar\");\r\n        Assert.assertTrue(jarB.createNewFile());\r\n        File jarC = new File(dirA, \"c.jar\");\r\n        Assert.assertTrue(jarC.createNewFile());\r\n        File txtD = new File(dirA, \"d.txt\");\r\n        Assert.assertTrue(txtD.createNewFile());\r\n        File jarD = new File(dirB, \"d.jar\");\r\n        Assert.assertTrue(jarD.createNewFile());\r\n        File txtE = new File(dirB, \"e.txt\");\r\n        Assert.assertTrue(txtE.createNewFile());\r\n        FrameworkUploader uploader = new FrameworkUploader();\r\n        uploader.whitelist = \".*a\\\\.jar,.*b\\\\.jar,.*d\\\\.jar\";\r\n        uploader.blacklist = \".*b\\\\.jar\";\r\n        uploader.input = dirA.getAbsolutePath() + File.separatorChar + \"*\" + File.pathSeparatorChar + dirB.getAbsolutePath() + File.separatorChar + \"*\";\r\n        uploader.collectPackages();\r\n        Assert.assertEquals(\"Whitelist count error\", 3, uploader.whitelistedFiles.size());\r\n        Assert.assertEquals(\"Blacklist count error\", 1, uploader.blacklistedFiles.size());\r\n        Assert.assertTrue(\"File not collected\", uploader.filteredInputFiles.contains(jarA.getAbsolutePath()));\r\n        Assert.assertFalse(\"File collected\", uploader.filteredInputFiles.contains(jarB.getAbsolutePath()));\r\n        Assert.assertTrue(\"File not collected\", uploader.filteredInputFiles.contains(jarD.getAbsolutePath()));\r\n        Assert.assertEquals(\"Too many whitelists\", 2, uploader.filteredInputFiles.size());\r\n    } finally {\r\n        FileUtils.deleteDirectory(parent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testBuildTarBall",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testBuildTarBall() throws IOException, UploaderException, InterruptedException\n{\r\n    String[] testFiles = { \"upload.tar\", \"upload.tar.gz\" };\r\n    for (String testFile : testFiles) {\r\n        File parent = new File(testDir);\r\n        try {\r\n            parent.deleteOnExit();\r\n            FrameworkUploader uploader = prepareTree(parent);\r\n            File gzipFile = new File(parent.getAbsolutePath() + \"/\" + testFile);\r\n            gzipFile.deleteOnExit();\r\n            uploader.target = \"file:///\" + gzipFile.getAbsolutePath();\r\n            uploader.beginUpload();\r\n            uploader.buildPackage();\r\n            InputStream stream = new FileInputStream(gzipFile);\r\n            if (gzipFile.getName().endsWith(\".gz\")) {\r\n                stream = new GZIPInputStream(stream);\r\n            }\r\n            TarArchiveInputStream result = null;\r\n            try {\r\n                result = new TarArchiveInputStream(stream);\r\n                Set<String> fileNames = new HashSet<>();\r\n                Set<Long> sizes = new HashSet<>();\r\n                TarArchiveEntry entry1 = result.getNextTarEntry();\r\n                fileNames.add(entry1.getName());\r\n                sizes.add(entry1.getSize());\r\n                TarArchiveEntry entry2 = result.getNextTarEntry();\r\n                fileNames.add(entry2.getName());\r\n                sizes.add(entry2.getSize());\r\n                Assert.assertTrue(\"File name error\", fileNames.contains(\"a.jar\"));\r\n                Assert.assertTrue(\"File size error\", sizes.contains((long) 13));\r\n                Assert.assertTrue(\"File name error\", fileNames.contains(\"b.jar\"));\r\n                Assert.assertTrue(\"File size error\", sizes.contains((long) 14));\r\n            } finally {\r\n                if (result != null) {\r\n                    result.close();\r\n                }\r\n            }\r\n        } finally {\r\n            FileUtils.deleteDirectory(parent);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testUpload",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testUpload() throws IOException, UploaderException, InterruptedException\n{\r\n    final String fileName = \"/upload.tar.gz\";\r\n    File parent = new File(testDir);\r\n    try {\r\n        parent.deleteOnExit();\r\n        FrameworkUploader uploader = prepareTree(parent);\r\n        uploader.target = \"file://\" + parent.getAbsolutePath() + fileName;\r\n        uploader.buildPackage();\r\n        try (TarArchiveInputStream archiveInputStream = new TarArchiveInputStream(new GZIPInputStream(new FileInputStream(parent.getAbsolutePath() + fileName)))) {\r\n            Set<String> fileNames = new HashSet<>();\r\n            Set<Long> sizes = new HashSet<>();\r\n            TarArchiveEntry entry1 = archiveInputStream.getNextTarEntry();\r\n            fileNames.add(entry1.getName());\r\n            sizes.add(entry1.getSize());\r\n            TarArchiveEntry entry2 = archiveInputStream.getNextTarEntry();\r\n            fileNames.add(entry2.getName());\r\n            sizes.add(entry2.getSize());\r\n            Assert.assertTrue(\"File name error\", fileNames.contains(\"a.jar\"));\r\n            Assert.assertTrue(\"File size error\", sizes.contains((long) 13));\r\n            Assert.assertTrue(\"File name error\", fileNames.contains(\"b.jar\"));\r\n            Assert.assertTrue(\"File size error\", sizes.contains((long) 14));\r\n        }\r\n    } finally {\r\n        FileUtils.deleteDirectory(parent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "prepareTree",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "FrameworkUploader prepareTree(File parent) throws FileNotFoundException\n{\r\n    Assert.assertTrue(parent.mkdirs());\r\n    File dirA = new File(parent, \"A\");\r\n    Assert.assertTrue(dirA.mkdirs());\r\n    File jarA = new File(parent, \"a.jar\");\r\n    PrintStream printStream = new PrintStream(new FileOutputStream(jarA));\r\n    printStream.println(\"Hello World!\");\r\n    printStream.close();\r\n    File jarB = new File(dirA, \"b.jar\");\r\n    printStream = new PrintStream(new FileOutputStream(jarB));\r\n    printStream.println(\"Hello Galaxy!\");\r\n    printStream.close();\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    uploader.filteredInputFiles.add(jarA.getAbsolutePath());\r\n    uploader.filteredInputFiles.add(jarB.getAbsolutePath());\r\n    return uploader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testEnvironmentReplacement",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEnvironmentReplacement() throws UploaderException\n{\r\n    String input = \"C/$A/B,$B,D\";\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(\"A\", \"X\");\r\n    map.put(\"B\", \"Y\");\r\n    map.put(\"C\", \"Z\");\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    String output = uploader.expandEnvironmentVariables(input, map);\r\n    Assert.assertEquals(\"Environment not expanded\", \"C/X/B,Y,D\", output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testRecursiveEnvironmentReplacement",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRecursiveEnvironmentReplacement() throws UploaderException\n{\r\n    String input = \"C/$A/B,$B,D\";\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(\"A\", \"X\");\r\n    map.put(\"B\", \"$C\");\r\n    map.put(\"C\", \"Y\");\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    String output = uploader.expandEnvironmentVariables(input, map);\r\n    Assert.assertEquals(\"Environment not expanded\", \"C/X/B,Y,D\", output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testNativeIO",
  "errType" : [ "UnsupportedOperationException", "UnsupportedOperationException", "UnsupportedOperationException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testNativeIO() throws IOException\n{\r\n    FrameworkUploader uploader = new FrameworkUploader();\r\n    File parent = new File(testDir);\r\n    try {\r\n        parent.deleteOnExit();\r\n        Assert.assertTrue(parent.mkdirs());\r\n        File targetFile = new File(parent, \"a.txt\");\r\n        try (FileOutputStream os = new FileOutputStream(targetFile)) {\r\n            IOUtils.writeLines(Lists.newArrayList(\"a\", \"b\"), null, os, StandardCharsets.UTF_8);\r\n        }\r\n        Assert.assertFalse(uploader.checkSymlink(targetFile));\r\n        File symlinkToTarget = new File(parent, \"symlinkToTarget.txt\");\r\n        try {\r\n            Files.createSymbolicLink(Paths.get(symlinkToTarget.getAbsolutePath()), Paths.get(targetFile.getAbsolutePath()));\r\n        } catch (UnsupportedOperationException e) {\r\n            Assume.assumeTrue(false);\r\n        }\r\n        Assert.assertTrue(uploader.checkSymlink(symlinkToTarget));\r\n        symlinkToTarget = new File(parent.getAbsolutePath() + \"/./symlinkToTarget2.txt\");\r\n        try {\r\n            Files.createSymbolicLink(Paths.get(symlinkToTarget.getAbsolutePath()), Paths.get(targetFile.getAbsolutePath()));\r\n        } catch (UnsupportedOperationException e) {\r\n            Assume.assumeTrue(false);\r\n        }\r\n        Assert.assertTrue(uploader.checkSymlink(symlinkToTarget));\r\n        File symlinkOutside = new File(parent, \"symlinkToParent.txt\");\r\n        try {\r\n            Files.createSymbolicLink(Paths.get(symlinkOutside.getAbsolutePath()), Paths.get(parent.getAbsolutePath()));\r\n        } catch (UnsupportedOperationException e) {\r\n            Assume.assumeTrue(false);\r\n        }\r\n        Assert.assertFalse(uploader.checkSymlink(symlinkOutside));\r\n    } finally {\r\n        FileUtils.forceDelete(parent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 5,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-uploader\\src\\test\\java\\org\\apache\\hadoop\\mapred\\uploader",
  "methodName" : "testPermissionSettingsOnRestrictiveUmask",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testPermissionSettingsOnRestrictiveUmask() throws Exception\n{\r\n    File parent = new File(testDir);\r\n    parent.deleteOnExit();\r\n    MiniDFSCluster cluster = null;\r\n    try {\r\n        Assert.assertTrue(\"Directory creation failed\", parent.mkdirs());\r\n        Configuration hdfsConf = new HdfsConfiguration();\r\n        String namenodeDir = new File(MiniDFSCluster.getBaseDirectory(), \"name\").getAbsolutePath();\r\n        hdfsConf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, namenodeDir);\r\n        hdfsConf.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY, namenodeDir);\r\n        hdfsConf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, \"027\");\r\n        cluster = new MiniDFSCluster.Builder(hdfsConf).numDataNodes(1).build();\r\n        DistributedFileSystem dfs = cluster.getFileSystem();\r\n        cluster.waitActive();\r\n        File file1 = new File(parent, \"a.jar\");\r\n        file1.createNewFile();\r\n        File file2 = new File(parent, \"b.jar\");\r\n        file2.createNewFile();\r\n        File file3 = new File(parent, \"c.jar\");\r\n        file3.createNewFile();\r\n        FrameworkUploader uploader = new FrameworkUploader();\r\n        uploader.whitelist = \"\";\r\n        uploader.blacklist = \"\";\r\n        uploader.input = parent.getAbsolutePath() + File.separatorChar + \"*\";\r\n        String hdfsUri = hdfsConf.get(FS_DEFAULT_NAME_KEY);\r\n        String targetPath = \"/test.tar.gz\";\r\n        uploader.target = hdfsUri + targetPath;\r\n        uploader.acceptableReplication = 1;\r\n        uploader.setConf(hdfsConf);\r\n        uploader.collectPackages();\r\n        uploader.buildPackage();\r\n        FileStatus fileStatus = dfs.getFileStatus(new Path(targetPath));\r\n        FsPermission perm = fileStatus.getPermission();\r\n        Assert.assertEquals(\"Permissions\", new FsPermission(0644), perm);\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.close();\r\n        }\r\n        FileUtils.deleteDirectory(parent);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
} ]
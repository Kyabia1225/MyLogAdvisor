[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (conf instanceof JobConf) {\r\n        this.conf = (JobConf) conf;\r\n    } else {\r\n        this.conf = new JobConf(conf, HadoopArchives.class);\r\n    }\r\n    String testJar = System.getProperty(TEST_HADOOP_ARCHIVES_JAR_PATH, null);\r\n    if (testJar != null) {\r\n        this.conf.setJar(testJar);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return this.conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkPaths",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkPaths(Configuration conf, List<Path> paths) throws IOException\n{\r\n    for (Path p : paths) {\r\n        FileSystem fs = p.getFileSystem(conf);\r\n        fs.getFileStatus(p);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "recursivels",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void recursivels(FileSystem fs, FileStatusDir fdir, List<FileStatusDir> out) throws IOException\n{\r\n    if (fdir.getFileStatus().isFile()) {\r\n        out.add(fdir);\r\n        return;\r\n    } else {\r\n        out.add(fdir);\r\n        FileStatus[] listStatus = fs.listStatus(fdir.getFileStatus().getPath());\r\n        fdir.setChildren(listStatus);\r\n        for (FileStatus stat : listStatus) {\r\n            FileStatusDir fstatDir = new FileStatusDir(stat, null);\r\n            recursivels(fs, fstatDir, out);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkValidName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean checkValidName(String name)\n{\r\n    Path tmp = new Path(name);\r\n    if (tmp.depth() != 1) {\r\n        return false;\r\n    }\r\n    if (name.endsWith(\".har\"))\r\n        return true;\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "largestDepth",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path largestDepth(List<Path> paths)\n{\r\n    Path deepest = paths.get(0);\r\n    for (Path p : paths) {\r\n        if (p.depth() > deepest.depth()) {\r\n            deepest = p;\r\n        }\r\n    }\r\n    return deepest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "relPathToRoot",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Path relPathToRoot(Path fullPath, Path root)\n{\r\n    final Path justRoot = new Path(Path.SEPARATOR);\r\n    if (fullPath.depth() == root.depth()) {\r\n        return justRoot;\r\n    } else if (fullPath.depth() > root.depth()) {\r\n        Path retPath = new Path(fullPath.getName());\r\n        Path parent = fullPath.getParent();\r\n        for (int i = 0; i < (fullPath.depth() - root.depth() - 1); i++) {\r\n            retPath = new Path(parent.getName(), retPath);\r\n            parent = parent.getParent();\r\n        }\r\n        return new Path(justRoot, retPath);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "writeTopLevelDirs",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void writeTopLevelDirs(SequenceFile.Writer srcWriter, List<Path> paths, Path parentPath) throws IOException\n{\r\n    List<Path> justPaths = new ArrayList<Path>();\r\n    for (Path p : paths) {\r\n        justPaths.add(new Path(p.toUri().getPath()));\r\n    }\r\n    TreeMap<String, HashSet<String>> allpaths = new TreeMap<String, HashSet<String>>();\r\n    Path deepest = largestDepth(paths);\r\n    Path root = new Path(Path.SEPARATOR);\r\n    for (int i = parentPath.depth(); i < deepest.depth(); i++) {\r\n        List<Path> parents = new ArrayList<Path>();\r\n        for (Path p : justPaths) {\r\n            if (p.compareTo(root) == 0) {\r\n            } else {\r\n                Path parent = p.getParent();\r\n                if (null != parent) {\r\n                    if (allpaths.containsKey(parent.toString())) {\r\n                        HashSet<String> children = allpaths.get(parent.toString());\r\n                        children.add(p.getName());\r\n                    } else {\r\n                        HashSet<String> children = new HashSet<String>();\r\n                        children.add(p.getName());\r\n                        allpaths.put(parent.toString(), children);\r\n                    }\r\n                    parents.add(parent);\r\n                }\r\n            }\r\n        }\r\n        justPaths = parents;\r\n    }\r\n    Set<Map.Entry<String, HashSet<String>>> keyVals = allpaths.entrySet();\r\n    for (Map.Entry<String, HashSet<String>> entry : keyVals) {\r\n        final Path relPath = relPathToRoot(new Path(entry.getKey()), parentPath);\r\n        if (relPath != null) {\r\n            final String[] children = new String[entry.getValue().size()];\r\n            int i = 0;\r\n            for (String child : entry.getValue()) {\r\n                children[i++] = child;\r\n            }\r\n            append(srcWriter, 0L, relPath.toString(), children);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void append(SequenceFile.Writer srcWriter, long len, String path, String[] children) throws IOException\n{\r\n    srcWriter.append(new LongWritable(len), new HarEntry(path, children));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "archive",
  "errType" : [ "InterruptedException", "IOException" ],
  "containingMethodsNum" : 52,
  "sourceCodeText" : "void archive(Path parentPath, List<Path> srcPaths, String archiveName, Path dest) throws IOException\n{\r\n    checkPaths(conf, srcPaths);\r\n    int numFiles = 0;\r\n    long totalSize = 0;\r\n    FileSystem fs = parentPath.getFileSystem(conf);\r\n    this.blockSize = conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\r\n    this.partSize = conf.getLong(HAR_PARTSIZE_LABEL, partSize);\r\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\r\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\r\n    conf.set(DST_HAR_LABEL, archiveName);\r\n    conf.set(SRC_PARENT_LABEL, fs.makeQualified(parentPath).toString());\r\n    conf.setInt(HAR_REPLICATION_LABEL, repl);\r\n    Path outputPath = new Path(dest, archiveName);\r\n    FileOutputFormat.setOutputPath(conf, outputPath);\r\n    FileSystem outFs = outputPath.getFileSystem(conf);\r\n    if (outFs.exists(outputPath)) {\r\n        throw new IOException(\"Archive path: \" + outputPath.toString() + \" already exists\");\r\n    }\r\n    if (outFs.isFile(dest)) {\r\n        throw new IOException(\"Destination \" + dest.toString() + \" should be a directory but is a file\");\r\n    }\r\n    conf.set(DST_DIR_LABEL, outputPath.toString());\r\n    Path stagingArea;\r\n    try {\r\n        stagingArea = JobSubmissionFiles.getStagingDir(new Cluster(conf), conf);\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n    Path jobDirectory = new Path(stagingArea, NAME + \"_\" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\r\n    FsPermission mapredSysPerms = new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\r\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory, mapredSysPerms);\r\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\r\n    FileSystem jobfs = jobDirectory.getFileSystem(conf);\r\n    Path srcFiles = new Path(jobDirectory, \"_har_src_files\");\r\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\r\n    SequenceFile.Writer srcWriter = SequenceFile.createWriter(jobfs, conf, srcFiles, LongWritable.class, HarEntry.class, SequenceFile.CompressionType.NONE);\r\n    try {\r\n        writeTopLevelDirs(srcWriter, srcPaths, parentPath);\r\n        srcWriter.sync();\r\n        for (Path src : srcPaths) {\r\n            ArrayList<FileStatusDir> allFiles = new ArrayList<FileStatusDir>();\r\n            FileStatus fstatus = fs.getFileStatus(src);\r\n            FileStatusDir fdir = new FileStatusDir(fstatus, null);\r\n            recursivels(fs, fdir, allFiles);\r\n            for (FileStatusDir statDir : allFiles) {\r\n                FileStatus stat = statDir.getFileStatus();\r\n                long len = stat.isDirectory() ? 0 : stat.getLen();\r\n                final Path path = relPathToRoot(stat.getPath(), parentPath);\r\n                final String[] children;\r\n                if (stat.isDirectory()) {\r\n                    FileStatus[] list = statDir.getChildren();\r\n                    children = new String[list.length];\r\n                    for (int i = 0; i < list.length; i++) {\r\n                        children[i] = list[i].getPath().getName();\r\n                    }\r\n                } else {\r\n                    children = null;\r\n                }\r\n                append(srcWriter, len, path.toString(), children);\r\n                srcWriter.sync();\r\n                numFiles++;\r\n                totalSize += len;\r\n            }\r\n        }\r\n    } finally {\r\n        srcWriter.close();\r\n    }\r\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\r\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\r\n    int numMaps = (int) (totalSize / partSize);\r\n    conf.setNumMapTasks(numMaps == 0 ? 1 : numMaps);\r\n    conf.setNumReduceTasks(1);\r\n    conf.setInputFormat(HArchiveInputFormat.class);\r\n    conf.setOutputFormat(NullOutputFormat.class);\r\n    conf.setMapperClass(HArchivesMapper.class);\r\n    conf.setReducerClass(HArchivesReducer.class);\r\n    conf.setMapOutputKeyClass(IntWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    FileInputFormat.addInputPath(conf, jobDirectory);\r\n    conf.setSpeculativeExecution(false);\r\n    JobClient.runJob(conf);\r\n    try {\r\n        jobfs.delete(jobDirectory, true);\r\n    } catch (IOException ie) {\r\n        LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printUsage(Options opts, boolean printDetailed)\n{\r\n    HelpFormatter helpFormatter = new HelpFormatter();\r\n    if (printDetailed) {\r\n        helpFormatter.printHelp(usage.length() + 10, usage, null, opts, null, false);\r\n    } else {\r\n        System.out.println(usage);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "run",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 34,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    try {\r\n        Options options = new Options();\r\n        options.addOption(ARCHIVE_NAME, true, \"Name of the Archive. This is mandatory option\");\r\n        options.addOption(PARENT_PATH, true, \"Parent path of sources. This is mandatory option\");\r\n        options.addOption(REPLICATION, true, \"Replication factor archive files\");\r\n        options.addOption(HELP, false, \"Show the usage\");\r\n        Parser parser = new GnuParser();\r\n        CommandLine commandLine = parser.parse(options, args, true);\r\n        if (commandLine.hasOption(HELP)) {\r\n            printUsage(options, true);\r\n            return 0;\r\n        }\r\n        if (!commandLine.hasOption(ARCHIVE_NAME)) {\r\n            printUsage(options, false);\r\n            throw new IOException(\"Archive Name not specified.\");\r\n        }\r\n        String archiveName = commandLine.getOptionValue(ARCHIVE_NAME);\r\n        if (!checkValidName(archiveName)) {\r\n            printUsage(options, false);\r\n            throw new IOException(\"Invalid name for archives. \" + archiveName);\r\n        }\r\n        if (!commandLine.hasOption(PARENT_PATH)) {\r\n            printUsage(options, false);\r\n            throw new IOException(\"Parent path not specified.\");\r\n        }\r\n        Path parentPath = new Path(commandLine.getOptionValue(PARENT_PATH));\r\n        if (!parentPath.isAbsolute()) {\r\n            parentPath = parentPath.getFileSystem(getConf()).makeQualified(parentPath);\r\n        }\r\n        if (commandLine.hasOption(REPLICATION)) {\r\n            repl = Short.parseShort(commandLine.getOptionValue(REPLICATION));\r\n        }\r\n        args = commandLine.getArgs();\r\n        List<Path> srcPaths = new ArrayList<Path>();\r\n        Path destPath = null;\r\n        for (int i = 0; i < args.length; i++) {\r\n            if (i == (args.length - 1)) {\r\n                destPath = new Path(args[i]);\r\n                if (!destPath.isAbsolute()) {\r\n                    destPath = destPath.getFileSystem(getConf()).makeQualified(destPath);\r\n                }\r\n            } else {\r\n                Path argPath = new Path(args[i]);\r\n                if (argPath.isAbsolute()) {\r\n                    printUsage(options, false);\r\n                    throw new IOException(\"Source path \" + argPath + \" is not relative to \" + parentPath);\r\n                }\r\n                srcPaths.add(new Path(parentPath, argPath));\r\n            }\r\n        }\r\n        if (destPath == null) {\r\n            printUsage(options, false);\r\n            throw new IOException(\"Destination path not specified.\");\r\n        }\r\n        if (srcPaths.size() == 0) {\r\n            srcPaths.add(parentPath);\r\n        }\r\n        List<Path> globPaths = new ArrayList<Path>();\r\n        for (Path p : srcPaths) {\r\n            FileSystem fs = p.getFileSystem(getConf());\r\n            FileStatus[] statuses = fs.globStatus(p);\r\n            if (statuses != null) {\r\n                for (FileStatus status : statuses) {\r\n                    globPaths.add(fs.makeQualified(status.getPath()));\r\n                }\r\n            }\r\n        }\r\n        if (globPaths.isEmpty()) {\r\n            throw new IOException(\"The resolved paths set is empty.\" + \"  Please check whether the srcPaths exist, where srcPaths = \" + srcPaths);\r\n        }\r\n        archive(parentPath, globPaths, archiveName, destPath);\r\n    } catch (IOException ie) {\r\n        System.err.println(ie.getLocalizedMessage());\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    JobConf job = new JobConf(HadoopArchives.class);\r\n    HadoopArchives harchives = new HadoopArchives(job);\r\n    int ret = 0;\r\n    try {\r\n        ret = ToolRunner.run(harchives, args);\r\n    } catch (Exception e) {\r\n        LOG.debug(\"Exception in archives  \", e);\r\n        System.err.println(e.getClass().getSimpleName() + \" in archives\");\r\n        final String s = e.getLocalizedMessage();\r\n        if (s != null) {\r\n            System.err.println(s);\r\n        } else {\r\n            e.printStackTrace(System.err);\r\n        }\r\n        System.exit(1);\r\n    }\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
} ]
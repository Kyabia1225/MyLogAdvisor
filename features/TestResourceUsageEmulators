[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testResourceUsageMatcher",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testResourceUsageMatcher() throws Exception\n{\r\n    ResourceUsageMatcher matcher = new ResourceUsageMatcher();\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, TestResourceUsageEmulatorPlugin.class, ResourceUsageEmulatorPlugin.class);\r\n    long currentTime = System.currentTimeMillis();\r\n    matcher.configure(conf, null, null, null);\r\n    matcher.matchResourceUsage();\r\n    String id = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;\r\n    long result = TestResourceUsageEmulatorPlugin.testInitialization(id, conf);\r\n    assertTrue(\"Resource usage matcher failed to initialize the configured\" + \" plugin\", result > currentTime);\r\n    result = TestResourceUsageEmulatorPlugin.testEmulation(id, conf);\r\n    assertTrue(\"Resource usage matcher failed to load and emulate the\" + \" configured plugin\", result > currentTime);\r\n    conf.setStrings(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, TestCpu.class.getName() + \",\" + TestOthers.class.getName());\r\n    matcher.configure(conf, null, null, null);\r\n    long time1 = TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);\r\n    long time2 = TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, conf);\r\n    assertTrue(\"Resource usage matcher failed to initialize the configured\" + \" plugins in order\", time1 < time2);\r\n    matcher.matchResourceUsage();\r\n    time1 = TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);\r\n    time2 = TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, conf);\r\n    assertTrue(\"Resource usage matcher failed to load the configured plugins\", time1 < time2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testResourceUsageMatcherRunner",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testResourceUsageMatcherRunner() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FakeProgressive progress = new FakeProgressive();\r\n    conf.setClass(TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN, DummyResourceCalculatorPlugin.class, ResourceCalculatorPlugin.class);\r\n    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, TestResourceUsageEmulatorPlugin.class, ResourceUsageEmulatorPlugin.class);\r\n    long currentTime = System.currentTimeMillis();\r\n    TaskAttemptID id = new TaskAttemptID(\"test\", 1, TaskType.MAP, 1, 1);\r\n    StatusReporter reporter = new DummyReporter(progress);\r\n    TaskInputOutputContext context = new MapContextImpl(conf, id, null, null, null, reporter, null);\r\n    FakeResourceUsageMatcherRunner matcher = new FakeResourceUsageMatcherRunner(context, null);\r\n    String identifier = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;\r\n    long initTime = TestResourceUsageEmulatorPlugin.testInitialization(identifier, conf);\r\n    assertTrue(\"ResourceUsageMatcherRunner failed to initialize the\" + \" configured plugin\", initTime > currentTime);\r\n    assertEquals(\"Progress mismatch in ResourceUsageMatcherRunner\", 0, progress.getProgress(), 0D);\r\n    progress.setProgress(0.01f);\r\n    currentTime = System.currentTimeMillis();\r\n    matcher.test();\r\n    long emulateTime = TestResourceUsageEmulatorPlugin.testEmulation(identifier, conf);\r\n    assertTrue(\"ProgressBasedResourceUsageMatcher failed to load and emulate\" + \" the configured plugin\", emulateTime > currentTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testCpuUsageEmulator",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCpuUsageEmulator() throws IOException\n{\r\n    long target = 100000L;\r\n    int unitUsage = 50;\r\n    FakeCpuUsageEmulatorCore fakeCpuEmulator = new FakeCpuUsageEmulatorCore();\r\n    fakeCpuEmulator.setUnitUsage(unitUsage);\r\n    FakeResourceUsageMonitor fakeMonitor = new FakeResourceUsageMonitor(fakeCpuEmulator);\r\n    fakeCpuEmulator.calibrate(fakeMonitor, target);\r\n    assertEquals(\"Fake calibration failed\", 100, fakeMonitor.getCumulativeCpuTime());\r\n    assertEquals(\"Fake calibration failed\", 100, fakeCpuEmulator.getCpuUsage());\r\n    assertEquals(\"Fake calibration failed\", 2, fakeCpuEmulator.getNumCalls());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "createMetrics",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ResourceUsageMetrics createMetrics(long target)\n{\r\n    ResourceUsageMetrics metrics = new ResourceUsageMetrics();\r\n    metrics.setCumulativeCpuUsage(target);\r\n    metrics.setVirtualMemoryUsage(target);\r\n    metrics.setPhysicalMemoryUsage(target);\r\n    metrics.setHeapUsage(target);\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testCumulativeCpuUsageEmulatorPlugin",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void testCumulativeCpuUsageEmulatorPlugin() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    long targetCpuUsage = 1000L;\r\n    int unitCpuUsage = 50;\r\n    FakeProgressive fakeProgress = new FakeProgressive();\r\n    FakeCpuUsageEmulatorCore fakeCore = new FakeCpuUsageEmulatorCore();\r\n    fakeCore.setUnitUsage(unitCpuUsage);\r\n    CumulativeCpuUsageEmulatorPlugin cpuPlugin = new CumulativeCpuUsageEmulatorPlugin(fakeCore);\r\n    ResourceUsageMetrics invalidUsage = createMetrics(0);\r\n    cpuPlugin.initialize(conf, invalidUsage, null, null);\r\n    int numCallsPre = fakeCore.getNumCalls();\r\n    long cpuUsagePre = fakeCore.getCpuUsage();\r\n    cpuPlugin.emulate();\r\n    int numCallsPost = fakeCore.getNumCalls();\r\n    long cpuUsagePost = fakeCore.getCpuUsage();\r\n    assertEquals(\"Disabled cumulative CPU usage emulation plugin works!\", numCallsPre, numCallsPost);\r\n    assertEquals(\"Disabled cumulative CPU usage emulation plugin works!\", cpuUsagePre, cpuUsagePost);\r\n    float progress = cpuPlugin.getProgress();\r\n    assertEquals(\"Invalid progress of disabled cumulative CPU usage emulation \" + \"plugin!\", 1.0f, progress, 0f);\r\n    ResourceUsageMetrics metrics = createMetrics(targetCpuUsage);\r\n    ResourceCalculatorPlugin monitor = new FakeResourceUsageMonitor(fakeCore);\r\n    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, targetCpuUsage, targetCpuUsage / unitCpuUsage);\r\n    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL, 0.2F);\r\n    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, targetCpuUsage, targetCpuUsage / unitCpuUsage);\r\n    fakeProgress = new FakeProgressive();\r\n    fakeCore.reset();\r\n    fakeCore.setUnitUsage(1);\r\n    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL, 0.25F);\r\n    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);\r\n    long initCpuUsage = monitor.getCumulativeCpuTime();\r\n    long initNumCalls = fakeCore.getNumCalls();\r\n    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, initNumCalls, \"[no-op, 0 progress]\");\r\n    testEmulationBoundary(0.24F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, initNumCalls, \"[no-op, 24% progress]\");\r\n    testEmulationBoundary(0.25F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, initNumCalls, \"[op, 25% progress]\");\r\n    testEmulationBoundary(0.80F, fakeCore, fakeProgress, cpuPlugin, 410, 410, \"[op, 80% progress]\");\r\n    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage, targetCpuUsage, \"[op, 100% progress]\");\r\n    fakeProgress = new FakeProgressive();\r\n    fakeCore.reset();\r\n    fakeCore.setUnitUsage(unitCpuUsage);\r\n    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL, 0.40F);\r\n    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);\r\n    initCpuUsage = monitor.getCumulativeCpuTime();\r\n    initNumCalls = fakeCore.getNumCalls();\r\n    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, initNumCalls, \"[no-op, 0 progress]\");\r\n    testEmulationBoundary(0.39F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, initNumCalls, \"[no-op, 39% progress]\");\r\n    testEmulationBoundary(0.40F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, initNumCalls, \"[op, 40% progress]\");\r\n    testEmulationBoundary(0.90F, fakeCore, fakeProgress, cpuPlugin, 700, 700 / unitCpuUsage, \"[op, 90% progress]\");\r\n    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage, targetCpuUsage / unitCpuUsage, \"[op, 100% progress]\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testEmulationAccuracy",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testEmulationAccuracy(Configuration conf, FakeCpuUsageEmulatorCore fakeCore, ResourceCalculatorPlugin monitor, ResourceUsageMetrics metrics, CumulativeCpuUsageEmulatorPlugin cpuPlugin, long expectedTotalCpuUsage, long expectedTotalNumCalls) throws Exception\n{\r\n    FakeProgressive fakeProgress = new FakeProgressive();\r\n    fakeCore.reset();\r\n    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);\r\n    int numLoops = 0;\r\n    while (fakeProgress.getProgress() < 1) {\r\n        ++numLoops;\r\n        float progress = (float) numLoops / 100;\r\n        fakeProgress.setProgress(progress);\r\n        cpuPlugin.emulate();\r\n    }\r\n    assertEquals(\"Cumulative cpu usage emulator plugin failed (num calls)!\", expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);\r\n    assertEquals(\"Cumulative cpu usage emulator plugin failed (total usage)!\", expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testEmulationBoundary",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testEmulationBoundary(float progress, FakeCpuUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, CumulativeCpuUsageEmulatorPlugin cpuPlugin, long expectedTotalCpuUsage, long expectedTotalNumCalls, String info) throws Exception\n{\r\n    fakeProgress.setProgress(progress);\r\n    cpuPlugin.emulate();\r\n    assertEquals(\"Emulation interval test for cpu usage failed \" + info + \"!\", expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);\r\n    assertEquals(\"Emulation interval test for num calls failed \" + info + \"!\", expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "createRootDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createRootDir() throws IOException\n{\r\n    conf = new Configuration();\r\n    fs = FileSystem.getLocal(conf);\r\n    rootDir = new Path(fs.makeQualified(new Path(System.getProperty(\"test.build.data\", \"/tmp\"))), \"gridmixUserResolve\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "writeUserList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeUserList(Path usersFilePath, String usersFileContent) throws IOException\n{\r\n    FSDataOutputStream out = null;\r\n    try {\r\n        out = fs.create(usersFilePath, true);\r\n        out.writeBytes(usersFileContent);\r\n    } finally {\r\n        if (out != null) {\r\n            out.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateBadUsersFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateBadUsersFile(UserResolver rslv, URI userRsrc, String expectedErrorMsg)\n{\r\n    boolean fail = false;\r\n    try {\r\n        rslv.setTargetUsers(userRsrc, conf);\r\n    } catch (IOException e) {\r\n        assertTrue(\"Exception message from RoundRobinUserResolver is wrong\", e.getMessage().equals(expectedErrorMsg));\r\n        fail = true;\r\n    }\r\n    assertTrue(\"User list required for RoundRobinUserResolver\", fail);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRoundRobinResolver",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRoundRobinResolver() throws Exception\n{\r\n    final UserResolver rslv = new RoundRobinUserResolver();\r\n    Path usersFilePath = new Path(rootDir, \"users\");\r\n    URI userRsrc = new URI(usersFilePath.toString());\r\n    fs.delete(usersFilePath, false);\r\n    String expectedErrorMsg = \"File \" + userRsrc + \" does not exist\";\r\n    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);\r\n    writeUserList(usersFilePath, \"\");\r\n    expectedErrorMsg = RoundRobinUserResolver.buildEmptyUsersErrorMsg(userRsrc);\r\n    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);\r\n    writeUserList(usersFilePath, \"user0,groupA,groupB,groupC\\nuser1,groupA,groupC\\n\");\r\n    validateValidUsersFile(rslv, userRsrc);\r\n    writeUserList(usersFilePath, \"user0,groupA,groupB\\nuser1,\");\r\n    validateValidUsersFile(rslv, userRsrc);\r\n    writeUserList(usersFilePath, \"user0\\nuser1\");\r\n    validateValidUsersFile(rslv, userRsrc);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateValidUsersFile",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void validateValidUsersFile(UserResolver rslv, URI userRsrc) throws IOException\n{\r\n    assertTrue(rslv.setTargetUsers(userRsrc, conf));\r\n    UserGroupInformation ugi1 = UserGroupInformation.createRemoteUser(\"hfre0\");\r\n    assertEquals(\"user0\", rslv.getTargetUgi(ugi1).getUserName());\r\n    assertEquals(\"user1\", rslv.getTargetUgi(UserGroupInformation.createRemoteUser(\"hfre1\")).getUserName());\r\n    assertEquals(\"user0\", rslv.getTargetUgi(UserGroupInformation.createRemoteUser(\"hfre2\")).getUserName());\r\n    assertEquals(\"user0\", rslv.getTargetUgi(ugi1).getUserName());\r\n    assertEquals(\"user1\", rslv.getTargetUgi(UserGroupInformation.createRemoteUser(\"hfre3\")).getUserName());\r\n    assertEquals(\"user0\", rslv.getTargetUgi(UserGroupInformation.createRemoteUser(\"hfre0\")).getUserName());\r\n    assertEquals(\"user0\", rslv.getTargetUgi(UserGroupInformation.createRemoteUser(\"hfre5\")).getUserName());\r\n    assertEquals(\"user0\", rslv.getTargetUgi(UserGroupInformation.createRemoteUser(\"hfre0\")).getUserName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testSubmitterResolver",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSubmitterResolver() throws Exception\n{\r\n    final UserResolver rslv = new SubmitterUserResolver();\r\n    assertFalse(rslv.needsTargetUsersList());\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    assertEquals(ugi, rslv.getTargetUgi((UserGroupInformation) null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testLoadSplit",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testLoadSplit() throws Exception\n{\r\n    LoadSplit test = getLoadSplit();\r\n    ByteArrayOutputStream data = new ByteArrayOutputStream();\r\n    DataOutputStream out = new DataOutputStream(data);\r\n    test.write(out);\r\n    LoadSplit copy = new LoadSplit();\r\n    copy.readFields(new DataInputStream(new ByteArrayInputStream(data.toByteArray())));\r\n    assertEquals(test.getId(), copy.getId());\r\n    assertEquals(test.getMapCount(), copy.getMapCount());\r\n    assertEquals(test.getInputRecords(), copy.getInputRecords());\r\n    assertEquals(test.getOutputBytes()[0], copy.getOutputBytes()[0]);\r\n    assertEquals(test.getOutputRecords()[0], copy.getOutputRecords()[0]);\r\n    assertEquals(test.getReduceBytes(0), copy.getReduceBytes(0));\r\n    assertEquals(test.getReduceRecords(0), copy.getReduceRecords(0));\r\n    assertEquals(test.getMapResourceUsageMetrics().getCumulativeCpuUsage(), copy.getMapResourceUsageMetrics().getCumulativeCpuUsage());\r\n    assertEquals(test.getReduceResourceUsageMetrics(0).getCumulativeCpuUsage(), copy.getReduceResourceUsageMetrics(0).getCumulativeCpuUsage());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testGridmixSplit",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGridmixSplit() throws Exception\n{\r\n    Path[] files = { new Path(\"one\"), new Path(\"two\") };\r\n    long[] start = { 1, 2 };\r\n    long[] lengths = { 100, 200 };\r\n    String[] locations = { \"locOne\", \"loctwo\" };\r\n    CombineFileSplit cfSplit = new CombineFileSplit(files, start, lengths, locations);\r\n    ResourceUsageMetrics metrics = new ResourceUsageMetrics();\r\n    metrics.setCumulativeCpuUsage(200);\r\n    double[] reduceBytes = { 8.1d, 8.2d };\r\n    double[] reduceRecords = { 9.1d, 9.2d };\r\n    long[] reduceOutputBytes = { 101L, 102L };\r\n    long[] reduceOutputRecords = { 111L, 112L };\r\n    GridmixSplit test = new GridmixSplit(cfSplit, 2, 3, 4L, 5L, 6L, 7L, reduceBytes, reduceRecords, reduceOutputBytes, reduceOutputRecords);\r\n    ByteArrayOutputStream data = new ByteArrayOutputStream();\r\n    DataOutputStream out = new DataOutputStream(data);\r\n    test.write(out);\r\n    GridmixSplit copy = new GridmixSplit();\r\n    copy.readFields(new DataInputStream(new ByteArrayInputStream(data.toByteArray())));\r\n    assertEquals(test.getId(), copy.getId());\r\n    assertEquals(test.getMapCount(), copy.getMapCount());\r\n    assertEquals(test.getInputRecords(), copy.getInputRecords());\r\n    assertEquals(test.getOutputBytes()[0], copy.getOutputBytes()[0]);\r\n    assertEquals(test.getOutputRecords()[0], copy.getOutputRecords()[0]);\r\n    assertEquals(test.getReduceBytes(0), copy.getReduceBytes(0));\r\n    assertEquals(test.getReduceRecords(0), copy.getReduceRecords(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testLoadMapper",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testLoadMapper() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JobContext.NUM_REDUCES, 2);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);\r\n    TaskAttemptID taskId = new TaskAttemptID();\r\n    RecordReader<NullWritable, GridmixRecord> reader = new FakeRecordReader();\r\n    LoadRecordGkGrWriter writer = new LoadRecordGkGrWriter();\r\n    OutputCommitter committer = new CustomOutputCommitter();\r\n    StatusReporter reporter = new TaskAttemptContextImpl.DummyReporter();\r\n    LoadSplit split = getLoadSplit();\r\n    MapContext<NullWritable, GridmixRecord, GridmixKey, GridmixRecord> mapContext = new MapContextImpl<NullWritable, GridmixRecord, GridmixKey, GridmixRecord>(conf, taskId, reader, writer, committer, reporter, split);\r\n    Context ctx = new WrappedMapper<NullWritable, GridmixRecord, GridmixKey, GridmixRecord>().getMapContext(mapContext);\r\n    reader.initialize(split, ctx);\r\n    ctx.getConfiguration().setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(ctx.getConfiguration(), true);\r\n    LoadJob.LoadMapper mapper = new LoadJob.LoadMapper();\r\n    mapper.run(ctx);\r\n    Map<GridmixKey, GridmixRecord> data = writer.getData();\r\n    assertEquals(2, data.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getLoadSplit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LoadSplit getLoadSplit() throws Exception\n{\r\n    Path[] files = { new Path(\"one\"), new Path(\"two\") };\r\n    long[] start = { 1, 2 };\r\n    long[] lengths = { 100, 200 };\r\n    String[] locations = { \"locOne\", \"loctwo\" };\r\n    CombineFileSplit cfSplit = new CombineFileSplit(files, start, lengths, locations);\r\n    ResourceUsageMetrics metrics = new ResourceUsageMetrics();\r\n    metrics.setCumulativeCpuUsage(200);\r\n    ResourceUsageMetrics[] rMetrics = { metrics };\r\n    double[] reduceBytes = { 8.1d, 8.2d };\r\n    double[] reduceRecords = { 9.1d, 9.2d };\r\n    long[] reduceOutputBytes = { 101L, 102L };\r\n    long[] reduceOutputRecords = { 111L, 112L };\r\n    return new LoadSplit(cfSplit, 2, 1, 4L, 5L, 6L, 7L, reduceBytes, reduceRecords, reduceOutputBytes, reduceOutputRecords, metrics, rMetrics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testLoadJobLoadSortComparator",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testLoadJobLoadSortComparator() throws Exception\n{\r\n    LoadJob.LoadSortComparator test = new LoadJob.LoadSortComparator();\r\n    ByteArrayOutputStream data = new ByteArrayOutputStream();\r\n    DataOutputStream dos = new DataOutputStream(data);\r\n    WritableUtils.writeVInt(dos, 2);\r\n    WritableUtils.writeVInt(dos, 1);\r\n    WritableUtils.writeVInt(dos, 4);\r\n    WritableUtils.writeVInt(dos, 7);\r\n    WritableUtils.writeVInt(dos, 4);\r\n    byte[] b1 = data.toByteArray();\r\n    byte[] b2 = data.toByteArray();\r\n    assertEquals(0, test.compare(b1, 0, 1, b2, 0, 1));\r\n    b2[2] = 5;\r\n    assertEquals(-1, test.compare(b1, 0, 1, b2, 0, 1));\r\n    b2[2] = 2;\r\n    assertEquals(2, test.compare(b1, 0, 1, b2, 0, 1));\r\n    b2[2] = 4;\r\n    assertEquals(1, test.compare(b1, 0, 1, b2, 1, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testGridmixJobSpecGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGridmixJobSpecGroupingComparator() throws Exception\n{\r\n    GridmixJob.SpecGroupingComparator test = new GridmixJob.SpecGroupingComparator();\r\n    ByteArrayOutputStream data = new ByteArrayOutputStream();\r\n    DataOutputStream dos = new DataOutputStream(data);\r\n    WritableUtils.writeVInt(dos, 2);\r\n    WritableUtils.writeVInt(dos, 1);\r\n    WritableUtils.writeVInt(dos, 0);\r\n    WritableUtils.writeVInt(dos, 7);\r\n    WritableUtils.writeVInt(dos, 4);\r\n    byte[] b1 = data.toByteArray();\r\n    byte[] b2 = data.toByteArray();\r\n    assertEquals(0, test.compare(b1, 0, 1, b2, 0, 1));\r\n    b2[2] = 1;\r\n    assertEquals(-1, test.compare(b1, 0, 1, b2, 0, 1));\r\n    b2[2] = 1;\r\n    assertEquals(-1, test.compare(b1, 0, 1, b2, 0, 1));\r\n    assertEquals(0, test.compare(new GridmixKey(GridmixKey.DATA, 100, 2), new GridmixKey(GridmixKey.DATA, 100, 2)));\r\n    assertEquals(-1, test.compare(new GridmixKey(GridmixKey.REDUCE_SPEC, 100, 2), new GridmixKey(GridmixKey.DATA, 100, 2)));\r\n    assertEquals(1, test.compare(new GridmixKey(GridmixKey.DATA, 100, 2), new GridmixKey(GridmixKey.REDUCE_SPEC, 100, 2)));\r\n    assertEquals(2, test.compare(new GridmixKey(GridmixKey.DATA, 102, 2), new GridmixKey(GridmixKey.DATA, 100, 2)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testCompareGridmixJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCompareGridmixJob() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    Path outRoot = new Path(\"target\");\r\n    JobStory jobDesc = mock(JobStory.class);\r\n    when(jobDesc.getName()).thenReturn(\"JobName\");\r\n    when(jobDesc.getJobConf()).thenReturn(new JobConf(conf));\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    GridmixJob j1 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 0);\r\n    GridmixJob j2 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 0);\r\n    GridmixJob j3 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 1);\r\n    GridmixJob j4 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 1);\r\n    assertTrue(j1.equals(j2));\r\n    assertEquals(0, j1.compareTo(j2));\r\n    assertFalse(j1.equals(j3));\r\n    assertEquals(-1, j1.compareTo(j3));\r\n    assertEquals(-1, j1.compareTo(j4));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testReadRecordFactory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testReadRecordFactory() throws Exception\n{\r\n    RecordFactory rf = new FakeRecordFactory();\r\n    FakeInputStream input = new FakeInputStream();\r\n    ReadRecordFactory test = new ReadRecordFactory(rf, input, new Configuration());\r\n    GridmixKey key = new GridmixKey(GridmixKey.DATA, 100, 2);\r\n    GridmixRecord val = new GridmixRecord(200, 2);\r\n    while (test.next(key, val)) {\r\n    }\r\n    assertEquals(3000, input.getCounter());\r\n    assertEquals(-1, rf.getProgress(), 0.01);\r\n    test.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testLoadJobLoadRecordReader",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testLoadJobLoadRecordReader() throws Exception\n{\r\n    LoadJob.LoadRecordReader test = new LoadJob.LoadRecordReader();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs1 = mock(FileSystem.class);\r\n    when(fs1.open(any(Path.class))).thenReturn(new FakeFSDataInputStream(new FakeInputStream()));\r\n    Path p1 = mock(Path.class);\r\n    when(p1.getFileSystem(any())).thenReturn(fs1);\r\n    FileSystem fs2 = mock(FileSystem.class);\r\n    when(fs2.open(any(Path.class))).thenReturn(new FakeFSDataInputStream(new FakeInputStream()));\r\n    Path p2 = mock(Path.class);\r\n    when(p2.getFileSystem(any())).thenReturn(fs2);\r\n    Path[] paths = { p1, p2 };\r\n    long[] start = { 0, 0 };\r\n    long[] lengths = { 1000, 1000 };\r\n    String[] locations = { \"temp1\", \"temp2\" };\r\n    CombineFileSplit cfsplit = new CombineFileSplit(paths, start, lengths, locations);\r\n    double[] reduceBytes = { 100, 100 };\r\n    double[] reduceRecords = { 2, 2 };\r\n    long[] reduceOutputBytes = { 500, 500 };\r\n    long[] reduceOutputRecords = { 2, 2 };\r\n    ResourceUsageMetrics metrics = new ResourceUsageMetrics();\r\n    ResourceUsageMetrics[] rMetrics = { new ResourceUsageMetrics(), new ResourceUsageMetrics() };\r\n    LoadSplit input = new LoadSplit(cfsplit, 2, 3, 1500L, 2L, 3000L, 2L, reduceBytes, reduceRecords, reduceOutputBytes, reduceOutputRecords, metrics, rMetrics);\r\n    TaskAttemptID taskId = new TaskAttemptID();\r\n    TaskAttemptContext ctx = new TaskAttemptContextImpl(conf, taskId);\r\n    test.initialize(input, ctx);\r\n    GridmixRecord gr = test.getCurrentValue();\r\n    int counter = 0;\r\n    while (test.nextKeyValue()) {\r\n        gr = test.getCurrentValue();\r\n        if (counter == 0) {\r\n            assertEquals(0.5, test.getProgress(), 0.001);\r\n        } else if (counter == 1) {\r\n            assertEquals(1.0, test.getProgress(), 0.001);\r\n        }\r\n        assertEquals(1000, gr.getSize());\r\n        counter++;\r\n    }\r\n    assertEquals(1000, gr.getSize());\r\n    assertEquals(2, counter);\r\n    test.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testLoadJobLoadReducer",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testLoadJobLoadReducer() throws Exception\n{\r\n    LoadJob.LoadReducer test = new LoadJob.LoadReducer();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JobContext.NUM_REDUCES, 2);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    conf.setBoolean(FileOutputFormat.COMPRESS, true);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);\r\n    TaskAttemptID taskid = new TaskAttemptID();\r\n    RawKeyValueIterator input = new FakeRawKeyValueIterator();\r\n    Counter counter = new GenericCounter();\r\n    Counter inputValueCounter = new GenericCounter();\r\n    LoadRecordWriter output = new LoadRecordWriter();\r\n    OutputCommitter committer = new CustomOutputCommitter();\r\n    StatusReporter reporter = new DummyReporter();\r\n    RawComparator<GridmixKey> comparator = new FakeRawComparator();\r\n    ReduceContext<GridmixKey, GridmixRecord, NullWritable, GridmixRecord> reduceContext = new ReduceContextImpl<GridmixKey, GridmixRecord, NullWritable, GridmixRecord>(conf, taskid, input, counter, inputValueCounter, output, committer, reporter, comparator, GridmixKey.class, GridmixRecord.class);\r\n    reduceContext.nextKeyValue();\r\n    org.apache.hadoop.mapreduce.Reducer<GridmixKey, GridmixRecord, NullWritable, GridmixRecord>.Context context = new WrappedReducer<GridmixKey, GridmixRecord, NullWritable, GridmixRecord>().getReducerContext(reduceContext);\r\n    test.run(context);\r\n    assertEquals(9, counter.getValue());\r\n    assertEquals(10, inputValueCounter.getValue());\r\n    assertEquals(1, output.getData().size());\r\n    GridmixRecord record = output.getData().values().iterator().next();\r\n    assertEquals(1593, record.getSize());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testSerialReaderThread",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testSerialReaderThread() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    File fin = new File(\"src\" + File.separator + \"test\" + File.separator + \"resources\" + File.separator + \"data\" + File.separator + \"wordcount2.json\");\r\n    JobStoryProducer jobProducer = new ZombieJobProducer(new Path(fin.getAbsolutePath()), null, conf);\r\n    CountDownLatch startFlag = new CountDownLatch(1);\r\n    UserResolver resolver = new SubmitterUserResolver();\r\n    FakeJobSubmitter submitter = new FakeJobSubmitter();\r\n    File ws = new File(\"target\" + File.separator + this.getClass().getName());\r\n    if (!ws.exists()) {\r\n        Assert.assertTrue(ws.mkdirs());\r\n    }\r\n    SerialJobFactory jobFactory = new SerialJobFactory(submitter, jobProducer, new Path(ws.getAbsolutePath()), conf, startFlag, resolver);\r\n    Path ioPath = new Path(ws.getAbsolutePath());\r\n    jobFactory.setDistCacheEmulator(new DistributedCacheEmulator(conf, ioPath));\r\n    Thread test = jobFactory.createReaderThread();\r\n    test.start();\r\n    Thread.sleep(1000);\r\n    assertEquals(0, submitter.getJobs().size());\r\n    startFlag.countDown();\r\n    while (test.isAlive()) {\r\n        Thread.sleep(1000);\r\n        jobFactory.update(null);\r\n    }\r\n    assertEquals(2, submitter.getJobs().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testSleepMapper",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSleepMapper() throws Exception\n{\r\n    SleepJob.SleepMapper test = new SleepJob.SleepMapper();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JobContext.NUM_REDUCES, 2);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);\r\n    TaskAttemptID taskId = new TaskAttemptID();\r\n    FakeRecordLLReader reader = new FakeRecordLLReader();\r\n    LoadRecordGkNullWriter writer = new LoadRecordGkNullWriter();\r\n    OutputCommitter committer = new CustomOutputCommitter();\r\n    StatusReporter reporter = new TaskAttemptContextImpl.DummyReporter();\r\n    SleepSplit split = getSleepSplit();\r\n    MapContext<LongWritable, LongWritable, GridmixKey, NullWritable> mapcontext = new MapContextImpl<LongWritable, LongWritable, GridmixKey, NullWritable>(conf, taskId, reader, writer, committer, reporter, split);\r\n    Context context = new WrappedMapper<LongWritable, LongWritable, GridmixKey, NullWritable>().getMapContext(mapcontext);\r\n    long start = System.currentTimeMillis();\r\n    LOG.info(\"start:\" + start);\r\n    LongWritable key = new LongWritable(start + 2000);\r\n    LongWritable value = new LongWritable(start + 2000);\r\n    test.map(key, value, context);\r\n    LOG.info(\"finish:\" + System.currentTimeMillis());\r\n    assertTrue(System.currentTimeMillis() >= (start + 2000));\r\n    test.cleanup(context);\r\n    assertEquals(1, writer.getData().size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getSleepSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SleepSplit getSleepSplit() throws Exception\n{\r\n    String[] locations = { \"locOne\", \"loctwo\" };\r\n    long[] reduceDurations = { 101L, 102L };\r\n    return new SleepSplit(0, 2000L, reduceDurations, 2, locations);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testSleepReducer",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSleepReducer() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(JobContext.NUM_REDUCES, 2);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    conf.setBoolean(FileOutputFormat.COMPRESS, true);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);\r\n    TaskAttemptID taskId = new TaskAttemptID();\r\n    RawKeyValueIterator input = new FakeRawKeyValueReducerIterator();\r\n    Counter counter = new GenericCounter();\r\n    Counter inputValueCounter = new GenericCounter();\r\n    RecordWriter<NullWritable, NullWritable> output = new LoadRecordReduceWriter();\r\n    OutputCommitter committer = new CustomOutputCommitter();\r\n    StatusReporter reporter = new DummyReporter();\r\n    RawComparator<GridmixKey> comparator = new FakeRawComparator();\r\n    ReduceContext<GridmixKey, NullWritable, NullWritable, NullWritable> reducecontext = new ReduceContextImpl<GridmixKey, NullWritable, NullWritable, NullWritable>(conf, taskId, input, counter, inputValueCounter, output, committer, reporter, comparator, GridmixKey.class, NullWritable.class);\r\n    org.apache.hadoop.mapreduce.Reducer<GridmixKey, NullWritable, NullWritable, NullWritable>.Context context = new WrappedReducer<GridmixKey, NullWritable, NullWritable, NullWritable>().getReducerContext(reducecontext);\r\n    SleepReducer test = new SleepReducer();\r\n    long start = System.currentTimeMillis();\r\n    test.setup(context);\r\n    long sleeper = context.getCurrentKey().getReduceOutputBytes();\r\n    assertEquals(\"Sleeping... \" + sleeper + \" ms left\", context.getStatus());\r\n    assertTrue(System.currentTimeMillis() >= (start + sleeper));\r\n    test.cleanup(context);\r\n    assertEquals(\"Slept for \" + sleeper, context.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getBaseDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getBaseDir()\n{\r\n    try {\r\n        final Configuration conf = new Configuration();\r\n        final FileSystem fs = FileSystem.getLocal(conf).getRaw();\r\n        return fs.makeQualified(new Path(System.getProperty(\"test.build.data\", \"/tmp\"), \"testFilePool\"));\r\n    } catch (IOException e) {\r\n        fail();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = FileSystem.getLocal(conf).getRaw();\r\n    fs.delete(base, true);\r\n    final Random r = new Random();\r\n    final long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    LOG.info(\"seed: \" + seed);\r\n    fs.mkdirs(base);\r\n    for (int i = 0; i < NFILES; ++i) {\r\n        Path file = base;\r\n        for (double d = 0.6; d > 0.0; d *= 0.8) {\r\n            if (r.nextDouble() < d) {\r\n                file = new Path(base, Integer.toString(r.nextInt(3)));\r\n                continue;\r\n            }\r\n            break;\r\n        }\r\n        OutputStream out = null;\r\n        try {\r\n            out = fs.create(new Path(file, \"\" + (char) ('A' + i)));\r\n            final byte[] b = new byte[1024];\r\n            Arrays.fill(b, (byte) ('A' + i));\r\n            for (int len = ((i % 13) + 1) * 1024; len > 0; len -= 1024) {\r\n                out.write(b);\r\n            }\r\n        } finally {\r\n            if (out != null) {\r\n                out.close();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = FileSystem.getLocal(conf).getRaw();\r\n    fs.delete(base, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testUnsuitable",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUnsuitable() throws Exception\n{\r\n    try {\r\n        final Configuration conf = new Configuration();\r\n        conf.setLong(FilePool.GRIDMIX_MIN_FILE, 14 * 1024);\r\n        final FilePool pool = new FilePool(conf, base);\r\n        pool.refresh();\r\n    } catch (IOException e) {\r\n        return;\r\n    }\r\n    fail();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testPool",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testPool() throws Exception\n{\r\n    final Random r = new Random();\r\n    final Configuration conf = new Configuration();\r\n    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);\r\n    final FilePool pool = new FilePool(conf, base);\r\n    pool.refresh();\r\n    final ArrayList<FileStatus> files = new ArrayList<FileStatus>();\r\n    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;\r\n    assertEquals(expectedPoolSize, pool.getInputFiles(Long.MAX_VALUE, files));\r\n    assertEquals(NFILES - 4, files.size());\r\n    files.clear();\r\n    assertEquals(expectedPoolSize, pool.getInputFiles(expectedPoolSize, files));\r\n    files.clear();\r\n    final long rand = r.nextInt(expectedPoolSize);\r\n    assertTrue(\"Missed: \" + rand, (NFILES / 2) * 1024 > rand - pool.getInputFiles(rand, files));\r\n    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 0);\r\n    pool.refresh();\r\n    files.clear();\r\n    assertEquals((NFILES / 2 * (NFILES / 2 + 1)) * 1024, pool.getInputFiles(Long.MAX_VALUE, files));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "checkSplitEq",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkSplitEq(FileSystem fs, CombineFileSplit split, long bytes) throws Exception\n{\r\n    long splitBytes = 0L;\r\n    HashSet<Path> uniq = new HashSet<Path>();\r\n    for (int i = 0; i < split.getNumPaths(); ++i) {\r\n        splitBytes += split.getLength(i);\r\n        assertTrue(split.getLength(i) <= fs.getFileStatus(split.getPath(i)).getLen());\r\n        assertFalse(uniq.contains(split.getPath(i)));\r\n        uniq.add(split.getPath(i));\r\n    }\r\n    assertEquals(bytes, splitBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testStriper",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testStriper() throws Exception\n{\r\n    final Random r = new Random();\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = FileSystem.getLocal(conf).getRaw();\r\n    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);\r\n    final FilePool pool = new FilePool(conf, base) {\r\n\r\n        @Override\r\n        public BlockLocation[] locationsFor(FileStatus stat, long start, long len) throws IOException {\r\n            return new BlockLocation[] { new BlockLocation() };\r\n        }\r\n    };\r\n    pool.refresh();\r\n    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;\r\n    final InputStriper striper = new InputStriper(pool, expectedPoolSize);\r\n    int last = 0;\r\n    for (int i = 0; i < expectedPoolSize; last = Math.min(expectedPoolSize - i, r.nextInt(expectedPoolSize))) {\r\n        checkSplitEq(fs, striper.splitFor(pool, last, 0), last);\r\n        i += last;\r\n    }\r\n    final InputStriper striper2 = new InputStriper(pool, expectedPoolSize);\r\n    checkSplitEq(fs, striper2.splitFor(pool, expectedPoolSize, 0), expectedPoolSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "doSubmission",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void doSubmission(String jobCreatorName, boolean defaultOutputPath) throws Exception\n{\r\n    final Path in = new Path(\"foo\").makeQualified(GridmixTestUtils.dfs.getUri(), GridmixTestUtils.dfs.getWorkingDirectory());\r\n    final Path out = GridmixTestUtils.DEST.makeQualified(GridmixTestUtils.dfs.getUri(), GridmixTestUtils.dfs.getWorkingDirectory());\r\n    final Path root = new Path(workspace.getName()).makeQualified(GridmixTestUtils.dfs.getUri(), GridmixTestUtils.dfs.getWorkingDirectory());\r\n    if (!workspace.exists()) {\r\n        assertTrue(workspace.mkdirs());\r\n    }\r\n    Configuration conf = null;\r\n    try {\r\n        ArrayList<String> argsList = new ArrayList<String>();\r\n        argsList.add(\"-D\" + FilePool.GRIDMIX_MIN_FILE + \"=0\");\r\n        argsList.add(\"-D\" + Gridmix.GRIDMIX_USR_RSV + \"=\" + EchoUserResolver.class.getName());\r\n        if (jobCreatorName != null) {\r\n            argsList.add(\"-D\" + JobCreator.GRIDMIX_JOB_TYPE + \"=\" + jobCreatorName);\r\n        }\r\n        if (!defaultOutputPath) {\r\n            argsList.add(\"-D\" + Gridmix.GRIDMIX_OUT_DIR + \"=\" + out);\r\n        }\r\n        argsList.add(\"-generate\");\r\n        argsList.add(String.valueOf(GENDATA) + \"m\");\r\n        argsList.add(in.toString());\r\n        argsList.add(\"-\");\r\n        String[] argv = argsList.toArray(new String[argsList.size()]);\r\n        DebugGridmix client = new DebugGridmix();\r\n        conf = GridmixTestUtils.mrvl.getConfig();\r\n        CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n        conf.setEnum(GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, policy);\r\n        conf.setBoolean(GridmixJob.GRIDMIX_USE_QUEUE_IN_TRACE, true);\r\n        UserGroupInformation ugi = UserGroupInformation.getLoginUser();\r\n        conf.set(MRJobConfig.USER_NAME, ugi.getUserName());\r\n        GridmixTestUtils.dfs.mkdirs(root, new FsPermission((short) 777));\r\n        GridmixTestUtils.dfs.setPermission(root, new FsPermission((short) 777));\r\n        int res = ToolRunner.run(conf, client, argv);\r\n        assertEquals(\"Client exited with nonzero status\", 0, res);\r\n        client.checkMonitor();\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n    } finally {\r\n        in.getFileSystem(conf).delete(in, true);\r\n        out.getFileSystem(conf).delete(out, true);\r\n        root.getFileSystem(conf).delete(root, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testPseudoLocalFsFileSize",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testPseudoLocalFsFileSize() throws Exception\n{\r\n    long fileSize = 10000;\r\n    Path path = PseudoLocalFs.generateFilePath(\"myPsedoFile\", fileSize);\r\n    PseudoLocalFs pfs = new PseudoLocalFs();\r\n    pfs.create(path);\r\n    InputStream in = pfs.open(path, 0);\r\n    long totalSize = 0;\r\n    while (in.read() >= 0) {\r\n        ++totalSize;\r\n    }\r\n    in.close();\r\n    assertEquals(\"File size mismatch with read().\", fileSize, totalSize);\r\n    in = pfs.open(path, 0);\r\n    totalSize = 0;\r\n    byte[] b = new byte[1024];\r\n    int bytesRead = in.read(b);\r\n    while (bytesRead >= 0) {\r\n        totalSize += bytesRead;\r\n        bytesRead = in.read(b);\r\n    }\r\n    assertEquals(\"File size mismatch with read(byte[]).\", fileSize, totalSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateGetFileStatus",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateGetFileStatus(FileSystem pfs, Path path, boolean shouldSucceed) throws IOException\n{\r\n    boolean expectedExceptionSeen = false;\r\n    FileStatus stat = null;\r\n    try {\r\n        stat = pfs.getFileStatus(path);\r\n    } catch (FileNotFoundException e) {\r\n        expectedExceptionSeen = true;\r\n    }\r\n    if (shouldSucceed) {\r\n        assertFalse(\"getFileStatus() has thrown Exception for valid file name \" + path, expectedExceptionSeen);\r\n        assertNotNull(\"Missing file status for a valid file.\", stat);\r\n        String[] parts = path.toUri().getPath().split(\"\\\\.\");\r\n        long expectedFileSize = Long.parseLong(parts[parts.length - 1]);\r\n        assertEquals(\"Invalid file size.\", expectedFileSize, stat.getLen());\r\n    } else {\r\n        assertTrue(\"getFileStatus() did not throw Exception for invalid file \" + \" name \" + path, expectedExceptionSeen);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateCreate",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateCreate(FileSystem pfs, Path path, boolean shouldSucceed) throws IOException\n{\r\n    boolean expectedExceptionSeen = false;\r\n    try {\r\n        pfs.create(path);\r\n    } catch (IOException e) {\r\n        expectedExceptionSeen = true;\r\n    }\r\n    if (shouldSucceed) {\r\n        assertFalse(\"create() has thrown Exception for valid file name \" + path, expectedExceptionSeen);\r\n    } else {\r\n        assertTrue(\"create() did not throw Exception for invalid file name \" + path, expectedExceptionSeen);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateOpen",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateOpen(FileSystem pfs, Path path, boolean shouldSucceed) throws IOException\n{\r\n    boolean expectedExceptionSeen = false;\r\n    try {\r\n        pfs.open(path);\r\n    } catch (IOException e) {\r\n        expectedExceptionSeen = true;\r\n    }\r\n    if (shouldSucceed) {\r\n        assertFalse(\"open() has thrown Exception for valid file name \" + path, expectedExceptionSeen);\r\n    } else {\r\n        assertTrue(\"open() did not throw Exception for invalid file name \" + path, expectedExceptionSeen);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateExists",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateExists(FileSystem pfs, Path path, boolean shouldSucceed) throws IOException\n{\r\n    boolean ret = pfs.exists(path);\r\n    if (shouldSucceed) {\r\n        assertTrue(\"exists() returned false for valid file name \" + path, ret);\r\n    } else {\r\n        assertFalse(\"exists() returned true for invalid file name \" + path, ret);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testPseudoLocalFsFileNames",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testPseudoLocalFsFileNames() throws IOException\n{\r\n    PseudoLocalFs pfs = new PseudoLocalFs();\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(\"fs.pseudo.impl\", PseudoLocalFs.class, FileSystem.class);\r\n    Path path = new Path(\"pseudo:///myPsedoFile.1234\");\r\n    FileSystem testFs = path.getFileSystem(conf);\r\n    assertEquals(\"Failed to obtain a pseudo local file system object from path\", pfs.getUri().getScheme(), testFs.getUri().getScheme());\r\n    path = new Path(\"file:///myPsedoFile.12345\");\r\n    validateGetFileStatus(pfs, path, false);\r\n    validateCreate(pfs, path, false);\r\n    validateOpen(pfs, path, false);\r\n    validateExists(pfs, path, false);\r\n    path = new Path(\"pseudo:///myPsedoFile\");\r\n    validateGetFileStatus(pfs, path, false);\r\n    validateCreate(pfs, path, false);\r\n    validateOpen(pfs, path, false);\r\n    validateExists(pfs, path, false);\r\n    path = new Path(\"pseudo:///myPsedoFile.txt\");\r\n    validateGetFileStatus(pfs, path, false);\r\n    validateCreate(pfs, path, false);\r\n    validateOpen(pfs, path, false);\r\n    validateExists(pfs, path, false);\r\n    long fileSize = 231456;\r\n    path = PseudoLocalFs.generateFilePath(\"my.Psedo.File\", fileSize);\r\n    assertEquals(\"generateFilePath() failed.\", fileSize, pfs.validateFileNameFormat(path));\r\n    validateGetFileStatus(pfs, path, true);\r\n    validateCreate(pfs, path, true);\r\n    validateOpen(pfs, path, true);\r\n    validateExists(pfs, path, true);\r\n    path = new Path(\"myPsedoFile.1237\");\r\n    path = pfs.makeQualified(path);\r\n    validateGetFileStatus(pfs, path, true);\r\n    validateCreate(pfs, path, true);\r\n    validateOpen(pfs, path, true);\r\n    validateExists(pfs, path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getFactory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobFactory<?> getFactory(JobSubmitter submitter, Path scratch, int numJobs, Configuration conf, CountDownLatch startFlag, UserResolver resolver) throws IOException\n{\r\n    GridmixJobSubmissionPolicy policy = GridmixJobSubmissionPolicy.getPolicy(conf, GridmixJobSubmissionPolicy.STRESS);\r\n    if (policy == GridmixJobSubmissionPolicy.REPLAY) {\r\n        return new DebugReplayJobFactory(submitter, scratch, numJobs, conf, startFlag, resolver);\r\n    } else if (policy == GridmixJobSubmissionPolicy.STRESS) {\r\n        return new DebugStressJobFactory(submitter, scratch, numJobs, conf, startFlag, resolver);\r\n    } else if (policy == GridmixJobSubmissionPolicy.SERIAL) {\r\n        return new DebugSerialJobFactory(submitter, scratch, numJobs, conf, startFlag, resolver);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testHeapUsageEmulator",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testHeapUsageEmulator() throws IOException\n{\r\n    FakeHeapUsageEmulatorCore heapEmulator = new FakeHeapUsageEmulatorCore();\r\n    long testSizeInMB = 10;\r\n    long previousHeap = heapEmulator.getHeapUsageInMB();\r\n    heapEmulator.load(testSizeInMB);\r\n    long currentHeap = heapEmulator.getHeapUsageInMB();\r\n    assertEquals(\"Default heap emulator failed to load 10mb\", previousHeap + testSizeInMB, currentHeap);\r\n    heapEmulator.resetFake();\r\n    assertEquals(\"Default heap emulator failed to reset\", 0, heapEmulator.getHeapUsageInMB());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testTotalHeapUsageEmulatorPlugin",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void testTotalHeapUsageEmulatorPlugin() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    ResourceCalculatorPlugin monitor = new DummyResourceCalculatorPlugin();\r\n    long maxHeapUsage = 1024 * TotalHeapUsageEmulatorPlugin.ONE_MB;\r\n    conf.setLong(DummyResourceCalculatorPlugin.MAXPMEM_TESTING_PROPERTY, maxHeapUsage);\r\n    monitor.setConf(conf);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);\r\n    long targetHeapUsageInMB = 200;\r\n    FakeProgressive fakeProgress = new FakeProgressive();\r\n    FakeHeapUsageEmulatorCore fakeCore = new FakeHeapUsageEmulatorCore();\r\n    FakeHeapUsageEmulatorPlugin heapPlugin = new FakeHeapUsageEmulatorPlugin(fakeCore);\r\n    ResourceUsageMetrics invalidUsage = TestResourceUsageEmulators.createMetrics(0);\r\n    heapPlugin.initialize(conf, invalidUsage, null, null);\r\n    int numCallsPre = fakeCore.getNumCalls();\r\n    long heapUsagePre = fakeCore.getHeapUsageInMB();\r\n    heapPlugin.emulate();\r\n    int numCallsPost = fakeCore.getNumCalls();\r\n    long heapUsagePost = fakeCore.getHeapUsageInMB();\r\n    assertEquals(\"Disabled heap usage emulation plugin works!\", numCallsPre, numCallsPost);\r\n    assertEquals(\"Disabled heap usage emulation plugin works!\", heapUsagePre, heapUsagePost);\r\n    float progress = heapPlugin.getProgress();\r\n    assertEquals(\"Invalid progress of disabled cumulative heap usage emulation \" + \"plugin!\", 1.0f, progress, 0f);\r\n    Boolean failed = null;\r\n    invalidUsage = TestResourceUsageEmulators.createMetrics(maxHeapUsage + TotalHeapUsageEmulatorPlugin.ONE_MB);\r\n    try {\r\n        heapPlugin.initialize(conf, invalidUsage, monitor, null);\r\n        failed = false;\r\n    } catch (Exception e) {\r\n        failed = true;\r\n    }\r\n    assertNotNull(\"Fail case failure!\", failed);\r\n    assertTrue(\"Expected failure!\", failed);\r\n    ResourceUsageMetrics metrics = TestResourceUsageEmulators.createMetrics(targetHeapUsageInMB * TotalHeapUsageEmulatorPlugin.ONE_MB);\r\n    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 10);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL, 0.2F);\r\n    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 5);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.5F);\r\n    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 120, 2);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);\r\n    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 10);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.25F);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);\r\n    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 162, 6);\r\n    fakeProgress = new FakeProgressive();\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);\r\n    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL, 0.25F);\r\n    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);\r\n    fakeCore.resetFake();\r\n    long initHeapUsage = fakeCore.getHeapUsageInMB();\r\n    long initNumCallsUsage = fakeCore.getNumCalls();\r\n    testEmulationBoundary(0F, fakeCore, fakeProgress, heapPlugin, initHeapUsage, initNumCallsUsage, \"[no-op, 0 progress]\");\r\n    testEmulationBoundary(0.24F, fakeCore, fakeProgress, heapPlugin, initHeapUsage, initNumCallsUsage, \"[no-op, 24% progress]\");\r\n    testEmulationBoundary(0.25F, fakeCore, fakeProgress, heapPlugin, targetHeapUsageInMB / 4, 1, \"[op, 25% progress]\");\r\n    testEmulationBoundary(0.80F, fakeCore, fakeProgress, heapPlugin, (targetHeapUsageInMB * 4) / 5, 2, \"[op, 80% progress]\");\r\n    testEmulationBoundary(1F, fakeCore, fakeProgress, heapPlugin, targetHeapUsageInMB, 3, \"[op, 100% progress]\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testEmulationAccuracy",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testEmulationAccuracy(Configuration conf, FakeHeapUsageEmulatorCore fakeCore, ResourceCalculatorPlugin monitor, ResourceUsageMetrics metrics, TotalHeapUsageEmulatorPlugin heapPlugin, long expectedTotalHeapUsageInMB, long expectedTotalNumCalls) throws Exception\n{\r\n    FakeProgressive fakeProgress = new FakeProgressive();\r\n    fakeCore.resetFake();\r\n    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);\r\n    int numLoops = 0;\r\n    while (fakeProgress.getProgress() < 1) {\r\n        ++numLoops;\r\n        float progress = numLoops / 100.0F;\r\n        fakeProgress.setProgress(progress);\r\n        heapPlugin.emulate();\r\n    }\r\n    assertEquals(\"Cumulative heap usage emulator plugin failed (total usage)!\", expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 1L);\r\n    assertEquals(\"Cumulative heap usage emulator plugin failed (num calls)!\", expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testEmulationBoundary",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testEmulationBoundary(float progress, FakeHeapUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, TotalHeapUsageEmulatorPlugin heapPlugin, long expectedTotalHeapUsageInMB, long expectedTotalNumCalls, String info) throws Exception\n{\r\n    fakeProgress.setProgress(progress);\r\n    heapPlugin.emulate();\r\n    assertEquals(\"Emulation interval test for heap usage failed \" + info + \"!\", expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 0L);\r\n    assertEquals(\"Emulation interval test for heap usage failed \" + info + \"!\", expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testJavaHeapOptions",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testJavaHeapOptions(String mapOptions, String reduceOptions, String taskOptions, String defaultMapOptions, String defaultReduceOptions, String defaultTaskOptions, String expectedMapOptions, String expectedReduceOptions, String expectedTaskOptions) throws Exception\n{\r\n    Configuration simulatedConf = new Configuration();\r\n    simulatedConf.unset(MRJobConfig.MAP_JAVA_OPTS);\r\n    simulatedConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);\r\n    simulatedConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);\r\n    if (defaultMapOptions != null) {\r\n        simulatedConf.set(MRJobConfig.MAP_JAVA_OPTS, defaultMapOptions);\r\n    }\r\n    if (defaultReduceOptions != null) {\r\n        simulatedConf.set(MRJobConfig.REDUCE_JAVA_OPTS, defaultReduceOptions);\r\n    }\r\n    if (defaultTaskOptions != null) {\r\n        simulatedConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, defaultTaskOptions);\r\n    }\r\n    Configuration originalConf = new Configuration();\r\n    originalConf.unset(MRJobConfig.MAP_JAVA_OPTS);\r\n    originalConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);\r\n    originalConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);\r\n    if (mapOptions != null) {\r\n        originalConf.set(MRJobConfig.MAP_JAVA_OPTS, mapOptions);\r\n    }\r\n    if (reduceOptions != null) {\r\n        originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, reduceOptions);\r\n    }\r\n    if (taskOptions != null) {\r\n        originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, taskOptions);\r\n    }\r\n    GridmixJob.configureTaskJVMOptions(originalConf, simulatedConf);\r\n    assertEquals(\"Map heap options mismatch!\", expectedMapOptions, simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));\r\n    assertEquals(\"Reduce heap options mismatch!\", expectedReduceOptions, simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));\r\n    assertEquals(\"Task heap options mismatch!\", expectedTaskOptions, simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testJavaHeapOptions",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJavaHeapOptions() throws Exception\n{\r\n    testJavaHeapOptions(null, null, null, null, null, null, null, null, null);\r\n    testJavaHeapOptions(\"-Xms10m\", \"-Xms20m\", \"-Xms30m\", null, null, null, null, null, null);\r\n    testJavaHeapOptions(null, null, null, \"-Xms10m\", \"-Xms20m\", \"-Xms30m\", \"-Xms10m\", \"-Xms20m\", \"-Xms30m\");\r\n    testJavaHeapOptions(\"\", \"\", \"\", null, null, null, null, null, null);\r\n    testJavaHeapOptions(null, null, null, \"\", \"\", \"\", \"\", \"\", \"\");\r\n    testJavaHeapOptions(\"\", \"\", \"\", \"-Xmx10m -Xms1m\", \"-Xmx50m -Xms2m\", \"-Xms2m -Xmx100m\", \"-Xmx10m -Xms1m\", \"-Xmx50m -Xms2m\", \"-Xms2m -Xmx100m\");\r\n    testJavaHeapOptions(\"-Xmx10m\", \"-Xmx20m\", \"-Xmx30m\", null, null, null, \"-Xmx10m\", \"-Xmx20m\", \"-Xmx30m\");\r\n    testJavaHeapOptions(\"-Xms5m -Xmx200m\", \"-Xms15m -Xmx300m\", \"-Xms25m -Xmx50m\", \"-XXabc\", \"-XXxyz\", \"-XXdef\", \"-XXabc -Xmx200m\", \"-XXxyz -Xmx300m\", \"-XXdef -Xmx50m\");\r\n    testJavaHeapOptions(\"-Xms5m -Xmx200m\", \"-Xms15m -Xmx300m\", \"-Xms25m -Xmx50m\", \"-XXabc -Xmx500m\", \"-XXxyz -Xmx600m\", \"-XXdef -Xmx700m\", \"-XXabc -Xmx200m\", \"-XXxyz -Xmx300m\", \"-XXdef -Xmx50m\");\r\n    testJavaHeapOptions(\"-Xmx10m\", \"-Xmx20m\", \"-Xmx50m\", \"-Xms2m\", \"-Xms3m\", \"-Xms5m\", \"-Xms2m -Xmx10m\", \"-Xms3m -Xmx20m\", \"-Xms5m -Xmx50m\");\r\n    testJavaHeapOptions(\"-Xmx10m\", \"-Xmx20m\", \"-Xmx50m\", \"-Xmx2m\", \"-Xmx3m\", \"-Xmx5m\", \"-Xmx10m\", \"-Xmx20m\", \"-Xmx50m\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testJavaHeapOptionsDisabled",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testJavaHeapOptionsDisabled() throws Exception\n{\r\n    Configuration gridmixConf = new Configuration();\r\n    gridmixConf.setBoolean(GridmixJob.GRIDMIX_TASK_JVM_OPTIONS_ENABLE, false);\r\n    gridmixConf.set(MRJobConfig.MAP_JAVA_OPTS, \"-Xmx1m\");\r\n    gridmixConf.set(MRJobConfig.REDUCE_JAVA_OPTS, \"-Xmx2m\");\r\n    gridmixConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, \"-Xmx3m\");\r\n    final JobConf originalConf = new JobConf();\r\n    originalConf.set(MRJobConfig.MAP_JAVA_OPTS, \"-Xmx10m\");\r\n    originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, \"-Xmx20m\");\r\n    originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, \"-Xmx30m\");\r\n    MockJob story = new MockJob(originalConf) {\r\n\r\n        public JobConf getJobConf() {\r\n            return originalConf;\r\n        }\r\n    };\r\n    GridmixJob job = new DummyGridmixJob(gridmixConf, story);\r\n    Job simulatedJob = job.getJob();\r\n    Configuration simulatedConf = simulatedJob.getConfiguration();\r\n    assertEquals(\"Map heap options works when disabled!\", \"-Xmx1m\", simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));\r\n    assertEquals(\"Reduce heap options works when disabled!\", \"-Xmx2m\", simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));\r\n    assertEquals(\"Task heap options works when disabled!\", \"-Xmx3m\", simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void init() throws IOException\n{\r\n    GridmixTestUtils.initCluster(TestDistCacheEmulation.class);\r\n    File target = new File(\"target\" + File.separator + TestDistCacheEmulation.class.getName());\r\n    if (!target.exists()) {\r\n        assertTrue(target.mkdirs());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "shutDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutDown() throws IOException\n{\r\n    GridmixTestUtils.shutdownCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateDistCacheData",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateDistCacheData(Configuration jobConf, long[] sortedFileSizes) throws FileNotFoundException, IOException\n{\r\n    Path distCachePath = dce.getDistributedCacheDir();\r\n    String filesListFile = jobConf.get(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_LIST);\r\n    FileSystem fs = FileSystem.get(jobConf);\r\n    Path listFile = new Path(filesListFile);\r\n    assertTrue(\"Path of Distributed Cache files list file is wrong.\", distCachePath.equals(listFile.getParent().makeQualified(fs.getUri(), fs.getWorkingDirectory())));\r\n    assertTrue(\"Failed to delete distributed Cache files list file \" + listFile, fs.delete(listFile, true));\r\n    List<Long> fileSizes = new ArrayList<Long>();\r\n    for (long size : sortedFileSizes) {\r\n        fileSizes.add(size);\r\n    }\r\n    validateDistCacheFiles(fileSizes, distCachePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateDistCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void validateDistCacheFiles(List<Long> filesSizesExpected, Path distCacheDir) throws FileNotFoundException, IOException\n{\r\n    FileStatus[] statuses = GridmixTestUtils.dfs.listStatus(distCacheDir);\r\n    int numFiles = filesSizesExpected.size();\r\n    assertEquals(\"Number of files under distributed cache dir is wrong.\", numFiles, statuses.length);\r\n    for (int i = 0; i < numFiles; i++) {\r\n        FileStatus stat = statuses[i];\r\n        assertTrue(\"File size of distributed cache file \" + stat.getPath().toUri().getPath() + \" is wrong.\", filesSizesExpected.remove(stat.getLen()));\r\n        FsPermission perm = stat.getPermission();\r\n        assertEquals(\"Wrong permissions for distributed cache file \" + stat.getPath().toUri().getPath(), new FsPermission(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_PERM), perm);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "configureDummyDistCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long[] configureDummyDistCacheFiles(Configuration conf) throws IOException\n{\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    conf.set(\"user.name\", user);\r\n    String[] distCacheFiles = { \"hdfs:///tmp/file1.txt\", \"/tmp/\" + user + \"/.staging/job_1/file2.txt\", \"hdfs:///user/user1/file3.txt\", \"/home/user2/file4.txt\", \"subdir1/file5.txt\", \"subdir2/file6.gz\" };\r\n    String[] fileSizes = { \"400\", \"2500\", \"700\", \"1200\", \"1500\", \"500\" };\r\n    String[] visibilities = { \"true\", \"false\", \"false\", \"true\", \"true\", \"false\" };\r\n    String[] timeStamps = { \"1234\", \"2345\", \"34567\", \"5434\", \"125\", \"134\" };\r\n    conf.setStrings(MRJobConfig.CACHE_FILES, distCacheFiles);\r\n    conf.setStrings(MRJobConfig.CACHE_FILES_SIZES, fileSizes);\r\n    conf.setStrings(JobContext.CACHE_FILE_VISIBILITIES, visibilities);\r\n    conf.setStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS, timeStamps);\r\n    long[] sortedFileSizes = new long[] { 1500, 1200, 700, 500, 400 };\r\n    return sortedFileSizes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "runSetupGenerateDistCacheData",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Configuration runSetupGenerateDistCacheData(boolean generate, long[] sortedFileSizes) throws IOException, InterruptedException\n{\r\n    Configuration conf = new Configuration();\r\n    long[] fileSizes = configureDummyDistCacheFiles(conf);\r\n    System.arraycopy(fileSizes, 0, sortedFileSizes, 0, fileSizes.length);\r\n    final int numJobs = 3;\r\n    DebugJobProducer jobProducer = new DebugJobProducer(numJobs, conf);\r\n    Configuration jobConf = GridmixTestUtils.mrvl.getConfig();\r\n    Path ioPath = new Path(\"testSetupGenerateDistCacheData\").makeQualified(GridmixTestUtils.dfs.getUri(), GridmixTestUtils.dfs.getWorkingDirectory());\r\n    FileSystem fs = FileSystem.get(jobConf);\r\n    if (fs.exists(ioPath)) {\r\n        fs.delete(ioPath, true);\r\n    }\r\n    FileSystem.mkdirs(fs, ioPath, new FsPermission((short) 0777));\r\n    dce = createDistributedCacheEmulator(jobConf, ioPath, generate);\r\n    int exitCode = dce.setupGenerateDistCacheData(jobProducer);\r\n    int expectedExitCode = generate ? 0 : Gridmix.MISSING_DIST_CACHE_FILES_ERROR;\r\n    assertEquals(\"setupGenerateDistCacheData failed.\", expectedExitCode, exitCode);\r\n    resetDistCacheConfigProperties(jobConf);\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "resetDistCacheConfigProperties",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void resetDistCacheConfigProperties(Configuration jobConf)\n{\r\n    jobConf.setStrings(MRJobConfig.CACHE_FILES, \"\");\r\n    jobConf.setStrings(MRJobConfig.CACHE_FILES_SIZES, \"\");\r\n    jobConf.setStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS, \"\");\r\n    jobConf.setStrings(JobContext.CACHE_FILE_VISIBILITIES, \"\");\r\n    jobConf.setStrings(\"mapred.cache.files\", \"\");\r\n    jobConf.setStrings(\"mapred.cache.files.filesizes\", \"\");\r\n    jobConf.setStrings(\"mapred.cache.files.visibilities\", \"\");\r\n    jobConf.setStrings(\"mapred.cache.files.timestamps\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testGenerateDistCacheData",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGenerateDistCacheData() throws Exception\n{\r\n    long[] sortedFileSizes = new long[5];\r\n    Configuration jobConf = runSetupGenerateDistCacheData(true, sortedFileSizes);\r\n    GridmixJob gridmixJob = new GenerateDistCacheData(jobConf);\r\n    Job job = gridmixJob.call();\r\n    assertEquals(\"Number of reduce tasks in GenerateDistCacheData is not 0.\", 0, job.getNumReduceTasks());\r\n    assertTrue(\"GenerateDistCacheData job failed.\", job.waitForCompletion(false));\r\n    validateDistCacheData(jobConf, sortedFileSizes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "validateSetupGenDC",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void validateSetupGenDC(Configuration jobConf, long[] sortedFileSizes) throws IOException, InterruptedException\n{\r\n    long sumOfFileSizes = 0;\r\n    for (int i = 0; i < sortedFileSizes.length; i++) {\r\n        sumOfFileSizes += sortedFileSizes[i];\r\n    }\r\n    FileSystem fs = FileSystem.get(jobConf);\r\n    assertEquals(\"Number of distributed cache files to be generated is wrong.\", sortedFileSizes.length, jobConf.getInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, -1));\r\n    assertEquals(\"Total size of dist cache files to be generated is wrong.\", sumOfFileSizes, jobConf.getLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, -1));\r\n    Path filesListFile = new Path(jobConf.get(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_LIST));\r\n    FileStatus stat = fs.getFileStatus(filesListFile);\r\n    assertEquals(\"Wrong permissions of dist Cache files list file \" + filesListFile, new FsPermission((short) 0644), stat.getPermission());\r\n    InputSplit split = new FileSplit(filesListFile, 0, stat.getLen(), (String[]) null);\r\n    TaskAttemptContext taskContext = MapReduceTestUtil.createDummyMapTaskAttemptContext(jobConf);\r\n    RecordReader<LongWritable, BytesWritable> reader = new GenerateDistCacheData.GenDCDataFormat().createRecordReader(split, taskContext);\r\n    MapContext<LongWritable, BytesWritable, NullWritable, BytesWritable> mapContext = new MapContextImpl<LongWritable, BytesWritable, NullWritable, BytesWritable>(jobConf, taskContext.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n    reader.initialize(split, mapContext);\r\n    doValidateSetupGenDC(reader, fs, sortedFileSizes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "doValidateSetupGenDC",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void doValidateSetupGenDC(RecordReader<LongWritable, BytesWritable> reader, FileSystem fs, long[] sortedFileSizes) throws IOException, InterruptedException\n{\r\n    Path distCacheDir = dce.getDistributedCacheDir();\r\n    assertEquals(\"Wrong permissions for distributed cache dir \" + distCacheDir, fs.getFileStatus(distCacheDir).getPermission().getOtherAction().and(FsAction.EXECUTE), FsAction.EXECUTE);\r\n    LongWritable key = new LongWritable();\r\n    BytesWritable val = new BytesWritable();\r\n    for (int i = 0; i < sortedFileSizes.length; i++) {\r\n        assertTrue(\"Number of files written to the sequence file by \" + \"setupGenerateDistCacheData is less than the expected.\", reader.nextKeyValue());\r\n        key = reader.getCurrentKey();\r\n        val = reader.getCurrentValue();\r\n        long fileSize = key.get();\r\n        String file = new String(val.getBytes(), 0, val.getLength());\r\n        assertEquals(\"Dist cache file size is wrong.\", sortedFileSizes[i], fileSize);\r\n        Path parent = new Path(file).getParent().makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n        assertTrue(\"Public dist cache file path is wrong.\", distCacheDir.equals(parent));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testSetupGenerateDistCacheData",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSetupGenerateDistCacheData() throws IOException, InterruptedException\n{\r\n    long[] sortedFileSizes = new long[5];\r\n    Configuration jobConf = runSetupGenerateDistCacheData(true, sortedFileSizes);\r\n    validateSetupGenDC(jobConf, sortedFileSizes);\r\n    runSetupGenerateDistCacheData(false, sortedFileSizes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "createDistributedCacheEmulator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DistributedCacheEmulator createDistributedCacheEmulator(Configuration conf, Path ioPath, boolean generate) throws IOException\n{\r\n    DistributedCacheEmulator dce = new DistributedCacheEmulator(conf, ioPath);\r\n    JobCreator jobCreator = JobCreator.getPolicy(conf, JobCreator.LOADJOB);\r\n    jobCreator.setDistCacheEmulator(dce);\r\n    dce.init(\"dummytrace\", jobCreator, generate);\r\n    return dce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testDistCacheEmulationConfigurability",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDistCacheEmulationConfigurability() throws IOException\n{\r\n    Configuration jobConf = GridmixTestUtils.mrvl.getConfig();\r\n    Path ioPath = new Path(\"testDistCacheEmulationConfigurability\").makeQualified(GridmixTestUtils.dfs.getUri(), GridmixTestUtils.dfs.getWorkingDirectory());\r\n    FileSystem fs = FileSystem.get(jobConf);\r\n    FileSystem.mkdirs(fs, ioPath, new FsPermission((short) 0777));\r\n    dce = createDistributedCacheEmulator(jobConf, ioPath, false);\r\n    assertTrue(\"Default configuration of \" + DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE + \" is wrong.\", dce.shouldEmulateDistCacheLoad());\r\n    jobConf.setBoolean(DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE, false);\r\n    dce = createDistributedCacheEmulator(jobConf, ioPath, false);\r\n    assertFalse(\"Disabling of emulation of distributed cache load by setting \" + DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE + \" to false is not working.\", dce.shouldEmulateDistCacheLoad());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testDistCacheEmulator",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDistCacheEmulator() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    configureDummyDistCacheFiles(conf);\r\n    File ws = new File(\"target\" + File.separator + this.getClass().getName());\r\n    Path ioPath = new Path(ws.getAbsolutePath());\r\n    DistributedCacheEmulator dce = new DistributedCacheEmulator(conf, ioPath);\r\n    JobConf jobConf = new JobConf(conf);\r\n    jobConf.setUser(UserGroupInformation.getCurrentUser().getShortUserName());\r\n    File fin = new File(\"src\" + File.separator + \"test\" + File.separator + \"resources\" + File.separator + \"data\" + File.separator + \"wordcount.json\");\r\n    dce.init(fin.getAbsolutePath(), JobCreator.LOADJOB, true);\r\n    dce.configureDistCacheFiles(conf, jobConf);\r\n    String[] caches = conf.getStrings(MRJobConfig.CACHE_FILES);\r\n    String[] tmpfiles = conf.getStrings(\"tmpfiles\");\r\n    assertEquals(6, ((caches == null ? 0 : caches.length) + (tmpfiles == null ? 0 : tmpfiles.length)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "initCluster",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void initCluster(Class<?> caller) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"mapred.queue.names\", \"default\");\r\n    conf.set(PREFIX + \"root.queues\", \"default\");\r\n    conf.set(PREFIX + \"root.default.capacity\", \"100.0\");\r\n    conf.setBoolean(GRIDMIX_USE_QUEUE_IN_TRACE, false);\r\n    conf.set(GRIDMIX_DEFAULT_QUEUE, \"default\");\r\n    dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();\r\n    dfs = dfsCluster.getFileSystem();\r\n    conf.set(JTConfig.JT_RETIREJOBS, \"false\");\r\n    mrvl = MiniMRClientClusterFactory.create(caller, 2, conf);\r\n    conf = mrvl.getConfig();\r\n    String[] files = conf.getStrings(MRJobConfig.CACHE_FILES);\r\n    if (files != null) {\r\n        String[] timestamps = new String[files.length];\r\n        for (int i = 0; i < files.length; i++) {\r\n            timestamps[i] = Long.toString(System.currentTimeMillis());\r\n        }\r\n        conf.setStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS, timestamps);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "shutdownCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdownCluster() throws IOException\n{\r\n    if (mrvl != null) {\r\n        mrvl.stop();\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "createHomeAndStagingDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void createHomeAndStagingDirectory(String user, Configuration conf)\n{\r\n    try {\r\n        FileSystem fs = dfsCluster.getFileSystem();\r\n        String path = \"/user/\" + user;\r\n        Path homeDirectory = new Path(path);\r\n        if (!fs.exists(homeDirectory)) {\r\n            LOG.info(\"Creating Home directory : \" + homeDirectory);\r\n            fs.mkdirs(homeDirectory);\r\n            changePermission(user, homeDirectory, fs);\r\n        }\r\n        changePermission(user, homeDirectory, fs);\r\n        Path stagingArea = new Path(conf.get(\"mapreduce.jobtracker.staging.root.dir\", \"/tmp/hadoop/mapred/staging\"));\r\n        LOG.info(\"Creating Staging root directory : \" + stagingArea);\r\n        fs.mkdirs(stagingArea);\r\n        fs.setPermission(stagingArea, new FsPermission((short) 0777));\r\n    } catch (IOException ioe) {\r\n        ioe.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "changePermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void changePermission(String user, Path homeDirectory, FileSystem fs) throws IOException\n{\r\n    fs.setOwner(homeDirectory, user, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init() throws IOException\n{\r\n    GridmixTestUtils.initCluster(TestGridmixSubmission.class);\r\n    System.setProperty(\"src.test.data\", inSpace.getAbsolutePath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "shutDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutDown() throws IOException\n{\r\n    GridmixTestUtils.shutdownCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "verifyWordCountJobStory",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void verifyWordCountJobStory(JobStory js)\n{\r\n    assertNotNull(\"Null JobStory\", js);\r\n    String expectedJobStory = \"WordCount:johndoe:default:1285322645148:3:1\";\r\n    String actualJobStory = js.getName() + \":\" + js.getUser() + \":\" + js.getQueueName() + \":\" + js.getSubmissionTime() + \":\" + js.getNumberMaps() + \":\" + js.getNumberReduces();\r\n    assertEquals(\"Unexpected JobStory\", expectedJobStory, actualJobStory);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "expandGzippedTrace",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void expandGzippedTrace(FileSystem fs, Path in, Path out) throws Exception\n{\r\n    byte[] buff = new byte[4096];\r\n    GZIPInputStream gis = new GZIPInputStream(fs.open(in));\r\n    FSDataOutputStream fsdOs = fs.create(out);\r\n    int numRead;\r\n    while ((numRead = gis.read(buff, 0, buff.length)) != -1) {\r\n        fsdOs.write(buff, 0, numRead);\r\n    }\r\n    gis.close();\r\n    fsdOs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testTraceReader",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTraceReader() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    Path rootInputDir = new Path(System.getProperty(\"src.test.data\"));\r\n    rootInputDir = rootInputDir.makeQualified(lfs.getUri(), lfs.getWorkingDirectory());\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", System.getProperty(\"java.io.tmpdir\")), \"testTraceReader\");\r\n    rootTempDir = rootTempDir.makeQualified(lfs.getUri(), lfs.getWorkingDirectory());\r\n    Path inputFile = new Path(rootInputDir, \"wordcount.json.gz\");\r\n    Path tempFile = new Path(rootTempDir, \"gridmix3-wc.json\");\r\n    InputStream origStdIn = System.in;\r\n    InputStream tmpIs = null;\r\n    try {\r\n        DebugGridmix dgm = new DebugGridmix();\r\n        JobStoryProducer jsp = dgm.createJobStoryProducer(inputFile.toString(), conf);\r\n        LOG.info(\"Verifying JobStory from compressed trace...\");\r\n        verifyWordCountJobStory(jsp.getNextJob());\r\n        expandGzippedTrace(lfs, inputFile, tempFile);\r\n        jsp = dgm.createJobStoryProducer(tempFile.toString(), conf);\r\n        LOG.info(\"Verifying JobStory from uncompressed trace...\");\r\n        verifyWordCountJobStory(jsp.getNextJob());\r\n        tmpIs = lfs.open(tempFile);\r\n        System.setIn(tmpIs);\r\n        LOG.info(\"Verifying JobStory from trace in standard input...\");\r\n        jsp = dgm.createJobStoryProducer(\"-\", conf);\r\n        verifyWordCountJobStory(jsp.getNextJob());\r\n    } finally {\r\n        System.setIn(origStdIn);\r\n        if (tmpIs != null) {\r\n            tmpIs.close();\r\n        }\r\n        lfs.delete(rootTempDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testReplaySubmit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testReplaySubmit() throws Exception\n{\r\n    policy = GridmixJobSubmissionPolicy.REPLAY;\r\n    LOG.info(\" Replay started at \" + System.currentTimeMillis());\r\n    doSubmission(null, false);\r\n    LOG.info(\" Replay ended at \" + System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testStressSubmit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testStressSubmit() throws Exception\n{\r\n    policy = GridmixJobSubmissionPolicy.STRESS;\r\n    LOG.info(\" Stress started at \" + System.currentTimeMillis());\r\n    doSubmission(null, false);\r\n    LOG.info(\" Stress ended at \" + System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testMain",
  "errType" : [ "ExitUtil.ExitException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testMain() throws Exception\n{\r\n    SecurityManager securityManager = System.getSecurityManager();\r\n    final ByteArrayOutputStream bytes = new ByteArrayOutputStream();\r\n    final PrintStream out = new PrintStream(bytes);\r\n    final PrintStream oldOut = System.out;\r\n    System.setErr(out);\r\n    ExitUtil.disableSystemExit();\r\n    try {\r\n        String[] argv = new String[0];\r\n        DebugGridmix.main(argv);\r\n    } catch (ExitUtil.ExitException e) {\r\n        assertExceptionContains(ExitUtil.EXIT_EXCEPTION_MESSAGE, e);\r\n        ExitUtil.resetFirstExitException();\r\n    } finally {\r\n        System.setErr(oldOut);\r\n        System.setSecurityManager(securityManager);\r\n    }\r\n    String print = bytes.toString();\r\n    assertTrue(print.contains(\"Usage: gridmix [-generate <MiB>] [-users URI] [-Dname=value ...] <iopath> <trace>\"));\r\n    assertTrue(print.contains(\"e.g. gridmix -generate 100m foo -\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init() throws IOException\n{\r\n    GridmixTestUtils.initCluster(TestSleepJob.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "shutDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutDown() throws IOException\n{\r\n    GridmixTestUtils.shutdownCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testMapTasksOnlySleepJobs",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMapTasksOnlySleepJobs() throws Exception\n{\r\n    Configuration configuration = GridmixTestUtils.mrvl.getConfig();\r\n    DebugJobProducer jobProducer = new DebugJobProducer(5, configuration);\r\n    configuration.setBoolean(SleepJob.SLEEPJOB_MAPTASK_ONLY, true);\r\n    UserGroupInformation ugi = UserGroupInformation.getLoginUser();\r\n    JobStory story;\r\n    int seq = 1;\r\n    while ((story = jobProducer.getNextJob()) != null) {\r\n        GridmixJob gridmixJob = JobCreator.SLEEPJOB.createGridmixJob(configuration, 0, story, new Path(\"ignored\"), ugi, seq++);\r\n        gridmixJob.buildSplits(null);\r\n        Job job = gridmixJob.call();\r\n        assertEquals(0, job.getNumReduceTasks());\r\n    }\r\n    jobProducer.close();\r\n    assertEquals(6, seq);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomLocation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRandomLocation() throws Exception\n{\r\n    UserGroupInformation ugi = UserGroupInformation.getLoginUser();\r\n    testRandomLocation(1, 10, ugi);\r\n    testRandomLocation(2, 10, ugi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testSerialSubmit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSerialSubmit() throws Exception\n{\r\n    policy = GridmixJobSubmissionPolicy.SERIAL;\r\n    LOG.info(\"Serial started at \" + System.currentTimeMillis());\r\n    doSubmission(JobCreator.SLEEPJOB.name(), false);\r\n    LOG.info(\"Serial ended at \" + System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testReplaySubmit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testReplaySubmit() throws Exception\n{\r\n    policy = GridmixJobSubmissionPolicy.REPLAY;\r\n    LOG.info(\" Replay started at \" + System.currentTimeMillis());\r\n    doSubmission(JobCreator.SLEEPJOB.name(), false);\r\n    LOG.info(\" Replay ended at \" + System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testStressSubmit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testStressSubmit() throws Exception\n{\r\n    policy = GridmixJobSubmissionPolicy.STRESS;\r\n    LOG.info(\" Replay started at \" + System.currentTimeMillis());\r\n    doSubmission(JobCreator.SLEEPJOB.name(), false);\r\n    LOG.info(\" Replay ended at \" + System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomLocation",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRandomLocation(int locations, int njobs, UserGroupInformation ugi) throws Exception\n{\r\n    Configuration configuration = new Configuration();\r\n    DebugJobProducer jobProducer = new DebugJobProducer(njobs, configuration);\r\n    Configuration jconf = GridmixTestUtils.mrvl.getConfig();\r\n    jconf.setInt(JobCreator.SLEEPJOB_RANDOM_LOCATIONS, locations);\r\n    JobStory story;\r\n    int seq = 1;\r\n    while ((story = jobProducer.getNextJob()) != null) {\r\n        GridmixJob gridmixJob = JobCreator.SLEEPJOB.createGridmixJob(jconf, 0, story, new Path(\"ignored\"), ugi, seq++);\r\n        gridmixJob.buildSplits(null);\r\n        List<InputSplit> splits = new SleepJob.SleepInputFormat().getSplits(gridmixJob.getJob());\r\n        for (InputSplit split : splits) {\r\n            assertEquals(locations, split.getLocations().length);\r\n        }\r\n    }\r\n    jobProducer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = FileSystem.getLocal(conf).getRaw();\r\n    final Path p = fs.makeQualified(new Path(System.getProperty(\"test.build.data\", \"/tmp\"), \"testFileQueue\"));\r\n    fs.delete(p, true);\r\n    final byte[] b = new byte[BLOCK];\r\n    for (int i = 0; i < NFILES; ++i) {\r\n        Arrays.fill(b, (byte) ('A' + i));\r\n        paths[i] = new Path(p, \"\" + (char) ('A' + i));\r\n        OutputStream f = null;\r\n        try {\r\n            f = fs.create(paths[i]);\r\n            f.write(b);\r\n        } finally {\r\n            if (f != null) {\r\n                f.close();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = FileSystem.getLocal(conf).getRaw();\r\n    final Path p = fs.makeQualified(new Path(System.getProperty(\"test.build.data\", \"/tmp\"), \"testFileQueue\"));\r\n    fs.delete(p, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "fillVerif",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ByteArrayOutputStream fillVerif() throws IOException\n{\r\n    final byte[] b = new byte[BLOCK];\r\n    final ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    for (int i = 0; i < NFILES; ++i) {\r\n        Arrays.fill(b, (byte) ('A' + i));\r\n        out.write(b, 0, (int) len[i]);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRepeat",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRepeat() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    Arrays.fill(loc, \"\");\r\n    Arrays.fill(start, 0L);\r\n    Arrays.fill(len, BLOCK);\r\n    final ByteArrayOutputStream out = fillVerif();\r\n    final FileQueue q = new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);\r\n    final byte[] verif = out.toByteArray();\r\n    final byte[] check = new byte[2 * NFILES * BLOCK];\r\n    q.read(check, 0, NFILES * BLOCK);\r\n    assertArrayEquals(verif, Arrays.copyOf(check, NFILES * BLOCK));\r\n    final byte[] verif2 = new byte[2 * NFILES * BLOCK];\r\n    System.arraycopy(verif, 0, verif2, 0, verif.length);\r\n    System.arraycopy(verif, 0, verif2, verif.length, verif.length);\r\n    q.read(check, 0, 2 * NFILES * BLOCK);\r\n    assertArrayEquals(verif2, check);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testUneven",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testUneven() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    Arrays.fill(loc, \"\");\r\n    Arrays.fill(start, 0L);\r\n    Arrays.fill(len, BLOCK);\r\n    final int B2 = BLOCK / 2;\r\n    for (int i = 0; i < NFILES; i += 2) {\r\n        start[i] += B2;\r\n        len[i] -= B2;\r\n    }\r\n    final FileQueue q = new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);\r\n    final ByteArrayOutputStream out = fillVerif();\r\n    final byte[] verif = out.toByteArray();\r\n    final byte[] check = new byte[NFILES / 2 * BLOCK + NFILES / 2 * B2];\r\n    q.read(check, 0, verif.length);\r\n    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));\r\n    q.read(check, 0, verif.length);\r\n    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testEmpty",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testEmpty() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileQueue q = new FileQueue(new CombineFileSplit(new Path[0], new long[0], new long[0], new String[0]), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "convertIntArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Integer> convertIntArray(int[] from)\n{\r\n    List<Integer> ret = new ArrayList<Integer>(from.length);\r\n    for (int v : from) {\r\n        ret.add(v);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomSelectSelector",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRandomSelectSelector(int niter, int m, int n)\n{\r\n    RandomAlgorithms.Selector selector = new RandomAlgorithms.Selector(n, (double) m / n, new Random());\r\n    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(niter);\r\n    for (int i = 0; i < niter; ++i, selector.reset()) {\r\n        int[] result = new int[m];\r\n        for (int j = 0; j < m; ++j) {\r\n            int v = selector.next();\r\n            if (v < 0)\r\n                break;\r\n            result[j] = v;\r\n        }\r\n        Arrays.sort(result);\r\n        List<Integer> resultAsList = convertIntArray(result);\r\n        Integer count = results.get(resultAsList);\r\n        if (count == null) {\r\n            results.put(resultAsList, 1);\r\n        } else {\r\n            results.put(resultAsList, ++count);\r\n        }\r\n    }\r\n    verifyResults(results, m, n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomSelect",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRandomSelect(int niter, int m, int n)\n{\r\n    Random random = new Random();\r\n    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(niter);\r\n    for (int i = 0; i < niter; ++i) {\r\n        int[] result = RandomAlgorithms.select(m, n, random);\r\n        Arrays.sort(result);\r\n        List<Integer> resultAsList = convertIntArray(result);\r\n        Integer count = results.get(resultAsList);\r\n        if (count == null) {\r\n            results.put(resultAsList, 1);\r\n        } else {\r\n            results.put(resultAsList, ++count);\r\n        }\r\n    }\r\n    verifyResults(results, m, n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "verifyResults",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void verifyResults(Map<List<Integer>, Integer> results, int m, int n)\n{\r\n    if (n >= 10) {\r\n        assertTrue(results.size() >= Math.min(m, 2));\r\n    }\r\n    for (List<Integer> result : results.keySet()) {\r\n        assertEquals(m, result.size());\r\n        Set<Integer> seen = new HashSet<Integer>();\r\n        for (int v : result) {\r\n            System.out.printf(\"%d \", v);\r\n            assertTrue((v >= 0) && (v < n));\r\n            assertTrue(seen.add(v));\r\n        }\r\n        System.out.printf(\" ==> %d\\n\", results.get(result));\r\n    }\r\n    System.out.println(\"====\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomSelect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRandomSelect()\n{\r\n    for (int[] param : parameters) {\r\n        testRandomSelect(param[0], param[1], param[2]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomSelectSelector",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRandomSelectSelector()\n{\r\n    for (int[] param : parameters) {\r\n        testRandomSelectSelector(param[0], param[1], param[2]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "lengthTest",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void lengthTest(GridmixRecord x, GridmixRecord y, int min, int max) throws Exception\n{\r\n    final Random r = new Random();\r\n    final long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    LOG.info(\"length: \" + seed);\r\n    final DataInputBuffer in = new DataInputBuffer();\r\n    final DataOutputBuffer out1 = new DataOutputBuffer();\r\n    final DataOutputBuffer out2 = new DataOutputBuffer();\r\n    for (int i = min; i < max; ++i) {\r\n        setSerialize(x, r.nextLong(), i, out1);\r\n        assertEquals(i, out1.getLength());\r\n        x.write(out2);\r\n        in.reset(out1.getData(), 0, out1.getLength());\r\n        y.readFields(in);\r\n        assertEquals(i, x.getSize());\r\n        assertEquals(i, y.getSize());\r\n    }\r\n    in.reset(out2.getData(), 0, out2.getLength());\r\n    for (int i = min; i < max; ++i) {\r\n        y.readFields(in);\r\n        assertEquals(i, y.getSize());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "randomReplayTest",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void randomReplayTest(GridmixRecord x, GridmixRecord y, int min, int max) throws Exception\n{\r\n    final Random r = new Random();\r\n    final long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    LOG.info(\"randReplay: \" + seed);\r\n    final DataOutputBuffer out1 = new DataOutputBuffer();\r\n    for (int i = min; i < max; ++i) {\r\n        final int s = out1.getLength();\r\n        x.setSeed(r.nextLong());\r\n        x.setSize(i);\r\n        x.write(out1);\r\n        assertEquals(i, out1.getLength() - s);\r\n    }\r\n    final DataInputBuffer in = new DataInputBuffer();\r\n    in.reset(out1.getData(), 0, out1.getLength());\r\n    final DataOutputBuffer out2 = new DataOutputBuffer();\r\n    for (int i = min; i < max; ++i) {\r\n        final int s = in.getPosition();\r\n        y.readFields(in);\r\n        assertEquals(i, in.getPosition() - s);\r\n        y.write(out2);\r\n    }\r\n    assertEquals(out1.getLength(), out2.getLength());\r\n    assertEquals(\"Bad test\", out1.getData().length, out2.getData().length);\r\n    assertArrayEquals(out1.getData(), out2.getData());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "eqSeedTest",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void eqSeedTest(GridmixRecord x, GridmixRecord y, int max) throws Exception\n{\r\n    final Random r = new Random();\r\n    final long s = r.nextLong();\r\n    r.setSeed(s);\r\n    LOG.info(\"eqSeed: \" + s);\r\n    assertEquals(x.fixedBytes(), y.fixedBytes());\r\n    final int min = x.fixedBytes() + 1;\r\n    final DataOutputBuffer out1 = new DataOutputBuffer();\r\n    final DataOutputBuffer out2 = new DataOutputBuffer();\r\n    for (int i = min; i < max; ++i) {\r\n        final long seed = r.nextLong();\r\n        setSerialize(x, seed, i, out1);\r\n        setSerialize(y, seed, i, out2);\r\n        assertEquals(x, y);\r\n        assertEquals(x.hashCode(), y.hashCode());\r\n        assertEquals(out1.getLength(), out2.getLength());\r\n        assertEquals(\"Bad test\", out1.getData().length, out2.getData().length);\r\n        assertArrayEquals(out1.getData(), out2.getData());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "binSortTest",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void binSortTest(GridmixRecord x, GridmixRecord y, int min, int max, WritableComparator cmp) throws Exception\n{\r\n    final Random r = new Random();\r\n    final long s = r.nextLong();\r\n    r.setSeed(s);\r\n    LOG.info(\"sort: \" + s);\r\n    final DataOutputBuffer out1 = new DataOutputBuffer();\r\n    final DataOutputBuffer out2 = new DataOutputBuffer();\r\n    for (int i = min; i < max; ++i) {\r\n        final long seed1 = r.nextLong();\r\n        setSerialize(x, seed1, i, out1);\r\n        assertEquals(0, x.compareSeed(seed1, Math.max(0, i - x.fixedBytes())));\r\n        final long seed2 = r.nextLong();\r\n        setSerialize(y, seed2, i, out2);\r\n        assertEquals(0, y.compareSeed(seed2, Math.max(0, i - x.fixedBytes())));\r\n        final int chk = WritableComparator.compareBytes(out1.getData(), 0, out1.getLength(), out2.getData(), 0, out2.getLength());\r\n        assertEquals(Integer.signum(chk), Integer.signum(x.compareTo(y)));\r\n        assertEquals(Integer.signum(chk), Integer.signum(cmp.compare(out1.getData(), 0, out1.getLength(), out2.getData(), 0, out2.getLength())));\r\n        final int s1 = out1.getLength();\r\n        x.write(out1);\r\n        assertEquals(0, cmp.compare(out1.getData(), 0, s1, out1.getData(), s1, out1.getLength() - s1));\r\n        final int s2 = out2.getLength();\r\n        y.write(out2);\r\n        assertEquals(0, cmp.compare(out2.getData(), 0, s2, out2.getData(), s2, out2.getLength() - s2));\r\n        assertEquals(Integer.signum(chk), Integer.signum(cmp.compare(out1.getData(), 0, s1, out2.getData(), s2, out2.getLength() - s2)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "checkSpec",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void checkSpec(GridmixKey a, GridmixKey b) throws Exception\n{\r\n    final Random r = new Random();\r\n    final long s = r.nextLong();\r\n    r.setSeed(s);\r\n    LOG.info(\"spec: \" + s);\r\n    final DataInputBuffer in = new DataInputBuffer();\r\n    final DataOutputBuffer out = new DataOutputBuffer();\r\n    a.setType(GridmixKey.REDUCE_SPEC);\r\n    b.setType(GridmixKey.REDUCE_SPEC);\r\n    for (int i = 0; i < 100; ++i) {\r\n        final int in_rec = r.nextInt(Integer.MAX_VALUE);\r\n        a.setReduceInputRecords(in_rec);\r\n        final int out_rec = r.nextInt(Integer.MAX_VALUE);\r\n        a.setReduceOutputRecords(out_rec);\r\n        final int out_bytes = r.nextInt(Integer.MAX_VALUE);\r\n        a.setReduceOutputBytes(out_bytes);\r\n        final int min = WritableUtils.getVIntSize(in_rec) + WritableUtils.getVIntSize(out_rec) + WritableUtils.getVIntSize(out_bytes) + WritableUtils.getVIntSize(0);\r\n        assertEquals(min + 2, a.fixedBytes());\r\n        final int size = r.nextInt(1024) + a.fixedBytes() + 1;\r\n        setSerialize(a, r.nextLong(), size, out);\r\n        assertEquals(size, out.getLength());\r\n        assertTrue(a.equals(a));\r\n        assertEquals(0, a.compareTo(a));\r\n        in.reset(out.getData(), 0, out.getLength());\r\n        b.readFields(in);\r\n        assertEquals(size, b.getSize());\r\n        assertEquals(in_rec, b.getReduceInputRecords());\r\n        assertEquals(out_rec, b.getReduceOutputRecords());\r\n        assertEquals(out_bytes, b.getReduceOutputBytes());\r\n        assertTrue(a.equals(b));\r\n        assertEquals(0, a.compareTo(b));\r\n        assertEquals(a.hashCode(), b.hashCode());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "setSerialize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setSerialize(GridmixRecord x, long seed, int size, DataOutputBuffer out) throws IOException\n{\r\n    x.setSeed(seed);\r\n    x.setSize(size);\r\n    out.reset();\r\n    x.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testKeySpec",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testKeySpec() throws Exception\n{\r\n    final int min = 6;\r\n    final int max = 300;\r\n    final GridmixKey a = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);\r\n    final GridmixKey b = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);\r\n    lengthTest(a, b, min, max);\r\n    randomReplayTest(a, b, min, max);\r\n    binSortTest(a, b, min, max, new GridmixKey.Comparator());\r\n    eqSeedTest(a, b, max);\r\n    checkSpec(a, b);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testKeyData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testKeyData() throws Exception\n{\r\n    final int min = 2;\r\n    final int max = 300;\r\n    final GridmixKey a = new GridmixKey(GridmixKey.DATA, 1, 0L);\r\n    final GridmixKey b = new GridmixKey(GridmixKey.DATA, 1, 0L);\r\n    lengthTest(a, b, min, max);\r\n    randomReplayTest(a, b, min, max);\r\n    binSortTest(a, b, min, max, new GridmixKey.Comparator());\r\n    eqSeedTest(a, b, 300);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testBaseRecord",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testBaseRecord() throws Exception\n{\r\n    final int min = 1;\r\n    final int max = 300;\r\n    final GridmixRecord a = new GridmixRecord();\r\n    final GridmixRecord b = new GridmixRecord();\r\n    lengthTest(a, b, min, max);\r\n    randomReplayTest(a, b, min, max);\r\n    binSortTest(a, b, min, max, new GridmixRecord.Comparator());\r\n    eqSeedTest(a, b, 300);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "main",
  "errType" : [ "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    boolean fail = false;\r\n    final TestGridmixRecord test = new TestGridmixRecord();\r\n    try {\r\n        test.testKeySpec();\r\n    } catch (Exception e) {\r\n        fail = true;\r\n        e.printStackTrace();\r\n    }\r\n    try {\r\n        test.testKeyData();\r\n    } catch (Exception e) {\r\n        fail = true;\r\n        e.printStackTrace();\r\n    }\r\n    try {\r\n        test.testBaseRecord();\r\n    } catch (Exception e) {\r\n        fail = true;\r\n        e.printStackTrace();\r\n    }\r\n    System.exit(fail ? -1 : 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "printDebug",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void printDebug(GridmixRecord a, GridmixRecord b) throws IOException\n{\r\n    DataOutputBuffer out = new DataOutputBuffer();\r\n    a.write(out);\r\n    System.out.println(\"A \" + Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));\r\n    out.reset();\r\n    b.write(out);\r\n    System.out.println(\"B \" + Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testFactory",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testFactory(long targetBytes, long targetRecs) throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    final GridmixKey key = new GridmixKey();\r\n    final GridmixRecord val = new GridmixRecord();\r\n    LOG.info(\"Target bytes/records: \" + targetBytes + \"/\" + targetRecs);\r\n    final RecordFactory f = new AvgRecordFactory(targetBytes, targetRecs, conf);\r\n    targetRecs = targetRecs <= 0 && targetBytes >= 0 ? Math.max(1, targetBytes / conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64 * 1024)) : targetRecs;\r\n    long records = 0L;\r\n    final DataOutputBuffer out = new DataOutputBuffer();\r\n    while (f.next(key, val)) {\r\n        ++records;\r\n        key.write(out);\r\n        val.write(out);\r\n    }\r\n    assertEquals(targetRecs, records);\r\n    assertEquals(targetBytes, out.getLength());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandom",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRandom() throws Exception\n{\r\n    final Random r = new Random();\r\n    final long targetBytes = r.nextInt(1 << 20) + 3 * (1 << 14);\r\n    final long targetRecs = r.nextInt(1 << 14);\r\n    testFactory(targetBytes, targetRecs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testAvg",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAvg() throws Exception\n{\r\n    final Random r = new Random();\r\n    final long avgsize = r.nextInt(1 << 10) + 1;\r\n    final long targetRecs = r.nextInt(1 << 14);\r\n    testFactory(targetRecs * avgsize, targetRecs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testZero",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testZero() throws Exception\n{\r\n    final Random r = new Random();\r\n    final long targetBytes = r.nextInt(1 << 20);\r\n    testFactory(targetBytes, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomCompressedTextDataGenerator",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testRandomCompressedTextDataGenerator() throws Exception\n{\r\n    int wordSize = 10;\r\n    int listSize = 20;\r\n    long dataSize = 10 * 1024 * 1024;\r\n    Configuration conf = new Configuration();\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);\r\n    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, listSize);\r\n    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, wordSize);\r\n    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);\r\n    conf.set(\"mapreduce.job.hdfs-servers\", \"\");\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).makeQualified(lfs.getUri(), lfs.getWorkingDirectory());\r\n    Path tempDir = new Path(rootTempDir, \"TestRandomCompressedTextDataGenr\");\r\n    lfs.delete(tempDir, true);\r\n    runDataGenJob(conf, tempDir);\r\n    FileStatus[] files = lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());\r\n    long size = 0;\r\n    long maxLineSize = 0;\r\n    for (FileStatus status : files) {\r\n        InputStream in = CompressionEmulationUtil.getPossiblyDecompressedInputStream(status.getPath(), conf, 0);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(in));\r\n        String line = reader.readLine();\r\n        if (line != null) {\r\n            long lineSize = line.getBytes().length;\r\n            if (lineSize > maxLineSize) {\r\n                maxLineSize = lineSize;\r\n            }\r\n            while (line != null) {\r\n                for (String word : line.split(\"\\\\s\")) {\r\n                    size += word.getBytes().length;\r\n                }\r\n                line = reader.readLine();\r\n            }\r\n        }\r\n        reader.close();\r\n    }\r\n    assertTrue(size >= dataSize);\r\n    assertTrue(size <= dataSize + maxLineSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "runDataGenJob",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void runDataGenJob(Configuration conf, Path tempDir) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    JobClient client = new JobClient(conf);\r\n    conf.setInt(MRJobConfig.NUM_MAPS, 1);\r\n    Job job = Job.getInstance(conf);\r\n    CompressionEmulationUtil.configure(job);\r\n    job.setInputFormatClass(CustomInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, tempDir);\r\n    job.submit();\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    assertEquals(\"Job Failed\", 0, ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testCompressionRatioConfigure",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCompressionRatioConfigure(float ratio) throws Exception\n{\r\n    long dataSize = 10 * 1024 * 1024;\r\n    Configuration conf = new Configuration();\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);\r\n    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);\r\n    conf.set(\"mapreduce.job.hdfs-servers\", \"\");\r\n    float expectedRatio = CompressionEmulationUtil.DEFAULT_COMPRESSION_RATIO;\r\n    if (ratio > 0) {\r\n        CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);\r\n        expectedRatio = CompressionEmulationUtil.standardizeCompressionRatio(ratio);\r\n    }\r\n    CompressionEmulationUtil.setupDataGeneratorConfig(conf);\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).makeQualified(lfs.getUri(), lfs.getWorkingDirectory());\r\n    Path tempDir = new Path(rootTempDir, \"TestCustomRandomCompressedTextDataGenr\");\r\n    lfs.delete(tempDir, true);\r\n    runDataGenJob(conf, tempDir);\r\n    FileStatus[] files = lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());\r\n    long size = 0;\r\n    for (FileStatus status : files) {\r\n        size += status.getLen();\r\n    }\r\n    float compressionRatio = ((float) size) / dataSize;\r\n    float stdRatio = CompressionEmulationUtil.standardizeCompressionRatio(compressionRatio);\r\n    assertEquals(expectedRatio, stdRatio, 0.0D);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testCompressionRatios",
  "errType" : [ "RuntimeException", "RuntimeException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCompressionRatios() throws Exception\n{\r\n    testCompressionRatioConfigure(0F);\r\n    testCompressionRatioConfigure(0.2F);\r\n    testCompressionRatioConfigure(0.4F);\r\n    testCompressionRatioConfigure(0.65F);\r\n    testCompressionRatioConfigure(0.682F);\r\n    testCompressionRatioConfigure(0.567F);\r\n    boolean failed = false;\r\n    try {\r\n        testCompressionRatioConfigure(0.01F);\r\n    } catch (RuntimeException re) {\r\n        failed = true;\r\n    }\r\n    assertTrue(\"Compression ratio min value (0.07) check failed!\", failed);\r\n    failed = false;\r\n    try {\r\n        testCompressionRatioConfigure(0.7F);\r\n    } catch (RuntimeException re) {\r\n        failed = true;\r\n    }\r\n    assertTrue(\"Compression ratio max value (0.68) check failed!\", failed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testCompressionRatioStandardization",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCompressionRatioStandardization() throws Exception\n{\r\n    assertEquals(0.55F, CompressionEmulationUtil.standardizeCompressionRatio(0.55F), 0.0D);\r\n    assertEquals(0.65F, CompressionEmulationUtil.standardizeCompressionRatio(0.652F), 0.0D);\r\n    assertEquals(0.78F, CompressionEmulationUtil.standardizeCompressionRatio(0.777F), 0.0D);\r\n    assertEquals(0.86F, CompressionEmulationUtil.standardizeCompressionRatio(0.855F), 0.0D);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testInputCompressionRatioConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testInputCompressionRatioConfiguration() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    float ratio = 0.567F;\r\n    CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);\r\n    assertEquals(ratio, CompressionEmulationUtil.getMapInputCompressionEmulationRatio(conf), 0.0D);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testIntermediateCompressionRatioConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testIntermediateCompressionRatioConfiguration() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    float ratio = 0.567F;\r\n    CompressionEmulationUtil.setMapOutputCompressionEmulationRatio(conf, ratio);\r\n    assertEquals(ratio, CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf), 0.0D);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testOutputCompressionRatioConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testOutputCompressionRatioConfiguration() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    float ratio = 0.567F;\r\n    CompressionEmulationUtil.setJobOutputCompressionEmulationRatio(conf, ratio);\r\n    assertEquals(ratio, CompressionEmulationUtil.getJobOutputCompressionEmulationRatio(conf), 0.0D);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testCompressibleGridmixRecord",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testCompressibleGridmixRecord() throws IOException\n{\r\n    JobConf conf = new JobConf();\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    int dataSize = 1024 * 1024 * 10;\r\n    float ratio = 0.357F;\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).makeQualified(lfs.getUri(), lfs.getWorkingDirectory());\r\n    Path tempDir = new Path(rootTempDir, \"TestPossiblyCompressibleGridmixRecord\");\r\n    lfs.delete(tempDir, true);\r\n    GridmixRecord record = new GridmixRecord(dataSize, 0);\r\n    record.setCompressibility(true, ratio);\r\n    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);\r\n    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);\r\n    Path recordFile = new Path(tempDir, \"record\");\r\n    OutputStream outStream = CompressionEmulationUtil.getPossiblyCompressedOutputStream(recordFile, conf);\r\n    DataOutputStream out = new DataOutputStream(outStream);\r\n    record.write(out);\r\n    out.close();\r\n    outStream.close();\r\n    Path actualRecordFile = recordFile.suffix(\".gz\");\r\n    InputStream in = CompressionEmulationUtil.getPossiblyDecompressedInputStream(actualRecordFile, conf, 0);\r\n    long compressedFileSize = lfs.listStatus(actualRecordFile)[0].getLen();\r\n    GridmixRecord recordRead = new GridmixRecord();\r\n    recordRead.readFields(new DataInputStream(in));\r\n    assertEquals(\"Record size mismatch in a compressible GridmixRecord\", dataSize, recordRead.getSize());\r\n    assertTrue(\"Failed to generate a compressible GridmixRecord\", recordRead.getSize() > compressedFileSize);\r\n    float seenRatio = ((float) compressedFileSize) / dataSize;\r\n    assertEquals(CompressionEmulationUtil.standardizeCompressionRatio(ratio), CompressionEmulationUtil.standardizeCompressionRatio(seenRatio), 1.0D);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testIsCompressionEmulationEnabled",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testIsCompressionEmulationEnabled()\n{\r\n    Configuration conf = new Configuration();\r\n    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);\r\n    assertFalse(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testPossiblyCompressedDecompressedStreams",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPossiblyCompressedDecompressedStreams() throws IOException\n{\r\n    JobConf conf = new JobConf();\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    String inputLine = \"Hi Hello!\";\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);\r\n    conf.setBoolean(FileOutputFormat.COMPRESS, true);\r\n    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).makeQualified(lfs.getUri(), lfs.getWorkingDirectory());\r\n    Path tempDir = new Path(rootTempDir, \"TestPossiblyCompressedDecompressedStreams\");\r\n    lfs.delete(tempDir, true);\r\n    Path compressedFile = new Path(tempDir, \"test\");\r\n    OutputStream out = CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, conf);\r\n    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));\r\n    writer.write(inputLine);\r\n    writer.close();\r\n    compressedFile = compressedFile.suffix(\".gz\");\r\n    InputStream in = CompressionEmulationUtil.getPossiblyDecompressedInputStream(compressedFile, conf, 0);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(in));\r\n    String readLine = reader.readLine();\r\n    assertEquals(\"Compression/Decompression error\", inputLine, readLine);\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testExtractCompressionConfigs",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testExtractCompressionConfigs()\n{\r\n    JobConf source = new JobConf();\r\n    JobConf target = new JobConf();\r\n    source.setBoolean(FileOutputFormat.COMPRESS, false);\r\n    source.set(FileOutputFormat.COMPRESS_CODEC, \"MyDefaultCodec\");\r\n    source.set(FileOutputFormat.COMPRESS_TYPE, \"MyDefaultType\");\r\n    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);\r\n    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, \"MyDefaultCodec2\");\r\n    CompressionEmulationUtil.configureCompressionEmulation(source, target);\r\n    assertFalse(target.getBoolean(FileOutputFormat.COMPRESS, true));\r\n    assertEquals(\"MyDefaultCodec\", target.get(FileOutputFormat.COMPRESS_CODEC));\r\n    assertEquals(\"MyDefaultType\", target.get(FileOutputFormat.COMPRESS_TYPE));\r\n    assertFalse(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true));\r\n    assertEquals(\"MyDefaultCodec2\", target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));\r\n    assertFalse(CompressionEmulationUtil.isInputCompressionEmulationEnabled(target));\r\n    source.setBoolean(FileOutputFormat.COMPRESS, true);\r\n    source.set(FileOutputFormat.COMPRESS_CODEC, \"MyCodec\");\r\n    source.set(FileOutputFormat.COMPRESS_TYPE, \"MyType\");\r\n    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);\r\n    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, \"MyCodec2\");\r\n    org.apache.hadoop.mapred.FileInputFormat.setInputPaths(source, \"file.gz\");\r\n    target = new JobConf();\r\n    CompressionEmulationUtil.configureCompressionEmulation(source, target);\r\n    assertTrue(target.getBoolean(FileOutputFormat.COMPRESS, false));\r\n    assertEquals(\"MyCodec\", target.get(FileOutputFormat.COMPRESS_CODEC));\r\n    assertEquals(\"MyType\", target.get(FileOutputFormat.COMPRESS_TYPE));\r\n    assertTrue(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false));\r\n    assertEquals(\"MyCodec2\", target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));\r\n    assertTrue(CompressionEmulationUtil.isInputCompressionEmulationEnabled(target));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testFileQueueDecompression",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testFileQueueDecompression() throws IOException\n{\r\n    JobConf conf = new JobConf();\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    String inputLine = \"Hi Hello!\";\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);\r\n    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);\r\n    org.apache.hadoop.mapred.FileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).makeQualified(lfs.getUri(), lfs.getWorkingDirectory());\r\n    Path tempDir = new Path(rootTempDir, \"TestFileQueueDecompression\");\r\n    lfs.delete(tempDir, true);\r\n    Path compressedFile = new Path(tempDir, \"test\");\r\n    OutputStream out = CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, conf);\r\n    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));\r\n    writer.write(inputLine);\r\n    writer.close();\r\n    compressedFile = compressedFile.suffix(\".gz\");\r\n    long fileSize = lfs.listStatus(compressedFile)[0].getLen();\r\n    CombineFileSplit split = new CombineFileSplit(new Path[] { compressedFile }, new long[] { fileSize });\r\n    FileQueue queue = new FileQueue(split, conf);\r\n    byte[] bytes = new byte[inputLine.getBytes().length];\r\n    queue.read(bytes);\r\n    queue.close();\r\n    String readLine = new String(bytes);\r\n    assertEquals(\"Compression/Decompression error\", inputLine, readLine);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testComputeUncompressedInputBytes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testComputeUncompressedInputBytes()\n{\r\n    long possiblyCompressedInputBytes = 100000;\r\n    float compressionRatio = 0.45F;\r\n    Configuration conf = new Configuration();\r\n    CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, compressionRatio);\r\n    long result = CompressionEmulationUtil.getUncompressedInputBytes(possiblyCompressedInputBytes, conf);\r\n    assertEquals(possiblyCompressedInputBytes, result);\r\n    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);\r\n    result = CompressionEmulationUtil.getUncompressedInputBytes(possiblyCompressedInputBytes, conf);\r\n    assertEquals((long) (possiblyCompressedInputBytes / compressionRatio), result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testDataStatistics",
  "errType" : [ "RuntimeException", "RuntimeException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testDataStatistics() throws Exception\n{\r\n    DataStatistics stats = new DataStatistics(10, 2, true);\r\n    assertEquals(\"Data size mismatch\", 10, stats.getDataSize());\r\n    assertEquals(\"Num files mismatch\", 2, stats.getNumFiles());\r\n    assertTrue(\"Compression configuration mismatch\", stats.isDataCompressed());\r\n    stats = new DataStatistics(100, 5, false);\r\n    assertEquals(\"Data size mismatch\", 100, stats.getDataSize());\r\n    assertEquals(\"Num files mismatch\", 5, stats.getNumFiles());\r\n    assertFalse(\"Compression configuration mismatch\", stats.isDataCompressed());\r\n    Configuration conf = new Configuration();\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\"));\r\n    Path testDir = new Path(rootTempDir, \"testDataStatistics\");\r\n    FileSystem fs = testDir.getFileSystem(conf);\r\n    fs.delete(testDir, true);\r\n    Path testInputDir = new Path(testDir, \"test\");\r\n    fs.mkdirs(testInputDir);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    Boolean failed = null;\r\n    try {\r\n        GenerateData.publishDataStatistics(testInputDir, 1024L, conf);\r\n        failed = false;\r\n    } catch (RuntimeException e) {\r\n        failed = true;\r\n    }\r\n    assertNotNull(\"Expected failure!\", failed);\r\n    assertTrue(\"Compression data publishing error\", failed);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);\r\n    stats = GenerateData.publishDataStatistics(testInputDir, 1024L, conf);\r\n    assertEquals(\"Data size mismatch\", 0, stats.getDataSize());\r\n    assertEquals(\"Num files mismatch\", 0, stats.getNumFiles());\r\n    assertFalse(\"Compression configuration mismatch\", stats.isDataCompressed());\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);\r\n    Path inputDataFile = new Path(testInputDir, \"test\");\r\n    long size = UtilsForTests.createTmpFileDFS(fs, inputDataFile, FsPermission.createImmutable((short) 777), \"hi hello bye\").size();\r\n    stats = GenerateData.publishDataStatistics(testInputDir, -1, conf);\r\n    assertEquals(\"Data size mismatch\", size, stats.getDataSize());\r\n    assertEquals(\"Num files mismatch\", 1, stats.getNumFiles());\r\n    assertFalse(\"Compression configuration mismatch\", stats.isDataCompressed());\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    failed = null;\r\n    try {\r\n        GenerateData.publishDataStatistics(testInputDir, 1234L, conf);\r\n        failed = false;\r\n    } catch (RuntimeException e) {\r\n        failed = true;\r\n    }\r\n    assertNotNull(\"Expected failure!\", failed);\r\n    assertTrue(\"Compression data publishing error\", failed);\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);\r\n    fs.delete(inputDataFile, false);\r\n    inputDataFile = new Path(testInputDir, \"test.gz\");\r\n    size = UtilsForTests.createTmpFileDFS(fs, inputDataFile, FsPermission.createImmutable((short) 777), \"hi hello\").size();\r\n    stats = GenerateData.publishDataStatistics(testInputDir, 1234L, conf);\r\n    assertEquals(\"Data size mismatch\", size, stats.getDataSize());\r\n    assertEquals(\"Num files mismatch\", 1, stats.getNumFiles());\r\n    assertFalse(\"Compression configuration mismatch\", stats.isDataCompressed());\r\n    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);\r\n    stats = GenerateData.publishDataStatistics(testInputDir, 1234L, conf);\r\n    assertEquals(\"Data size mismatch\", size, stats.getDataSize());\r\n    assertEquals(\"Num files mismatch\", 1, stats.getNumFiles());\r\n    assertTrue(\"Compression configuration mismatch\", stats.isDataCompressed());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testExecutionSummarizer",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 57,
  "sourceCodeText" : "void testExecutionSummarizer() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    ExecutionSummarizer es = new ExecutionSummarizer();\r\n    assertEquals(\"ExecutionSummarizer init failed\", Summarizer.NA, es.getCommandLineArgsString());\r\n    long startTime = System.currentTimeMillis();\r\n    String[] initArgs = new String[] { \"-Xmx20m\", \"-Dtest.args='test'\" };\r\n    es = new ExecutionSummarizer(initArgs);\r\n    assertEquals(\"ExecutionSummarizer init failed\", \"-Xmx20m -Dtest.args='test'\", es.getCommandLineArgsString());\r\n    assertTrue(\"Start time mismatch\", es.getStartTime() >= startTime);\r\n    assertTrue(\"Start time mismatch\", es.getStartTime() <= System.currentTimeMillis());\r\n    es.update(null);\r\n    assertEquals(\"ExecutionSummarizer init failed\", 0, es.getSimulationStartTime());\r\n    testExecutionSummarizer(0, 0, 0, 0, 0, 0, 0, es);\r\n    long simStartTime = System.currentTimeMillis();\r\n    es.start(null);\r\n    assertTrue(\"Simulation start time mismatch\", es.getSimulationStartTime() >= simStartTime);\r\n    assertTrue(\"Simulation start time mismatch\", es.getSimulationStartTime() <= System.currentTimeMillis());\r\n    JobStats stats = generateFakeJobStats(1, 10, true, false);\r\n    es.update(stats);\r\n    testExecutionSummarizer(1, 10, 0, 1, 1, 0, 0, es);\r\n    stats = generateFakeJobStats(5, 1, false, false);\r\n    es.update(stats);\r\n    testExecutionSummarizer(6, 11, 0, 2, 1, 1, 0, es);\r\n    stats = generateFakeJobStats(1, 1, true, true);\r\n    es.update(stats);\r\n    testExecutionSummarizer(7, 12, 0, 3, 1, 1, 1, es);\r\n    stats = generateFakeJobStats(2, 2, false, true);\r\n    es.update(stats);\r\n    testExecutionSummarizer(9, 14, 0, 4, 1, 1, 2, es);\r\n    JobFactory factory = new FakeJobFactory(conf);\r\n    factory.numJobsInTrace = 3;\r\n    Path rootTempDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\"));\r\n    Path testDir = new Path(rootTempDir, \"testGridmixSummary\");\r\n    Path testTraceFile = new Path(testDir, \"test-trace.json\");\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    fs.create(testTraceFile).close();\r\n    UserResolver resolver = new RoundRobinUserResolver();\r\n    DataStatistics dataStats = new DataStatistics(100, 2, true);\r\n    String policy = GridmixJobSubmissionPolicy.REPLAY.name();\r\n    conf.set(GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, policy);\r\n    es.finalize(factory, testTraceFile.toString(), 1024L, resolver, dataStats, conf);\r\n    assertEquals(\"Mismtach in num jobs in trace\", 3, es.getNumJobsInTrace());\r\n    String tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());\r\n    assertEquals(\"Mismatch in trace signature\", tid, es.getInputTraceSignature());\r\n    Path qPath = fs.makeQualified(testTraceFile);\r\n    assertEquals(\"Mismatch in trace filename\", qPath.toString(), es.getInputTraceLocation());\r\n    assertEquals(\"Mismatch in expected data size\", \"1 K\", es.getExpectedDataSize());\r\n    assertEquals(\"Mismatch in input data statistics\", ExecutionSummarizer.stringifyDataStatistics(dataStats), es.getInputDataStatistics());\r\n    assertEquals(\"Mismatch in user resolver\", resolver.getClass().getName(), es.getUserResolver());\r\n    assertEquals(\"Mismatch in policy\", policy, es.getJobSubmissionPolicy());\r\n    es.finalize(factory, testTraceFile.toString(), 1024 * 1024 * 1024 * 10L, resolver, dataStats, conf);\r\n    assertEquals(\"Mismatch in expected data size\", \"10 G\", es.getExpectedDataSize());\r\n    fs.delete(testTraceFile, false);\r\n    try {\r\n        Thread.sleep(1000);\r\n    } catch (InterruptedException ie) {\r\n    }\r\n    fs.create(testTraceFile).close();\r\n    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, conf);\r\n    assertEquals(\"Mismatch in trace data size\", Summarizer.NA, es.getExpectedDataSize());\r\n    assertFalse(\"Mismatch in trace signature\", tid.equals(es.getInputTraceSignature()));\r\n    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());\r\n    assertEquals(\"Mismatch in trace signature\", tid, es.getInputTraceSignature());\r\n    testTraceFile = new Path(testDir, \"test-trace2.json\");\r\n    fs.create(testTraceFile).close();\r\n    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, conf);\r\n    assertFalse(\"Mismatch in trace signature\", tid.equals(es.getInputTraceSignature()));\r\n    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());\r\n    assertEquals(\"Mismatch in trace signature\", tid, es.getInputTraceSignature());\r\n    es.finalize(factory, \"-\", 0L, resolver, dataStats, conf);\r\n    assertEquals(\"Mismatch in trace signature\", Summarizer.NA, es.getInputTraceSignature());\r\n    assertEquals(\"Mismatch in trace file location\", Summarizer.NA, es.getInputTraceLocation());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testExecutionSummarizer",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testExecutionSummarizer(int numMaps, int numReds, int totalJobsInTrace, int totalJobSubmitted, int numSuccessfulJob, int numFailedJobs, int numLostJobs, ExecutionSummarizer es)\n{\r\n    assertEquals(\"ExecutionSummarizer test failed [num-maps]\", numMaps, es.getNumMapTasksLaunched());\r\n    assertEquals(\"ExecutionSummarizer test failed [num-reducers]\", numReds, es.getNumReduceTasksLaunched());\r\n    assertEquals(\"ExecutionSummarizer test failed [num-jobs-in-trace]\", totalJobsInTrace, es.getNumJobsInTrace());\r\n    assertEquals(\"ExecutionSummarizer test failed [num-submitted jobs]\", totalJobSubmitted, es.getNumSubmittedJobs());\r\n    assertEquals(\"ExecutionSummarizer test failed [num-successful-jobs]\", numSuccessfulJob, es.getNumSuccessfulJobs());\r\n    assertEquals(\"ExecutionSummarizer test failed [num-failed jobs]\", numFailedJobs, es.getNumFailedJobs());\r\n    assertEquals(\"ExecutionSummarizer test failed [num-lost jobs]\", numLostJobs, es.getNumLostJobs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "generateFakeJobStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobStats generateFakeJobStats(final int numMaps, final int numReds, final boolean isSuccessful, final boolean lost) throws IOException\n{\r\n    Job fakeJob = new Job() {\r\n\r\n        @Override\r\n        public int getNumReduceTasks() {\r\n            return numReds;\r\n        }\r\n\r\n        @Override\r\n        public boolean isSuccessful() throws IOException {\r\n            if (lost) {\r\n                throw new IOException(\"Test failure!\");\r\n            }\r\n            return isSuccessful;\r\n        }\r\n    };\r\n    return new JobStats(numMaps, numReds, fakeJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testClusterSummarizer",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testClusterSummarizer() throws IOException\n{\r\n    ClusterSummarizer cs = new ClusterSummarizer();\r\n    Configuration conf = new Configuration();\r\n    String jt = \"test-jt:1234\";\r\n    String nn = \"test-nn:5678\";\r\n    conf.set(JTConfig.JT_IPC_ADDRESS, jt);\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, nn);\r\n    cs.start(conf);\r\n    assertEquals(\"JT name mismatch\", jt, cs.getJobTrackerInfo());\r\n    assertEquals(\"NN name mismatch\", nn, cs.getNamenodeInfo());\r\n    ClusterStats cStats = ClusterStats.getClusterStats();\r\n    conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, \"local\");\r\n    JobClient jc = new JobClient(conf);\r\n    cStats.setClusterMetric(jc.getClusterStatus());\r\n    cs.update(cStats);\r\n    assertEquals(\"Cluster summary test failed!\", 1, cs.getMaxMapTasks());\r\n    assertEquals(\"Cluster summary test failed!\", 1, cs.getMaxReduceTasks());\r\n    assertEquals(\"Cluster summary test failed!\", 1, cs.getNumActiveTrackers());\r\n    assertEquals(\"Cluster summary test failed!\", 0, cs.getNumBlacklistedTrackers());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getVirtualMemorySize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getVirtualMemorySize()\n{\r\n    return getConf().getLong(MAXVMEM_TESTING_PROPERTY, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getPhysicalMemorySize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPhysicalMemorySize()\n{\r\n    return getConf().getLong(MAXPMEM_TESTING_PROPERTY, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getAvailableVirtualMemorySize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getAvailableVirtualMemorySize()\n{\r\n    return getConf().getLong(MAXVMEM_TESTING_PROPERTY, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getAvailablePhysicalMemorySize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getAvailablePhysicalMemorySize()\n{\r\n    return getConf().getLong(MAXPMEM_TESTING_PROPERTY, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getNumProcessors",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumProcessors()\n{\r\n    return getConf().getInt(NUM_PROCESSORS, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getNumCores",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumCores()\n{\r\n    return getNumProcessors();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getCpuFrequency",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCpuFrequency()\n{\r\n    return getConf().getLong(CPU_FREQUENCY, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getCumulativeCpuTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCumulativeCpuTime()\n{\r\n    return getConf().getLong(CUMULATIVE_CPU_TIME, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getCpuUsagePercentage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getCpuUsagePercentage()\n{\r\n    return getConf().getFloat(CPU_USAGE, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getNetworkBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNetworkBytesRead()\n{\r\n    return getConf().getLong(NETWORK_BYTES_READ, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getNetworkBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNetworkBytesWritten()\n{\r\n    return getConf().getLong(NETWORK_BYTES_WRITTEN, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getStorageBytesRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getStorageBytesRead()\n{\r\n    return getConf().getLong(STORAGE_BYTES_READ, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getStorageBytesWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getStorageBytesWritten()\n{\r\n    return getConf().getLong(STORAGE_BYTES_WRITTEN, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomTextDataGenerator",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRandomTextDataGenerator()\n{\r\n    RandomTextDataGenerator rtdg = new RandomTextDataGenerator(10, 0L, 5);\r\n    List<String> words = rtdg.getRandomWords();\r\n    assertEquals(\"List size mismatch\", 10, words.size());\r\n    Set<String> wordsSet = new HashSet<String>(words);\r\n    assertEquals(\"List size mismatch due to duplicates\", 10, wordsSet.size());\r\n    for (String word : wordsSet) {\r\n        assertEquals(\"Word size mismatch\", 5, word.length());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomTextDataGeneratorRepeatability",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRandomTextDataGeneratorRepeatability()\n{\r\n    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 0L, 5);\r\n    List<String> words1 = rtdg1.getRandomWords();\r\n    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);\r\n    List<String> words2 = rtdg2.getRandomWords();\r\n    assertTrue(\"List mismatch\", words1.equals(words2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testRandomTextDataGeneratorUniqueness",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRandomTextDataGeneratorUniqueness()\n{\r\n    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 1L, 5);\r\n    Set<String> words1 = new HashSet(rtdg1.getRandomWords());\r\n    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);\r\n    Set<String> words2 = new HashSet(rtdg2.getRandomWords());\r\n    assertFalse(\"List size mismatch across lists\", words1.equals(words2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getNextJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobStory getNextJob() throws IOException\n{\r\n    if (numJobs.getAndDecrement() > 0) {\r\n        final MockJob ret = new MockJob(conf);\r\n        submitted.add(ret);\r\n        return ret;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "getDistr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "double[] getDistr(Random r, double mindist, int size)\n{\r\n    assert 0.0 <= mindist && mindist <= 1.0;\r\n    final double min = mindist / size;\r\n    final double rem = 1.0 - min * size;\r\n    final double[] tmp = new double[size];\r\n    for (int i = 0; i < tmp.length - 1; ++i) {\r\n        tmp[i] = r.nextDouble() * rem;\r\n    }\r\n    tmp[tmp.length - 1] = rem;\r\n    Arrays.sort(tmp);\r\n    final double[] ret = new double[size];\r\n    ret[0] = tmp[0] + min;\r\n    for (int i = 1; i < size; ++i) {\r\n        ret[i] = tmp[i] - tmp[i - 1] + min;\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testHighRamConfig",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testHighRamConfig(long jobMapMB, long jobReduceMB, long clusterMapMB, long clusterReduceMB, long simulatedClusterMapMB, long simulatedClusterReduceMB, long expectedMapMB, long expectedReduceMB, Configuration gConf) throws IOException\n{\r\n    Configuration simulatedJobConf = new Configuration(gConf);\r\n    simulatedJobConf.setLong(MRConfig.MAPMEMORY_MB, simulatedClusterMapMB);\r\n    simulatedJobConf.setLong(MRConfig.REDUCEMEMORY_MB, simulatedClusterReduceMB);\r\n    Configuration sourceConf = new Configuration();\r\n    sourceConf.setLong(MRJobConfig.MAP_MEMORY_MB, jobMapMB);\r\n    sourceConf.setLong(MRConfig.MAPMEMORY_MB, clusterMapMB);\r\n    sourceConf.setLong(MRJobConfig.REDUCE_MEMORY_MB, jobReduceMB);\r\n    sourceConf.setLong(MRConfig.REDUCEMEMORY_MB, clusterReduceMB);\r\n    MockJob story = new MockJob(sourceConf);\r\n    GridmixJob job = new DummyGridmixJob(simulatedJobConf, story);\r\n    Job simulatedJob = job.getJob();\r\n    JobConf simulatedConf = (JobConf) simulatedJob.getConfiguration();\r\n    assertEquals(expectedMapMB, simulatedConf.getMemoryRequired(TaskType.MAP));\r\n    assertEquals(expectedReduceMB, simulatedConf.getMemoryRequired(TaskType.REDUCE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testHighRamFeatureEmulation",
  "errType" : [ "Exception", "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testHighRamFeatureEmulation() throws IOException\n{\r\n    Configuration gridmixConf = new Configuration();\r\n    gridmixConf.setBoolean(GridmixJob.GRIDMIX_HIGHRAM_EMULATION_ENABLE, false);\r\n    testHighRamConfig(10, 20, 5, 10, MRJobConfig.DEFAULT_MAP_MEMORY_MB, MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, MRJobConfig.DEFAULT_MAP_MEMORY_MB, MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, gridmixConf);\r\n    gridmixConf = new Configuration();\r\n    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 20 * 1024 * 1024);\r\n    testHighRamConfig(10, 20, 5, 10, 5, 10, 10, 20, gridmixConf);\r\n    gridmixConf = new Configuration();\r\n    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 100);\r\n    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 300);\r\n    testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);\r\n    gridmixConf = new Configuration();\r\n    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 70 * 1024 * 1024);\r\n    Boolean failed = null;\r\n    try {\r\n        testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);\r\n        failed = false;\r\n    } catch (Exception e) {\r\n        failed = true;\r\n    }\r\n    assertNotNull(failed);\r\n    assertTrue(\"Exception expected for exceeding map memory limit \" + \"(deprecation)!\", failed);\r\n    gridmixConf = new Configuration();\r\n    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 150 * 1024 * 1024);\r\n    failed = null;\r\n    try {\r\n        testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);\r\n        failed = false;\r\n    } catch (Exception e) {\r\n        failed = true;\r\n    }\r\n    assertNotNull(failed);\r\n    assertTrue(\"Exception expected for exceeding reduce memory limit \" + \"(deprecation)!\", failed);\r\n    gridmixConf = new Configuration();\r\n    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 70);\r\n    failed = null;\r\n    try {\r\n        testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);\r\n        failed = false;\r\n    } catch (Exception e) {\r\n        failed = true;\r\n    }\r\n    assertNotNull(failed);\r\n    assertTrue(\"Exception expected for exceeding map memory limit!\", failed);\r\n    gridmixConf = new Configuration();\r\n    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 200);\r\n    failed = null;\r\n    try {\r\n        testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);\r\n        failed = false;\r\n    } catch (Exception e) {\r\n        failed = true;\r\n    }\r\n    assertNotNull(failed);\r\n    assertTrue(\"Exception expected for exceeding reduce memory limit!\", failed);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init() throws IOException\n{\r\n    GridmixTestUtils.initCluster(TestLoadJob.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "shutDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutDown() throws IOException\n{\r\n    GridmixTestUtils.shutdownCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testSerialSubmit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSerialSubmit() throws Exception\n{\r\n    policy = GridmixJobSubmissionPolicy.SERIAL;\r\n    LOG.info(\"Serial started at \" + System.currentTimeMillis());\r\n    doSubmission(JobCreator.LOADJOB.name(), false);\r\n    LOG.info(\"Serial ended at \" + System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-gridmix\\src\\test\\java\\org\\apache\\hadoop\\mapred\\gridmix",
  "methodName" : "testReplaySubmit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testReplaySubmit() throws Exception\n{\r\n    policy = GridmixJobSubmissionPolicy.REPLAY;\r\n    LOG.info(\" Replay started at \" + System.currentTimeMillis());\r\n    doSubmission(JobCreator.LOADJOB.name(), false);\r\n    LOG.info(\" Replay ended at \" + System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
} ]
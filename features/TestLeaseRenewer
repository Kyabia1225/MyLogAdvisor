[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "setupMocksAndRenewer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupMocksAndRenewer() throws IOException\n{\r\n    MOCK_DFSCLIENT = createMockClient();\r\n    renewer = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);\r\n    renewer.setGraceSleepPeriod(FAST_GRACE_PERIOD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "createMockClient",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DFSClient createMockClient()\n{\r\n    final DfsClientConf mockConf = Mockito.mock(DfsClientConf.class);\r\n    Mockito.doReturn((int) FAST_GRACE_PERIOD).when(mockConf).getHdfsTimeout();\r\n    DFSClient mock = Mockito.mock(DFSClient.class);\r\n    Mockito.doReturn(true).when(mock).isClientRunning();\r\n    Mockito.doReturn(mockConf).when(mock).getConf();\r\n    Mockito.doReturn(\"myclient\").when(mock).getClientName();\r\n    return mock;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "testInstanceSharing",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testInstanceSharing() throws IOException\n{\r\n    LeaseRenewer lr = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);\r\n    LeaseRenewer lr2 = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);\r\n    Assert.assertSame(lr, lr2);\r\n    LeaseRenewer lr3 = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_B, MOCK_DFSCLIENT);\r\n    Assert.assertNotSame(lr, lr3);\r\n    LeaseRenewer lr4 = LeaseRenewer.getInstance(\"someOtherAuthority\", FAKE_UGI_B, MOCK_DFSCLIENT);\r\n    Assert.assertNotSame(lr, lr4);\r\n    Assert.assertNotSame(lr3, lr4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "testRenewal",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRenewal() throws Exception\n{\r\n    final AtomicInteger leaseRenewalCount = new AtomicInteger();\r\n    Mockito.doAnswer(new Answer<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean answer(InvocationOnMock invocation) throws Throwable {\r\n            leaseRenewalCount.incrementAndGet();\r\n            return true;\r\n        }\r\n    }).when(MOCK_DFSCLIENT).renewLease();\r\n    DFSOutputStream mockStream = Mockito.mock(DFSOutputStream.class);\r\n    long fileId = 123L;\r\n    renewer.put(MOCK_DFSCLIENT);\r\n    long failTime = Time.monotonicNow() + 5000;\r\n    while (Time.monotonicNow() < failTime && leaseRenewalCount.get() == 0) {\r\n        Thread.sleep(50);\r\n    }\r\n    if (leaseRenewalCount.get() == 0) {\r\n        Assert.fail(\"Did not renew lease at all!\");\r\n    }\r\n    renewer.closeClient(MOCK_DFSCLIENT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "testManyDfsClientsWhereSomeNotOpen",
  "errType" : [ "AssertionError", "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testManyDfsClientsWhereSomeNotOpen() throws Exception\n{\r\n    final DFSClient mockClient1 = createMockClient();\r\n    Mockito.doReturn(false).when(mockClient1).renewLease();\r\n    assertSame(renewer, LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, mockClient1));\r\n    long fileId = 456L;\r\n    renewer.put(mockClient1);\r\n    final DFSClient mockClient2 = createMockClient();\r\n    Mockito.doReturn(true).when(mockClient2).renewLease();\r\n    assertSame(renewer, LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, mockClient2));\r\n    renewer.put(mockClient2);\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            try {\r\n                Mockito.verify(mockClient1, Mockito.atLeastOnce()).renewLease();\r\n                Mockito.verify(mockClient2, Mockito.atLeastOnce()).renewLease();\r\n                return true;\r\n            } catch (AssertionError err) {\r\n                LeaseRenewer.LOG.warn(\"Not yet satisfied\", err);\r\n                return false;\r\n            } catch (IOException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        }\r\n    }, 100, 10000);\r\n    renewer.closeClient(mockClient1);\r\n    renewer.closeClient(mockClient2);\r\n    renewer.closeClient(MOCK_DFSCLIENT);\r\n    Thread.sleep(FAST_GRACE_PERIOD * 2);\r\n    Assert.assertTrue(!renewer.isRunning());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "testThreadName",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testThreadName() throws Exception\n{\r\n    Assert.assertFalse(\"Renewer not initially running\", renewer.isRunning());\r\n    renewer.put(MOCK_DFSCLIENT);\r\n    Assert.assertTrue(\"Renewer should have started running\", renewer.isRunning());\r\n    String threadName = renewer.getDaemonName();\r\n    Assert.assertEquals(\"LeaseRenewer:myuser@hdfs://nn1/\", threadName);\r\n    renewer.closeClient(MOCK_DFSCLIENT);\r\n    renewer.setEmptyTime(Time.monotonicNow());\r\n    long failTime = Time.monotonicNow() + 5000;\r\n    while (renewer.isRunning() && Time.monotonicNow() < failTime) {\r\n        Thread.sleep(50);\r\n    }\r\n    Assert.assertFalse(renewer.isRunning());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "testDaemonThreadLeak",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testDaemonThreadLeak() throws Exception\n{\r\n    Assert.assertFalse(\"Renewer not initially running\", renewer.isRunning());\r\n    renewer.put(MOCK_DFSCLIENT);\r\n    Assert.assertTrue(\"Renewer should have started running\", renewer.isRunning());\r\n    Pattern daemonThreadNamePattern = Pattern.compile(\"LeaseRenewer:\\\\S+\");\r\n    Assert.assertEquals(1, countThreadMatching(daemonThreadNamePattern));\r\n    LeaseRenewer lastRenewer = renewer;\r\n    renewer = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);\r\n    Assert.assertEquals(lastRenewer, renewer);\r\n    renewer.closeClient(MOCK_DFSCLIENT);\r\n    Assert.assertEquals(1, countThreadMatching(daemonThreadNamePattern));\r\n    renewer.setEmptyTime(0);\r\n    renewer = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);\r\n    renewer.setGraceSleepPeriod(FAST_GRACE_PERIOD);\r\n    boolean success = renewer.put(MOCK_DFSCLIENT);\r\n    if (!success) {\r\n        LeaseRenewer.remove(renewer);\r\n        renewer = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);\r\n        renewer.setGraceSleepPeriod(FAST_GRACE_PERIOD);\r\n        renewer.put(MOCK_DFSCLIENT);\r\n    }\r\n    int threadCount = countThreadMatching(daemonThreadNamePattern);\r\n    Assert.assertTrue(1 == threadCount || 2 == threadCount);\r\n    Thread.sleep(FAST_GRACE_PERIOD * 2);\r\n    lastRenewer = renewer;\r\n    renewer = LeaseRenewer.getInstance(FAKE_AUTHORITY, FAKE_UGI_A, MOCK_DFSCLIENT);\r\n    Assert.assertEquals(lastRenewer, renewer);\r\n    renewer.setGraceSleepPeriod(FAST_GRACE_PERIOD);\r\n    renewer.closeClient(MOCK_DFSCLIENT);\r\n    renewer.setEmptyTime(0);\r\n    Thread.sleep(FAST_GRACE_PERIOD * 2);\r\n    Assert.assertEquals(\"LeaseRenewer#daemon thread leaks\", 0, countThreadMatching(daemonThreadNamePattern));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\client\\impl",
  "methodName" : "countThreadMatching",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int countThreadMatching(Pattern pattern)\n{\r\n    ThreadMXBean threadBean = ManagementFactory.getThreadMXBean();\r\n    ThreadInfo[] infos = threadBean.getThreadInfo(threadBean.getAllThreadIds(), 1);\r\n    int count = 0;\r\n    for (ThreadInfo info : infos) {\r\n        if (info == null) {\r\n            continue;\r\n        }\r\n        if (pattern.matcher(info.getThreadName()).matches()) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testEquals",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testEquals()\n{\r\n    assertEquals(new ExtendedBlock(POOL_A, BLOCK_1_GS1), new ExtendedBlock(POOL_A, BLOCK_1_GS1));\r\n    assertNotEquals(new ExtendedBlock(POOL_A, BLOCK_1_GS1), new ExtendedBlock(POOL_B, BLOCK_1_GS1));\r\n    assertNotEquals(new ExtendedBlock(POOL_A, BLOCK_1_GS1), new ExtendedBlock(POOL_A, BLOCK_2_GS1));\r\n    assertEquals(new ExtendedBlock(POOL_A, BLOCK_1_GS1), new ExtendedBlock(POOL_A, BLOCK_1_GS2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testHashcode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testHashcode()\n{\r\n    assertNotEquals(new ExtendedBlock(POOL_A, BLOCK_1_GS1).hashCode(), new ExtendedBlock(POOL_B, BLOCK_1_GS1).hashCode());\r\n    assertNotEquals(new ExtendedBlock(POOL_A, BLOCK_1_GS1).hashCode(), new ExtendedBlock(POOL_A, BLOCK_2_GS1).hashCode());\r\n    assertEquals(new ExtendedBlock(POOL_A, BLOCK_1_GS1).hashCode(), new ExtendedBlock(POOL_A, BLOCK_1_GS1).hashCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "assertNotEquals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertNotEquals(Object a, Object b)\n{\r\n    assertFalse(\"expected not equal: '\" + a + \"' and '\" + b + \"'\", a.equals(b));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testGetBlockType",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testGetBlockType() throws Exception\n{\r\n    assertEquals(BlockType.fromBlockId(0x0000000000000000L), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x1000000000000000L), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x2000000000000000L), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x4000000000000000L), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x7000000000000000L), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x00000000ffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x10000000ffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x20000000ffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x40000000ffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x70000000ffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x70000000ffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x0fffffffffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x1fffffffffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x2fffffffffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x4fffffffffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x7fffffffffffffffL), CONTIGUOUS);\r\n    assertEquals(BlockType.fromBlockId(0x8000000000000000L), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0x9000000000000000L), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0xa000000000000000L), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0xf000000000000000L), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0x80000000ffffffffL), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0x90000000ffffffffL), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0xa0000000ffffffffL), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0xf0000000ffffffffL), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0x8fffffffffffffffL), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0x9fffffffffffffffL), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0xafffffffffffffffL), STRIPED);\r\n    assertEquals(BlockType.fromBlockId(0xffffffffffffffffL), STRIPED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "buildConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration buildConf(String refreshToken, String tokenExpires, String clientId, String refreshURL)\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(OAUTH_REFRESH_TOKEN_KEY, refreshToken);\r\n    conf.set(OAUTH_REFRESH_TOKEN_EXPIRES_KEY, tokenExpires);\r\n    conf.set(OAUTH_CLIENT_ID_KEY, clientId);\r\n    conf.set(OAUTH_REFRESH_URL_KEY, refreshURL);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "refreshUrlIsCorrect",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void refreshUrlIsCorrect() throws IOException\n{\r\n    final int PORT = 7552;\r\n    final String REFRESH_ADDRESS = \"http://localhost:\" + PORT + \"/refresh\";\r\n    long tokenExpires = 0;\r\n    Configuration conf = buildConf(\"refresh token key\", Long.toString(tokenExpires), \"joebob\", REFRESH_ADDRESS);\r\n    Timer mockTimer = mock(Timer.class);\r\n    when(mockTimer.now()).thenReturn(tokenExpires + 1000l);\r\n    AccessTokenProvider tokenProvider = new ConfRefreshTokenBasedAccessTokenProvider(mockTimer);\r\n    tokenProvider.setConf(conf);\r\n    ClientAndServer mockServer = startClientAndServer(PORT);\r\n    HttpRequest expectedRequest = request().withMethod(\"POST\").withPath(\"/refresh\").withBody(ParameterBody.params(Parameter.param(CLIENT_ID, \"joebob\"), Parameter.param(GRANT_TYPE, REFRESH_TOKEN), Parameter.param(REFRESH_TOKEN, \"refresh token key\")));\r\n    MockServerClient mockServerClient = new MockServerClient(\"localhost\", PORT);\r\n    Map<String, Object> map = new TreeMap<>();\r\n    map.put(EXPIRES_IN, \"0987654321\");\r\n    map.put(TOKEN_TYPE, BEARER);\r\n    map.put(ACCESS_TOKEN, \"new access token\");\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    HttpResponse resp = response().withStatusCode(HttpStatus.SC_OK).withHeaders(CONTENT_TYPE_APPLICATION_JSON).withBody(mapper.writeValueAsString(map));\r\n    mockServerClient.when(expectedRequest, exactly(1)).respond(resp);\r\n    assertEquals(\"new access token\", tokenProvider.getAccessToken());\r\n    mockServerClient.verify(expectedRequest);\r\n    mockServerClient.clear(expectedRequest);\r\n    mockServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testCounter",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCounter() throws Exception\n{\r\n    final long countResetTimePeriodMs = 200L;\r\n    final Counter c = new Counter(countResetTimePeriodMs);\r\n    final int n = ThreadLocalRandom.current().nextInt(512) + 512;\r\n    final List<Future<Integer>> futures = new ArrayList<Future<Integer>>(n);\r\n    final ExecutorService pool = Executors.newFixedThreadPool(32);\r\n    try {\r\n        for (int i = 0; i < n; i++) {\r\n            futures.add(pool.submit(new Callable<Integer>() {\r\n\r\n                @Override\r\n                public Integer call() throws Exception {\r\n                    return (int) c.increment();\r\n                }\r\n            }));\r\n        }\r\n        Collections.sort(futures, CMP);\r\n    } finally {\r\n        pool.shutdown();\r\n    }\r\n    Assert.assertEquals(n, futures.size());\r\n    for (int i = 0; i < n; i++) {\r\n        Assert.assertEquals(i + 1, futures.get(i).get().intValue());\r\n    }\r\n    Assert.assertEquals(n, c.getCount());\r\n    Thread.sleep(countResetTimePeriodMs + 100);\r\n    Assert.assertEquals(1, c.increment());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testAllocateRecycle",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testAllocateRecycle() throws Exception\n{\r\n    final int countThreshold = 4;\r\n    final int countLimit = 8;\r\n    final long countResetTimePeriodMs = 200L;\r\n    final ByteArrayManager.Impl bam = new ByteArrayManager.Impl(new ByteArrayManager.Conf(countThreshold, countLimit, countResetTimePeriodMs));\r\n    final CounterMap counters = bam.getCounters();\r\n    final ManagerMap managers = bam.getManagers();\r\n    final int[] uncommonArrays = { 0, 1, 2, 4, 8, 16, 32, 64 };\r\n    final int arrayLength = 1024;\r\n    final Allocator allocator = new Allocator(bam);\r\n    final Recycler recycler = new Recycler(bam);\r\n    try {\r\n        {\r\n            for (int i = 0; i < countThreshold; i++) {\r\n                allocator.submit(arrayLength);\r\n            }\r\n            waitForAll(allocator.futures);\r\n            Assert.assertEquals(countThreshold, counters.get(arrayLength, false).getCount());\r\n            Assert.assertNull(managers.get(arrayLength, false));\r\n            for (int n : uncommonArrays) {\r\n                Assert.assertNull(counters.get(n, false));\r\n                Assert.assertNull(managers.get(n, false));\r\n            }\r\n        }\r\n        {\r\n            for (int i = 0; i < countThreshold / 2; i++) {\r\n                recycler.submit(removeLast(allocator.futures).get());\r\n            }\r\n            for (Future<Integer> f : recycler.furtures) {\r\n                Assert.assertEquals(-1, f.get().intValue());\r\n            }\r\n            recycler.furtures.clear();\r\n        }\r\n        {\r\n            allocator.submit(arrayLength).get();\r\n            Assert.assertEquals(countThreshold + 1, counters.get(arrayLength, false).getCount());\r\n            Assert.assertNotNull(managers.get(arrayLength, false));\r\n        }\r\n        {\r\n            final int n = allocator.recycleAll(recycler);\r\n            recycler.verify(n);\r\n        }\r\n        {\r\n            for (int i = 0; i < countLimit; i++) {\r\n                allocator.submit(arrayLength);\r\n            }\r\n            waitForAll(allocator.futures);\r\n            final AllocatorThread t = new AllocatorThread(arrayLength, bam);\r\n            t.start();\r\n            for (int i = 0; i < 5; i++) {\r\n                Thread.sleep(100);\r\n                final Thread.State threadState = t.getState();\r\n                if (threadState != Thread.State.RUNNABLE && threadState != Thread.State.WAITING && threadState != Thread.State.TIMED_WAITING) {\r\n                    Assert.fail(\"threadState = \" + threadState);\r\n                }\r\n            }\r\n            recycler.submit(removeLast(allocator.futures).get());\r\n            Assert.assertEquals(1, removeLast(recycler.furtures).get().intValue());\r\n            Thread.sleep(100);\r\n            Assert.assertEquals(Thread.State.TERMINATED, t.getState());\r\n            Assert.assertEquals(countLimit - 1, allocator.recycleAll(recycler));\r\n            recycler.submit(t.array);\r\n            recycler.verify(countLimit);\r\n            Assert.assertEquals(countLimit, bam.release(new byte[arrayLength]));\r\n        }\r\n    } finally {\r\n        allocator.pool.shutdown();\r\n        recycler.pool.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "removeLast",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Future<T> removeLast(List<Future<T>> furtures) throws Exception\n{\r\n    return remove(furtures, furtures.size() - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Future<T> remove(List<Future<T>> furtures, int i) throws Exception\n{\r\n    return furtures.isEmpty() ? null : furtures.remove(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "waitForAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForAll(List<Future<T>> furtures) throws Exception\n{\r\n    for (Future<T> f : furtures) {\r\n        f.get();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testByteArrayManager",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testByteArrayManager() throws Exception\n{\r\n    final int countThreshold = 32;\r\n    final int countLimit = 64;\r\n    final long countResetTimePeriodMs = 10000L;\r\n    final ByteArrayManager.Impl bam = new ByteArrayManager.Impl(new ByteArrayManager.Conf(countThreshold, countLimit, countResetTimePeriodMs));\r\n    final CounterMap counters = bam.getCounters();\r\n    final ManagerMap managers = bam.getManagers();\r\n    final ExecutorService pool = Executors.newFixedThreadPool(128);\r\n    final Runner[] runners = new Runner[Runner.NUM_RUNNERS];\r\n    final Thread[] threads = new Thread[runners.length];\r\n    final int num = 1 << 10;\r\n    for (int i = 0; i < runners.length; i++) {\r\n        runners[i] = new Runner(i, countThreshold, countLimit, pool, i, bam);\r\n        threads[i] = runners[i].start(num);\r\n    }\r\n    final List<Exception> exceptions = new ArrayList<Exception>();\r\n    final Thread randomRecycler = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            LOG.info(\"randomRecycler start\");\r\n            for (int i = 0; shouldRun(); i++) {\r\n                final int j = ThreadLocalRandom.current().nextInt(runners.length);\r\n                try {\r\n                    runners[j].recycle();\r\n                } catch (Exception e) {\r\n                    e.printStackTrace();\r\n                    exceptions.add(new Exception(this + \" has an exception\", e));\r\n                }\r\n                if ((i & 0xFF) == 0) {\r\n                    LOG.info(\"randomRecycler sleep, i=\" + i);\r\n                    sleepMs(100);\r\n                }\r\n            }\r\n            LOG.info(\"randomRecycler done\");\r\n        }\r\n\r\n        boolean shouldRun() {\r\n            for (int i = 0; i < runners.length; i++) {\r\n                if (threads[i].isAlive()) {\r\n                    return true;\r\n                }\r\n                if (!runners[i].isEmpty()) {\r\n                    return true;\r\n                }\r\n            }\r\n            return false;\r\n        }\r\n    };\r\n    randomRecycler.start();\r\n    randomRecycler.join();\r\n    Assert.assertTrue(exceptions.isEmpty());\r\n    Assert.assertNull(counters.get(0, false));\r\n    for (int i = 1; i < runners.length; i++) {\r\n        if (!runners[i].assertionErrors.isEmpty()) {\r\n            for (AssertionError e : runners[i].assertionErrors) {\r\n                LOG.error(\"AssertionError \" + i, e);\r\n            }\r\n            Assert.fail(runners[i].assertionErrors.size() + \" AssertionError(s)\");\r\n        }\r\n        final int arrayLength = Runner.index2arrayLength(i);\r\n        final boolean exceedCountThreshold = counters.get(arrayLength, false).getCount() > countThreshold;\r\n        final FixedLengthManager m = managers.get(arrayLength, false);\r\n        if (exceedCountThreshold) {\r\n            Assert.assertNotNull(m);\r\n        } else {\r\n            Assert.assertNull(m);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "sleepMs",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void sleepMs(long ms)\n{\r\n    try {\r\n        Thread.sleep(ms);\r\n    } catch (InterruptedException e) {\r\n        e.printStackTrace();\r\n        Assert.fail(\"Sleep is interrupted: \" + e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    GenericTestUtils.disableLog(LoggerFactory.getLogger(ByteArrayManager.class));\r\n    final int arrayLength = 64 * 1024;\r\n    final int nThreads = 512;\r\n    final int nAllocations = 1 << 15;\r\n    final int maxArrays = 1 << 10;\r\n    final int nTrials = 5;\r\n    System.out.println(\"arrayLength=\" + arrayLength + \", nThreads=\" + nThreads + \", nAllocations=\" + nAllocations + \", maxArrays=\" + maxArrays);\r\n    final ByteArrayManager[] impls = { new ByteArrayManager.NewByteArrayWithoutLimit(), new NewByteArrayWithLimit(maxArrays), new ByteArrayManager.Impl(new ByteArrayManager.Conf(HdfsClientConfigKeys.Write.ByteArrayManager.COUNT_THRESHOLD_DEFAULT, maxArrays, HdfsClientConfigKeys.Write.ByteArrayManager.COUNT_RESET_TIME_PERIOD_MS_DEFAULT)) };\r\n    final double[] avg = new double[impls.length];\r\n    for (int i = 0; i < impls.length; i++) {\r\n        double duration = 0;\r\n        printf(\"%26s:\", impls[i].getClass().getSimpleName());\r\n        for (int j = 0; j < nTrials; j++) {\r\n            final int[] sleepTime = new int[nAllocations];\r\n            for (int k = 0; k < sleepTime.length; k++) {\r\n                sleepTime[k] = ThreadLocalRandom.current().nextInt(100);\r\n            }\r\n            final long elapsed = performanceTest(arrayLength, maxArrays, nThreads, sleepTime, impls[i]);\r\n            duration += elapsed;\r\n            printf(\"%5d, \", elapsed);\r\n        }\r\n        avg[i] = duration / nTrials;\r\n        printf(\"avg=%6.3fs\", avg[i] / 1000);\r\n        for (int j = 0; j < i; j++) {\r\n            printf(\" (%6.2f%%)\", percentageDiff(avg[j], avg[i]));\r\n        }\r\n        printf(\"\\n\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "percentageDiff",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double percentageDiff(double original, double newValue)\n{\r\n    return (newValue - original) / original * 100;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "printf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printf(String format, Object... args)\n{\r\n    System.out.printf(format, args);\r\n    System.out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "performanceTest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long performanceTest(final int arrayLength, final int maxArrays, final int nThreads, final int[] sleepTimeMSs, final ByteArrayManager impl) throws Exception\n{\r\n    final ExecutorService pool = Executors.newFixedThreadPool(nThreads);\r\n    final List<Future<Void>> futures = new ArrayList<Future<Void>>(sleepTimeMSs.length);\r\n    final long startTime = Time.monotonicNow();\r\n    for (int i = 0; i < sleepTimeMSs.length; i++) {\r\n        final long sleepTime = sleepTimeMSs[i];\r\n        futures.add(pool.submit(new Callable<Void>() {\r\n\r\n            @Override\r\n            public Void call() throws Exception {\r\n                byte[] array = impl.newByteArray(arrayLength);\r\n                sleepMs(sleepTime);\r\n                impl.release(array);\r\n                return null;\r\n            }\r\n        }));\r\n    }\r\n    for (Future<Void> f : futures) {\r\n        f.get();\r\n    }\r\n    final long endTime = Time.monotonicNow();\r\n    pool.shutdown();\r\n    return endTime - startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testConcurrency",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConcurrency() throws Exception\n{\r\n    for (int i = 0; i < RUNS; i++) {\r\n        singleRun();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "singleRun",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void singleRun() throws Exception\n{\r\n    final FsUrlStreamHandlerFactory factory = new FsUrlStreamHandlerFactory();\r\n    final Random random = new Random();\r\n    ExecutorService executor = Executors.newFixedThreadPool(THREADS);\r\n    ArrayList<Future<?>> futures = new ArrayList<Future<?>>(TASKS);\r\n    for (int i = 0; i < TASKS; i++) {\r\n        final int aux = i;\r\n        futures.add(executor.submit(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                int rand = aux + random.nextInt(3);\r\n                factory.createURLStreamHandler(String.valueOf(rand));\r\n            }\r\n        }));\r\n    }\r\n    executor.shutdown();\r\n    for (Future future : futures) {\r\n        if (!future.isDone()) {\r\n            break;\r\n        }\r\n        future.get();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFsUrlStreamHandlerFactory",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testFsUrlStreamHandlerFactory() throws IOException\n{\r\n    File myFile = new File(GenericTestUtils.getTestDir(), \"foo bar.txt\");\r\n    myFile.createNewFile();\r\n    URL myUrl = myFile.toURI().toURL();\r\n    myUrl.openStream().close();\r\n    URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());\r\n    URL myUrl2 = myFile.toURI().toURL();\r\n    myUrl2.openStream();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testGetAddressFromString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetAddressFromString() throws Exception\n{\r\n    assertEquals(HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT, DFSUtilClient.getNNAddress(\"foo\").getPort());\r\n    assertEquals(HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT, DFSUtilClient.getNNAddress(\"hdfs://foo/\").getPort());\r\n    assertEquals(555, DFSUtilClient.getNNAddress(\"hdfs://foo:555\").getPort());\r\n    assertEquals(555, DFSUtilClient.getNNAddress(\"foo:555\").getPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testGetAddressFromConf",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetAddressFromConf() throws Exception\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    FileSystem.setDefaultUri(conf, \"hdfs://foo/\");\r\n    assertEquals(HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT, DFSUtilClient.getNNAddress(conf).getPort());\r\n    FileSystem.setDefaultUri(conf, \"hdfs://foo:555/\");\r\n    assertEquals(555, DFSUtilClient.getNNAddress(conf).getPort());\r\n    FileSystem.setDefaultUri(conf, \"foo\");\r\n    assertEquals(HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT, DFSUtilClient.getNNAddress(conf).getPort());\r\n    FileSystem.setDefaultUri(conf, \"foo:555\");\r\n    assertEquals(555, DFSUtilClient.getNNAddress(conf).getPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testGetUri",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetUri()\n{\r\n    assertEquals(URI.create(\"hdfs://foo:555\"), DFSUtilClient.getNNUri(new InetSocketAddress(\"foo\", 555)));\r\n    assertEquals(URI.create(\"hdfs://foo\"), DFSUtilClient.getNNUri(new InetSocketAddress(\"foo\", HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup()\n{\r\n    for (OpType opType : OpType.values()) {\r\n        expectedOpsCountMap.put(opType, new AtomicLong());\r\n    }\r\n    incrementOpsCountByRandomNumbers();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testOpTypeSymbolsAreUnique",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testOpTypeSymbolsAreUnique()\n{\r\n    final Set<String> opTypeSymbols = new HashSet<>();\r\n    for (OpType opType : OpType.values()) {\r\n        assertFalse(opTypeSymbols.contains(opType.getSymbol()));\r\n        opTypeSymbols.add(opType.getSymbol());\r\n    }\r\n    assertEquals(OpType.values().length, opTypeSymbols.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testGetLongStatistics",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGetLongStatistics()\n{\r\n    short iterations = 0;\r\n    final Iterator<LongStatistic> iter = statistics.getLongStatistics();\r\n    while (iter.hasNext()) {\r\n        final LongStatistic longStat = iter.next();\r\n        assertNotNull(longStat);\r\n        final OpType opType = OpType.fromSymbol(longStat.getName());\r\n        assertNotNull(opType);\r\n        assertTrue(expectedOpsCountMap.containsKey(opType));\r\n        assertEquals(expectedOpsCountMap.get(opType).longValue(), longStat.getValue());\r\n        iterations++;\r\n    }\r\n    assertEquals(OpType.values().length, iterations);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testGetLong",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetLong()\n{\r\n    assertNull(statistics.getLong(null));\r\n    assertNull(statistics.getLong(NO_SUCH_OP));\r\n    verifyStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testIsTracked",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testIsTracked()\n{\r\n    assertFalse(statistics.isTracked(null));\r\n    assertFalse(statistics.isTracked(NO_SUCH_OP));\r\n    final Iterator<LongStatistic> iter = statistics.getLongStatistics();\r\n    while (iter.hasNext()) {\r\n        final LongStatistic longStatistic = iter.next();\r\n        assertTrue(statistics.isTracked(longStatistic.getName()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testReset",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testReset()\n{\r\n    statistics.reset();\r\n    for (OpType opType : OpType.values()) {\r\n        expectedOpsCountMap.get(opType).set(0);\r\n    }\r\n    final Iterator<LongStatistic> iter = statistics.getLongStatistics();\r\n    while (iter.hasNext()) {\r\n        final LongStatistic longStat = iter.next();\r\n        assertEquals(0, longStat.getValue());\r\n    }\r\n    incrementOpsCountByRandomNumbers();\r\n    verifyStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testCurrentAccess",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCurrentAccess() throws InterruptedException\n{\r\n    final int numThreads = 10;\r\n    final ExecutorService threadPool = newFixedThreadPool(numThreads);\r\n    try {\r\n        final CountDownLatch allReady = new CountDownLatch(numThreads);\r\n        final CountDownLatch startBlocker = new CountDownLatch(1);\r\n        final CountDownLatch allDone = new CountDownLatch(numThreads);\r\n        final AtomicReference<Throwable> childError = new AtomicReference<>();\r\n        for (int i = 0; i < numThreads; i++) {\r\n            threadPool.submit(new Runnable() {\r\n\r\n                @Override\r\n                public void run() {\r\n                    allReady.countDown();\r\n                    try {\r\n                        startBlocker.await();\r\n                        incrementOpsCountByRandomNumbers();\r\n                    } catch (Throwable t) {\r\n                        LOG.error(\"Child failed when calling mkdir\", t);\r\n                        childError.compareAndSet(null, t);\r\n                    } finally {\r\n                        allDone.countDown();\r\n                    }\r\n                }\r\n            });\r\n        }\r\n        allReady.await();\r\n        startBlocker.countDown();\r\n        allDone.await();\r\n        assertNull(\"Child failed with exception.\", childError.get());\r\n        verifyStatistics();\r\n    } finally {\r\n        threadPool.shutdownNow();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "incrementOpsCountByRandomNumbers",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void incrementOpsCountByRandomNumbers()\n{\r\n    for (OpType opType : OpType.values()) {\r\n        final Long randomCount = RandomUtils.nextLong(0, 100);\r\n        expectedOpsCountMap.get(opType).addAndGet(randomCount);\r\n        for (long i = 0; i < randomCount; i++) {\r\n            statistics.incrementOpCounter(opType);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "verifyStatistics",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyStatistics()\n{\r\n    for (OpType opType : OpType.values()) {\r\n        assertNotNull(expectedOpsCountMap.get(opType));\r\n        assertNotNull(statistics.getLong(opType.getSymbol()));\r\n        assertEquals(\"Not expected count for operation \" + opType.getSymbol(), expectedOpsCountMap.get(opType).longValue(), statistics.getLong(opType.getSymbol()).longValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testPolicyAndStateCantBeNull",
  "errType" : [ "NullPointerException", "NullPointerException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testPolicyAndStateCantBeNull()\n{\r\n    try {\r\n        new ErasureCodingPolicyInfo(null);\r\n        fail(\"Null policy should fail\");\r\n    } catch (NullPointerException expected) {\r\n    }\r\n    try {\r\n        new ErasureCodingPolicyInfo(SystemErasureCodingPolicies.getByID(RS_6_3_POLICY_ID), null);\r\n        fail(\"Null policy state should fail\");\r\n    } catch (NullPointerException expected) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testStates",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testStates()\n{\r\n    ErasureCodingPolicyInfo info = new ErasureCodingPolicyInfo(SystemErasureCodingPolicies.getByID(RS_6_3_POLICY_ID));\r\n    info.setState(ENABLED);\r\n    assertFalse(info.isDisabled());\r\n    assertTrue(info.isEnabled());\r\n    assertFalse(info.isRemoved());\r\n    info.setState(REMOVED);\r\n    assertFalse(info.isDisabled());\r\n    assertFalse(info.isEnabled());\r\n    assertTrue(info.isRemoved());\r\n    info.setState(DISABLED);\r\n    assertTrue(info.isDisabled());\r\n    assertFalse(info.isEnabled());\r\n    assertFalse(info.isRemoved());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    GenericTestUtils.setLogLevel(RequestHedgingProxyProvider.LOG, Level.TRACE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void setup() throws URISyntaxException\n{\r\n    ns1 = \"mycluster-1-\" + Time.monotonicNow();\r\n    ns1Uri = new URI(\"hdfs://\" + ns1);\r\n    conf = new Configuration();\r\n    conf.set(HdfsClientConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ns1, \"nn1,nn2,nn3\");\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns1 + \".nn1\", ns1nn1Hostname + \":\" + rpcPort);\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns1 + \".nn2\", ns1nn2Hostname + \":\" + rpcPort);\r\n    conf.set(HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + ns1, ConfiguredFailoverProxyProvider.class.getName());\r\n    conf.setBoolean(HdfsClientConfigKeys.Failover.RANDOM_ORDER + \".\" + ns1, false);\r\n    ns2 = \"myroutercluster-2-\" + Time.monotonicNow();\r\n    ns2Uri = new URI(\"hdfs://\" + ns2);\r\n    conf.set(HdfsClientConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ns2, \"nn1,nn2,nn3\");\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns2 + \".nn1\", ns2nn1Hostname + \":\" + rpcPort);\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns2 + \".nn2\", ns2nn2Hostname + \":\" + rpcPort);\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns2 + \".nn3\", ns2nn3Hostname + \":\" + rpcPort);\r\n    conf.set(HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + ns2, ConfiguredFailoverProxyProvider.class.getName());\r\n    conf.setBoolean(HdfsClientConfigKeys.Failover.RANDOM_ORDER + \".\" + ns2, true);\r\n    ns3 = \"mycluster-3-\" + Time.monotonicNow();\r\n    ns3Uri = new URI(\"hdfs://\" + ns3);\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMESERVICES, String.join(\",\", ns1, ns2, ns3));\r\n    conf.set(\"fs.defaultFS\", \"hdfs://\" + ns1);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "addDNSSettings",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addDNSSettings(Configuration config, boolean hostResolvable, boolean useFQDN)\n{\r\n    config.set(HdfsClientConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ns3, \"nn\");\r\n    String domain = hostResolvable ? MockDomainNameResolver.DOMAIN : MockDomainNameResolver.UNKNOW_DOMAIN;\r\n    config.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns3 + \".nn\", domain + \":\" + rpcPort);\r\n    config.set(HdfsClientConfigKeys.Failover.PROXY_PROVIDER_KEY_PREFIX + \".\" + ns3, ConfiguredFailoverProxyProvider.class.getName());\r\n    config.setBoolean(HdfsClientConfigKeys.Failover.RESOLVE_ADDRESS_NEEDED_KEY + \".\" + ns3, true);\r\n    config.set(HdfsClientConfigKeys.Failover.RESOLVE_SERVICE_KEY + \".\" + ns3, MockDomainNameResolver.class.getName());\r\n    config.setBoolean(HdfsClientConfigKeys.Failover.RANDOM_ORDER + \".\" + ns3, true);\r\n    config.setBoolean(HdfsClientConfigKeys.Failover.RESOLVE_ADDRESS_TO_FQDN + \".\" + ns3, useFQDN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testNonRandomGetProxy",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testNonRandomGetProxy() throws Exception\n{\r\n    final AtomicInteger nn1Count = new AtomicInteger(0);\r\n    final AtomicInteger nn2Count = new AtomicInteger(0);\r\n    Map<InetSocketAddress, ClientProtocol> proxyMap = new HashMap<>();\r\n    final ClientProtocol nn1Mock = mock(ClientProtocol.class);\r\n    when(nn1Mock.getStats()).thenAnswer(createAnswer(nn1Count, 1));\r\n    proxyMap.put(ns1nn1, nn1Mock);\r\n    final ClientProtocol nn2Mock = mock(ClientProtocol.class);\r\n    when(nn2Mock.getStats()).thenAnswer(createAnswer(nn2Count, 2));\r\n    proxyMap.put(ns1nn2, nn2Mock);\r\n    ConfiguredFailoverProxyProvider<ClientProtocol> provider1 = new ConfiguredFailoverProxyProvider<>(conf, ns1Uri, ClientProtocol.class, createFactory(proxyMap));\r\n    ClientProtocol proxy1 = provider1.getProxy().proxy;\r\n    proxy1.getStats();\r\n    assertEquals(1, nn1Count.get());\r\n    assertEquals(0, nn2Count.get());\r\n    proxy1.getStats();\r\n    assertEquals(2, nn1Count.get());\r\n    assertEquals(0, nn2Count.get());\r\n    nn1Count.set(0);\r\n    nn2Count.set(0);\r\n    for (int i = 0; i < NUM_ITERATIONS; i++) {\r\n        ConfiguredFailoverProxyProvider<ClientProtocol> provider2 = new ConfiguredFailoverProxyProvider<>(conf, ns1Uri, ClientProtocol.class, createFactory(proxyMap));\r\n        ClientProtocol proxy2 = provider2.getProxy().proxy;\r\n        proxy2.getStats();\r\n    }\r\n    assertEquals(NUM_ITERATIONS, nn1Count.get());\r\n    assertEquals(0, nn2Count.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testRandomGetProxy",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testRandomGetProxy() throws Exception\n{\r\n    final AtomicInteger nn1Count = new AtomicInteger(0);\r\n    final AtomicInteger nn2Count = new AtomicInteger(0);\r\n    final AtomicInteger nn3Count = new AtomicInteger(0);\r\n    Map<InetSocketAddress, ClientProtocol> proxyMap = new HashMap<>();\r\n    final ClientProtocol nn1Mock = mock(ClientProtocol.class);\r\n    when(nn1Mock.getStats()).thenAnswer(createAnswer(nn1Count, 1));\r\n    proxyMap.put(ns2nn1, nn1Mock);\r\n    final ClientProtocol nn2Mock = mock(ClientProtocol.class);\r\n    when(nn2Mock.getStats()).thenAnswer(createAnswer(nn2Count, 2));\r\n    proxyMap.put(ns2nn2, nn2Mock);\r\n    final ClientProtocol nn3Mock = mock(ClientProtocol.class);\r\n    when(nn3Mock.getStats()).thenAnswer(createAnswer(nn3Count, 3));\r\n    proxyMap.put(ns2nn3, nn3Mock);\r\n    for (int i = 0; i < NUM_ITERATIONS; i++) {\r\n        ConfiguredFailoverProxyProvider<ClientProtocol> provider = new ConfiguredFailoverProxyProvider<>(conf, ns2Uri, ClientProtocol.class, createFactory(proxyMap));\r\n        ClientProtocol proxy = provider.getProxy().proxy;\r\n        proxy.getStats();\r\n    }\r\n    assertTrue(nn1Count.get() < NUM_ITERATIONS && nn1Count.get() > 0);\r\n    assertTrue(nn2Count.get() < NUM_ITERATIONS && nn2Count.get() > 0);\r\n    assertTrue(nn3Count.get() < NUM_ITERATIONS && nn3Count.get() > 0);\r\n    assertEquals(NUM_ITERATIONS, nn1Count.get() + nn2Count.get() + nn3Count.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testResolveDomainNameUsingDNS",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testResolveDomainNameUsingDNS(boolean useFQDN) throws Exception\n{\r\n    Configuration dnsConf = new Configuration(conf);\r\n    addDNSSettings(dnsConf, true, useFQDN);\r\n    Map<InetSocketAddress, ClientProtocol> proxyMap = new HashMap<>();\r\n    final AtomicInteger nn1Count = addClientMock(useFQDN ? MockDomainNameResolver.FQDN_1 : MockDomainNameResolver.ADDR_1, proxyMap);\r\n    final AtomicInteger nn2Count = addClientMock(useFQDN ? MockDomainNameResolver.FQDN_2 : MockDomainNameResolver.ADDR_2, proxyMap);\r\n    final Map<String, AtomicInteger> proxyResults = new HashMap<>();\r\n    for (int i = 0; i < NUM_ITERATIONS; i++) {\r\n        @SuppressWarnings(\"resource\")\r\n        ConfiguredFailoverProxyProvider<ClientProtocol> provider = new ConfiguredFailoverProxyProvider<>(dnsConf, ns3Uri, ClientProtocol.class, createFactory(proxyMap));\r\n        ClientProtocol proxy = provider.getProxy().proxy;\r\n        String proxyAddress = provider.getProxy().proxyInfo;\r\n        if (proxyResults.containsKey(proxyAddress)) {\r\n            proxyResults.get(proxyAddress).incrementAndGet();\r\n        } else {\r\n            proxyResults.put(proxyAddress, new AtomicInteger(1));\r\n        }\r\n        proxy.getStats();\r\n    }\r\n    String resolvedHost1 = useFQDN ? MockDomainNameResolver.FQDN_1 : \"/\" + MockDomainNameResolver.ADDR_1;\r\n    String resolvedHost2 = useFQDN ? MockDomainNameResolver.FQDN_2 : \"/\" + MockDomainNameResolver.ADDR_2;\r\n    assertEquals(2, proxyResults.size());\r\n    if (Shell.isJavaVersionAtLeast(14) && useFQDN) {\r\n        assertTrue(\"nn1 wasn't returned: \" + proxyResults, proxyResults.containsKey(resolvedHost1 + \"/<unresolved>:8020\"));\r\n        assertTrue(\"nn2 wasn't returned: \" + proxyResults, proxyResults.containsKey(resolvedHost2 + \"/<unresolved>:8020\"));\r\n    } else {\r\n        assertTrue(\"nn1 wasn't returned: \" + proxyResults, proxyResults.containsKey(resolvedHost1 + \":8020\"));\r\n        assertTrue(\"nn2 wasn't returned: \" + proxyResults, proxyResults.containsKey(resolvedHost2 + \":8020\"));\r\n    }\r\n    assertEquals(NUM_ITERATIONS, nn1Count.get() + nn2Count.get());\r\n    assertTrue(\"nn1 was selected too much:\" + nn1Count.get(), nn1Count.get() < NUM_ITERATIONS);\r\n    assertTrue(\"nn1 should have been selected: \" + nn1Count.get(), nn1Count.get() > 0);\r\n    assertTrue(\"nn2 was selected too much:\" + nn2Count.get(), nn2Count.get() < NUM_ITERATIONS);\r\n    assertTrue(\"nn2 should have been selected: \" + nn2Count.get(), nn2Count.get() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testResolveDomainNameUsingDNS",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testResolveDomainNameUsingDNS() throws Exception\n{\r\n    testResolveDomainNameUsingDNS(false);\r\n    testResolveDomainNameUsingDNS(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testResolveDomainNameUsingDNSUnknownHost",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testResolveDomainNameUsingDNSUnknownHost() throws Exception\n{\r\n    Configuration dnsConf = new Configuration(conf);\r\n    addDNSSettings(dnsConf, false, false);\r\n    Map<InetSocketAddress, ClientProtocol> proxyMap = new HashMap<>();\r\n    exception.expect(RuntimeException.class);\r\n    ConfiguredFailoverProxyProvider<ClientProtocol> provider = new ConfiguredFailoverProxyProvider<>(dnsConf, ns3Uri, ClientProtocol.class, createFactory(proxyMap));\r\n    assertNull(\"failover proxy cannot be created due to unknownhost\", provider);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "addClientMock",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AtomicInteger addClientMock(String host, Map<InetSocketAddress, ClientProtocol> proxyMap) throws Exception\n{\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    InetSocketAddress inetSockerAddr = new InetSocketAddress(host, rpcPort);\r\n    final ClientProtocol cpMock = mock(ClientProtocol.class);\r\n    when(cpMock.getStats()).thenAnswer(createAnswer(counter, 1));\r\n    proxyMap.put(inetSockerAddr, cpMock);\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "createAnswer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Answer<long[]> createAnswer(final AtomicInteger counter, final long retVal)\n{\r\n    return new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            counter.incrementAndGet();\r\n            return new long[] { retVal };\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "createFactory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HAProxyFactory<ClientProtocol> createFactory(final Map<InetSocketAddress, ClientProtocol> proxies)\n{\r\n    final Map<InetSocketAddress, ClientProtocol> proxyMap = proxies;\r\n    return new HAProxyFactory<ClientProtocol>() {\r\n\r\n        @Override\r\n        public ClientProtocol createProxy(Configuration cfg, InetSocketAddress nnAddr, Class<ClientProtocol> xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth) throws IOException {\r\n            if (proxyMap.containsKey(nnAddr)) {\r\n                return proxyMap.get(nnAddr);\r\n            } else {\r\n                throw new IOException(\"Name node address not found\");\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public ClientProtocol createProxy(Configuration cfg, InetSocketAddress nnAddr, Class<ClientProtocol> xface, UserGroupInformation ugi, boolean withRetries) throws IOException {\r\n            if (proxyMap.containsKey(nnAddr)) {\r\n                return proxyMap.get(nnAddr);\r\n            } else {\r\n                throw new IOException(\"Name node address not found\");\r\n            }\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "before",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void before()\n{\r\n    Assume.assumeTrue(null == SharedFileDescriptorFactory.getLoadingFailureReason());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "testStartupShutdown",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testStartupShutdown() throws Exception\n{\r\n    File path = new File(TEST_BASE, \"testStartupShutdown\");\r\n    path.mkdirs();\r\n    SharedFileDescriptorFactory factory = SharedFileDescriptorFactory.create(\"shm_\", new String[] { path.getAbsolutePath() });\r\n    FileInputStream stream = factory.createDescriptor(\"testStartupShutdown\", 4096);\r\n    ShortCircuitShm shm = new ShortCircuitShm(ShmId.createRandom(), stream);\r\n    shm.free();\r\n    stream.close();\r\n    FileUtil.fullyDelete(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\shortcircuit",
  "methodName" : "testAllocateSlots",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testAllocateSlots() throws Exception\n{\r\n    File path = new File(TEST_BASE, \"testAllocateSlots\");\r\n    path.mkdirs();\r\n    SharedFileDescriptorFactory factory = SharedFileDescriptorFactory.create(\"shm_\", new String[] { path.getAbsolutePath() });\r\n    FileInputStream stream = factory.createDescriptor(\"testAllocateSlots\", 4096);\r\n    ShortCircuitShm shm = new ShortCircuitShm(ShmId.createRandom(), stream);\r\n    int numSlots = 0;\r\n    ArrayList<Slot> slots = new ArrayList<Slot>();\r\n    while (!shm.isFull()) {\r\n        Slot slot = shm.allocAndRegisterSlot(new ExtendedBlockId(123L, \"test_bp1\"));\r\n        slots.add(slot);\r\n        numSlots++;\r\n    }\r\n    LOG.info(\"allocated \" + numSlots + \" slots before running out.\");\r\n    int slotIdx = 0;\r\n    for (Iterator<Slot> iter = shm.slotIterator(); iter.hasNext(); ) {\r\n        Assert.assertTrue(slots.contains(iter.next()));\r\n    }\r\n    for (Slot slot : slots) {\r\n        Assert.assertFalse(slot.addAnchor());\r\n        Assert.assertEquals(slotIdx++, slot.getSlotIdx());\r\n    }\r\n    for (Slot slot : slots) {\r\n        slot.makeAnchorable();\r\n    }\r\n    for (Slot slot : slots) {\r\n        Assert.assertTrue(slot.addAnchor());\r\n    }\r\n    for (Slot slot : slots) {\r\n        slot.removeAnchor();\r\n    }\r\n    for (Slot slot : slots) {\r\n        shm.unregisterSlot(slot.getSlotIdx());\r\n        slot.makeInvalid();\r\n    }\r\n    shm.free();\r\n    stream.close();\r\n    FileUtil.fullyDelete(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testAddAndRetrieve",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAddAndRetrieve() throws Exception\n{\r\n    PeerCache cache = new PeerCache(3, 100000);\r\n    DatanodeID dnId = new DatanodeID(\"192.168.0.1\", \"fakehostname\", \"fake_datanode_id\", 100, 101, 102, 103);\r\n    FakePeer peer = new FakePeer(dnId, false);\r\n    cache.put(dnId, peer);\r\n    assertTrue(!peer.isClosed());\r\n    assertEquals(1, cache.size());\r\n    assertEquals(peer, cache.get(dnId, false));\r\n    assertEquals(0, cache.size());\r\n    cache.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testExpiry",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testExpiry() throws Exception\n{\r\n    final int CAPACITY = 3;\r\n    final int EXPIRY_PERIOD = 10;\r\n    PeerCache cache = new PeerCache(CAPACITY, EXPIRY_PERIOD);\r\n    DatanodeID[] dnIds = new DatanodeID[CAPACITY];\r\n    FakePeer[] peers = new FakePeer[CAPACITY];\r\n    for (int i = 0; i < CAPACITY; ++i) {\r\n        dnIds[i] = new DatanodeID(\"192.168.0.1\", \"fakehostname_\" + i, \"fake_datanode_id\", 100, 101, 102, 103);\r\n        peers[i] = new FakePeer(dnIds[i], false);\r\n    }\r\n    for (int i = 0; i < CAPACITY; ++i) {\r\n        cache.put(dnIds[i], peers[i]);\r\n    }\r\n    Thread.sleep(EXPIRY_PERIOD * 50);\r\n    assertEquals(0, cache.size());\r\n    for (int i = 0; i < CAPACITY; ++i) {\r\n        assertTrue(peers[i].isClosed());\r\n    }\r\n    Thread.sleep(EXPIRY_PERIOD * 50);\r\n    cache.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testEviction",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testEviction() throws Exception\n{\r\n    final int CAPACITY = 3;\r\n    PeerCache cache = new PeerCache(CAPACITY, 100000);\r\n    DatanodeID[] dnIds = new DatanodeID[CAPACITY + 1];\r\n    FakePeer[] peers = new FakePeer[CAPACITY + 1];\r\n    for (int i = 0; i < dnIds.length; ++i) {\r\n        dnIds[i] = new DatanodeID(\"192.168.0.1\", \"fakehostname_\" + i, \"fake_datanode_id_\" + i, 100, 101, 102, 103);\r\n        peers[i] = new FakePeer(dnIds[i], false);\r\n    }\r\n    for (int i = 0; i < CAPACITY; ++i) {\r\n        cache.put(dnIds[i], peers[i]);\r\n    }\r\n    assertEquals(CAPACITY, cache.size());\r\n    cache.put(dnIds[CAPACITY], peers[CAPACITY]);\r\n    assertEquals(CAPACITY, cache.size());\r\n    assertSame(null, cache.get(dnIds[0], false));\r\n    for (int i = 1; i < CAPACITY; ++i) {\r\n        Peer peer = cache.get(dnIds[i], false);\r\n        assertSame(peers[i], peer);\r\n        assertTrue(!peer.isClosed());\r\n        peer.close();\r\n    }\r\n    assertEquals(1, cache.size());\r\n    cache.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testMultiplePeersWithSameKey",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testMultiplePeersWithSameKey() throws Exception\n{\r\n    final int CAPACITY = 3;\r\n    PeerCache cache = new PeerCache(CAPACITY, 100000);\r\n    DatanodeID dnId = new DatanodeID(\"192.168.0.1\", \"fakehostname\", \"fake_datanode_id\", 100, 101, 102, 103);\r\n    HashMultiset<FakePeer> peers = HashMultiset.create(CAPACITY);\r\n    for (int i = 0; i < CAPACITY; ++i) {\r\n        FakePeer peer = new FakePeer(dnId, false);\r\n        peers.add(peer);\r\n        cache.put(dnId, peer);\r\n    }\r\n    assertEquals(CAPACITY, cache.size());\r\n    while (!peers.isEmpty()) {\r\n        Peer peer = cache.get(dnId, false);\r\n        assertTrue(peer != null);\r\n        assertTrue(!peer.isClosed());\r\n        peers.remove(peer);\r\n    }\r\n    assertEquals(0, cache.size());\r\n    cache.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testDomainSocketPeers",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testDomainSocketPeers() throws Exception\n{\r\n    final int CAPACITY = 3;\r\n    PeerCache cache = new PeerCache(CAPACITY, 100000);\r\n    DatanodeID dnId = new DatanodeID(\"192.168.0.1\", \"fakehostname\", \"fake_datanode_id\", 100, 101, 102, 103);\r\n    HashMultiset<FakePeer> peers = HashMultiset.create(CAPACITY);\r\n    for (int i = 0; i < CAPACITY; ++i) {\r\n        FakePeer peer = new FakePeer(dnId, i == CAPACITY - 1);\r\n        peers.add(peer);\r\n        cache.put(dnId, peer);\r\n    }\r\n    assertEquals(CAPACITY, cache.size());\r\n    Peer peer = cache.get(dnId, true);\r\n    assertTrue(peer.getDomainSocket() != null);\r\n    peers.remove(peer);\r\n    peer = cache.get(dnId, true);\r\n    assertTrue(peer == null);\r\n    while (!peers.isEmpty()) {\r\n        peer = cache.get(dnId, false);\r\n        assertTrue(peer != null);\r\n        assertTrue(!peer.isClosed());\r\n        peers.remove(peer);\r\n    }\r\n    assertEquals(0, cache.size());\r\n    cache.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getActionFromTokenAspect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RenewAction<?> getActionFromTokenAspect(TokenAspect<DummyFs> tokenAspect)\n{\r\n    return (RenewAction<?>) Whitebox.getInternalState(tokenAspect, \"action\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testCachedInitialization",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCachedInitialization() throws IOException, URISyntaxException\n{\r\n    Configuration conf = new Configuration();\r\n    DummyFs fs = spy(new DummyFs());\r\n    Token<TokenIdentifier> token = new Token<TokenIdentifier>(new byte[0], new byte[0], DummyFs.TOKEN_KIND, new Text(\"127.0.0.1:1234\"));\r\n    doReturn(token).when(fs).getDelegationToken(any());\r\n    doReturn(token).when(fs).getRenewToken();\r\n    fs.emulateSecurityEnabled = true;\r\n    fs.initialize(new URI(\"dummyfs://127.0.0.1:1234\"), conf);\r\n    fs.tokenAspect.ensureTokenInitialized();\r\n    verify(fs, times(1)).getDelegationToken(null);\r\n    verify(fs, times(1)).setDelegationToken(token);\r\n    fs.tokenAspect.ensureTokenInitialized();\r\n    verify(fs, times(1)).getDelegationToken(null);\r\n    verify(fs, times(1)).setDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testGetRemoteToken",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetRemoteToken() throws IOException, URISyntaxException\n{\r\n    Configuration conf = new Configuration();\r\n    DummyFs fs = spy(new DummyFs());\r\n    Token<TokenIdentifier> token = new Token<TokenIdentifier>(new byte[0], new byte[0], DummyFs.TOKEN_KIND, new Text(\"127.0.0.1:1234\"));\r\n    doReturn(token).when(fs).getDelegationToken(any());\r\n    doReturn(token).when(fs).getRenewToken();\r\n    fs.initialize(new URI(\"dummyfs://127.0.0.1:1234\"), conf);\r\n    fs.tokenAspect.ensureTokenInitialized();\r\n    verify(fs).setDelegationToken(token);\r\n    assertNotNull(Whitebox.getInternalState(fs.tokenAspect, \"dtRenewer\"));\r\n    assertNotNull(Whitebox.getInternalState(fs.tokenAspect, \"action\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testGetRemoteTokenFailure",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetRemoteTokenFailure() throws IOException, URISyntaxException\n{\r\n    Configuration conf = new Configuration();\r\n    DummyFs fs = spy(new DummyFs());\r\n    IOException e = new IOException();\r\n    doThrow(e).when(fs).getDelegationToken(anyString());\r\n    fs.emulateSecurityEnabled = true;\r\n    fs.initialize(new URI(\"dummyfs://127.0.0.1:1234\"), conf);\r\n    try {\r\n        fs.tokenAspect.ensureTokenInitialized();\r\n    } catch (IOException exc) {\r\n        assertEquals(e, exc);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testInitWithNoTokens",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testInitWithNoTokens() throws IOException, URISyntaxException\n{\r\n    Configuration conf = new Configuration();\r\n    DummyFs fs = spy(new DummyFs());\r\n    doReturn(null).when(fs).getDelegationToken(anyString());\r\n    fs.initialize(new URI(\"dummyfs://127.0.0.1:1234\"), conf);\r\n    fs.tokenAspect.ensureTokenInitialized();\r\n    verify(fs, never()).setDelegationToken(Mockito.<Token<? extends TokenIdentifier>>any());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testInitWithUGIToken",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testInitWithUGIToken() throws IOException, URISyntaxException\n{\r\n    Configuration conf = new Configuration();\r\n    DummyFs fs = spy(new DummyFs());\r\n    doReturn(null).when(fs).getDelegationToken(anyString());\r\n    Token<TokenIdentifier> token = new Token<TokenIdentifier>(new byte[0], new byte[0], DummyFs.TOKEN_KIND, new Text(\"127.0.0.1:1234\"));\r\n    fs.ugi.addToken(token);\r\n    fs.ugi.addToken(new Token<TokenIdentifier>(new byte[0], new byte[0], new Text(\"Other token\"), new Text(\"127.0.0.1:8021\")));\r\n    assertEquals(\"wrong tokens in user\", 2, fs.ugi.getTokens().size());\r\n    fs.emulateSecurityEnabled = true;\r\n    fs.initialize(new URI(\"dummyfs://127.0.0.1:1234\"), conf);\r\n    fs.tokenAspect.ensureTokenInitialized();\r\n    verify(fs).setDelegationToken(token);\r\n    verify(fs, never()).getDelegationToken(anyString());\r\n    assertNull(Whitebox.getInternalState(fs.tokenAspect, \"dtRenewer\"));\r\n    assertNull(Whitebox.getInternalState(fs.tokenAspect, \"action\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testRenewal",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testRenewal() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    Token<?> token1 = mock(Token.class);\r\n    Token<?> token2 = mock(Token.class);\r\n    final long renewCycle = 100;\r\n    DelegationTokenRenewer.renewCycle = renewCycle;\r\n    UserGroupInformation ugi = UserGroupInformation.createUserForTesting(\"foo\", new String[] { \"bar\" });\r\n    DummyFs fs = spy(new DummyFs());\r\n    doReturn(token1).doReturn(token2).when(fs).getDelegationToken(null);\r\n    doReturn(token1).when(fs).getRenewToken();\r\n    doThrow(new IOException(\"renew failed\")).when(token1).renew(conf);\r\n    doThrow(new IOException(\"get failed\")).when(fs).addDelegationTokens(null, null);\r\n    final URI uri = new URI(\"dummyfs://127.0.0.1:1234\");\r\n    TokenAspect<DummyFs> tokenAspect = new TokenAspect<DummyFs>(fs, SecurityUtil.buildTokenService(uri), DummyFs.TOKEN_KIND);\r\n    fs.initialize(uri, conf);\r\n    tokenAspect.initDelegationToken(ugi);\r\n    tokenAspect.ensureTokenInitialized();\r\n    DelegationTokenRenewer.RenewAction<?> action = getActionFromTokenAspect(tokenAspect);\r\n    verify(fs).setDelegationToken(token1);\r\n    assertTrue(action.isValid());\r\n    Thread.sleep(renewCycle * 2);\r\n    assertSame(action, getActionFromTokenAspect(tokenAspect));\r\n    assertFalse(action.isValid());\r\n    tokenAspect.ensureTokenInitialized();\r\n    verify(fs, times(2)).getDelegationToken(any());\r\n    verify(fs).setDelegationToken(token2);\r\n    assertNotSame(action, getActionFromTokenAspect(tokenAspect));\r\n    action = getActionFromTokenAspect(tokenAspect);\r\n    assertTrue(action.isValid());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    GenericTestUtils.setLogLevel(RequestHedgingProxyProvider.LOG, Level.TRACE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws URISyntaxException\n{\r\n    ns = \"mycluster-\" + Time.monotonicNow();\r\n    nnUri = new URI(\"hdfs://\" + ns);\r\n    conf = new Configuration();\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMESERVICES, ns);\r\n    conf.set(HdfsClientConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ns, \"nn1,nn2\");\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns + \".nn1\", \"machine1.foo.bar:8020\");\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns + \".nn2\", \"machine2.foo.bar:8020\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testHedgingWhenOneFails",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testHedgingWhenOneFails() throws Exception\n{\r\n    final ClientProtocol goodMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(goodMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            Thread.sleep(1000);\r\n            return new long[] { 1 };\r\n        }\r\n    });\r\n    final ClientProtocol badMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(badMock.getStats()).thenThrow(new IOException(\"Bad mock !!\"));\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(badMock, goodMock));\r\n    long[] stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Mockito.verify(badMock).getStats();\r\n    Mockito.verify(goodMock).getStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testRequestNNAfterOneSuccess",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testRequestNNAfterOneSuccess() throws Exception\n{\r\n    final AtomicInteger goodCount = new AtomicInteger(0);\r\n    final AtomicInteger badCount = new AtomicInteger(0);\r\n    final ClientProtocol goodMock = mock(ClientProtocol.class);\r\n    when(goodMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            goodCount.incrementAndGet();\r\n            Thread.sleep(1000);\r\n            return new long[] { 1 };\r\n        }\r\n    });\r\n    final ClientProtocol badMock = mock(ClientProtocol.class);\r\n    when(badMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            badCount.incrementAndGet();\r\n            throw new IOException(\"Bad mock !!\");\r\n        }\r\n    });\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(badMock, goodMock));\r\n    ClientProtocol proxy = provider.getProxy().proxy;\r\n    proxy.getStats();\r\n    assertEquals(1, goodCount.get());\r\n    assertEquals(1, badCount.get());\r\n    proxy.getStats();\r\n    assertEquals(2, goodCount.get());\r\n    assertEquals(1, badCount.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testExceptionInfo",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testExceptionInfo() throws Exception\n{\r\n    final ClientProtocol goodMock = mock(ClientProtocol.class);\r\n    when(goodMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        private boolean first = true;\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            if (first) {\r\n                Thread.sleep(1000);\r\n                first = false;\r\n                return new long[] { 1 };\r\n            } else {\r\n                throw new IOException(\"Expected Exception Info\");\r\n            }\r\n        }\r\n    });\r\n    final ClientProtocol badMock = mock(ClientProtocol.class);\r\n    when(badMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            throw new IOException(\"Bad Mock! This is Standby!\");\r\n        }\r\n    });\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(badMock, goodMock));\r\n    ClientProtocol proxy = provider.getProxy().proxy;\r\n    proxy.getStats();\r\n    try {\r\n        proxy.getStats();\r\n    } catch (IOException e) {\r\n        assertExceptionContains(\"Expected Exception Info\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testHedgingWhenOneIsSlow",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testHedgingWhenOneIsSlow() throws Exception\n{\r\n    final ClientProtocol goodMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(goodMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            Thread.sleep(1000);\r\n            return new long[] { 1 };\r\n        }\r\n    });\r\n    final ClientProtocol badMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(badMock.getStats()).thenThrow(new IOException(\"Bad mock !!\"));\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(goodMock, badMock));\r\n    long[] stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(1, stats[0]);\r\n    Mockito.verify(badMock).getStats();\r\n    Mockito.verify(goodMock).getStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testHedgingWhenBothFail",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testHedgingWhenBothFail() throws Exception\n{\r\n    ClientProtocol badMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(badMock.getStats()).thenThrow(new IOException(\"Bad mock !!\"));\r\n    ClientProtocol worseMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(worseMock.getStats()).thenThrow(new IOException(\"Worse mock !!\"));\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(badMock, worseMock));\r\n    try {\r\n        provider.getProxy().proxy.getStats();\r\n        Assert.fail(\"Should fail since both namenodes throw IOException !!\");\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(e instanceof MultiException);\r\n    }\r\n    Mockito.verify(badMock).getStats();\r\n    Mockito.verify(worseMock).getStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testPerformFailover",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testPerformFailover() throws Exception\n{\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    final int[] isGood = { 1 };\r\n    final ClientProtocol goodMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(goodMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            counter.incrementAndGet();\r\n            if (isGood[0] == 1) {\r\n                Thread.sleep(1000);\r\n                return new long[] { 1 };\r\n            }\r\n            throw new IOException(\"Was Good mock !!\");\r\n        }\r\n    });\r\n    final ClientProtocol badMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(badMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            counter.incrementAndGet();\r\n            if (isGood[0] == 2) {\r\n                Thread.sleep(1000);\r\n                return new long[] { 2 };\r\n            }\r\n            throw new IOException(\"Bad mock !!\");\r\n        }\r\n    });\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(goodMock, badMock));\r\n    long[] stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(1, stats[0]);\r\n    Assert.assertEquals(2, counter.get());\r\n    Mockito.verify(badMock).getStats();\r\n    Mockito.verify(goodMock).getStats();\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(1, stats[0]);\r\n    Mockito.verifyNoMoreInteractions(badMock);\r\n    Assert.assertEquals(3, counter.get());\r\n    isGood[0] = 2;\r\n    try {\r\n        provider.getProxy().proxy.getStats();\r\n        Assert.fail(\"Should fail since previously successful proxy now fails \");\r\n    } catch (Exception ex) {\r\n        Assert.assertTrue(ex instanceof IOException);\r\n    }\r\n    Assert.assertEquals(4, counter.get());\r\n    provider.performFailover(provider.getProxy().proxy);\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(2, stats[0]);\r\n    Assert.assertEquals(5, counter.get());\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(2, stats[0]);\r\n    Assert.assertEquals(6, counter.get());\r\n    isGood[0] = 1;\r\n    try {\r\n        provider.getProxy().proxy.getStats();\r\n        Assert.fail(\"Should fail since previously successful proxy now fails \");\r\n    } catch (Exception ex) {\r\n        Assert.assertTrue(ex instanceof IOException);\r\n    }\r\n    Assert.assertEquals(7, counter.get());\r\n    provider.performFailover(provider.getProxy().proxy);\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(1, stats[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testFileNotFoundExceptionWithSingleProxy",
  "errType" : [ "MultiException", "RemoteException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testFileNotFoundExceptionWithSingleProxy() throws Exception\n{\r\n    ClientProtocol active = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(active.getBlockLocations(anyString(), anyLong(), anyLong())).thenThrow(new RemoteException(\"java.io.FileNotFoundException\", \"File does not exist!\"));\r\n    ClientProtocol standby = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(standby.getBlockLocations(anyString(), anyLong(), anyLong())).thenThrow(new RemoteException(\"org.apache.hadoop.ipc.StandbyException\", \"Standby NameNode\"));\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(standby, active));\r\n    try {\r\n        provider.getProxy().proxy.getBlockLocations(\"/tmp/test.file\", 0L, 20L);\r\n        Assert.fail(\"Should fail since the active namenode throws\" + \" FileNotFoundException!\");\r\n    } catch (MultiException me) {\r\n        for (Exception ex : me.getExceptions().values()) {\r\n            Exception rEx = ((RemoteException) ex).unwrapRemoteException();\r\n            if (rEx instanceof StandbyException) {\r\n                continue;\r\n            }\r\n            Assert.assertTrue(rEx instanceof FileNotFoundException);\r\n        }\r\n    }\r\n    provider.performFailover(active);\r\n    try {\r\n        provider.getProxy().proxy.getBlockLocations(\"/tmp/test.file\", 0L, 20L);\r\n        Assert.fail(\"Should fail since the active namenode throws\" + \" FileNotFoundException!\");\r\n    } catch (RemoteException ex) {\r\n        Exception rEx = ex.unwrapRemoteException();\r\n        if (rEx instanceof StandbyException) {\r\n            Mockito.verify(active).getBlockLocations(anyString(), anyLong(), anyLong());\r\n            Mockito.verify(standby, Mockito.times(2)).getBlockLocations(anyString(), anyLong(), anyLong());\r\n        } else {\r\n            Assert.assertTrue(rEx instanceof FileNotFoundException);\r\n            Mockito.verify(active, Mockito.times(2)).getBlockLocations(anyString(), anyLong(), anyLong());\r\n            Mockito.verify(standby).getBlockLocations(anyString(), anyLong(), anyLong());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testSingleProxyFailover",
  "errType" : [ "RemoteException", "RemoteException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testSingleProxyFailover() throws Exception\n{\r\n    String singleNS = \"mycluster-\" + Time.monotonicNow();\r\n    URI singleNNUri = new URI(\"hdfs://\" + singleNS);\r\n    Configuration singleConf = new Configuration();\r\n    singleConf.set(HdfsClientConfigKeys.DFS_NAMESERVICES, singleNS);\r\n    singleConf.set(HdfsClientConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + singleNS, \"nn1\");\r\n    singleConf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + singleNS + \".nn1\", RandomStringUtils.randomAlphabetic(8) + \".foo.bar:9820\");\r\n    ClientProtocol active = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(active.getBlockLocations(anyString(), anyLong(), anyLong())).thenThrow(new RemoteException(\"java.io.FileNotFoundException\", \"File does not exist!\"));\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(singleConf, singleNNUri, ClientProtocol.class, createFactory(active));\r\n    try {\r\n        provider.getProxy().proxy.getBlockLocations(\"/tmp/test.file\", 0L, 20L);\r\n        Assert.fail(\"Should fail since the active namenode throws\" + \" FileNotFoundException!\");\r\n    } catch (RemoteException ex) {\r\n        Exception rEx = ex.unwrapRemoteException();\r\n        Assert.assertTrue(rEx instanceof FileNotFoundException);\r\n    }\r\n    provider.performFailover(active);\r\n    try {\r\n        provider.getProxy().proxy.getBlockLocations(\"/tmp/test.file\", 0L, 20L);\r\n        Assert.fail(\"Should fail since the active namenode throws\" + \" FileNotFoundException!\");\r\n    } catch (RemoteException ex) {\r\n        Exception rEx = ex.unwrapRemoteException();\r\n        Assert.assertTrue(rEx instanceof IOException);\r\n        Assert.assertTrue(rEx.getMessage().equals(\"No valid proxies left.\" + \" All NameNode proxies have failed over.\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testPerformFailoverWith3Proxies",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 48,
  "sourceCodeText" : "void testPerformFailoverWith3Proxies() throws Exception\n{\r\n    conf.set(HdfsClientConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + \".\" + ns, \"nn1,nn2,nn3\");\r\n    conf.set(HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY + \".\" + ns + \".nn3\", \"machine3.foo.bar:8020\");\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    final int[] isGood = { 1 };\r\n    final ClientProtocol goodMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(goodMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            counter.incrementAndGet();\r\n            if (isGood[0] == 1) {\r\n                Thread.sleep(1000);\r\n                return new long[] { 1 };\r\n            }\r\n            throw new IOException(\"Was Good mock !!\");\r\n        }\r\n    });\r\n    final ClientProtocol badMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(badMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            counter.incrementAndGet();\r\n            if (isGood[0] == 2) {\r\n                Thread.sleep(1000);\r\n                return new long[] { 2 };\r\n            }\r\n            throw new IOException(\"Bad mock !!\");\r\n        }\r\n    });\r\n    final ClientProtocol worseMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(worseMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            counter.incrementAndGet();\r\n            if (isGood[0] == 3) {\r\n                Thread.sleep(1000);\r\n                return new long[] { 3 };\r\n            }\r\n            throw new IOException(\"Worse mock !!\");\r\n        }\r\n    });\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(goodMock, badMock, worseMock));\r\n    long[] stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(1, stats[0]);\r\n    Assert.assertEquals(3, counter.get());\r\n    Mockito.verify(badMock).getStats();\r\n    Mockito.verify(goodMock).getStats();\r\n    Mockito.verify(worseMock).getStats();\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(1, stats[0]);\r\n    Mockito.verifyNoMoreInteractions(badMock);\r\n    Mockito.verifyNoMoreInteractions(worseMock);\r\n    Assert.assertEquals(4, counter.get());\r\n    isGood[0] = 2;\r\n    try {\r\n        provider.getProxy().proxy.getStats();\r\n        Assert.fail(\"Should fail since previously successful proxy now fails \");\r\n    } catch (Exception ex) {\r\n        Assert.assertTrue(ex instanceof IOException);\r\n    }\r\n    Assert.assertEquals(5, counter.get());\r\n    provider.performFailover(provider.getProxy().proxy);\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(2, stats[0]);\r\n    Assert.assertEquals(7, counter.get());\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(2, stats[0]);\r\n    Assert.assertEquals(8, counter.get());\r\n    isGood[0] = 3;\r\n    try {\r\n        provider.getProxy().proxy.getStats();\r\n        Assert.fail(\"Should fail since previously successful proxy now fails \");\r\n    } catch (Exception ex) {\r\n        Assert.assertTrue(ex instanceof IOException);\r\n    }\r\n    Assert.assertEquals(9, counter.get());\r\n    provider.performFailover(provider.getProxy().proxy);\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(3, stats[0]);\r\n    Assert.assertEquals(11, counter.get());\r\n    stats = provider.getProxy().proxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(3, stats[0]);\r\n    Assert.assertEquals(12, counter.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testHedgingWhenFileNotFoundException",
  "errType" : [ "MultiException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testHedgingWhenFileNotFoundException() throws Exception\n{\r\n    ClientProtocol active = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(active.getBlockLocations(anyString(), anyLong(), anyLong())).thenThrow(new RemoteException(\"java.io.FileNotFoundException\", \"File does not exist!\"));\r\n    ClientProtocol standby = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(standby.getBlockLocations(anyString(), anyLong(), anyLong())).thenThrow(new RemoteException(\"org.apache.hadoop.ipc.StandbyException\", \"Standby NameNode\"));\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(active, standby));\r\n    try {\r\n        provider.getProxy().proxy.getBlockLocations(\"/tmp/test.file\", 0L, 20L);\r\n        Assert.fail(\"Should fail since the active namenode throws\" + \" FileNotFoundException!\");\r\n    } catch (MultiException me) {\r\n        for (Exception ex : me.getExceptions().values()) {\r\n            Exception rEx = ((RemoteException) ex).unwrapRemoteException();\r\n            if (rEx instanceof StandbyException) {\r\n                continue;\r\n            }\r\n            Assert.assertTrue(rEx instanceof FileNotFoundException);\r\n        }\r\n    }\r\n    Mockito.verify(active).getBlockLocations(anyString(), anyLong(), anyLong());\r\n    Mockito.verify(standby).getBlockLocations(anyString(), anyLong(), anyLong());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testHedgingWhenConnectException",
  "errType" : [ "MultiException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testHedgingWhenConnectException() throws Exception\n{\r\n    ClientProtocol active = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(active.getStats()).thenThrow(new ConnectException());\r\n    ClientProtocol standby = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(standby.getStats()).thenThrow(new RemoteException(\"org.apache.hadoop.ipc.StandbyException\", \"Standby NameNode\"));\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(active, standby));\r\n    try {\r\n        provider.getProxy().proxy.getStats();\r\n        Assert.fail(\"Should fail since the active namenode throws\" + \" ConnectException!\");\r\n    } catch (MultiException me) {\r\n        for (Exception ex : me.getExceptions().values()) {\r\n            if (ex instanceof RemoteException) {\r\n                Exception rEx = ((RemoteException) ex).unwrapRemoteException();\r\n                Assert.assertTrue(\"Unexpected RemoteException: \" + rEx.getMessage(), rEx instanceof StandbyException);\r\n            } else {\r\n                Assert.assertTrue(ex instanceof ConnectException);\r\n            }\r\n        }\r\n    }\r\n    Mockito.verify(active).getStats();\r\n    Mockito.verify(standby).getStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testHedgingWhenConnectAndEOFException",
  "errType" : [ "MultiException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testHedgingWhenConnectAndEOFException() throws Exception\n{\r\n    ClientProtocol active = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(active.getStats()).thenThrow(new EOFException());\r\n    ClientProtocol standby = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(standby.getStats()).thenThrow(new ConnectException());\r\n    RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(active, standby));\r\n    try {\r\n        provider.getProxy().proxy.getStats();\r\n        Assert.fail(\"Should fail since both active and standby namenodes throw\" + \" Exceptions!\");\r\n    } catch (MultiException me) {\r\n        for (Exception ex : me.getExceptions().values()) {\r\n            if (!(ex instanceof ConnectException) && !(ex instanceof EOFException)) {\r\n                Assert.fail(\"Unexpected Exception \" + ex.getMessage());\r\n            }\r\n        }\r\n    }\r\n    Mockito.verify(active).getStats();\r\n    Mockito.verify(standby).getStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "testHedgingMultiThreads",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testHedgingMultiThreads() throws Exception\n{\r\n    final AtomicInteger counter = new AtomicInteger(0);\r\n    final ClientProtocol delayMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(delayMock.getStats()).thenAnswer(new Answer<long[]>() {\r\n\r\n        @Override\r\n        public long[] answer(InvocationOnMock invocation) throws Throwable {\r\n            int flag = counter.incrementAndGet();\r\n            Thread.sleep(2000);\r\n            if (flag == 1) {\r\n                return new long[] { 1 };\r\n            } else {\r\n                throw new IOException(\"Exception for test.\");\r\n            }\r\n        }\r\n    });\r\n    final ClientProtocol badMock = Mockito.mock(ClientProtocol.class);\r\n    Mockito.when(badMock.getStats()).thenThrow(new IOException(\"Bad mock !!\"));\r\n    final RequestHedgingProxyProvider<ClientProtocol> provider = new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class, createFactory(delayMock, badMock));\r\n    final ClientProtocol delayProxy = provider.getProxy().proxy;\r\n    long[] stats = delayProxy.getStats();\r\n    Assert.assertTrue(stats.length == 1);\r\n    Assert.assertEquals(1, stats[0]);\r\n    Assert.assertEquals(1, counter.get());\r\n    Thread t = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                Thread.sleep(1000);\r\n                provider.performFailover(delayProxy);\r\n            } catch (Exception e) {\r\n                e.printStackTrace();\r\n            }\r\n        }\r\n    };\r\n    t.start();\r\n    LambdaTestUtils.intercept(IOException.class, \"Exception for test.\", delayProxy::getStats);\r\n    t.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\server\\namenode\\ha",
  "methodName" : "createFactory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HAProxyFactory<ClientProtocol> createFactory(ClientProtocol... protos)\n{\r\n    final Iterator<ClientProtocol> iterator = Lists.newArrayList(protos).iterator();\r\n    return new HAProxyFactory<ClientProtocol>() {\r\n\r\n        @Override\r\n        public ClientProtocol createProxy(Configuration conf, InetSocketAddress nnAddr, Class<ClientProtocol> xface, UserGroupInformation ugi, boolean withRetries, AtomicBoolean fallbackToSimpleAuth) throws IOException {\r\n            return iterator.next();\r\n        }\r\n\r\n        @Override\r\n        public ClientProtocol createProxy(Configuration conf, InetSocketAddress nnAddr, Class<ClientProtocol> xface, UserGroupInformation ugi, boolean withRetries) throws IOException {\r\n            return iterator.next();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "buildConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration buildConf(String credential, String tokenExpires, String clientId, String refreshURL)\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(CredentialBasedAccessTokenProvider.OAUTH_CREDENTIAL_KEY, credential);\r\n    conf.set(ACCESS_TOKEN_PROVIDER_KEY, ConfCredentialBasedAccessTokenProvider.class.getName());\r\n    conf.set(OAUTH_CLIENT_ID_KEY, clientId);\r\n    conf.set(OAUTH_REFRESH_URL_KEY, refreshURL);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "refreshUrlIsCorrect",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void refreshUrlIsCorrect() throws IOException\n{\r\n    final int PORT = ServerSocketUtil.getPort(0, 20);\r\n    final String REFRESH_ADDRESS = \"http://localhost:\" + PORT + \"/refresh\";\r\n    long tokenExpires = 0;\r\n    Configuration conf = buildConf(\"myreallycoolcredential\", Long.toString(tokenExpires), CLIENT_ID_FOR_TESTING, REFRESH_ADDRESS);\r\n    Timer mockTimer = mock(Timer.class);\r\n    when(mockTimer.now()).thenReturn(tokenExpires + 1000l);\r\n    AccessTokenProvider credProvider = new ConfCredentialBasedAccessTokenProvider(mockTimer);\r\n    credProvider.setConf(conf);\r\n    ClientAndServer mockServer = startClientAndServer(PORT);\r\n    HttpRequest expectedRequest = request().withMethod(\"POST\").withPath(\"/refresh\").withBody(ParameterBody.params(Parameter.param(CLIENT_SECRET, \"myreallycoolcredential\"), Parameter.param(GRANT_TYPE, CLIENT_CREDENTIALS), Parameter.param(CLIENT_ID, CLIENT_ID_FOR_TESTING)));\r\n    MockServerClient mockServerClient = new MockServerClient(\"localhost\", PORT);\r\n    Map<String, Object> map = new TreeMap<>();\r\n    map.put(EXPIRES_IN, \"0987654321\");\r\n    map.put(TOKEN_TYPE, \"bearer\");\r\n    map.put(ACCESS_TOKEN, \"new access token\");\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    HttpResponse resp = response().withStatusCode(HttpStatus.SC_OK).withHeaders(CONTENT_TYPE_APPLICATION_JSON).withBody(mapper.writeValueAsString(map));\r\n    mockServerClient.when(expectedRequest, exactly(1)).respond(resp);\r\n    assertEquals(\"new access token\", credProvider.getAccessToken());\r\n    mockServerClient.verify(expectedRequest);\r\n    mockServerClient.clear(expectedRequest);\r\n    mockServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testInvalid",
  "errType" : [ "NullPointerException", "NullPointerException", "IllegalArgumentException", "NullPointerException", "IllegalArgumentException", "NullPointerException", "IllegalArgumentException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testInvalid()\n{\r\n    try {\r\n        new ErasureCodingPolicy(null, SCHEMA_1, 123, (byte) -1);\r\n        fail(\"Instantiated invalid ErasureCodingPolicy\");\r\n    } catch (NullPointerException e) {\r\n    }\r\n    try {\r\n        new ErasureCodingPolicy(\"policy\", null, 123, (byte) -1);\r\n        fail(\"Instantiated invalid ErasureCodingPolicy\");\r\n    } catch (NullPointerException e) {\r\n    }\r\n    try {\r\n        new ErasureCodingPolicy(\"policy\", SCHEMA_1, -1, (byte) -1);\r\n        fail(\"Instantiated invalid ErasureCodingPolicy\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"cellSize\", e);\r\n    }\r\n    try {\r\n        new ErasureCodingPolicy(null, 1024, (byte) -1);\r\n        fail(\"Instantiated invalid ErasureCodingPolicy\");\r\n    } catch (NullPointerException e) {\r\n    }\r\n    try {\r\n        new ErasureCodingPolicy(SCHEMA_1, -1, (byte) -1);\r\n        fail(\"Instantiated invalid ErasureCodingPolicy\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"cellSize\", e);\r\n    }\r\n    try {\r\n        new ErasureCodingPolicy(null, 1024);\r\n        fail(\"Instantiated invalid ErasureCodingPolicy\");\r\n    } catch (NullPointerException e) {\r\n    }\r\n    try {\r\n        new ErasureCodingPolicy(SCHEMA_1, -1);\r\n        fail(\"Instantiated invalid ErasureCodingPolicy\");\r\n    } catch (IllegalArgumentException e) {\r\n        GenericTestUtils.assertExceptionContains(\"cellSize\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 7,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testEqualsAndHashCode",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testEqualsAndHashCode()\n{\r\n    ErasureCodingPolicy[] policies = new ErasureCodingPolicy[] { new ErasureCodingPolicy(\"one\", SCHEMA_1, 1024, (byte) 1), new ErasureCodingPolicy(\"two\", SCHEMA_1, 1024, (byte) 1), new ErasureCodingPolicy(\"one\", SCHEMA_2, 1024, (byte) 1), new ErasureCodingPolicy(\"one\", SCHEMA_1, 2048, (byte) 1), new ErasureCodingPolicy(\"one\", SCHEMA_1, 1024, (byte) 3) };\r\n    for (int i = 0; i < policies.length; i++) {\r\n        final ErasureCodingPolicy ei = policies[i];\r\n        ErasureCodingPolicy temp = new ErasureCodingPolicy(ei.getName(), ei.getSchema(), ei.getCellSize(), ei.getId());\r\n        assertEquals(ei, temp);\r\n        assertEquals(ei.hashCode(), temp.hashCode());\r\n        for (int j = 0; j < policies.length; j++) {\r\n            final ErasureCodingPolicy ej = policies[j];\r\n            if (i == j) {\r\n                assertEquals(ei, ej);\r\n                assertEquals(ei.hashCode(), ej.hashCode());\r\n            } else {\r\n                assertNotEquals(ei, ej);\r\n                assertNotEquals(ei, ej.hashCode());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testReadOnly",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReadOnly()\n{\r\n    for (Method m : ALL_METHODS) {\r\n        boolean expected = READONLY_METHOD_NAMES.contains(m.getName());\r\n        checkIsReadOnly(m.getName(), expected);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "checkIsReadOnly",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkIsReadOnly(String methodName, boolean expected)\n{\r\n    for (Method m : ALL_METHODS) {\r\n        if (m.getName().equals(methodName)) {\r\n            assertEquals(\"Expected ReadOnly for method '\" + methodName + \"' to be \" + expected, m.isAnnotationPresent(ReadOnly.class), expected);\r\n            return;\r\n        }\r\n    }\r\n    throw new IllegalArgumentException(\"Unknown method name: \" + methodName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testPacket",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPacket() throws Exception\n{\r\n    Random r = new Random(12345L);\r\n    byte[] data = new byte[chunkSize];\r\n    r.nextBytes(data);\r\n    byte[] checksum = new byte[checksumSize];\r\n    r.nextBytes(checksum);\r\n    DataOutputBuffer os = new DataOutputBuffer(data.length * 2);\r\n    byte[] packetBuf = new byte[data.length * 2];\r\n    DFSPacket p = new DFSPacket(packetBuf, maxChunksPerPacket, 0, 0, checksumSize, false);\r\n    p.setSyncBlock(true);\r\n    p.writeData(data, 0, data.length);\r\n    p.writeChecksum(checksum, 0, checksum.length);\r\n    p.writeTo(os);\r\n    int headerLen = PacketHeader.PKT_MAX_HEADER_LEN;\r\n    byte[] readBuf = os.getData();\r\n    assertArrayRegionsEqual(readBuf, headerLen, checksum, 0, checksum.length);\r\n    assertArrayRegionsEqual(readBuf, headerLen + checksum.length, data, 0, data.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "assertArrayRegionsEqual",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertArrayRegionsEqual(byte[] buf1, int off1, byte[] buf2, int off2, int len)\n{\r\n    for (int i = 0; i < len; i++) {\r\n        if (buf1[off1 + i] != buf2[off2 + i]) {\r\n            Assert.fail(\"arrays differ at byte \" + i + \". \" + \"The first array has \" + (int) buf1[off1 + i] + \", but the second array has \" + (int) buf2[off2 + i]);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testRemoveOffset",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRemoveOffset() throws IOException\n{\r\n    {\r\n        String s = \"http://test/Abc?Length=99\";\r\n        assertEquals(s, WebHdfsFileSystem.removeOffsetParam(new URL(s)).toString());\r\n    }\r\n    {\r\n        String s = \"http://test/Abc\";\r\n        assertEquals(s, WebHdfsFileSystem.removeOffsetParam(new URL(s)).toString());\r\n    }\r\n    {\r\n        String s = \"http://test/Abc?offset=10&Length=99\";\r\n        assertEquals(\"http://test/Abc?Length=99\", WebHdfsFileSystem.removeOffsetParam(new URL(s)).toString());\r\n    }\r\n    {\r\n        String s = \"http://test/Abc?op=read&OFFset=10&Length=99\";\r\n        assertEquals(\"http://test/Abc?op=read&Length=99\", WebHdfsFileSystem.removeOffsetParam(new URL(s)).toString());\r\n    }\r\n    {\r\n        String s = \"http://test/Abc?Length=99&offset=10\";\r\n        assertEquals(\"http://test/Abc?Length=99\", WebHdfsFileSystem.removeOffsetParam(new URL(s)).toString());\r\n    }\r\n    {\r\n        String s = \"http://test/Abc?offset=10\";\r\n        assertEquals(\"http://test/Abc\", WebHdfsFileSystem.removeOffsetParam(new URL(s)).toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testConnConfiguratior",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testConnConfiguratior() throws IOException\n{\r\n    final URL u = new URL(\"http://localhost\");\r\n    final List<HttpURLConnection> conns = Lists.newArrayList();\r\n    URLConnectionFactory fc = new URLConnectionFactory(new ConnectionConfigurator() {\r\n\r\n        @Override\r\n        public HttpURLConnection configure(HttpURLConnection conn) throws IOException {\r\n            Assert.assertEquals(u, conn.getURL());\r\n            conns.add(conn);\r\n            return conn;\r\n        }\r\n    });\r\n    fc.openConnection(u);\r\n    Assert.assertEquals(1, conns.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testSSLInitFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSSLInitFailure() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(SSLFactory.SSL_HOSTNAME_VERIFIER_KEY, \"foo\");\r\n    GenericTestUtils.LogCapturer logs = GenericTestUtils.LogCapturer.captureLogs(LoggerFactory.getLogger(URLConnectionFactory.class));\r\n    URLConnectionFactory.newDefaultURLConnectionFactory(conf);\r\n    Assert.assertTrue(\"Expected log for ssl init failure not found!\", logs.getOutput().contains(\"Cannot load customized ssl related configuration\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testSSLFactoryCleanup",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testSSLFactoryCleanup() throws Exception\n{\r\n    String baseDir = GenericTestUtils.getTempPath(TestURLConnectionFactory.class.getSimpleName());\r\n    File base = new File(baseDir);\r\n    FileUtil.fullyDelete(base);\r\n    base.mkdirs();\r\n    String keystoreDir = new File(baseDir).getAbsolutePath();\r\n    String sslConfDir = KeyStoreTestUtil.getClasspathDir(TestURLConnectionFactory.class);\r\n    Configuration conf = new Configuration();\r\n    KeyStoreTestUtil.setupSSLConfig(keystoreDir, sslConfDir, conf, false, true);\r\n    Configuration sslConf = KeyStoreTestUtil.getSslConfig();\r\n    sslConf.set(\"fs.defaultFS\", \"swebhdfs://localhost\");\r\n    FileSystem fs = FileSystem.get(sslConf);\r\n    ThreadGroup threadGroup = Thread.currentThread().getThreadGroup();\r\n    while (threadGroup.getParent() != null) {\r\n        threadGroup = threadGroup.getParent();\r\n    }\r\n    Thread[] threads = new Thread[threadGroup.activeCount()];\r\n    threadGroup.enumerate(threads);\r\n    Thread reloaderThread = null;\r\n    for (Thread thread : threads) {\r\n        if ((thread.getName() != null) && (thread.getName().contains(SSL_MONITORING_THREAD_NAME))) {\r\n            reloaderThread = thread;\r\n        }\r\n    }\r\n    Assert.assertTrue(\"Reloader is not alive\", reloaderThread.isAlive());\r\n    fs.close();\r\n    boolean reloaderStillAlive = true;\r\n    for (int i = 0; i < 10; i++) {\r\n        reloaderStillAlive = reloaderThread.isAlive();\r\n        if (!reloaderStillAlive) {\r\n            break;\r\n        }\r\n        Thread.sleep(1000);\r\n    }\r\n    Assert.assertFalse(\"Reloader is still alive\", reloaderStillAlive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    listenSocket = new ServerSocket();\r\n    listenSocket.bind(null);\r\n    bindAddr = NetUtils.getHostPortString((InetSocketAddress) listenSocket.getLocalSocketAddress());\r\n    redirectResponse = \"HTTP/1.1 307 Redirect\\r\\n\" + \"Location: http://\" + bindAddr + \"/path\\r\\n\" + \"Connection: close\\r\\n\\r\\n\";\r\n    p = new Path(\"webhdfs://\" + bindAddr + \"/path\");\r\n    fs = p.getFileSystem(new Configuration());\r\n    executor = Executors.newSingleThreadExecutor();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void teardown() throws IOException\n{\r\n    if (listenSocket != null) {\r\n        listenSocket.close();\r\n    }\r\n    if (executor != null) {\r\n        executor.shutdownNow();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testGetOp",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetOp() throws Exception\n{\r\n    Future<String> future = contentLengthFuture(errResponse);\r\n    try {\r\n        fs.getFileStatus(p);\r\n        Assert.fail();\r\n    } catch (IOException ioe) {\r\n    }\r\n    Assert.assertEquals(null, getContentLength(future));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testGetOpWithRedirect",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetOpWithRedirect()\n{\r\n    Future<String> future1 = contentLengthFuture(redirectResponse);\r\n    Future<String> future2 = contentLengthFuture(errResponse);\r\n    Future<String> future3 = contentLengthFuture(errResponse);\r\n    try {\r\n        fs.open(p).read();\r\n        Assert.fail();\r\n    } catch (IOException ioe) {\r\n    }\r\n    Assert.assertEquals(null, getContentLength(future1));\r\n    Assert.assertEquals(null, getContentLength(future2));\r\n    Assert.assertEquals(null, getContentLength(future3));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testPutOp",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPutOp()\n{\r\n    Future<String> future = contentLengthFuture(errResponse);\r\n    try {\r\n        fs.mkdirs(p);\r\n        Assert.fail();\r\n    } catch (IOException ioe) {\r\n    }\r\n    Assert.assertEquals(\"0\", getContentLength(future));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testPutOpWithRedirect",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPutOpWithRedirect()\n{\r\n    Future<String> future1 = contentLengthFuture(redirectResponse);\r\n    Future<String> future2 = contentLengthFuture(errResponse);\r\n    try {\r\n        FSDataOutputStream os = fs.create(p);\r\n        os.write(new byte[] { 0 });\r\n        os.close();\r\n        Assert.fail();\r\n    } catch (IOException ioe) {\r\n    }\r\n    Assert.assertEquals(\"0\", getContentLength(future1));\r\n    Assert.assertEquals(\"chunked\", getContentLength(future2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testPostOp",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testPostOp()\n{\r\n    Future<String> future = contentLengthFuture(errResponse);\r\n    try {\r\n        fs.concat(p, new Path[] { p });\r\n        Assert.fail();\r\n    } catch (IOException ioe) {\r\n    }\r\n    Assert.assertEquals(\"0\", getContentLength(future));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testPostOpWithRedirect",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testPostOpWithRedirect()\n{\r\n    Future<String> future1 = contentLengthFuture(redirectResponse);\r\n    Future<String> future2 = contentLengthFuture(errResponse);\r\n    try {\r\n        FSDataOutputStream os = fs.append(p);\r\n        os.write(new byte[] { 0 });\r\n        os.close();\r\n        Assert.fail();\r\n    } catch (IOException ioe) {\r\n    }\r\n    Assert.assertEquals(\"0\", getContentLength(future1));\r\n    Assert.assertEquals(\"chunked\", getContentLength(future2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testDelete",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDelete()\n{\r\n    Future<String> future = contentLengthFuture(errResponse);\r\n    try {\r\n        fs.delete(p, false);\r\n        Assert.fail();\r\n    } catch (IOException ioe) {\r\n    }\r\n    Assert.assertEquals(null, getContentLength(future));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getContentLength",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getContentLength(Future<String> future)\n{\r\n    String request = null;\r\n    try {\r\n        request = future.get(2, TimeUnit.SECONDS);\r\n    } catch (Exception e) {\r\n        Assert.fail(e.toString());\r\n    }\r\n    Matcher matcher = contentLengthPattern.matcher(request);\r\n    return matcher.find() ? matcher.group(2) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "contentLengthFuture",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Future<String> contentLengthFuture(final String response)\n{\r\n    return executor.submit(new Callable<String>() {\r\n\r\n        @Override\r\n        public String call() throws Exception {\r\n            Socket client = listenSocket.accept();\r\n            client.setSoTimeout(2000);\r\n            try {\r\n                client.getOutputStream().write(response.getBytes());\r\n                client.shutdownOutput();\r\n                byte[] buf = new byte[4 * 1024];\r\n                StringBuilder sb = new StringBuilder();\r\n                for (; ; ) {\r\n                    int n = client.getInputStream().read(buf);\r\n                    if (n <= 0) {\r\n                        break;\r\n                    }\r\n                    sb.append(new String(buf, 0, n, \"UTF-8\"));\r\n                }\r\n                return sb.toString();\r\n            } finally {\r\n                client.close();\r\n            }\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testLoadECPolicy",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void testLoadECPolicy() throws Exception\n{\r\n    PrintWriter out = new PrintWriter(new FileWriter(POLICY_FILE));\r\n    out.println(\"<?xml version=\\\"1.0\\\"?>\");\r\n    out.println(\"<configuration>\");\r\n    out.println(\"<layoutversion>1</layoutversion>\");\r\n    out.println(\"<schemas>\");\r\n    out.println(\"  <schema id=\\\"RSk12m4\\\">\");\r\n    out.println(\"    <codec>rs</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"  <schema id=\\\"RS-legacyk12m4\\\">\");\r\n    out.println(\"    <codec>rs-legacy</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"</schemas>\");\r\n    out.println(\"<policies>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RSk12m4</schema>\");\r\n    out.println(\"    <cellsize>131072</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RS-legacyk12m4</schema>\");\r\n    out.println(\"    <cellsize>262144</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"</policies>\");\r\n    out.println(\"</configuration>\");\r\n    out.close();\r\n    ECPolicyLoader ecPolicyLoader = new ECPolicyLoader();\r\n    List<ErasureCodingPolicy> policies = ecPolicyLoader.loadPolicy(POLICY_FILE);\r\n    assertEquals(2, policies.size());\r\n    ErasureCodingPolicy policy1 = policies.get(0);\r\n    ECSchema schema1 = policy1.getSchema();\r\n    assertEquals(131072, policy1.getCellSize());\r\n    assertEquals(0, schema1.getExtraOptions().size());\r\n    assertEquals(12, schema1.getNumDataUnits());\r\n    assertEquals(4, schema1.getNumParityUnits());\r\n    assertEquals(\"rs\", schema1.getCodecName());\r\n    ErasureCodingPolicy policy2 = policies.get(1);\r\n    ECSchema schema2 = policy2.getSchema();\r\n    assertEquals(262144, policy2.getCellSize());\r\n    assertEquals(0, schema2.getExtraOptions().size());\r\n    assertEquals(12, schema2.getNumDataUnits());\r\n    assertEquals(4, schema2.getNumParityUnits());\r\n    assertEquals(\"rs-legacy\", schema2.getCodecName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testNullECSchemaOptionValue",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testNullECSchemaOptionValue() throws Exception\n{\r\n    PrintWriter out = new PrintWriter(new FileWriter(POLICY_FILE));\r\n    out.println(\"<?xml version=\\\"1.0\\\"?>\");\r\n    out.println(\"<configuration>\");\r\n    out.println(\"<layoutversion>1</layoutversion>\");\r\n    out.println(\"<schemas>\");\r\n    out.println(\"  <schema id=\\\"RSk12m4\\\">\");\r\n    out.println(\"    <codec>RS</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"  <schema id=\\\"RS-legacyk12m4\\\">\");\r\n    out.println(\"    <codec>RS-legacy</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"    <option></option>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"</schemas>\");\r\n    out.println(\"<policies>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RS-legacyk12m4</schema>\");\r\n    out.println(\"    <cellsize>1024</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RSk12m4</schema>\");\r\n    out.println(\"    <cellsize>20480</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"</policies>\");\r\n    out.println(\"</configuration>\");\r\n    out.close();\r\n    ECPolicyLoader ecPolicyLoader = new ECPolicyLoader();\r\n    try {\r\n        ecPolicyLoader.loadPolicy(POLICY_FILE);\r\n        fail(\"IllegalArgumentException should be thrown for null value\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"Value of <option> is null\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testRepeatECSchema",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testRepeatECSchema() throws Exception\n{\r\n    PrintWriter out = new PrintWriter(new FileWriter(POLICY_FILE));\r\n    out.println(\"<?xml version=\\\"1.0\\\"?>\");\r\n    out.println(\"<configuration>\");\r\n    out.println(\"<layoutversion>1</layoutversion>\");\r\n    out.println(\"<schemas>\");\r\n    out.println(\"  <schema id=\\\"RSk12m4\\\">\");\r\n    out.println(\"    <codec>RS-legacy</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"  <schema id=\\\"RS-legacyk12m4\\\">\");\r\n    out.println(\"    <codec>RS-legacy</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"</schemas>\");\r\n    out.println(\"<policies>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RS-legacyk12m4</schema>\");\r\n    out.println(\"    <cellsize>1024</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RSk12m4</schema>\");\r\n    out.println(\"    <cellsize>20480</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"</policies>\");\r\n    out.println(\"</configuration>\");\r\n    out.close();\r\n    ECPolicyLoader ecPolicyLoader = new ECPolicyLoader();\r\n    try {\r\n        ecPolicyLoader.loadPolicy(POLICY_FILE);\r\n        fail(\"RuntimeException should be thrown for repetitive elements\");\r\n    } catch (RuntimeException e) {\r\n        assertExceptionContains(\"Repetitive schemas in EC policy\" + \" configuration file: RS-legacyk12m4\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testBadECLayoutVersion",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testBadECLayoutVersion() throws Exception\n{\r\n    PrintWriter out = new PrintWriter(new FileWriter(POLICY_FILE));\r\n    out.println(\"<?xml version=\\\"1.0\\\"?>\");\r\n    out.println(\"<configuration>\");\r\n    out.println(\"<layoutversion>3</layoutversion>\");\r\n    out.println(\"<schemas>\");\r\n    out.println(\"  <schema id=\\\"RSk12m4\\\">\");\r\n    out.println(\"    <codec>RS</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"  <schema id=\\\"RS-legacyk12m4\\\">\");\r\n    out.println(\"    <codec>RS-legacy</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"</schemas>\");\r\n    out.println(\"<policies>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RSk12m4</schema>\");\r\n    out.println(\"    <cellsize>1024</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"</policies>\");\r\n    out.println(\"</configuration>\");\r\n    out.close();\r\n    ECPolicyLoader ecPolicyLoader = new ECPolicyLoader();\r\n    try {\r\n        ecPolicyLoader.loadPolicy(POLICY_FILE);\r\n        fail(\"RuntimeException should be thrown for bad layoutversion\");\r\n    } catch (RuntimeException e) {\r\n        assertExceptionContains(\"The parse failed because of \" + \"bad layoutversion value\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testBadECCellsize",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testBadECCellsize() throws Exception\n{\r\n    PrintWriter out = new PrintWriter(new FileWriter(POLICY_FILE));\r\n    out.println(\"<?xml version=\\\"1.0\\\"?>\");\r\n    out.println(\"<configuration>\");\r\n    out.println(\"<layoutversion>1</layoutversion>\");\r\n    out.println(\"<schemas>\");\r\n    out.println(\"  <schema id=\\\"RSk12m4\\\">\");\r\n    out.println(\"    <codec>RS</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"  <schema id=\\\"RS-legacyk12m4\\\">\");\r\n    out.println(\"    <codec>RS-legacy</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"</schemas>\");\r\n    out.println(\"<policies>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RSk12m4</schema>\");\r\n    out.println(\"    <cellsize>free</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"</policies>\");\r\n    out.println(\"</configuration>\");\r\n    out.close();\r\n    ECPolicyLoader ecPolicyLoader = new ECPolicyLoader();\r\n    try {\r\n        ecPolicyLoader.loadPolicy(POLICY_FILE);\r\n        fail(\"IllegalArgumentException should be thrown for bad policy\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertExceptionContains(\"Bad EC policy cellsize value free is found.\" + \" It should be an integer\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\util",
  "methodName" : "testBadECPolicy",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testBadECPolicy() throws Exception\n{\r\n    PrintWriter out = new PrintWriter(new FileWriter(POLICY_FILE));\r\n    out.println(\"<?xml version=\\\"1.0\\\"?>\");\r\n    out.println(\"<configuration>\");\r\n    out.println(\"<layoutversion>1</layoutversion>\");\r\n    out.println(\"<schemas>\");\r\n    out.println(\"  <schema id=\\\"RSk12m4\\\">\");\r\n    out.println(\"    <codec>RS</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"  <schema id=\\\"RS-legacyk12m4\\\">\");\r\n    out.println(\"    <codec>RS-legacy</codec>\");\r\n    out.println(\"    <k>12</k>\");\r\n    out.println(\"    <m>4</m>\");\r\n    out.println(\"  </schema>\");\r\n    out.println(\"</schemas>\");\r\n    out.println(\"<policies>\");\r\n    out.println(\"  <policy>\");\r\n    out.println(\"    <schema>RSk12m4</schema>\");\r\n    out.println(\"    <cellsize>-1025</cellsize>\");\r\n    out.println(\"  </policy>\");\r\n    out.println(\"</policies>\");\r\n    out.println(\"</configuration>\");\r\n    out.close();\r\n    ECPolicyLoader ecPolicyLoader = new ECPolicyLoader();\r\n    try {\r\n        ecPolicyLoader.loadPolicy(POLICY_FILE);\r\n        fail(\"RuntimeException should be thrown for bad policy\");\r\n    } catch (RuntimeException e) {\r\n        assertExceptionContains(\"Bad policy is found in EC policy\" + \" configuration file\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "startMockOAuthServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startMockOAuthServer()\n{\r\n    mockOAuthServer = startClientAndServer(OAUTH_PORT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "startMockWebHDFSServer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void startMockWebHDFSServer()\n{\r\n    System.setProperty(\"hadoop.home.dir\", System.getProperty(\"user.dir\"));\r\n    mockWebHDFS = startClientAndServer(WEBHDFS_PORT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "listStatusReturnsAsExpected",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void listStatusReturnsAsExpected() throws URISyntaxException, IOException\n{\r\n    MockServerClient mockWebHDFSServerClient = new MockServerClient(\"localhost\", WEBHDFS_PORT);\r\n    MockServerClient mockOAuthServerClient = new MockServerClient(\"localhost\", OAUTH_PORT);\r\n    HttpRequest oauthServerRequest = getOAuthServerMockRequest(mockOAuthServerClient);\r\n    HttpRequest fileSystemRequest = request().withMethod(\"GET\").withPath(WebHdfsFileSystem.PATH_PREFIX + \"/test1/test2\").withHeader(AUTH_TOKEN_HEADER);\r\n    try {\r\n        mockWebHDFSServerClient.when(fileSystemRequest, exactly(1)).respond(response().withStatusCode(HttpStatus.SC_OK).withHeaders(CONTENT_TYPE_APPLICATION_JSON).withBody(\"{\\n\" + \"  \\\"FileStatuses\\\":\\n\" + \"  {\\n\" + \"    \\\"FileStatus\\\":\\n\" + \"    [\\n\" + \"      {\\n\" + \"        \\\"accessTime\\\"      : 1320171722771,\\n\" + \"        \\\"blockSize\\\"       : 33554432,\\n\" + \"        \\\"group\\\"           : \\\"supergroup\\\",\\n\" + \"        \\\"length\\\"          : 24930,\\n\" + \"        \\\"modificationTime\\\": 1320171722771,\\n\" + \"        \\\"owner\\\"           : \\\"webuser\\\",\\n\" + \"        \\\"pathSuffix\\\"      : \\\"a.patch\\\",\\n\" + \"        \\\"permission\\\"      : \\\"644\\\",\\n\" + \"        \\\"replication\\\"     : 1,\\n\" + \"        \\\"type\\\"            : \\\"FILE\\\"\\n\" + \"      },\\n\" + \"      {\\n\" + \"        \\\"accessTime\\\"      : 0,\\n\" + \"        \\\"blockSize\\\"       : 0,\\n\" + \"        \\\"group\\\"           : \\\"supergroup\\\",\\n\" + \"        \\\"length\\\"          : 0,\\n\" + \"        \\\"modificationTime\\\": 1320895981256,\\n\" + \"        \\\"owner\\\"           : \\\"szetszwo\\\",\\n\" + \"        \\\"pathSuffix\\\"      : \\\"bar\\\",\\n\" + \"        \\\"permission\\\"      : \\\"711\\\",\\n\" + \"        \\\"replication\\\"     : 0,\\n\" + \"        \\\"type\\\"            : \\\"DIRECTORY\\\"\\n\" + \"      }\\n\" + \"    ]\\n\" + \"  }\\n\" + \"}\\n\"));\r\n        FileSystem fs = new WebHdfsFileSystem();\r\n        Configuration conf = getConfiguration();\r\n        conf.set(OAUTH_REFRESH_URL_KEY, \"http://localhost:\" + OAUTH_PORT + \"/refresh\");\r\n        conf.set(CredentialBasedAccessTokenProvider.OAUTH_CREDENTIAL_KEY, \"credential\");\r\n        URI uri = new URI(\"webhdfs://localhost:\" + WEBHDFS_PORT);\r\n        fs.initialize(uri, conf);\r\n        FileStatus[] ls = fs.listStatus(new Path(\"/test1/test2\"));\r\n        mockOAuthServer.verify(oauthServerRequest);\r\n        mockWebHDFSServerClient.verify(fileSystemRequest);\r\n        assertEquals(2, ls.length);\r\n        assertEquals(\"a.patch\", ls[0].getPath().getName());\r\n        assertEquals(\"bar\", ls[1].getPath().getName());\r\n        fs.close();\r\n    } finally {\r\n        mockWebHDFSServerClient.clear(fileSystemRequest);\r\n        mockOAuthServerClient.clear(oauthServerRequest);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getOAuthServerMockRequest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "HttpRequest getOAuthServerMockRequest(MockServerClient mockServerClient) throws IOException\n{\r\n    HttpRequest expectedRequest = request().withMethod(\"POST\").withPath(\"/refresh\").withBody(\"client_secret=credential&grant_type=client_credentials&client_id=MY_CLIENTID\");\r\n    Map<String, Object> map = new TreeMap<>();\r\n    map.put(EXPIRES_IN, \"0987654321\");\r\n    map.put(TOKEN_TYPE, \"bearer\");\r\n    map.put(ACCESS_TOKEN, AUTH_TOKEN);\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    HttpResponse resp = response().withStatusCode(HttpStatus.SC_OK).withHeaders(CONTENT_TYPE_APPLICATION_JSON).withBody(mapper.writeValueAsString(map));\r\n    mockServerClient.when(expectedRequest, exactly(1)).respond(resp);\r\n    return expectedRequest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY, true);\r\n    conf.set(OAUTH_CLIENT_ID_KEY, \"MY_CLIENTID\");\r\n    conf.set(ACCESS_TOKEN_PROVIDER_KEY, ConfCredentialBasedAccessTokenProvider.class.getName());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "stopMockWebHDFSServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stopMockWebHDFSServer()\n{\r\n    mockWebHDFS.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "stopMockOAuthServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stopMockOAuthServer()\n{\r\n    mockOAuthServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "testInterfaceSuperset",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInterfaceSuperset()\n{\r\n    Set<MethodSignature> fsM = signatures(FileStatus.class);\r\n    Set<MethodSignature> hfsM = signatures(HdfsFileStatus.class);\r\n    hfsM.addAll(Stream.of(HdfsFileStatus.class.getInterfaces()).flatMap(i -> Stream.of(i.getDeclaredMethods())).map(MethodSignature::new).collect(toSet()));\r\n    hfsM.addAll(signatures(Object.class));\r\n    assertTrue(fsM.removeAll(hfsM));\r\n    assertEquals(fsM.stream().map(MethodSignature::toString).collect(joining(\"\\n\")), Collections.emptySet(), fsM);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\protocol",
  "methodName" : "signatures",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<MethodSignature> signatures(Class<?> c)\n{\r\n    return Stream.of(c.getDeclaredMethods()).filter(m -> !Modifier.isStatic(m.getModifiers())).map(MethodSignature::new).collect(toSet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getMockURLOpener",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ByteRangeInputStream.URLOpener getMockURLOpener(URL url) throws IOException\n{\r\n    ByteRangeInputStream.URLOpener opener = mock(ByteRangeInputStream.URLOpener.class, CALLS_REAL_METHODS);\r\n    opener.setURL(url);\r\n    doReturn(getMockConnection(\"65535\")).when(opener).connect(anyLong(), anyBoolean());\r\n    return opener;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "getMockConnection",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HttpURLConnection getMockConnection(String length) throws IOException\n{\r\n    HttpURLConnection mockConnection = mock(HttpURLConnection.class);\r\n    doReturn(new ByteArrayInputStream(\"asdf\".getBytes())).when(mockConnection).getInputStream();\r\n    doReturn(length).when(mockConnection).getHeaderField(HttpHeaders.CONTENT_LENGTH);\r\n    return mockConnection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testByteRange",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testByteRange() throws IOException\n{\r\n    ByteRangeInputStream.URLOpener oMock = getMockURLOpener(new URL(\"http://test\"));\r\n    ByteRangeInputStream.URLOpener rMock = getMockURLOpener(null);\r\n    ByteRangeInputStream bris = new ByteRangeInputStreamImpl(oMock, rMock);\r\n    bris.seek(0);\r\n    assertEquals(\"getPos wrong\", 0, bris.getPos());\r\n    bris.read();\r\n    assertEquals(\"Initial call made incorrectly (offset check)\", 0, bris.startPos);\r\n    assertEquals(\"getPos should return 1 after reading one byte\", 1, bris.getPos());\r\n    verify(oMock, times(1)).connect(0, false);\r\n    bris.read();\r\n    assertEquals(\"getPos should return 2 after reading two bytes\", 2, bris.getPos());\r\n    verify(oMock, times(1)).connect(0, false);\r\n    rMock.setURL(new URL(\"http://resolvedurl/\"));\r\n    bris.seek(100);\r\n    bris.read();\r\n    assertEquals(\"Seek to 100 bytes made incorrectly (offset Check)\", 100, bris.startPos);\r\n    assertEquals(\"getPos should return 101 after reading one byte\", 101, bris.getPos());\r\n    verify(rMock, times(1)).connect(100, true);\r\n    bris.seek(101);\r\n    bris.read();\r\n    verify(rMock, times(1)).connect(100, true);\r\n    verify(rMock, times(0)).connect(101, true);\r\n    bris.seek(2500);\r\n    bris.read();\r\n    assertEquals(\"Seek to 2500 bytes made incorrectly (offset Check)\", 2500, bris.startPos);\r\n    doReturn(getMockConnection(null)).when(rMock).connect(anyLong(), anyBoolean());\r\n    bris.seek(500);\r\n    try {\r\n        bris.read();\r\n        fail(\"Exception should be thrown when content-length is not given\");\r\n    } catch (IOException e) {\r\n        assertTrue(\"Incorrect response message: \" + e.getMessage(), e.getMessage().startsWith(HttpHeaders.CONTENT_LENGTH + \" is missing: \"));\r\n    }\r\n    bris.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testPropagatedClose",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void testPropagatedClose() throws IOException\n{\r\n    ByteRangeInputStream bris = mock(ByteRangeInputStream.class, CALLS_REAL_METHODS);\r\n    InputStreamAndFileLength mockStream = new InputStreamAndFileLength(1L, mock(InputStream.class));\r\n    doReturn(mockStream).when(bris).openInputStream(Mockito.anyLong());\r\n    Whitebox.setInternalState(bris, \"status\", ByteRangeInputStream.StreamStatus.SEEK);\r\n    int brisOpens = 0;\r\n    int brisCloses = 0;\r\n    int isCloses = 0;\r\n    bris.getInputStream();\r\n    verify(bris, times(++brisOpens)).openInputStream(Mockito.anyLong());\r\n    verify(bris, times(brisCloses)).close();\r\n    verify(mockStream.in, times(isCloses)).close();\r\n    bris.getInputStream();\r\n    verify(bris, times(brisOpens)).openInputStream(Mockito.anyLong());\r\n    verify(bris, times(brisCloses)).close();\r\n    verify(mockStream.in, times(isCloses)).close();\r\n    bris.seek(1);\r\n    bris.getInputStream();\r\n    verify(bris, times(++brisOpens)).openInputStream(Mockito.anyLong());\r\n    verify(bris, times(brisCloses)).close();\r\n    verify(mockStream.in, times(++isCloses)).close();\r\n    bris.getInputStream();\r\n    verify(bris, times(brisOpens)).openInputStream(Mockito.anyLong());\r\n    verify(bris, times(brisCloses)).close();\r\n    verify(mockStream.in, times(isCloses)).close();\r\n    bris.seek(1);\r\n    bris.getInputStream();\r\n    verify(bris, times(brisOpens)).openInputStream(Mockito.anyLong());\r\n    verify(bris, times(brisCloses)).close();\r\n    verify(mockStream.in, times(isCloses)).close();\r\n    bris.close();\r\n    verify(bris, times(++brisCloses)).close();\r\n    verify(mockStream.in, times(++isCloses)).close();\r\n    bris.close();\r\n    verify(bris, times(++brisCloses)).close();\r\n    verify(mockStream.in, times(isCloses)).close();\r\n    boolean errored = false;\r\n    try {\r\n        bris.getInputStream();\r\n    } catch (IOException e) {\r\n        errored = true;\r\n        assertEquals(\"Stream closed\", e.getMessage());\r\n    } finally {\r\n        assertTrue(\"Read a closed steam\", errored);\r\n    }\r\n    verify(bris, times(brisOpens)).openInputStream(Mockito.anyLong());\r\n    verify(bris, times(brisCloses)).close();\r\n    verify(mockStream.in, times(isCloses)).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testAvailable",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testAvailable() throws IOException\n{\r\n    ByteRangeInputStream bris = mock(ByteRangeInputStream.class, CALLS_REAL_METHODS);\r\n    InputStreamAndFileLength mockStream = new InputStreamAndFileLength(65535L, mock(InputStream.class));\r\n    doReturn(mockStream).when(bris).openInputStream(Mockito.anyLong());\r\n    Whitebox.setInternalState(bris, \"status\", ByteRangeInputStream.StreamStatus.SEEK);\r\n    assertEquals(\"Before read or seek, available should be same as filelength\", 65535, bris.available());\r\n    verify(bris, times(1)).openInputStream(Mockito.anyLong());\r\n    bris.seek(10);\r\n    assertEquals(\"Seek 10 bytes, available should return filelength - 10\", 65525, bris.available());\r\n    bris.seek(65535);\r\n    assertEquals(\"Seek till end of file, available should return 0 bytes\", 0, bris.available());\r\n    bris.seek(0);\r\n    bris.read();\r\n    assertEquals(\"Read 1 byte, available must return  filelength - 1\", 65534, bris.available());\r\n    bris.read();\r\n    assertEquals(\"Read another 1 byte, available must return  filelength - 2\", 65533, bris.available());\r\n    bris.seek(100);\r\n    bris.read();\r\n    assertEquals(\"Seek to offset 100 and read 1 byte, available should return filelength - 101\", 65434, bris.available());\r\n    bris.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testAvailableLengthNotKnown",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAvailableLengthNotKnown() throws IOException\n{\r\n    ByteRangeInputStream bris = mock(ByteRangeInputStream.class, CALLS_REAL_METHODS);\r\n    InputStreamAndFileLength mockStream = new InputStreamAndFileLength(null, mock(InputStream.class));\r\n    doReturn(mockStream).when(bris).openInputStream(Mockito.anyLong());\r\n    Whitebox.setInternalState(bris, \"status\", ByteRangeInputStream.StreamStatus.SEEK);\r\n    assertEquals(Integer.MAX_VALUE, bris.available());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web",
  "methodName" : "testAvailableStreamClosed",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAvailableStreamClosed() throws IOException\n{\r\n    ByteRangeInputStream bris = mock(ByteRangeInputStream.class, CALLS_REAL_METHODS);\r\n    InputStreamAndFileLength mockStream = new InputStreamAndFileLength(null, mock(InputStream.class));\r\n    doReturn(mockStream).when(bris).openInputStream(Mockito.anyLong());\r\n    Whitebox.setInternalState(bris, \"status\", ByteRangeInputStream.StreamStatus.SEEK);\r\n    bris.close();\r\n    try {\r\n        bris.available();\r\n        fail(\"Exception should be thrown when stream is closed\");\r\n    } catch (IOException e) {\r\n        assertTrue(\"Exception when stream is closed\", e.getMessage().equals(\"Stream closed\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "expireConversionWorks",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void expireConversionWorks()\n{\r\n    Timer mockTimer = mock(Timer.class);\r\n    when(mockTimer.now()).thenReturn(5l);\r\n    AccessTokenTimer timer = new AccessTokenTimer(mockTimer);\r\n    timer.setExpiresIn(\"3\");\r\n    assertEquals(3005, timer.getNextRefreshMSSinceEpoch());\r\n    assertTrue(timer.shouldRefresh());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\hdfs\\web\\oauth2",
  "methodName" : "shouldRefreshIsCorrect",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void shouldRefreshIsCorrect()\n{\r\n    Timer mockTimer = mock(Timer.class);\r\n    when(mockTimer.now()).thenReturn(500l).thenReturn(1000000l + 500l);\r\n    AccessTokenTimer timer = new AccessTokenTimer(mockTimer);\r\n    timer.setExpiresInMSSinceEpoch(\"1000000\");\r\n    assertFalse(timer.shouldRefresh());\r\n    assertTrue(timer.shouldRefresh());\r\n    verify(mockTimer, times(2)).now();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    byte[] value = { 0x31, 0x32, 0x33 };\r\n    XATTR = new XAttr.Builder().setName(\"name\").setValue(value).build();\r\n    XATTR1 = new XAttr.Builder().setNameSpace(XAttr.NameSpace.USER).setName(\"name\").setValue(value).build();\r\n    XATTR2 = new XAttr.Builder().setNameSpace(XAttr.NameSpace.TRUSTED).setName(\"name\").setValue(value).build();\r\n    XATTR3 = new XAttr.Builder().setNameSpace(XAttr.NameSpace.SYSTEM).setName(\"name\").setValue(value).build();\r\n    XATTR4 = new XAttr.Builder().setNameSpace(XAttr.NameSpace.SECURITY).setName(\"name\").setValue(value).build();\r\n    XATTR5 = new XAttr.Builder().setNameSpace(XAttr.NameSpace.RAW).setName(\"name\").setValue(value).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testXAttrEquals",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testXAttrEquals()\n{\r\n    assertNotSame(XATTR1, XATTR2);\r\n    assertNotSame(XATTR2, XATTR3);\r\n    assertNotSame(XATTR3, XATTR4);\r\n    assertNotSame(XATTR4, XATTR5);\r\n    assertEquals(XATTR, XATTR1);\r\n    assertEquals(XATTR1, XATTR1);\r\n    assertEquals(XATTR2, XATTR2);\r\n    assertEquals(XATTR3, XATTR3);\r\n    assertEquals(XATTR4, XATTR4);\r\n    assertEquals(XATTR5, XATTR5);\r\n    assertFalse(XATTR1.equals(XATTR2));\r\n    assertFalse(XATTR2.equals(XATTR3));\r\n    assertFalse(XATTR3.equals(XATTR4));\r\n    assertFalse(XATTR4.equals(XATTR5));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-client\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testXAttrHashCode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testXAttrHashCode()\n{\r\n    assertEquals(XATTR.hashCode(), XATTR1.hashCode());\r\n    assertFalse(XATTR1.hashCode() == XATTR2.hashCode());\r\n    assertFalse(XATTR2.hashCode() == XATTR3.hashCode());\r\n    assertFalse(XATTR3.hashCode() == XATTR4.hashCode());\r\n    assertFalse(XATTR4.hashCode() == XATTR5.hashCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]